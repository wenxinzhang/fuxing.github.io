<!DOCTYPE HTML>
<html lang="zh-CN">

<head><meta name="generator" content="Hexo 3.9.0">
    <!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="福星">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <meta name="theme-version" content="1.2.3">
    <meta name="root" content="/">
    <link rel="dns-prefetch" href="http://zhangfuxin.cn">
    <!--SEO-->

<meta name="keywords" content="Sqoop">


<meta name="description" content="** Sqoop学习之路：** &lt;Excerpt in index | 首页摘要&gt;
​        Sqoop学习之路 （一）

&lt;The rest of contents ...">


<meta name="robots" content="all">
<meta name="google" content="all">
<meta name="googlebot" content="all">
<meta name="verify" content="all">
    <!--Title-->

<title>
    
    Sqoop学习之路 |
    
    福星
</title>

<link rel="alternate" href="/atom.xml" title="福星" type="application/atom+xml">


<link rel="icon" href="/favicon.ico">

    

<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7">
<link rel="stylesheet" href="/css/font-awesome.min.css?rev=4.7.0">
<link rel="stylesheet" href="/css/style.css?rev=@@hash">
    
<div class="hide">
    <script type="text/javascript">
    var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
    document.write(unescape("%3Cspan class='cnzz_stat_icon_1263868967 hide' %3E%3Cscript%20src%3D%22https%3A%2F%2Fs95.cnzz.com%2Fz_stat.php%3Fweb_id%3D1272564536%22%3E%3C%2Fscript%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1263868967%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
    </script>
</div>




    

<script>
(function() {
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    } else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

</head></html>
<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->
<body>
    <header class="main-header"  style="background-image:url(
    http://snippet.shenliyang.com/img/banner.jpg)"
     >
    <div class="main-header-box">
        <a class="header-avatar" href="/" title='福 星'>
            <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block">
        </a>
        <div class="branding">
            <!--<h2 class="text-hide">Snippet主题,从未如此简单有趣</h2>-->
            
            <img src="/img/branding.png" alt="Snippet 博客主题" class="img-responsive center-block">
            
        </div>
    </div>
</header>
    <nav class="main-navigation">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                        <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="navbar-brand" href="http://zhangfuxin.cn">
                        福星</a>
                </div>
                <div class="collapse navbar-collapse" id="main-menu">
                    <ul class="menu">
                        
                        <li role="presentation" class="text-center">
                            <a href="/"><i class="fa "></i>
                                首页</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Linux/"><i class="fa "></i>
                                Linux</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Hadoop/"><i class="fa "></i>
                                Hadoop</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Spark/"><i class="fa "></i>
                                Spark</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Java/"><i class="fa "></i>
                                Java</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Python/"><i class="fa "></i>
                                Python</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/algorithm/"><i class="fa "></i>
                                算法</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/工具/"><i class="fa "></i>
                                工具</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/archives/"><i class="fa "></i>
                                时间轴</a>
                        </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="Sqoop学习之路">
            
            Sqoop学习之路
            
        </h1>
        <div class="post-meta">
    
    <span class="categories-meta fa-wrap">
        <i class="fa fa-folder-open-o"></i>
        <a class="category-link" href="/categories/Hadoop/">Hadoop</a>
    </span>
    
    
    <span class="fa-wrap">
        <i class="fa fa-tags"></i>
        <span class="tags-meta">
            
            <a class="tag-link" href="/tags/Sqoop/">Sqoop</a>
            
        </span>
    </span>
    
    
    
    <span class="fa-wrap">
        <i class="fa fa-clock-o"></i>
        <span class="date-meta">
            2019/05/22</span>
    </span>
    
    <span class="fa-wrap">
        <i class="fa fa-eye"></i>
        <span id="busuanzi_value_page_pv"></span>
    </span>
    
    
</div>
        
        
    </div>
    
    <div class="post-body post-content">
        <p>** Sqoop学习之路：** &lt;Excerpt in index | 首页摘要&gt;</p>
<p>​        Sqoop学习之路 （一）</p>
<a id="more"></a>
<p>&lt;The rest of contents | 余下全文&gt;</p>
<h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>sqoop 是 apache 旗下一款“Hadoop 和关系数据库服务器之间传送数据”的工具。</p>
<p>核心的功能有两个：</p>
<p>导入、迁入</p>
<p>导出、迁出</p>
<p><strong>导入数据</strong>：MySQL，Oracle 导入数据到 Hadoop 的 HDFS、HIVE、HBASE 等数据存储系统</p>
<p><strong>导出数据</strong>：从 Hadoop 的文件系统中导出数据到关系数据库 mysql 等 Sqoop 的本质还是一个命令行工具，和 HDFS，Hive 相比，并没有什么高深的理论。</p>
<p>sqoop：</p>
<p>工具：本质就是迁移数据， 迁移的方式：就是把sqoop的迁移命令转换成MR程序</p>
<p>hive</p>
<p>工具，本质就是执行计算，依赖于HDFS存储数据，把SQL转换成MR程序</p>
<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412130640231-449939615.png" alt="img"></p>
<h2 id="二、工作机制"><a href="#二、工作机制" class="headerlink" title="二、工作机制"></a>二、工作机制</h2><p>将导入或导出命令翻译成 MapReduce 程序来实现 在翻译出的 MapReduce 中主要是对 InputFormat 和 OutputFormat 进行定制</p>
<h2 id="三、安装"><a href="#三、安装" class="headerlink" title="三、安装"></a>三、安装</h2><h3 id="1、前提概述"><a href="#1、前提概述" class="headerlink" title="1、前提概述"></a>1、前提概述</h3><p>将来sqoop在使用的时候有可能会跟那些系统或者组件打交道？</p>
<p>HDFS， MapReduce， YARN， ZooKeeper， Hive， HBase， MySQL</p>
<p><strong>sqoop就是一个工具， 只需要在一个节点上进行安装即可。</strong></p>
<blockquote>
<p>补充一点： 如果你的sqoop工具将来要进行hive或者hbase等等的系统和MySQL之间的交互</p>
<p>你安装的SQOOP软件的节点一定要包含以上你要使用的集群或者软件系统的安装包</p>
<p>补充一点： 将来要使用的azakban这个软件 除了会调度 hadoop的任务或者hbase或者hive的任务之外， 还会调度sqoop的任务</p>
<p>azkaban这个软件的安装节点也必须包含以上这些软件系统的客户端/2、</p>
</blockquote>
<h3 id="2、软件下载"><a href="#2、软件下载" class="headerlink" title="2、软件下载"></a>2、软件下载</h3><p>下载地址<a href="http://mirrors.hust.edu.cn/apache/" target="_blank" rel="noopener">http://mirrors.hust.edu.cn/apache/</a></p>
<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412131040413-312918279.png" alt="img"></p>
<p><strong>sqoop版本说明</strong></p>
<p>绝大部分企业所使用的sqoop的版本都是 sqoop1</p>
<p>sqoop-1.4.6 或者 sqoop-1.4.7 它是 sqoop1</p>
<p>sqoop-1.99.4—-都是 sqoop2</p>
<p>此处使用sqoop-1.4.6版本sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz</p>
<h3 id="3、安装步骤"><a href="#3、安装步骤" class="headerlink" title="3、安装步骤"></a>3、安装步骤</h3><h4 id="（1）上传解压缩安装包到指定目录"><a href="#（1）上传解压缩安装包到指定目录" class="headerlink" title="（1）上传解压缩安装包到指定目录"></a>（1）上传解压缩安装包到指定目录</h4><p>因为之前hive只是安装在hadoop3机器上，所以sqoop也同样安装在hadoop3机器上</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ tar -zxvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C apps/</span><br></pre></td></tr></table></figure>

<h4 id="（2）进入到-conf-文件夹，找到-sqoop-env-template-sh，修改其名称为-sqoop-env-sh-cd-conf"><a href="#（2）进入到-conf-文件夹，找到-sqoop-env-template-sh，修改其名称为-sqoop-env-sh-cd-conf" class="headerlink" title="（2）进入到 conf 文件夹，找到 sqoop-env-template.sh，修改其名称为 sqoop-env.sh cd conf"></a>（2）进入到 conf 文件夹，找到 sqoop-env-template.sh，修改其名称为 sqoop-env.sh cd conf</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop3 apps]$ ls</span><br><span class="line">apache-hive-2.3.3-bin  hadoop-2.7.5  hbase-1.2.6  sqoop-1.4.6.bin__hadoop-2.0.4-alpha  zookeeper-3.4.10</span><br><span class="line">[hadoop@hadoop3 apps]$ mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ sqoop-1.4.6</span><br><span class="line">[hadoop@hadoop3 apps]$ cd sqoop-1.4.6/conf/</span><br><span class="line">[hadoop@hadoop3 conf]$ ls</span><br><span class="line">oraoop-site-template.xml  sqoop-env-template.sh    sqoop-site.xml</span><br><span class="line">sqoop-env-template.cmd    sqoop-site-template.xml</span><br><span class="line">[hadoop@hadoop3 conf]$ mv sqoop-env-template.sh sqoop-env.sh</span><br></pre></td></tr></table></figure>

<h4 id="（3）修改-sqoop-env-sh"><a href="#（3）修改-sqoop-env-sh" class="headerlink" title="（3）修改 sqoop-env.sh"></a>（3）修改 sqoop-env.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 conf]$ vi sqoop-env.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_COMMON_HOME=/home/hadoop/apps/hadoop-2.7.5</span><br><span class="line"></span><br><span class="line">#Set path to where hadoop-*-core.jar is available</span><br><span class="line">export HADOOP_MAPRED_HOME=/home/hadoop/apps/hadoop-2.7.5</span><br><span class="line"></span><br><span class="line">#set the path to where bin/hbase is available</span><br><span class="line">export HBASE_HOME=/home/hadoop/apps/hbase-1.2.6</span><br><span class="line"></span><br><span class="line">#Set the path to where bin/hive is available</span><br><span class="line">export HIVE_HOME=/home/hadoop/apps/apache-hive-2.3.3-bin</span><br><span class="line"></span><br><span class="line">#Set the path for where zookeper config dir is</span><br><span class="line">export ZOOCFGDIR=/home/hadoop/apps/zookeeper-3.4.10/conf</span><br></pre></td></tr></table></figure>

<p>为什么在sqoop-env.sh 文件中会要求分别进行 common和mapreduce的配置呢？？？</p>
<blockquote>
<p>在apache的hadoop的安装中；四大组件都是安装在同一个hadoop_home中的</p>
<p>但是在CDH, HDP中， 这些组件都是可选的。</p>
<p>在安装hadoop的时候，可以选择性的只安装HDFS或者YARN，</p>
<p>CDH,HDP在安装hadoop的时候，会把HDFS和MapReduce有可能分别安装在不同的地方。</p>
</blockquote>
<h4 id="（4）加入-mysql-驱动包到-sqoop1-4-6-lib-目录下"><a href="#（4）加入-mysql-驱动包到-sqoop1-4-6-lib-目录下" class="headerlink" title="（4）加入 mysql 驱动包到 sqoop1.4.6/lib 目录下"></a>（4）加入 mysql 驱动包到 sqoop1.4.6/lib 目录下</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ cp mysql-connector-java-5.1.40-bin.jar apps/sqoop-1.4.6/lib/</span><br></pre></td></tr></table></figure>

<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412133204656-1647317834.png" alt="img"></p>
<h4 id="（5）配置系统环境变量"><a href="#（5）配置系统环境变量" class="headerlink" title="（5）配置系统环境变量"></a>（5）配置系统环境变量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ vi .bashrc </span><br><span class="line">#Sqoop</span><br><span class="line">export SQOOP_HOME=/home/hadoop/apps/sqoop-1.4.6</span><br><span class="line">export PATH=$PATH:$SQOOP_HOME/bin</span><br></pre></td></tr></table></figure>

<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412133442486-246649117.png" alt="img"></p>
<p>保存退出使其立即生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ source .bashrc</span><br></pre></td></tr></table></figure>

<h4 id="（6）验证安装是否成功"><a href="#（6）验证安装是否成功" class="headerlink" title="（6）验证安装是否成功"></a>（6）验证安装是否成功</h4><p> <strong>sqoop-version</strong> 或者 <strong>sqoop version</strong></p>
<p><strong><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412133617239-1494099736.png" alt="img"></strong></p>
<h2 id="四、Sqoop的基本命令"><a href="#四、Sqoop的基本命令" class="headerlink" title="四、Sqoop的基本命令"></a>四、Sqoop的基本命令</h2><h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><p>首先，我们可以使用 sqoop help 来查看，sqoop 支持哪些命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop help</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">18/04/12 13:37:19 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">usage: sqoop COMMAND [ARGS]</span><br><span class="line"></span><br><span class="line">Available commands:</span><br><span class="line">  codegen            Generate code to interact with database records</span><br><span class="line">  create-hive-table  Import a table definition into Hive</span><br><span class="line">  eval               Evaluate a SQL statement and display the results</span><br><span class="line">  export             Export an HDFS directory to a database table</span><br><span class="line">  help               List available commands</span><br><span class="line">  import             Import a table from a database to HDFS</span><br><span class="line">  import-all-tables  Import tables from a database to HDFS</span><br><span class="line">  import-mainframe   Import datasets from a mainframe server to HDFS</span><br><span class="line">  job                Work with saved jobs</span><br><span class="line">  list-databases     List available databases on a server</span><br><span class="line">  list-tables        List available tables in a database</span><br><span class="line">  merge              Merge results of incremental imports</span><br><span class="line">  metastore          Run a standalone Sqoop metastore</span><br><span class="line">  version            Display version information</span><br><span class="line"></span><br><span class="line">See 'sqoop help COMMAND' for information on a specific command.</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure>

<p>然后得到这些支持了的命令之后，如果不知道使用方式，可以使用 sqoop command 的方式 来查看某条具体命令的使用方式，比如：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop help import</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">18/04/12 13:38:29 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">usage: sqoop import [GENERIC-ARGS] [TOOL-ARGS]</span><br><span class="line"></span><br><span class="line">Common arguments:</span><br><span class="line">   --connect &lt;jdbc-uri&gt;                         Specify JDBC connect</span><br><span class="line">                                                string</span><br><span class="line">   --connection-manager &lt;class-name&gt;            Specify connection manager</span><br><span class="line">                                                class name</span><br><span class="line">   --connection-param-file &lt;properties-file&gt;    Specify connection</span><br><span class="line">                                                parameters file</span><br><span class="line">   --driver &lt;class-name&gt;                        Manually specify JDBC</span><br><span class="line">                                                driver class to use</span><br><span class="line">   --hadoop-home &lt;hdir&gt;                         Override</span><br><span class="line">                                                $HADOOP_MAPRED_HOME_ARG</span><br><span class="line">   --hadoop-mapred-home &lt;dir&gt;                   Override</span><br><span class="line">                                                $HADOOP_MAPRED_HOME_ARG</span><br><span class="line">   --help                                       Print usage instructions</span><br><span class="line">-P                                              Read password from console</span><br><span class="line">   --password &lt;password&gt;                        Set authentication</span><br><span class="line">                                                password</span><br><span class="line">   --password-alias &lt;password-alias&gt;            Credential provider</span><br><span class="line">                                                password alias</span><br><span class="line">   --password-file &lt;password-file&gt;              Set authentication</span><br><span class="line">                                                password file path</span><br><span class="line">   --relaxed-isolation                          Use read-uncommitted</span><br><span class="line">                                                isolation for imports</span><br><span class="line">   --skip-dist-cache                            Skip copying jars to</span><br><span class="line">                                                distributed cache</span><br><span class="line">   --username &lt;username&gt;                        Set authentication</span><br><span class="line">                                                username</span><br><span class="line">   --verbose                                    Print more information</span><br><span class="line">                                                while working</span><br><span class="line"></span><br><span class="line">Import control arguments:</span><br><span class="line">   --append                                                   Imports data</span><br><span class="line">                                                              in append</span><br><span class="line">                                                              mode</span><br><span class="line">   --as-avrodatafile                                          Imports data</span><br><span class="line">                                                              to Avro data</span><br><span class="line">                                                              files</span><br><span class="line">   --as-parquetfile                                           Imports data</span><br><span class="line">                                                              to Parquet</span><br><span class="line">                                                              files</span><br><span class="line">   --as-sequencefile                                          Imports data</span><br><span class="line">                                                              to</span><br><span class="line">                                                              SequenceFile</span><br><span class="line">                                                              s</span><br><span class="line">   --as-textfile                                              Imports data</span><br><span class="line">                                                              as plain</span><br><span class="line">                                                              text</span><br><span class="line">                                                              (default)</span><br><span class="line">   --autoreset-to-one-mapper                                  Reset the</span><br><span class="line">                                                              number of</span><br><span class="line">                                                              mappers to</span><br><span class="line">                                                              one mapper</span><br><span class="line">                                                              if no split</span><br><span class="line">                                                              key</span><br><span class="line">                                                              available</span><br><span class="line">   --boundary-query &lt;statement&gt;                               Set boundary</span><br><span class="line">                                                              query for</span><br><span class="line">                                                              retrieving</span><br><span class="line">                                                              max and min</span><br><span class="line">                                                              value of the</span><br><span class="line">                                                              primary key</span><br><span class="line">   --columns &lt;col,col,col...&gt;                                 Columns to</span><br><span class="line">                                                              import from</span><br><span class="line">                                                              table</span><br><span class="line">   --compression-codec &lt;codec&gt;                                Compression</span><br><span class="line">                                                              codec to use</span><br><span class="line">                                                              for import</span><br><span class="line">   --delete-target-dir                                        Imports data</span><br><span class="line">                                                              in delete</span><br><span class="line">                                                              mode</span><br><span class="line">   --direct                                                   Use direct</span><br><span class="line">                                                              import fast</span><br><span class="line">                                                              path</span><br><span class="line">   --direct-split-size &lt;n&gt;                                    Split the</span><br><span class="line">                                                              input stream</span><br><span class="line">                                                              every 'n'</span><br><span class="line">                                                              bytes when</span><br><span class="line">                                                              importing in</span><br><span class="line">                                                              direct mode</span><br><span class="line">-e,--query &lt;statement&gt;                                        Import</span><br><span class="line">                                                              results of</span><br><span class="line">                                                              SQL</span><br><span class="line">                                                              'statement'</span><br><span class="line">   --fetch-size &lt;n&gt;                                           Set number</span><br><span class="line">                                                              'n' of rows</span><br><span class="line">                                                              to fetch</span><br><span class="line">                                                              from the</span><br><span class="line">                                                              database</span><br><span class="line">                                                              when more</span><br><span class="line">                                                              rows are</span><br><span class="line">                                                              needed</span><br><span class="line">   --inline-lob-limit &lt;n&gt;                                     Set the</span><br><span class="line">                                                              maximum size</span><br><span class="line">                                                              for an</span><br><span class="line">                                                              inline LOB</span><br><span class="line">-m,--num-mappers &lt;n&gt;                                          Use 'n' map</span><br><span class="line">                                                              tasks to</span><br><span class="line">                                                              import in</span><br><span class="line">                                                              parallel</span><br><span class="line">   --mapreduce-job-name &lt;name&gt;                                Set name for</span><br><span class="line">                                                              generated</span><br><span class="line">                                                              mapreduce</span><br><span class="line">                                                              job</span><br><span class="line">   --merge-key &lt;column&gt;                                       Key column</span><br><span class="line">                                                              to use to</span><br><span class="line">                                                              join results</span><br><span class="line">   --split-by &lt;column-name&gt;                                   Column of</span><br><span class="line">                                                              the table</span><br><span class="line">                                                              used to</span><br><span class="line">                                                              split work</span><br><span class="line">                                                              units</span><br><span class="line">   --table &lt;table-name&gt;                                       Table to</span><br><span class="line">                                                              read</span><br><span class="line">   --target-dir &lt;dir&gt;                                         HDFS plain</span><br><span class="line">                                                              table</span><br><span class="line">                                                              destination</span><br><span class="line">   --validate                                                 Validate the</span><br><span class="line">                                                              copy using</span><br><span class="line">                                                              the</span><br><span class="line">                                                              configured</span><br><span class="line">                                                              validator</span><br><span class="line">   --validation-failurehandler &lt;validation-failurehandler&gt;    Fully</span><br><span class="line">                                                              qualified</span><br><span class="line">                                                              class name</span><br><span class="line">                                                              for</span><br><span class="line">                                                              ValidationFa</span><br><span class="line">                                                              ilureHandler</span><br><span class="line">   --validation-threshold &lt;validation-threshold&gt;              Fully</span><br><span class="line">                                                              qualified</span><br><span class="line">                                                              class name</span><br><span class="line">                                                              for</span><br><span class="line">                                                              ValidationTh</span><br><span class="line">                                                              reshold</span><br><span class="line">   --validator &lt;validator&gt;                                    Fully</span><br><span class="line">                                                              qualified</span><br><span class="line">                                                              class name</span><br><span class="line">                                                              for the</span><br><span class="line">                                                              Validator</span><br><span class="line">   --warehouse-dir &lt;dir&gt;                                      HDFS parent</span><br><span class="line">                                                              for table</span><br><span class="line">                                                              destination</span><br><span class="line">   --where &lt;where clause&gt;                                     WHERE clause</span><br><span class="line">                                                              to use</span><br><span class="line">                                                              during</span><br><span class="line">                                                              import</span><br><span class="line">-z,--compress                                                 Enable</span><br><span class="line">                                                              compression</span><br><span class="line"></span><br><span class="line">Incremental import arguments:</span><br><span class="line">   --check-column &lt;column&gt;        Source column to check for incremental</span><br><span class="line">                                  change</span><br><span class="line">   --incremental &lt;import-type&gt;    Define an incremental import of type</span><br><span class="line">                                  'append' or 'lastmodified'</span><br><span class="line">   --last-value &lt;value&gt;           Last imported value in the incremental</span><br><span class="line">                                  check column</span><br><span class="line"></span><br><span class="line">Output line formatting arguments:</span><br><span class="line">   --enclosed-by &lt;char&gt;               Sets a required field enclosing</span><br><span class="line">                                      character</span><br><span class="line">   --escaped-by &lt;char&gt;                Sets the escape character</span><br><span class="line">   --fields-terminated-by &lt;char&gt;      Sets the field separator character</span><br><span class="line">   --lines-terminated-by &lt;char&gt;       Sets the end-of-line character</span><br><span class="line">   --mysql-delimiters                 Uses MySQL's default delimiter set:</span><br><span class="line">                                      fields: ,  lines: \n  escaped-by: \</span><br><span class="line">                                      optionally-enclosed-by: '</span><br><span class="line">   --optionally-enclosed-by &lt;char&gt;    Sets a field enclosing character</span><br><span class="line"></span><br><span class="line">Input parsing arguments:</span><br><span class="line">   --input-enclosed-by &lt;char&gt;               Sets a required field encloser</span><br><span class="line">   --input-escaped-by &lt;char&gt;                Sets the input escape</span><br><span class="line">                                            character</span><br><span class="line">   --input-fields-terminated-by &lt;char&gt;      Sets the input field separator</span><br><span class="line">   --input-lines-terminated-by &lt;char&gt;       Sets the input end-of-line</span><br><span class="line">                                            char</span><br><span class="line">   --input-optionally-enclosed-by &lt;char&gt;    Sets a field enclosing</span><br><span class="line">                                            character</span><br><span class="line"></span><br><span class="line">Hive arguments:</span><br><span class="line">   --create-hive-table                         Fail if the target hive</span><br><span class="line">                                               table exists</span><br><span class="line">   --hive-database &lt;database-name&gt;             Sets the database name to</span><br><span class="line">                                               use when importing to hive</span><br><span class="line">   --hive-delims-replacement &lt;arg&gt;             Replace Hive record \0x01</span><br><span class="line">                                               and row delimiters (\n\r)</span><br><span class="line">                                               from imported string fields</span><br><span class="line">                                               with user-defined string</span><br><span class="line">   --hive-drop-import-delims                   Drop Hive record \0x01 and</span><br><span class="line">                                               row delimiters (\n\r) from</span><br><span class="line">                                               imported string fields</span><br><span class="line">   --hive-home &lt;dir&gt;                           Override $HIVE_HOME</span><br><span class="line">   --hive-import                               Import tables into Hive</span><br><span class="line">                                               (Uses Hive's default</span><br><span class="line">                                               delimiters if none are</span><br><span class="line">                                               set.)</span><br><span class="line">   --hive-overwrite                            Overwrite existing data in</span><br><span class="line">                                               the Hive table</span><br><span class="line">   --hive-partition-key &lt;partition-key&gt;        Sets the partition key to</span><br><span class="line">                                               use when importing to hive</span><br><span class="line">   --hive-partition-value &lt;partition-value&gt;    Sets the partition value to</span><br><span class="line">                                               use when importing to hive</span><br><span class="line">   --hive-table &lt;table-name&gt;                   Sets the table name to use</span><br><span class="line">                                               when importing to hive</span><br><span class="line">   --map-column-hive &lt;arg&gt;                     Override mapping for</span><br><span class="line">                                               specific column to hive</span><br><span class="line">                                               types.</span><br><span class="line"></span><br><span class="line">HBase arguments:</span><br><span class="line">   --column-family &lt;family&gt;    Sets the target column family for the</span><br><span class="line">                               import</span><br><span class="line">   --hbase-bulkload            Enables HBase bulk loading</span><br><span class="line">   --hbase-create-table        If specified, create missing HBase tables</span><br><span class="line">   --hbase-row-key &lt;col&gt;       Specifies which input column to use as the</span><br><span class="line">                               row key</span><br><span class="line">   --hbase-table &lt;table&gt;       Import to &lt;table&gt; in HBase</span><br><span class="line"></span><br><span class="line">HCatalog arguments:</span><br><span class="line">   --hcatalog-database &lt;arg&gt;                        HCatalog database name</span><br><span class="line">   --hcatalog-home &lt;hdir&gt;                           Override $HCAT_HOME</span><br><span class="line">   --hcatalog-partition-keys &lt;partition-key&gt;        Sets the partition</span><br><span class="line">                                                    keys to use when</span><br><span class="line">                                                    importing to hive</span><br><span class="line">   --hcatalog-partition-values &lt;partition-value&gt;    Sets the partition</span><br><span class="line">                                                    values to use when</span><br><span class="line">                                                    importing to hive</span><br><span class="line">   --hcatalog-table &lt;arg&gt;                           HCatalog table name</span><br><span class="line">   --hive-home &lt;dir&gt;                                Override $HIVE_HOME</span><br><span class="line">   --hive-partition-key &lt;partition-key&gt;             Sets the partition key</span><br><span class="line">                                                    to use when importing</span><br><span class="line">                                                    to hive</span><br><span class="line">   --hive-partition-value &lt;partition-value&gt;         Sets the partition</span><br><span class="line">                                                    value to use when</span><br><span class="line">                                                    importing to hive</span><br><span class="line">   --map-column-hive &lt;arg&gt;                          Override mapping for</span><br><span class="line">                                                    specific column to</span><br><span class="line">                                                    hive types.</span><br><span class="line"></span><br><span class="line">HCatalog import specific options:</span><br><span class="line">   --create-hcatalog-table            Create HCatalog before import</span><br><span class="line">   --hcatalog-storage-stanza &lt;arg&gt;    HCatalog storage stanza for table</span><br><span class="line">                                      creation</span><br><span class="line"></span><br><span class="line">Accumulo arguments:</span><br><span class="line">   --accumulo-batch-size &lt;size&gt;          Batch size in bytes</span><br><span class="line">   --accumulo-column-family &lt;family&gt;     Sets the target column family for</span><br><span class="line">                                         the import</span><br><span class="line">   --accumulo-create-table               If specified, create missing</span><br><span class="line">                                         Accumulo tables</span><br><span class="line">   --accumulo-instance &lt;instance&gt;        Accumulo instance name.</span><br><span class="line">   --accumulo-max-latency &lt;latency&gt;      Max write latency in milliseconds</span><br><span class="line">   --accumulo-password &lt;password&gt;        Accumulo password.</span><br><span class="line">   --accumulo-row-key &lt;col&gt;              Specifies which input column to</span><br><span class="line">                                         use as the row key</span><br><span class="line">   --accumulo-table &lt;table&gt;              Import to &lt;table&gt; in Accumulo</span><br><span class="line">   --accumulo-user &lt;user&gt;                Accumulo user name.</span><br><span class="line">   --accumulo-visibility &lt;vis&gt;           Visibility token to be applied to</span><br><span class="line">                                         all rows imported</span><br><span class="line">   --accumulo-zookeepers &lt;zookeepers&gt;    Comma-separated list of</span><br><span class="line">                                         zookeepers (host:port)</span><br><span class="line"></span><br><span class="line">Code generation arguments:</span><br><span class="line">   --bindir &lt;dir&gt;                        Output directory for compiled</span><br><span class="line">                                         objects</span><br><span class="line">   --class-name &lt;name&gt;                   Sets the generated class name.</span><br><span class="line">                                         This overrides --package-name.</span><br><span class="line">                                         When combined with --jar-file,</span><br><span class="line">                                         sets the input class.</span><br><span class="line">   --input-null-non-string &lt;null-str&gt;    Input null non-string</span><br><span class="line">                                         representation</span><br><span class="line">   --input-null-string &lt;null-str&gt;        Input null string representation</span><br><span class="line">   --jar-file &lt;file&gt;                     Disable code generation; use</span><br><span class="line">                                         specified jar</span><br><span class="line">   --map-column-java &lt;arg&gt;               Override mapping for specific</span><br><span class="line">                                         columns to java types</span><br><span class="line">   --null-non-string &lt;null-str&gt;          Null non-string representation</span><br><span class="line">   --null-string &lt;null-str&gt;              Null string representation</span><br><span class="line">   --outdir &lt;dir&gt;                        Output directory for generated</span><br><span class="line">                                         code</span><br><span class="line">   --package-name &lt;name&gt;                 Put auto-generated classes in</span><br><span class="line">                                         this package</span><br><span class="line"></span><br><span class="line">Generic Hadoop command-line arguments:</span><br><span class="line">(must preceed any tool-specific arguments)</span><br><span class="line">Generic options supported are</span><br><span class="line">-conf &lt;configuration file&gt;     specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;            use value for given property</span><br><span class="line">-fs &lt;local|namenode:port&gt;      specify a namenode</span><br><span class="line">-jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager</span><br><span class="line">-files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include in the classpath.</span><br><span class="line">-archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.</span><br><span class="line"></span><br><span class="line">The general command line syntax is</span><br><span class="line">bin/hadoop command [genericOptions] [commandOptions]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">At minimum, you must specify --connect and --table</span><br><span class="line">Arguments to mysqldump and other subprograms may be supplied</span><br><span class="line">after a '--' on the command line.</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure>

<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><h4 id="列出MySQL数据有哪些数据库"><a href="#列出MySQL数据有哪些数据库" class="headerlink" title="列出MySQL数据有哪些数据库"></a>列出MySQL数据有哪些数据库</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop list-databases \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --connect jdbc:mysql://hadoop1:3306/ \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --username root \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --password root</span></span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">18/04/12 13:43:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">18/04/12 13:43:51 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">18/04/12 13:43:51 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">information_schema</span><br><span class="line">hivedb</span><br><span class="line">mysql</span><br><span class="line">performance_schema</span><br><span class="line">test</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure>

<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412134538467-1445351095.png" alt="img"></p>
<h4 id="列出MySQL中的某个数据库有哪些数据表："><a href="#列出MySQL中的某个数据库有哪些数据表：" class="headerlink" title="列出MySQL中的某个数据库有哪些数据表："></a>列出MySQL中的某个数据库有哪些数据表：</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop list-tables **</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --connect jdbc:mysql://hadoop1:3306/mysql **</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --username root **</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --password root**</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop list-tables \</span><br><span class="line">&gt; --connect jdbc:mysql://hadoop1:3306/mysql \</span><br><span class="line">&gt; --username root \</span><br><span class="line">&gt; --password root</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">18/04/12 13:46:21 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">18/04/12 13:46:21 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">18/04/12 13:46:21 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">columns_priv</span><br><span class="line">db</span><br><span class="line">event</span><br><span class="line">func</span><br><span class="line">general_log</span><br><span class="line">help_category</span><br><span class="line">help_keyword</span><br><span class="line">help_relation</span><br><span class="line">help_topic</span><br><span class="line">innodb_index_stats</span><br><span class="line">innodb_table_stats</span><br><span class="line">ndb_binlog_index</span><br><span class="line">plugin</span><br><span class="line">proc</span><br><span class="line">procs_priv</span><br><span class="line">proxies_priv</span><br><span class="line">servers</span><br><span class="line">slave_master_info</span><br><span class="line">slave_relay_log_info</span><br><span class="line">slave_worker_info</span><br><span class="line">slow_log</span><br><span class="line">tables_priv</span><br><span class="line">time_zone</span><br><span class="line">time_zone_leap_second</span><br><span class="line">time_zone_name</span><br><span class="line">time_zone_transition</span><br><span class="line">time_zone_transition_type</span><br><span class="line">user</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure>



<h4 id="创建一张跟mysql中的help-keyword表一样的hive表hk："><a href="#创建一张跟mysql中的help-keyword表一样的hive表hk：" class="headerlink" title="创建一张跟mysql中的help_keyword表一样的hive表hk："></a>创建一张跟mysql中的help_keyword表一样的hive表hk：</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sqoop create-hive-table \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table help_keyword \</span><br><span class="line">--hive-table hk</span><br></pre></td></tr></table></figure>

 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop create-hive-table \</span><br><span class="line">&gt; --connect jdbc:mysql://hadoop1:3306/mysql \</span><br><span class="line">&gt; --username root \</span><br><span class="line">&gt; --password root \</span><br><span class="line">&gt; --table help_keyword \</span><br><span class="line">&gt; --hive-table hk</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">18/04/12 13:50:20 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">18/04/12 13:50:20 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">18/04/12 13:50:20 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override</span><br><span class="line">18/04/12 13:50:20 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.</span><br><span class="line">18/04/12 13:50:20 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">18/04/12 13:50:21 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 1</span><br><span class="line">18/04/12 13:50:21 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 1</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">18/04/12 13:50:23 INFO hive.HiveImport: Loading uploaded data into Hive</span><br><span class="line">18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/home/hadoop/apps/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">18/04/12 13:50:36 INFO hive.HiveImport: </span><br><span class="line">18/04/12 13:50:36 INFO hive.HiveImport: Logging initialized using configuration in jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/hive-common-2.3.3.jar!/hive-log4j2.properties Async: true</span><br><span class="line">18/04/12 13:50:50 INFO hive.HiveImport: OK</span><br><span class="line">18/04/12 13:50:50 INFO hive.HiveImport: Time taken: 11.651 seconds</span><br><span class="line">18/04/12 13:50:51 INFO hive.HiveImport: Hive import complete.</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure>

<h2 id="五、Sqoop的数据导入"><a href="#五、Sqoop的数据导入" class="headerlink" title="五、Sqoop的数据导入"></a>五、Sqoop的数据导入</h2><p>“导入工具”导入单个表从 RDBMS 到 HDFS。表中的每一行被视为 HDFS 的记录。所有记录 都存储为文本文件的文本数据（或者 Avro、sequence 文件等二进制数据） </p>
<h3 id="1、从RDBMS导入到HDFS中"><a href="#1、从RDBMS导入到HDFS中" class="headerlink" title="1、从RDBMS导入到HDFS中"></a>1、从RDBMS导入到HDFS中</h3><h4 id="语法格式"><a href="#语法格式" class="headerlink" title="语法格式"></a>语法格式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop import (generic-args) (import-args)</span><br></pre></td></tr></table></figure>

<p>常用参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">--connect &lt;jdbc-uri&gt; jdbc 连接地址</span><br><span class="line">--connection-manager &lt;class-name&gt; 连接管理者</span><br><span class="line">--driver &lt;class-name&gt; 驱动类</span><br><span class="line">--hadoop-mapred-home &lt;dir&gt; $HADOOP_MAPRED_HOME</span><br><span class="line">--help help 信息</span><br><span class="line">-P 从命令行输入密码</span><br><span class="line">--password &lt;password&gt; 密码</span><br><span class="line">--username &lt;username&gt; 账号</span><br><span class="line">--verbose 打印流程信息</span><br><span class="line">--connection-param-file &lt;filename&gt; 可选参数</span><br></pre></td></tr></table></figure>

<h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h4><p><strong>普通导入：导入mysql库中的help_keyword的数据到HDFS上</strong></p>
<p><strong>导入的默认路径：/user/hadoop/help_keyword</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sqoop import   \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql   \</span><br><span class="line">--username root  \</span><br><span class="line">--password root   \</span><br><span class="line">--table help_keyword   \</span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop import   \</span><br><span class="line">&gt; --connect jdbc:mysql://hadoop1:3306/mysql   \</span><br><span class="line">&gt; --username root  \</span><br><span class="line">&gt; --password root   \</span><br><span class="line">&gt; --table help_keyword   \</span><br><span class="line">&gt; -m 1</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">18/04/12 13:53:48 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">18/04/12 13:53:48 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">18/04/12 13:53:48 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">18/04/12 13:53:48 INFO tool.CodeGenTool: Beginning code generation</span><br><span class="line">18/04/12 13:53:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 1</span><br><span class="line">18/04/12 13:53:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 1</span><br><span class="line">18/04/12 13:53:49 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /home/hadoop/apps/hadoop-2.7.5</span><br><span class="line">注: /tmp/sqoop-hadoop/compile/979d87b9521d0a09ee6620060a112d60/help_keyword.java使用或覆盖了已过时的 API。</span><br><span class="line">注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。</span><br><span class="line">18/04/12 13:53:51 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/979d87b9521d0a09ee6620060a112d60/help_keyword.jar</span><br><span class="line">18/04/12 13:53:51 WARN manager.MySQLManager: It looks like you are importing from mysql.</span><br><span class="line">18/04/12 13:53:51 WARN manager.MySQLManager: This transfer can be faster! Use the --direct</span><br><span class="line">18/04/12 13:53:51 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.</span><br><span class="line">18/04/12 13:53:51 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)</span><br><span class="line">18/04/12 13:53:51 INFO mapreduce.ImportJobBase: Beginning import of help_keyword</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">18/04/12 13:53:52 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar</span><br><span class="line">18/04/12 13:53:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps</span><br><span class="line">18/04/12 13:53:58 INFO db.DBInputFormat: Using read commited transaction isolation</span><br><span class="line">18/04/12 13:53:58 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">18/04/12 13:53:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1523510178850_0001</span><br><span class="line">18/04/12 13:54:00 INFO impl.YarnClientImpl: Submitted application application_1523510178850_0001</span><br><span class="line">18/04/12 13:54:00 INFO mapreduce.Job: The url to track the job: http://hadoop3:8088/proxy/application_1523510178850_0001/</span><br><span class="line">18/04/12 13:54:00 INFO mapreduce.Job: Running job: job_1523510178850_0001</span><br><span class="line">18/04/12 13:54:17 INFO mapreduce.Job: Job job_1523510178850_0001 running in uber mode : false</span><br><span class="line">18/04/12 13:54:17 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">18/04/12 13:54:33 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">18/04/12 13:54:34 INFO mapreduce.Job: Job job_1523510178850_0001 completed successfully</span><br><span class="line">18/04/12 13:54:35 INFO mapreduce.Job: Counters: 30</span><br><span class="line">    File System Counters</span><br><span class="line">        FILE: Number of bytes read=0</span><br><span class="line">        FILE: Number of bytes written=142965</span><br><span class="line">        FILE: Number of read operations=0</span><br><span class="line">        FILE: Number of large read operations=0</span><br><span class="line">        FILE: Number of write operations=0</span><br><span class="line">        HDFS: Number of bytes read=87</span><br><span class="line">        HDFS: Number of bytes written=8264</span><br><span class="line">        HDFS: Number of read operations=4</span><br><span class="line">        HDFS: Number of large read operations=0</span><br><span class="line">        HDFS: Number of write operations=2</span><br><span class="line">    Job Counters </span><br><span class="line">        Launched map tasks=1</span><br><span class="line">        Other local map tasks=1</span><br><span class="line">        Total time spent by all maps in occupied slots (ms)=12142</span><br><span class="line">        Total time spent by all reduces in occupied slots (ms)=0</span><br><span class="line">        Total time spent by all map tasks (ms)=12142</span><br><span class="line">        Total vcore-milliseconds taken by all map tasks=12142</span><br><span class="line">        Total megabyte-milliseconds taken by all map tasks=12433408</span><br><span class="line">    Map-Reduce Framework</span><br><span class="line">        Map input records=619</span><br><span class="line">        Map output records=619</span><br><span class="line">        Input split bytes=87</span><br><span class="line">        Spilled Records=0</span><br><span class="line">        Failed Shuffles=0</span><br><span class="line">        Merged Map outputs=0</span><br><span class="line">        GC time elapsed (ms)=123</span><br><span class="line">        CPU time spent (ms)=1310</span><br><span class="line">        Physical memory (bytes) snapshot=93212672</span><br><span class="line">        Virtual memory (bytes) snapshot=2068234240</span><br><span class="line">        Total committed heap usage (bytes)=17567744</span><br><span class="line">    File Input Format Counters </span><br><span class="line">        Bytes Read=0</span><br><span class="line">    File Output Format Counters </span><br><span class="line">        Bytes Written=8264</span><br><span class="line">18/04/12 13:54:35 INFO mapreduce.ImportJobBase: Transferred 8.0703 KB in 41.8111 seconds (197.6507 bytes/sec)</span><br><span class="line">18/04/12 13:54:35 INFO mapreduce.ImportJobBase: Retrieved 619 records.</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure>

<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412135733601-512350541.png" alt="img"></p>
<p>查看导入的文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop4 ~]$ hadoop fs -cat /user/hadoop/help_keyword/part-m-00000</span><br></pre></td></tr></table></figure>

<p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412140014258-1069061594.png" alt="img"></p>
<p><strong>导入： 指定分隔符和导入路径</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop import   \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql   \</span><br><span class="line">--username root  \</span><br><span class="line">--password root   \</span><br><span class="line">--table help_keyword   \</span><br><span class="line">--target-dir /user/hadoop11/my_help_keyword1  \</span><br><span class="line">--fields-terminated-by &apos;\t&apos;  \</span><br><span class="line">-m 2</span><br></pre></td></tr></table></figure>



<p><strong>导入数据：带where条件</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop import   \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql   \</span><br><span class="line">--username root  \</span><br><span class="line">--password root   \</span><br><span class="line">--where &quot;name=&apos;STRING&apos; &quot; \</span><br><span class="line">--table help_keyword   \</span><br><span class="line">--target-dir /sqoop/hadoop11/myoutport1  \</span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure>



<p><strong>查询指定列</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import   \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql   \</span><br><span class="line">--username root  \</span><br><span class="line">--password root   \</span><br><span class="line">--columns &quot;name&quot; \</span><br><span class="line">--where &quot;name=&apos;STRING&apos; &quot; \</span><br><span class="line">--table help_keyword  \</span><br><span class="line">--target-dir /sqoop/hadoop11/myoutport22  \</span><br><span class="line">-m 1</span><br><span class="line">selct name from help_keyword where name = &quot;string&quot;</span><br></pre></td></tr></table></figure>



<p><strong>导入：指定自定义查询SQL</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sqoop import   \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/  \</span><br><span class="line">--username root  \</span><br><span class="line">--password root   \</span><br><span class="line">--target-dir /user/hadoop/myimport33_1  \</span><br><span class="line">--query &apos;select help_keyword_id,name from mysql.help_keyword where $CONDITIONS and name = &quot;STRING&quot;&apos; \</span><br><span class="line">--split-by  help_keyword_id \</span><br><span class="line">--fields-terminated-by &apos;\t&apos;  \</span><br><span class="line">-m 4</span><br></pre></td></tr></table></figure>



<p>在以上需要按照自定义SQL语句导出数据到HDFS的情况下：<br>1、引号问题，要么外层使用单引号，内层使用双引号，$CONDITIONS的$符号不用转义， 要么外层使用双引号，那么内层使用单引号，然后$CONDITIONS的$符号需要转义<br>2、自定义的SQL语句中必须带有WHERE $CONDITIONS</p>
<h3 id="2、把MySQL数据库中的表数据导入到Hive中"><a href="#2、把MySQL数据库中的表数据导入到Hive中" class="headerlink" title="2、把MySQL数据库中的表数据导入到Hive中"></a>2、把MySQL数据库中的表数据导入到Hive中</h3><p><strong>Sqoop 导入关系型数据到 hive 的过程是先导入到 hdfs，然后再 load 进入 hive</strong></p>
<p><strong>普通导入：数据存储在默认的default hive库中，表名就是对应的mysql的表名：</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop import   \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql   \</span><br><span class="line">--username root  \</span><br><span class="line">--password root   \</span><br><span class="line">--table help_keyword   \</span><br><span class="line">--hive-import \</span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure>

<p>导入过程</p>
<blockquote>
<p>第一步：导入mysql.help_keyword的数据到hdfs的默认路径<br>第二步：自动仿造mysql.help_keyword去创建一张hive表, 创建在默认的default库中<br>第三步：把临时目录中的数据导入到hive表中</p>
</blockquote>
<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412214708733-1227478125.png" alt="img"></p>
<p>查看数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ hadoop fs -cat /user/hive/warehouse/help_keyword/part-m-00000</span><br></pre></td></tr></table></figure>

<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412214921384-1664338223.png" alt="img"></p>
<p><strong>指定行分隔符和列分隔符，指定hive-import，指定覆盖导入，指定自动创建hive表，指定表名，指定删除中间结果数据目录</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sqoop import  \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql  \</span><br><span class="line">--username root  \</span><br><span class="line">--password root  \</span><br><span class="line">--table help_keyword  \</span><br><span class="line">--fields-terminated-by "\t"  \</span><br><span class="line">--lines-terminated-by "\n"  \</span><br><span class="line">--hive-import  \</span><br><span class="line">--hive-overwrite  \</span><br><span class="line">--create-hive-table  \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--hive-database  mydb_test \</span><br><span class="line">--hive-table new_help_keyword</span><br></pre></td></tr></table></figure>

<p> 报错原因是hive-import 当前这个导入命令。 sqoop会自动给创建hive的表。 但是不会自动创建不存在的库</p>
<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412215258496-1424370804.png" alt="img"></p>
<p>手动创建mydb_test数据块</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create database mydb_test;</span><br><span class="line">OK</span><br><span class="line">Time taken: 6.147 seconds</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure>

<p>之后再执行上面的语句没有报错</p>
<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412215621342-788714829.png" alt="img"></p>
<p>查询一下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from new_help_keyword limit 10;</span><br></pre></td></tr></table></figure>

<p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412215929162-1575352315.png" alt="img"></p>
<p>上面的导入语句等价于</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sqoop import  \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://hadoop1:3306/mysql  \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password root  \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t"  \</span></span><br><span class="line"><span class="comment">--lines-terminated-by "\n"  \</span></span><br><span class="line"><span class="comment">--hive-import  \</span></span><br><span class="line"><span class="comment">--hive-overwrite  \</span></span><br><span class="line"><span class="comment">--create-hive-table  \ </span></span><br><span class="line"><span class="comment">--hive-table  mydb_test.new_help_keyword  \</span></span><br><span class="line"><span class="comment">--delete-target-dir</span></span><br></pre></td></tr></table></figure>

<p><strong>增量导入</strong></p>
<p>执行增量导入之前，先清空hive数据库中的help_keyword表中的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">truncate table help_keyword;</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://hadoop1:3306/mysql   \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password root   \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--target-dir /user/hadoop/myimport_add  \</span></span><br><span class="line"><span class="comment">--incremental  append  \</span></span><br><span class="line"><span class="comment">--check-column  help_keyword_id \</span></span><br><span class="line"><span class="comment">--last-value 500  \</span></span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure>

<p>语句执行成功</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop import   \</span><br><span class="line">&gt; --connect jdbc:mysql://hadoop1:3306/mysql   \</span><br><span class="line">&gt; --username root  \</span><br><span class="line">&gt; --password root   \</span><br><span class="line">&gt; --table help_keyword  \</span><br><span class="line">&gt; --target-dir /user/hadoop/myimport_add  \</span><br><span class="line">&gt; --incremental  append  \</span><br><span class="line">&gt; --check-column  help_keyword_id \</span><br><span class="line">&gt; --last-value 500  \</span><br><span class="line">&gt; -m 1</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">18/04/12 22:01:07 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">18/04/12 22:01:08 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">18/04/12 22:01:08 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">18/04/12 22:01:08 INFO tool.CodeGenTool: Beginning code generation</span><br><span class="line">18/04/12 22:01:08 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 1</span><br><span class="line">18/04/12 22:01:08 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 1</span><br><span class="line">18/04/12 22:01:08 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /home/hadoop/apps/hadoop-2.7.5</span><br><span class="line">注: /tmp/sqoop-hadoop/compile/a51619d1ef8c6e4b112a209326ed9e0f/help_keyword.java使用或覆盖了已过时的 API。</span><br><span class="line">注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。</span><br><span class="line">18/04/12 22:01:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/a51619d1ef8c6e4b112a209326ed9e0f/help_keyword.jar</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">18/04/12 22:01:12 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(`help_keyword_id`) FROM `help_keyword`</span><br><span class="line">18/04/12 22:01:12 INFO tool.ImportTool: Incremental import based on column `help_keyword_id`</span><br><span class="line">18/04/12 22:01:12 INFO tool.ImportTool: Lower bound value: 500</span><br><span class="line">18/04/12 22:01:12 INFO tool.ImportTool: Upper bound value: 618</span><br><span class="line">18/04/12 22:01:12 WARN manager.MySQLManager: It looks like you are importing from mysql.</span><br><span class="line">18/04/12 22:01:12 WARN manager.MySQLManager: This transfer can be faster! Use the --direct</span><br><span class="line">18/04/12 22:01:12 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.</span><br><span class="line">18/04/12 22:01:12 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)</span><br><span class="line">18/04/12 22:01:12 INFO mapreduce.ImportJobBase: Beginning import of help_keyword</span><br><span class="line">18/04/12 22:01:12 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar</span><br><span class="line">18/04/12 22:01:12 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps</span><br><span class="line">18/04/12 22:01:17 INFO db.DBInputFormat: Using read commited transaction isolation</span><br><span class="line">18/04/12 22:01:17 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">18/04/12 22:01:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1523510178850_0010</span><br><span class="line">18/04/12 22:01:19 INFO impl.YarnClientImpl: Submitted application application_1523510178850_0010</span><br><span class="line">18/04/12 22:01:19 INFO mapreduce.Job: The url to track the job: http://hadoop3:8088/proxy/application_1523510178850_0010/</span><br><span class="line">18/04/12 22:01:19 INFO mapreduce.Job: Running job: job_1523510178850_0010</span><br><span class="line">18/04/12 22:01:30 INFO mapreduce.Job: Job job_1523510178850_0010 running in uber mode : false</span><br><span class="line">18/04/12 22:01:30 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">18/04/12 22:01:40 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">18/04/12 22:01:40 INFO mapreduce.Job: Job job_1523510178850_0010 completed successfully</span><br><span class="line">18/04/12 22:01:41 INFO mapreduce.Job: Counters: 30</span><br><span class="line">    File System Counters</span><br><span class="line">        FILE: Number of bytes read=0</span><br><span class="line">        FILE: Number of bytes written=143200</span><br><span class="line">        FILE: Number of read operations=0</span><br><span class="line">        FILE: Number of large read operations=0</span><br><span class="line">        FILE: Number of write operations=0</span><br><span class="line">        HDFS: Number of bytes read=87</span><br><span class="line">        HDFS: Number of bytes written=1576</span><br><span class="line">        HDFS: Number of read operations=4</span><br><span class="line">        HDFS: Number of large read operations=0</span><br><span class="line">        HDFS: Number of write operations=2</span><br><span class="line">    Job Counters </span><br><span class="line">        Launched map tasks=1</span><br><span class="line">        Other local map tasks=1</span><br><span class="line">        Total time spent by all maps in occupied slots (ms)=7188</span><br><span class="line">        Total time spent by all reduces in occupied slots (ms)=0</span><br><span class="line">        Total time spent by all map tasks (ms)=7188</span><br><span class="line">        Total vcore-milliseconds taken by all map tasks=7188</span><br><span class="line">        Total megabyte-milliseconds taken by all map tasks=7360512</span><br><span class="line">    Map-Reduce Framework</span><br><span class="line">        Map input records=118</span><br><span class="line">        Map output records=118</span><br><span class="line">        Input split bytes=87</span><br><span class="line">        Spilled Records=0</span><br><span class="line">        Failed Shuffles=0</span><br><span class="line">        Merged Map outputs=0</span><br><span class="line">        GC time elapsed (ms)=86</span><br><span class="line">        CPU time spent (ms)=870</span><br><span class="line">        Physical memory (bytes) snapshot=95576064</span><br><span class="line">        Virtual memory (bytes) snapshot=2068234240</span><br><span class="line">        Total committed heap usage (bytes)=18608128</span><br><span class="line">    File Input Format Counters </span><br><span class="line">        Bytes Read=0</span><br><span class="line">    File Output Format Counters </span><br><span class="line">        Bytes Written=1576</span><br><span class="line">18/04/12 22:01:41 INFO mapreduce.ImportJobBase: Transferred 1.5391 KB in 28.3008 seconds (55.6875 bytes/sec)</span><br><span class="line">18/04/12 22:01:41 INFO mapreduce.ImportJobBase: Retrieved 118 records.</span><br><span class="line">18/04/12 22:01:41 INFO util.AppendUtils: Creating missing output directory - myimport_add</span><br><span class="line">18/04/12 22:01:41 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:</span><br><span class="line">18/04/12 22:01:41 INFO tool.ImportTool:  --incremental append</span><br><span class="line">18/04/12 22:01:41 INFO tool.ImportTool:   --check-column help_keyword_id</span><br><span class="line">18/04/12 22:01:41 INFO tool.ImportTool:   --last-value 618</span><br><span class="line">18/04/12 22:01:41 INFO tool.ImportTool: (Consider saving this with &apos;sqoop job --create&apos;)</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure>

<p> 查看结果</p>
<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412221000267-1226635235.png" alt="img"></p>
<h3 id="3、把MySQL数据库中的表数据导入到hbase"><a href="#3、把MySQL数据库中的表数据导入到hbase" class="headerlink" title="3、把MySQL数据库中的表数据导入到hbase"></a>3、把MySQL数据库中的表数据导入到hbase</h3><p> 普通导入</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table help_keyword \</span><br><span class="line">--hbase-table new_help_keyword \</span><br><span class="line">--column-family person \</span><br><span class="line">--hbase-row-key help_keyword_id</span><br></pre></td></tr></table></figure>



<p>此时会报错，因为需要先创建Hbase里面的表，再执行导入的语句</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; create 'new_help_keyword', 'base_info'</span><br><span class="line">0 row(s) in 3.6280 seconds</span><br><span class="line"></span><br><span class="line">=&gt; Hbase::Table - new_help_keyword</span><br><span class="line">hbase(main):002:0&gt;</span><br></pre></td></tr></table></figure>




    </div>
    
    <div class="post-footer">
        <div>
            
            转载声明：
            商业转载请联系作者获得授权,非商业转载请注明出处 © <a href="https://github.com/wenxinzhang" target="_blank">福星</a>
            
            
        </div>
        <div>
            
        </div>
    </div>
</article>
<div class="article-nav prev-next-wrap clearfix">
    
    <a href="/2019-06-01-Spark学习之路 （一）Spark初识.html" class="pre-post btn btn-default" title='Spark学习之路 （一）Spark初识'>
        <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
        <span class="hidden-xs">
            Spark学习之路 （一）Spark初识</span>
    </a>
    
    
    <a href="/2019-05-10-Scala学习之路 （十）Scala的Actor.html" class="next-post btn btn-default" title='Scala学习之路 （十）Scala的Actor'>
        <span class="hidden-lg">下一篇</span>
        <span class="hidden-xs">
            Scala学习之路 （十）Scala的Actor</span><i class="fa fa-angle-right fa-fw"></i>
    </a>
    
</div>

<div id="comments">
    

<div id="vcomments" class="valine"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="/assets/valine.min.js"></script>
<script>
new Valine({
    av: AV,
    el: '#vcomments',
    appId: 'UckE9LEIQ8aoa3MH1Kio27rB-gzGzoHsz',
    appKey: '7HC9xCVYQdshKqFRDmULFm5G',
    placeholder: '说点什么吧',
    notify: false,
    verify: true,
    avatar: 'mm',
    meta: 'nick,mail'.split(','),
    pageSize: '10',
    path: window.location.pathname,
    lang: 'zh-CN'.toLowerCase()
})
</script>


</div>

                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">
            文章目录
        </h3>
        
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#一、概述"><span class="toc-text">一、概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#二、工作机制"><span class="toc-text">二、工作机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#三、安装"><span class="toc-text">三、安装</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、前提概述"><span class="toc-text">1、前提概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、软件下载"><span class="toc-text">2、软件下载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、安装步骤"><span class="toc-text">3、安装步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#（1）上传解压缩安装包到指定目录"><span class="toc-text">（1）上传解压缩安装包到指定目录</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#（2）进入到-conf-文件夹，找到-sqoop-env-template-sh，修改其名称为-sqoop-env-sh-cd-conf"><span class="toc-text">（2）进入到 conf 文件夹，找到 sqoop-env-template.sh，修改其名称为 sqoop-env.sh cd conf</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#（3）修改-sqoop-env-sh"><span class="toc-text">（3）修改 sqoop-env.sh</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#（4）加入-mysql-驱动包到-sqoop1-4-6-lib-目录下"><span class="toc-text">（4）加入 mysql 驱动包到 sqoop1.4.6/lib 目录下</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#（5）配置系统环境变量"><span class="toc-text">（5）配置系统环境变量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#（6）验证安装是否成功"><span class="toc-text">（6）验证安装是否成功</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#四、Sqoop的基本命令"><span class="toc-text">四、Sqoop的基本命令</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基本操作"><span class="toc-text">基本操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#示例"><span class="toc-text">示例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#列出MySQL数据有哪些数据库"><span class="toc-text">列出MySQL数据有哪些数据库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#列出MySQL中的某个数据库有哪些数据表："><span class="toc-text">列出MySQL中的某个数据库有哪些数据表：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#创建一张跟mysql中的help-keyword表一样的hive表hk："><span class="toc-text">创建一张跟mysql中的help_keyword表一样的hive表hk：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#五、Sqoop的数据导入"><span class="toc-text">五、Sqoop的数据导入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、从RDBMS导入到HDFS中"><span class="toc-text">1、从RDBMS导入到HDFS中</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#语法格式"><span class="toc-text">语法格式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#示例-1"><span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、把MySQL数据库中的表数据导入到Hive中"><span class="toc-text">2、把MySQL数据库中的表数据导入到Hive中</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、把MySQL数据库中的表数据导入到hbase"><span class="toc-text">3、把MySQL数据库中的表数据导入到hbase</span></a></li></ol></li></ol>
        
    </div>
</aside>
                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>
<a id="back-to-top" class="icon-btn hide">
    <i class="fa fa-chevron-up"></i>
</a>
    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="busuanzi">
    
    访问量:
    <strong id="busuanzi_value_site_pv">
        <i class="fa fa-spinner fa-spin"></i>
    </strong>
    &nbsp; | &nbsp;
    访客数:
    <strong id="busuanzi_value_site_uv">
        <i class="fa fa-spinner fa-spin"></i>
    </strong>
    
</div>
            </div>
            <div class="col-sm-12">
                <span>Copyright &copy;
                    2018
                </span> |
                <span>
                    Powered by <a href="//hexo.io" class="copyright-links" target="_blank" rel="nofollow">Hexo</a>
                </span> |
                <span>
                    Theme by <a href="//github.com/shenliyang/hexo-theme-snippet.git" class="copyright-links" target="_blank" rel="nofollow">Snippet</a>
                </span>
            </div>
        </div>
    </div>
</div>



<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<script src="/js/app.js?rev=@@hash"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":200,"height":350},"mobile":{"show":true}});</script></body>
</html>