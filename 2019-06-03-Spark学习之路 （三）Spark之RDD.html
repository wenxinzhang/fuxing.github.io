<!DOCTYPE HTML>
<html lang="zh-CN">

<head><meta name="generator" content="Hexo 3.9.0">
    <!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="福星">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <meta name="theme-version" content="1.2.3">
    <meta name="root" content="/">
    <link rel="dns-prefetch" href="http://zhangfuxin.cn">
    <!--SEO-->

<meta name="keywords" content="Spark">


<meta name="description" content="** Spark学习之路 （三）Spark之RDD：** &lt;Excerpt in index | 首页摘要&gt;
​        Spark学习之路 （三）Spark之RDD

&lt...">


<meta name="robots" content="all">
<meta name="google" content="all">
<meta name="googlebot" content="all">
<meta name="verify" content="all">
    <!--Title-->

<title>
    
    Spark学习之路 （三）Spark之RDD |
    
    福星
</title>

<link rel="alternate" href="/atom.xml" title="福星" type="application/atom+xml">


<link rel="icon" href="/favicon.ico">

    

<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7">
<link rel="stylesheet" href="/css/font-awesome.min.css?rev=4.7.0">
<link rel="stylesheet" href="/css/style.css?rev=@@hash">
    
<div class="hide">
    <script type="text/javascript">
    var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
    document.write(unescape("%3Cspan class='cnzz_stat_icon_1263868967 hide' %3E%3Cscript%20src%3D%22https%3A%2F%2Fs95.cnzz.com%2Fz_stat.php%3Fweb_id%3D1272564536%22%3E%3C%2Fscript%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1263868967%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
    </script>
</div>




    

<script>
(function() {
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    } else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

</head></html>
<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->
<body>
    <header class="main-header"  style="background-image:url(
    http://snippet.shenliyang.com/img/banner.jpg)"
     >
    <div class="main-header-box">
        <a class="header-avatar" href="/" title='福 星'>
            <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block">
        </a>
        <div class="branding">
            <!--<h2 class="text-hide">Snippet主题,从未如此简单有趣</h2>-->
            
            <img src="/img/branding.png" alt="Snippet 博客主题" class="img-responsive center-block">
            
        </div>
    </div>
</header>
    <nav class="main-navigation">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                        <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="navbar-brand" href="http://zhangfuxin.cn">
                        福星</a>
                </div>
                <div class="collapse navbar-collapse" id="main-menu">
                    <ul class="menu">
                        
                        <li role="presentation" class="text-center">
                            <a href="/"><i class="fa "></i>
                                首页</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Linux/"><i class="fa "></i>
                                Linux</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Hadoop/"><i class="fa "></i>
                                Hadoop</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Spark/"><i class="fa "></i>
                                Spark</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Java/"><i class="fa "></i>
                                Java</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Python/"><i class="fa "></i>
                                Python</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/algorithm/"><i class="fa "></i>
                                算法</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/工具/"><i class="fa "></i>
                                工具</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/archives/"><i class="fa "></i>
                                时间轴</a>
                        </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="Spark学习之路 （三）Spark之RDD">
            
            Spark学习之路 （三）Spark之RDD
            
        </h1>
        <div class="post-meta">
    
    <span class="categories-meta fa-wrap">
        <i class="fa fa-folder-open-o"></i>
        <a class="category-link" href="/categories/Spark/">Spark</a>
    </span>
    
    
    <span class="fa-wrap">
        <i class="fa fa-tags"></i>
        <span class="tags-meta">
            
            <a class="tag-link" href="/tags/Spark/">Spark</a>
            
        </span>
    </span>
    
    
    
    <span class="fa-wrap">
        <i class="fa fa-clock-o"></i>
        <span class="date-meta">
            2019/06/03</span>
    </span>
    
    <span class="fa-wrap">
        <i class="fa fa-eye"></i>
        <span id="busuanzi_value_page_pv"></span>
    </span>
    
    
</div>
        
        
    </div>
    
    <div class="post-body post-content">
        <p>** Spark学习之路 （三）Spark之RDD：** &lt;Excerpt in index | 首页摘要&gt;</p>
<p>​        Spark学习之路 （三）Spark之RDD</p>
<a id="more"></a>
<p>&lt;The rest of contents | 余下全文&gt;</p>
<h2 id="一、RDD的概述"><a href="#一、RDD的概述" class="headerlink" title="一、RDD的概述"></a>一、RDD的概述</h2><h3 id="1-1-什么是RDD"><a href="#1-1-什么是RDD" class="headerlink" title="1.1　什么是RDD"></a>1.1　什么是RDD</h3><p>​        <strong>RDD</strong>（Resilient Distributed Dataset）叫做<strong>弹性分布式数据集</strong>，<strong>是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。</strong>RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。</p>
<h3 id="1-2-RDD的属性"><a href="#1-2-RDD的属性" class="headerlink" title="1.2　RDD的属性"></a>1.2　RDD的属性</h3><p><a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala</a></p>
<blockquote>
<p> A list of partitions<br> A function for computing each split<br> A list of dependencies on other RDDs<br> Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<br> Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</p>
</blockquote>
<p>（1）一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</p>
<p>（2）一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</p>
<p>（3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p>
<p>（4）一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p>
<p>（5）一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</p>
<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421133911520-1150689001.png" alt="img"></p>
<p>其中hello.txt</p>
<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421134031551-1670646166.png" alt="img"></p>
<h2 id="二、RDD的创建方式"><a href="#二、RDD的创建方式" class="headerlink" title="二、RDD的创建方式"></a>二、RDD的创建方式</h2><h3 id="2-1-通过读取文件生成的"><a href="#2-1-通过读取文件生成的" class="headerlink" title="2.1　通过读取文件生成的"></a>2.1　通过读取文件生成的</h3><p>由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val file = sc.textFile(&quot;/spark/hello.txt&quot;)</span><br></pre></td></tr></table></figure>

<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421134515478-653027491.png" alt="img"></p>
<h3 id="2-2-通过并行化的方式创建RDD"><a href="#2-2-通过并行化的方式创建RDD" class="headerlink" title="2.2　通过并行化的方式创建RDD"></a>2.2　通过并行化的方式创建RDD</h3><p>由一个已经存在的Scala集合创建。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> array = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">array: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(array)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">27</span>] at parallelize at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>

<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421134820158-111255712.png" alt="img"></p>
<h3 id="2-3-其他方式"><a href="#2-3-其他方式" class="headerlink" title="2.3　其他方式"></a>2.3　其他方式</h3><p>读取数据库等等其他的操作。也可以生成RDD。RDD转换为ParallelCollectionRDD。</p>
<h2 id="三、RDD编程API"><a href="#三、RDD编程API" class="headerlink" title="三、RDD编程API"></a>三、RDD编程API</h2><p><strong>Spark支持两个类型（算子）操作：Transformation和Action</strong></p>
<h3 id="3-1-Transformation"><a href="#3-1-Transformation" class="headerlink" title="3.1　Transformation"></a>3.1　Transformation</h3><p>​    主要做的是就是将一个已有的RDD生成另外一个RDD。Transformation具有<strong>lazy**</strong>特性(延迟加载)**。Transformation算子的代码不会真正被执行。只有当我们的程序里面遇到一个action算子的时候，代码才会真正的被执行。这种设计让Spark更加有效率地运行。</p>
<p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds</a></p>
<p><strong>常用的Transformation</strong>：</p>
<table>
<thead>
<tr>
<th><strong>转换</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>map</strong>(func)</td>
<td>返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成</td>
</tr>
<tr>
<td><strong>filter</strong>(func)</td>
<td>返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成</td>
</tr>
<tr>
<td><strong>flatMap</strong>(func)</td>
<td>类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）</td>
</tr>
<tr>
<td><strong>mapPartitions</strong>(func)</td>
<td>类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]</td>
</tr>
<tr>
<td><strong>mapPartitionsWithIndex</strong>(func)</td>
<td>类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]</td>
</tr>
<tr>
<td><strong>sample</strong>(withReplacement, fraction, seed)</td>
<td>根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子</td>
</tr>
<tr>
<td><strong>union</strong>(otherDataset)</td>
<td>对源RDD和参数RDD求并集后返回一个新的RDD</td>
</tr>
<tr>
<td><strong>intersection</strong>(otherDataset)</td>
<td>对源RDD和参数RDD求交集后返回一个新的RDD</td>
</tr>
<tr>
<td><strong>distinct</strong>([numTasks]))</td>
<td>对源RDD进行去重后返回一个新的RDD</td>
</tr>
<tr>
<td><strong>groupByKey</strong>([numTasks])</td>
<td>在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD</td>
</tr>
<tr>
<td><strong>reduceByKey</strong>(func, [numTasks])</td>
<td>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置</td>
</tr>
<tr>
<td><strong>aggregateByKey</strong>(zeroValue)(seqOp, combOp, [numTasks])</td>
<td>先按分区聚合 再总的聚合   每次要跟初始值交流 例如：aggregateByKey(0)(<em>+</em>,<em>+</em>) 对k/y的RDD进行操作</td>
</tr>
<tr>
<td><strong>sortByKey</strong>([ascending], [numTasks])</td>
<td>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</td>
</tr>
<tr>
<td><strong>sortBy</strong>(func,[ascending], [numTasks])</td>
<td>与sortByKey类似，但是更灵活 第一个参数是根据什么排序  第二个是怎么排序 false倒序   第三个排序后分区数  默认与原RDD一样</td>
</tr>
<tr>
<td><strong>join</strong>(otherDataset, [numTasks])</td>
<td>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD  相当于内连接（求交集）</td>
</tr>
<tr>
<td><strong>cogroup</strong>(otherDataset, [numTasks])</td>
<td>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<v>,Iterable<w>))类型的RDD</w></v></td>
</tr>
<tr>
<td><strong>cartesian</strong>(otherDataset)</td>
<td>两个RDD的笛卡尔积  的成很多个K/V</td>
</tr>
<tr>
<td><strong>pipe</strong>(command, [envVars])</td>
<td>调用外部程序</td>
</tr>
<tr>
<td><strong>coalesce</strong>(numPartitions<strong>)</strong></td>
<td>重新分区 第一个参数是要分多少区，第二个参数是否shuffle 默认false  少分区变多分区 true   多分区变少分区 false</td>
</tr>
<tr>
<td><strong>repartition</strong>(numPartitions)</td>
<td>重新分区 必须shuffle  参数是要分多少区  少变多</td>
</tr>
<tr>
<td><strong>repartitionAndSortWithinPartitions</strong>(partitioner)</td>
<td>重新分区+排序  比先分区再排序效率高  对K/V的RDD进行操作</td>
</tr>
<tr>
<td><strong>foldByKey</strong>(zeroValue)(seqOp)</td>
<td>该函数用于K/V做折叠，合并处理 ，与aggregate类似   第一个括号的参数应用于每个V值  第二括号函数是聚合例如：<em>+</em></td>
</tr>
<tr>
<td><strong>combineByKey</strong></td>
<td>合并相同的key的值 rdd1.combineByKey(x =&gt; x, (a: Int, b: Int) =&gt; a + b, (m: Int, n: Int) =&gt; m + n)</td>
</tr>
<tr>
<td><strong>partitionBy**</strong>（partitioner）**</td>
<td>对RDD进行分区  partitioner是分区器 例如new HashPartition(2</td>
</tr>
<tr>
<td><strong>cache</strong></td>
<td>RDD缓存，可以避免重复计算从而减少时间，区别：cache内部调用了persist算子，cache默认就一个缓存级别MEMORY-ONLY ，而persist则可以选择缓存级别</td>
</tr>
<tr>
<td><strong>persist</strong></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Subtract**</strong>（rdd）**</td>
<td>返回前rdd元素不在后rdd的rdd</td>
</tr>
<tr>
<td><strong>leftOuterJoin</strong></td>
<td>leftOuterJoin类似于SQL中的左外关联left outer join，返回结果以前面的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可。</td>
</tr>
<tr>
<td><strong>rightOuterJoin</strong></td>
<td>rightOuterJoin类似于SQL中的有外关联right outer join，返回结果以参数中的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可</td>
</tr>
<tr>
<td>subtractByKey</td>
<td>substractByKey和基本转换操作中的subtract类似只不过这里是针对K的，返回在主RDD中出现，并且不在otherRDD中出现的元素</td>
</tr>
</tbody></table>
<h3 id="3-2-Action"><a href="#3-2-Action" class="headerlink" title="3.2　Action"></a>3.2　Action</h3><p>触发代码的运行，我们一段spark代码里面至少需要有一个action操作。</p>
<p><strong>常用的Action</strong>:</p>
<table>
<thead>
<tr>
<th><strong>动作</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>reduce</strong>(<em>func</em>)</td>
<td>通过func函数聚集RDD中的所有元素，这个功能必须是课交换且可并联的</td>
</tr>
<tr>
<td><strong>collect</strong>()</td>
<td>在驱动程序中，以数组的形式返回数据集的所有元素</td>
</tr>
<tr>
<td><strong>count</strong>()</td>
<td>返回RDD的元素个数</td>
</tr>
<tr>
<td><strong>first</strong>()</td>
<td>返回RDD的第一个元素（类似于take(1)）</td>
</tr>
<tr>
<td><strong>take</strong>(<em>n</em>)</td>
<td>返回一个由数据集的前n个元素组成的数组</td>
</tr>
<tr>
<td><strong>takeSample</strong>(<em>withReplacement</em>,<em>num</em>, [<em>seed</em>])</td>
<td>返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子</td>
</tr>
<tr>
<td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td>
<td></td>
</tr>
<tr>
<td><strong>saveAsTextFile</strong>(<em>path</em>)</td>
<td>将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本</td>
</tr>
<tr>
<td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td>
<td>将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。</td>
</tr>
<tr>
<td><strong>saveAsObjectFile</strong>(<em>path</em>)</td>
<td></td>
</tr>
<tr>
<td><strong>countByKey</strong>()</td>
<td>针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</td>
</tr>
<tr>
<td><strong>foreach</strong>(<em>func</em>)</td>
<td>在数据集的每一个元素上，运行函数func进行更新。</td>
</tr>
<tr>
<td><strong>aggregate</strong></td>
<td>先对分区进行操作，在总体操作</td>
</tr>
<tr>
<td><strong>reduceByKeyLocally</strong></td>
<td></td>
</tr>
<tr>
<td><strong>lookup</strong></td>
<td></td>
</tr>
<tr>
<td><strong>top</strong></td>
<td></td>
</tr>
<tr>
<td><strong>fold</strong></td>
<td></td>
</tr>
<tr>
<td><strong>foreachPartition</strong></td>
<td></td>
</tr>
</tbody></table>
<h3 id="3-3-Spark-WordCount代码编写"><a href="#3-3-Spark-WordCount代码编写" class="headerlink" title="3.3　Spark WordCount代码编写"></a>3.3　Spark WordCount代码编写</h3><p>使用maven进行项目构建</p>
<h4 id="（1）使用scala进行编写"><a href="#（1）使用scala进行编写" class="headerlink" title="（1）使用scala进行编写"></a>（1）使用scala进行编写</h4><p>查看官方网站，需要导入2个依赖包</p>
<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421144526496-1152731884.png" alt="img"></p>
<p>详细代码</p>
<p>SparkWordCountWithScala.scala</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkWordCountWithScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 如果这个参数不设置，默认认为你运行的是集群模式</span></span><br><span class="line"><span class="comment">      * 如果设置成local代表运行的是local模式</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    conf.setMaster(<span class="string">"local"</span>)</span><br><span class="line">    <span class="comment">//设置任务名</span></span><br><span class="line">    conf.setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">    <span class="comment">//创建SparkCore的程序入口</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//读取文件 生成RDD</span></span><br><span class="line">    <span class="keyword">val</span> file: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"E:\\hello.txt"</span>)</span><br><span class="line">    <span class="comment">//把每一行数据按照，分割</span></span><br><span class="line">    <span class="keyword">val</span> word: <span class="type">RDD</span>[<span class="type">String</span>] = file.flatMap(_.split(<span class="string">","</span>))</span><br><span class="line">    <span class="comment">//让每一个单词都出现一次</span></span><br><span class="line">    <span class="keyword">val</span> wordOne: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = word.map((_,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">//单词计数</span></span><br><span class="line">    <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.reduceByKey(_+_)</span><br><span class="line">    <span class="comment">//按照单词出现的次数 降序排序</span></span><br><span class="line">    <span class="keyword">val</span> sortRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordCount.sortBy(tuple =&gt; tuple._2,<span class="literal">false</span>)</span><br><span class="line">    <span class="comment">//将最终的结果进行保存</span></span><br><span class="line">    sortRdd.saveAsTextFile(<span class="string">"E:\\result"</span>)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>运行结果</p>
<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421151823241-1753369845.png" alt="img"></p>
<h4 id="（2）使用java-jdk7进行编写"><a href="#（2）使用java-jdk7进行编写" class="headerlink" title="（2）使用java jdk7进行编写"></a>（2）使用java jdk7进行编写</h4><p>SparkWordCountWithJava7.java</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaPairRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaSparkContext</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.<span class="type">FlatMapFunction</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.<span class="type">Function2</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.<span class="type">PairFunction</span>;</span><br><span class="line"><span class="keyword">import</span> scala.<span class="type">Tuple2</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Arrays</span>;</span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Iterator</span>;</span><br><span class="line"></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SparkWordCountWithJava7</span> </span>&#123;</span><br><span class="line">    public static void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line">        <span class="type">SparkConf</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>();</span><br><span class="line">        conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">        conf.setAppName(<span class="string">"WordCount"</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> sc = <span class="keyword">new</span> <span class="type">JavaSparkContext</span>(conf);</span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; fileRdd = sc.textFile(<span class="string">"E:\\hello.txt"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; wordRDD = fileRdd.flatMap(<span class="keyword">new</span> <span class="type">FlatMapFunction</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Iterator</span>&lt;<span class="type">String</span>&gt; call(<span class="type">String</span> line) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="type">Arrays</span>.asList(line.split(<span class="string">","</span>)).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordOneRDD = wordRDD.mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">String</span>, <span class="type">String</span>, <span class="type">Integer</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; call(<span class="type">String</span> word) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(word, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordCountRDD = wordOneRDD.reduceByKey(<span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Integer</span>, <span class="type">Integer</span>, <span class="type">Integer</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Integer</span> call(<span class="type">Integer</span> i1, <span class="type">Integer</span> i2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> i1 + i2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; count2WordRDD = wordCountRDD.mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt;, <span class="type">Integer</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; sortRDD = count2WordRDD.sortByKey(<span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; resultRDD = sortRDD.mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt;, <span class="type">String</span>, <span class="type">Integer</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        resultRDD.saveAsTextFile(<span class="string">"E:\\result7"</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="（3）使用java-jdk8进行编写"><a href="#（3）使用java-jdk8进行编写" class="headerlink" title="（3）使用java jdk8进行编写"></a>（3）使用java jdk8进行编写</h4><p>lambda表达式</p>
<p>SparkWordCountWithJava8.java</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaPairRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaSparkContext</span>;</span><br><span class="line"><span class="keyword">import</span> scala.<span class="type">Tuple2</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Arrays</span>;</span><br><span class="line"></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SparkWordCountWithJava8</span> </span>&#123;</span><br><span class="line">    public static void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line">        <span class="type">SparkConf</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>();</span><br><span class="line">        conf.setAppName(<span class="string">"WortCount"</span>);</span><br><span class="line">        conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> sc = <span class="keyword">new</span> <span class="type">JavaSparkContext</span>(conf);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; fileRDD = sc.textFile(<span class="string">"E:\\hello.txt"</span>);</span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; wordRdd = fileRDD.flatMap(line -&gt; <span class="type">Arrays</span>.asList(line.split(<span class="string">","</span>)).iterator());</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordOneRDD = wordRdd.mapToPair(word -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordCountRDD = wordOneRDD.reduceByKey((x, y) -&gt; x + y);</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; count2WordRDD = wordCountRDD.mapToPair(tuple -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1));</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; sortRDD = count2WordRDD.sortByKey(<span class="literal">false</span>);</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; resultRDD = sortRDD.mapToPair(tuple -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1));</span><br><span class="line">        resultRDD.saveAsTextFile(<span class="string">"E:\\result8"</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425153140543-8294264.png" alt="img"></p>
<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425153515149-1269337605.png" alt="img"></p>
<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425153556469-238789142.png" alt="img"></p>
<h2 id="四、RDD的宽依赖和窄依赖"><a href="#四、RDD的宽依赖和窄依赖" class="headerlink" title="四、RDD的宽依赖和窄依赖"></a>四、RDD的宽依赖和窄依赖</h2><h3 id="4-1-RDD依赖关系的本质内幕"><a href="#4-1-RDD依赖关系的本质内幕" class="headerlink" title="4.1　RDD依赖关系的本质内幕"></a>4.1　<strong>RDD依赖关系的本质内幕</strong></h3><p>由于RDD是粗粒度的操作数据集，每个Transformation操作都会生成一个新的RDD，所以RDD之间就会形成类似流水线的前后依赖关系；RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。如图所示显示了RDD之间的依赖关系。</p>
<p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425151022105-1121308065.png" alt="img"></p>
<p>从图中可知：</p>
<p><strong>窄依赖：</strong>是指每个父RDD的一个Partition最多被子RDD的一个Partition所使用，例如map、filter、union等操作都会产生窄依赖；（独生子女）</p>
<p><strong>宽依赖：</strong>是指一个父RDD的Partition会被多个子RDD的Partition所使用，例如groupByKey、reduceByKey、sortByKey等操作都会产生宽依赖；（超生）</p>
<p>需要特别说明的是对join操作有两种情况：</p>
<p>（1）图中左半部分join：如果两个RDD在进行join操作时，一个RDD的partition仅仅和另一个RDD中已知个数的Partition进行join，那么这种类型的join操作就是窄依赖，例如图1中左半部分的join操作(join with inputs co-partitioned)；</p>
<p>（2）图中右半部分join：其它情况的join操作就是宽依赖,例如图1中右半部分的join操作(join with inputs not co-partitioned)，由于是需要父RDD的所有partition进行join的转换，这就涉及到了shuffle，因此这种类型的join操作也是宽依赖。</p>
<p>总结：</p>
<blockquote>
<p>在这里我们是从父RDD的partition被使用的个数来定义窄依赖和宽依赖，因此可以用一句话概括下：如果父RDD的一个Partition被子RDD的一个Partition所使用就是窄依赖，否则的话就是宽依赖。因为是确定的partition数量的依赖关系，所以RDD之间的依赖关系就是窄依赖；由此我们可以得出一个推论：即窄依赖不仅包含一对一的窄依赖，还包含一对固定个数的窄依赖。</p>
<p>一对固定个数的窄依赖的理解：即子RDD的partition对父RDD依赖的Partition的数量不会随着RDD数据规模的改变而改变；换句话说，无论是有100T的数据量还是1P的数据量，在窄依赖中，子RDD所依赖的父RDD的partition的个数是确定的，而宽依赖是shuffle级别的，数据量越大，那么子RDD所依赖的父RDD的个数就越多，从而子RDD所依赖的父RDD的partition的个数也会变得越来越多。</p>
</blockquote>
<h3 id="4-2-依赖关系下的数据流视图"><a href="#4-2-依赖关系下的数据流视图" class="headerlink" title="4.2　依赖关系下的数据流视图"></a>4.2　<strong>依赖关系下的数据流视图</strong></h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425151747136-185909749.png" alt="img"></p>
<p>在spark中，会根据RDD之间的依赖关系将DAG图（有向无环图）划分为不同的阶段，对于窄依赖，由于partition依赖关系的确定性，partition的转换处理就可以在同一个线程里完成，窄依赖就被spark划分到同一个stage中，而对于宽依赖，只能等父RDD shuffle处理完成后，下一个stage才能开始接下来的计算。</p>
<p><strong>因此spark划分stage的整体思路是</strong>：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。因此在图2中RDD C,RDD D,RDD E,RDDF被构建在一个stage中,RDD A被构建在一个单独的Stage中,而RDD B和RDD G又被构建在同一个stage中。</p>
<p>在spark中，Task的类型分为2种：<strong>ShuffleMapTask</strong>和<strong>ResultTask</strong>；</p>
<p>简单来说，DAG的最后一个阶段会为每个结果的partition生成一个ResultTask，即每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！而其余所有阶段都会生成ShuffleMapTask；之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中；也就是说上图中的stage1和stage2相当于mapreduce中的Mapper,而ResultTask所代表的stage3就相当于mapreduce中的reducer。</p>
<p>在之前动手操作了一个wordcount程序，因此可知，Hadoop中MapReduce操作中的Mapper和Reducer在spark中的基本等量算子是map和reduceByKey;不过区别在于：Hadoop中的MapReduce天生就是排序的；而reduceByKey只是根据Key进行reduce，但spark除了这两个算子还有其他的算子；因此从这个意义上来说，Spark比Hadoop的计算算子更为丰富。</p>

    </div>
    
    <div class="post-footer">
        <div>
            
            转载声明：
            商业转载请联系作者获得授权,非商业转载请注明出处 © <a href="https://github.com/wenxinzhang" target="_blank">福星</a>
            
            
        </div>
        <div>
            
        </div>
    </div>
</article>
<div class="article-nav prev-next-wrap clearfix">
    
    <a href="/2019-06-04-Spark学习之路 （四）Spark的广播变量和累加器.html" class="pre-post btn btn-default" title='Spark学习之路 （四）Spark的广播变量和累加器'>
        <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
        <span class="hidden-xs">
            Spark学习之路 （四）Spark的广播变量和累加器</span>
    </a>
    
    
    <a href="/2019-06-02-Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler.html" class="next-post btn btn-default" title='Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler'>
        <span class="hidden-lg">下一篇</span>
        <span class="hidden-xs">
            Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler</span><i class="fa fa-angle-right fa-fw"></i>
    </a>
    
</div>

<div id="comments">
    

<div id="vcomments" class="valine"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="/assets/valine.min.js"></script>
<script>
new Valine({
    av: AV,
    el: '#vcomments',
    appId: 'UckE9LEIQ8aoa3MH1Kio27rB-gzGzoHsz',
    appKey: '7HC9xCVYQdshKqFRDmULFm5G',
    placeholder: '说点什么吧',
    notify: false,
    verify: true,
    avatar: 'mm',
    meta: 'nick,mail'.split(','),
    pageSize: '10',
    path: window.location.pathname,
    lang: 'zh-CN'.toLowerCase()
})
</script>


</div>

                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">
            文章目录
        </h3>
        
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#一、RDD的概述"><span class="toc-text">一、RDD的概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-什么是RDD"><span class="toc-text">1.1　什么是RDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-RDD的属性"><span class="toc-text">1.2　RDD的属性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#二、RDD的创建方式"><span class="toc-text">二、RDD的创建方式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-通过读取文件生成的"><span class="toc-text">2.1　通过读取文件生成的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-通过并行化的方式创建RDD"><span class="toc-text">2.2　通过并行化的方式创建RDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-其他方式"><span class="toc-text">2.3　其他方式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#三、RDD编程API"><span class="toc-text">三、RDD编程API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Transformation"><span class="toc-text">3.1　Transformation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Action"><span class="toc-text">3.2　Action</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Spark-WordCount代码编写"><span class="toc-text">3.3　Spark WordCount代码编写</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#（1）使用scala进行编写"><span class="toc-text">（1）使用scala进行编写</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#（2）使用java-jdk7进行编写"><span class="toc-text">（2）使用java jdk7进行编写</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#（3）使用java-jdk8进行编写"><span class="toc-text">（3）使用java jdk8进行编写</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#四、RDD的宽依赖和窄依赖"><span class="toc-text">四、RDD的宽依赖和窄依赖</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-RDD依赖关系的本质内幕"><span class="toc-text">4.1　RDD依赖关系的本质内幕</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-依赖关系下的数据流视图"><span class="toc-text">4.2　依赖关系下的数据流视图</span></a></li></ol></li></ol>
        
    </div>
</aside>
                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>
<a id="back-to-top" class="icon-btn hide">
    <i class="fa fa-chevron-up"></i>
</a>
    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="busuanzi">
    
    访问量:
    <strong id="busuanzi_value_site_pv">
        <i class="fa fa-spinner fa-spin"></i>
    </strong>
    &nbsp; | &nbsp;
    访客数:
    <strong id="busuanzi_value_site_uv">
        <i class="fa fa-spinner fa-spin"></i>
    </strong>
    
</div>
            </div>
            <div class="col-sm-12">
                <span>Copyright &copy;
                    2018
                </span> |
                <span>
                    Powered by <a href="//hexo.io" class="copyright-links" target="_blank" rel="nofollow">Hexo</a>
                </span> |
                <span>
                    Theme by <a href="//github.com/shenliyang/hexo-theme-snippet.git" class="copyright-links" target="_blank" rel="nofollow">Snippet</a>
                </span>
            </div>
        </div>
    </div>
</div>



<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<script src="/js/app.js?rev=@@hash"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":200,"height":350},"mobile":{"show":true}});</script></body>
</html>