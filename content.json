{"meta":{"title":"福星","subtitle":null,"description":null,"author":"福 星","url":"http://zhangfuxin.cn"},"pages":[],"posts":[{"title":"Scala打印九九乘法表的5种实现","slug":"2019-09-25-Scala打印九九乘法表的5种实现","date":"2019-09-25T01:13:14.000Z","updated":"2019-09-25T15:34:50.609Z","comments":true,"path":"2019-09-25-Scala打印九九乘法表的5种实现.html","link":"","permalink":"http://zhangfuxin.cn/2019-09-25-Scala打印九九乘法表的5种实现.html","excerpt":"** Scala打印九九乘法表的5种实现：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala打印九九乘法表的5种实现","text":"** Scala打印九九乘法表的5种实现：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala打印九九乘法表的5种实现 &lt;The rest of contents | 余下全文&gt; 使用scala打印九九乘法表，可以有多种实现方法，实现的过程充分的体现的scala语言的优势和巨大的简洁性和高效性， 下面我用了5种方法实现九九乘法表。 使用类似于java，c++等指令风格的的编程实现，源码如下： 12345678910111213141516171819202122//这里打印倒向九九乘法口诀表/*指令风格的编程实现九九乘法表*/def printMultiTable() &#123; var i = 1 //这里只有i在作用范围内 while (i &lt;= 9) &#123; var j = i //这里只有i和j在作用范围内 while (j &lt;= 9) &#123; val prod = (i * j).toString() //这里只有i和j和prod在作用范围内 var k = prod.length() //这里只有i和j和prod和k在作用范围内 while (k &lt; 4) &#123; print(\" \") k += 1 &#125; print(i + \"*\" + j + \"=\" + prod) j += 1 &#125; // i和j让在作用范围内，但是k已经不在作用范围内。 println() i += 1 &#125; //i仍在范围内，j，prod，和k脱离了范围&#125; 执行的结果如下： 11=1 12=2 13=3 14=4 15=5 16=6 17=7 18=8 19=9 22=4 23=6 24=8 25=10 26=12 27=14 28=16 29=18 33=9 34=12 35=15 36=18 37=21 38=24 39=27 44=16 45=20 46=24 47=28 48=32 49=36 55=25 56=30 57=35 58=40 59=45 66=36 67=42 68=48 69=54 77=49 78=56 79=63 88=64 89=72 9*9=81 发现是倒向的乘法口诀， 下面我们修改代码打印一个正向的九九乘法表，关键在while（j &lt;=i） 这个条件。 1234567891011121314151617181920212223/** * 打印正向的九九乘法表 */def printMultiTable2() &#123; var i = 1 //这里只有i在作用范围内 while (i &lt;= 9) &#123; var j = 1 //这里只有i和j在作用范围内 while (j &lt;= i) &#123; val prod = (i * j).toString() //这里只有i和j和prod在作用范围内 var k = prod.length() //这里只有i和j和prod和k在作用范围内 while (k &lt; 4) &#123; print(\" \") k += 1 &#125; print(j + \"*\" + i + \"=\" + prod) j += 1 &#125; // i和j让在作用范围内，但是k已经不在作用范围内。 println() i += 1 &#125; //i仍在范围内，j，prod，和k脱离了范围&#125; 执行结果如下： 11=1 12=2 22=4 13=3 23=6 33=9 14=4 24=8 34=12 44=16 15=5 25=10 35=15 45=20 55=25 16=6 26=12 36=18 46=24 56=30 66=36 17=7 27=14 37=21 47=28 57=35 67=42 77=49 18=8 28=16 38=24 48=32 58=40 68=48 78=56 88=64 19=9 29=18 39=27 49=36 59=45 69=54 79=63 89=72 9*9=81 scala的语法简洁性，和函数式的风格，我们可以使用函数风格实现该功能发现代码量会减少很多，逻辑也更加清晰： 源码如下： 12345678910111213//打印：打印乘法口诀发def makeRowSeq(row: Int) = for (col &lt;- 1 to row) yield &#123; val prod = (row * col).toString() val padding = \" \" * (4 - prod.length()) col + \"*\" + row + \"=\" + prod + padding &#125;def makeRow(row: Int) = makeRowSeq(row).mkString/*函数风格的编程实现九九乘法表*/def multiTable() = &#123; val tableSeq = for (row &lt;- 1 to 9) yield makeRow(row) println(tableSeq.mkString(\"\\n\"))&#125; 执行结果如下： 11=112=2 22=413=3 23=6 33=914=4 24=8 34=12 44=1615=5 25=10 35=15 45=20 55=2516=6 26=12 36=18 46=24 56=30 66=3617=7 27=14 37=21 47=28 57=35 67=42 77=4918=8 28=16 38=24 48=32 58=40 68=48 78=56 88=6419=9 29=18 39=27 49=36 59=45 69=54 79=63 89=72 9*9=81 使用scala的for循环嵌套的方式实现该功能，代码可以更加简洁，只需要5,6行代码即可实现， 充分体现了scala的语言的强大性。 12345678def multiTable2() = &#123; for(row &lt;- 1 to 9 ; col &lt;- 1 to row)&#123; val prod = (row * col).toString() val padding = \" \" * (4 - prod.length()) print(col + \"*\" + row + \"=\" + prod + padding) if(row == col) println() &#125;&#125; 执行结果如下： 11=112=2 22=413=3 23=6 33=914=4 24=8 34=12 44=1615=5 25=10 35=15 45=20 55=2516=6 26=12 36=18 46=24 56=30 66=3617=7 27=14 37=21 47=28 57=35 67=42 77=4918=8 28=16 38=24 48=32 58=40 68=48 78=56 88=6419=9 29=18 39=27 49=36 59=45 69=54 79=63 89=72 9*9=81 可以使用for嵌套循环和scala的s（）方法，使实现更加简单，scala果然博大精深， 源码如下： 12345678910def multiTable3 = &#123; ( for ( i &lt;- 1 to 9; j &lt;- 1 to i; ss = s\"$j*$i=$&#123;i * j&#125;\\t\" ) yield &#123; if (j == i) s\"$ss\\n\" else ss &#125;).foreach(print);&#125; 执行结果如下： 11=112=2 22=413=3 23=6 33=914=4 24=8 34=12 44=1615=5 25=10 35=15 45=20 55=2516=6 26=12 36=18 46=24 56=30 66=3617=7 27=14 37=21 47=28 57=35 67=42 77=4918=8 28=16 38=24 48=32 58=40 68=48 78=56 88=6419=9 29=18 39=27 49=36 59=45 69=54 79=63 89=72 9*9=81 作者： 丹江湖畔养蜂子的赵大爹出处：http://www.cnblogs.com/honeybee/","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://zhangfuxin.cn/tags/Scala/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Parquet文件存储格式","slug":"2019-09-18-Parquet文件存储格式","date":"2019-09-18T02:30:04.000Z","updated":"2019-09-18T16:01:41.367Z","comments":true,"path":"2019-09-18-Parquet文件存储格式.html","link":"","permalink":"http://zhangfuxin.cn/2019-09-18-Parquet文件存储格式.html","excerpt":"** Parquet文件存储格式：** &lt;Excerpt in index | 首页摘要&gt; ​ Parquet文件存储格式","text":"** Parquet文件存储格式：** &lt;Excerpt in index | 首页摘要&gt; ​ Parquet文件存储格式 &lt;The rest of contents | 余下全文&gt; 一、Parquet的组成Parquet仅仅是一种存储格式，它是语言、平台无关的，并且不需要和任何一种数据处理框架绑定，目前能够和Parquet适配的组件包括下面这些，可以看出基本上通常使用的查询引擎和计算框架都已适配，并且可以很方便的将其它序列化工具生成的数据转换成Parquet格式。 查询引擎: Hive, Impala, Pig, Presto, Drill, Tajo, HAWQ, IBM Big SQL 计算框架: MapReduce, Spark, Cascading, Crunch, Scalding, Kite 数据模型: Avro, Thrift, Protocol Buffers, POJOs 项目组成Parquet项目由以下几个子项目组成: parquet-format项目由java实现，它定义了所有Parquet元数据对象，Parquet的元数据是使用Apache Thrift进行序列化并存储在Parquet文件的尾部。 parquet-format项目由java实现，它包括多个模块，包括实现了读写Parquet文件的功能，并且提供一些和其它组件适配的工具，例如Hadoop Input/Output Formats、Hive Serde(目前Hive已经自带Parquet了)、Pig loaders等。 parquet-compatibility项目，包含不同编程语言之间(JAVA和C/C++)读写文件的测试代码。 parquet-cpp项目，它是用于用于读写Parquet文件的C++库。 下图展示了Parquet各个组件的层次以及从上到下交互的方式。 数据存储层定义了Parquet的文件格式，其中元数据在parquet-format中定义，包括Parquet原始类型定义、Page类型、编码类型、压缩类型等等。 对象转换层完成其他对象模型与Parquet内部数据模型的映射和转换，Parquet的编码方式使用的是striping and assembly算法。 对象模型层定义了如何读取Parquet文件的内容，这一层转换包括Avro、Thrift、PB等序列化格式、Hive serde等的适配。并且为了帮助大家理解和使用，Parquet提供了org.apache.parquet.example包实现了java对象和Parquet文件的转换。 数据模型Parquet支持嵌套的数据模型，类似于Protocol Buffers，每一个数据模型的schema包含多个字段，每一个字段又可以包含多个字段，每一个字段有三个属性：重复数、数据类型和字段名，重复数可以是以下三种：required(出现1次)，repeated(出现0次或多次)，optional(出现0次或1次)。每一个字段的数据类型可以分成两种：group(复杂类型)和primitive(基本类型)。例如Dremel中提供的Document的schema示例，它的定义如下： 1234567891011121314message Document &#123; required int64 DocId; optional group Links &#123; repeated int64 Backward; repeated int64 Forward; &#125; repeated group Name &#123; repeated group Language &#123; required string Code; optional string Country; &#125; optional string Url; &#125;&#125; 可以把这个Schema转换成树状结构，根节点可以理解为repeated类型，如下图: 可以看出在Schema中所有的基本类型字段都是叶子节点，在这个Schema中一共存在6个叶子节点，如果把这样的Schema转换成扁平式的关系模型，就可以理解为该表包含六个列。Parquet中没有Map、Array这样的复杂数据结构，但是可以通过repeated和group组合来实现这样的需求。在这个包含6个字段的表中有以下几个字段和每一条记录中它们可能出现的次数： 123456DocId int64 只能出现一次 Links.Backward int64 可能出现任意多次，但是如果出现0次则需要使用NULL标识 Links.Forward int64 同上 Name.Language.Code string 同上 Name.Language.Country string 同上 Name.Url string 同上 由于在一个表中可能存在出现任意多次的列，对于这些列需要标示出现多次或者等于NULL的情况，它是由Striping/Assembly算法实现的。 Striping/Assembly算法上文介绍了Parquet的数据模型，在Document中存在多个非required列，由于Parquet一条记录的数据分散的存储在不同的列中，如何组合不同的列值组成一条记录是由Striping/Assembly算法决定的，在该算法中列的每一个值都包含三部分：value、repetition level和definition level。 Repetition Levels为了支持repeated类型的节点，在写入的时候该值等于它和前面的值在哪一层节点是不共享的。在读取的时候根据该值可以推导出哪一层上需要创建一个新的节点，例如对于这样的一个schema和两条记录。 1234567message nested &#123; repeated group leve1 &#123; repeated string leve2; &#125;&#125;r1:[[a,b,c,] , [d,e,f,g]]r2:[[h] , [i,j]] 计算repetition level值的过程如下： value=a是一条记录的开始，和前面的值(已经没有值了)在根节点(第0层)上是不共享的，所以repeated level=0. value=b它和前面的值共享了level1这个节点，但是level2这个节点上是不共享的，所以repeated level=2. 同理value=c, repeated level=2. value=d和前面的值共享了根节点(属于相同记录)，但是在level1这个节点上是不共享的，所以repeated level=1. value=h和前面的值不属于同一条记录，也就是不共享任何节点，所以repeated level=0. 根据以上的分析每一个value需要记录的repeated level值如下： 在读取的时候，顺序的读取每一个值，然后根据它的repeated level创建对象，当读取value=a时repeated level=0，表示需要创建一个新的根节点(新记录)，value=b时repeated level=2，表示需要创建一个新的level2节点，value=d时repeated level=1，表示需要创建一个新的level1节点，当所有列读取完成之后可以创建一条新的记录。本例中当读取文件构建每条记录的结果如下： 可以看出repeated level=0表示一条记录的开始，并且repeated level的值只是针对路径上的repeated类型的节点，因此在计算该值的时候可以忽略非repeated类型的节点，在写入的时候将其理解为该节点和路径上的哪一个repeated节点是不共享的，读取的时候将其理解为需要在哪一层创建一个新的repeated节点，这样的话每一列最大的repeated level值就等于路径上的repeated节点的个数（不包括根节点）。减小repeated level的好处能够使得在存储使用更加紧凑的编码方式，节省存储空间。 Definition Levels有了repeated level我们就可以构造出一个记录了，为什么还需要definition levels呢？由于repeated和optional类型的存在，可能一条记录中某一列是没有值的，假设我们不记录这样的值就会导致本该属于下一条记录的值被当做当前记录的一部分，从而造成数据的错误，因此对于这种情况需要一个占位符标示这种情况。 definition level的值仅仅对于空值是有效的，表示在该值的路径上第几层开始是未定义的，对于非空的值它是没有意义的，因为非空值在叶子节点是定义的，所有的父节点也肯定是定义的，因此它总是等于该列最大的definition levels。例如下面的schema。 1234567message ExampleDefinitionLevel &#123; optional group a &#123; optional group b &#123; optional string c; &#125; &#125;&#125; 它包含一个列a.b.c，这个列的的每一个节点都是optional类型的，当c被定义时a和b肯定都是已定义的，当c未定义时我们就需要标示出在从哪一层开始时未定义的，如下面的值： 由于definition level只需要考虑未定义的值，而对于repeated类型的节点，只要父节点是已定义的，该节点就必须定义（例如Document中的DocId，每一条记录都该列都必须有值，同样对于Language节点，只要它定义了Code必须有值），所以计算definition level的值时可以忽略路径上的required节点，这样可以减小definition level的最大值，优化存储。 一个完整的例子本节我们使用Dremel论文中给的Document示例和给定的两个值r1和r2展示计算repeated level和definition level的过程，这里把未定义的值记录为NULL，使用R表示repeated level，D表示definition level。 首先看DocuId这一列，对于r1，DocId=10，由于它是记录的开始并且是已定义的，所以R=0，D=0，同样r2中的DocId=20，R=0，D=0。 对于Links.Forward这一列，在r1中，它是未定义的但是Links是已定义的，并且是该记录中的第一个值，所以R=0，D=1，在r1中该列有两个值，value1=10，R=0(记录中该列的第一个值)，D=2(该列的最大definition level)。 对于Name.Url这一列，r1中它有三个值，分别为url1=’http://A‘，它是r1中该列的第一个值并且是定义的，所以R=0，D=2；value2=’http://B‘，和上一个值value1在Name这一层是不相同的，所以R=1，D=2；value3=NULL，和上一个值value2在Name这一层是不相同的，所以R=1，但它是未定义的，而Name这一层是定义的，所以D=1。r2中该列只有一个值value3=’http://C‘，R=0，D=2. 最后看一下Name.Language.Code这一列，r1中有4个值，value1=’en-us’，它是r1中的第一个值并且是已定义的，所以R=0，D=2(由于Code是required类型，这一列repeated level的最大值等于2)；value2=’en’，它和value1在Language这个节点是不共享的，所以R=2，D=2；value3=NULL，它是未定义的，但是它和前一个值在Name这个节点是不共享的，在Name这个节点是已定义的，所以R=1，D=1；value4=’en-gb’，它和前一个值在Name这一层不共享，所以R=1，D=2。在r2中该列有一个值，它是未定义的，但是Name这一层是已定义的，所以R=0，D=1. Parquet文件格式Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。在HDFS文件系统和Parquet文件中存在如下几个概念。 HDFS块(Block)：它是HDFS上的最小的副本单位，HDFS会把一个Block存储在本地的一个文件并且维护分散在不同的机器上的多个副本，通常情况下一个Block的大小为256M、512M等。 HDFS文件(File)：一个HDFS的文件，包括数据和元数据，数据分散存储在多个Block中。 行组(Row Group)：按照行将数据物理上划分为多个单元，每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，Parquet读写的时候会将整个行组缓存在内存中，所以如果每一个行组的大小是由内存大的小决定的，例如记录占用空间比较小的Schema可以在每一个行组中存储更多的行。 列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。 页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。 文件格式通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示 上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页，但是在后面的版本中增加。 在执行MR任务的时候可能存在多个Mapper任务的输入是同一个Parquet文件的情况，每一个Mapper通过InputSplit标示处理的文件范围，如果多个InputSplit跨越了一个Row Group，Parquet能够保证一个Row Group只会被一个Mapper任务处理。 映射下推(Project PushDown)说到列式存储的优势，映射下推是最突出的，它意味着在获取表中原始数据时只需要扫描查询中需要的列，由于每一列的所有值都是连续存储的，所以分区取出每一列的所有值就可以实现TableScan算子，而避免扫描整个表文件内容。 在Parquet中原生就支持映射下推，执行查询的时候可以通过Configuration传递需要读取的列的信息，这些列必须是Schema的子集，映射每次会扫描一个Row Group的数据，然后一次性得将该Row Group里所有需要的列的Cloumn Chunk都读取到内存中，每次读取一个Row Group的数据能够大大降低随机读的次数，除此之外，Parquet在读取的时候会考虑列是否连续，如果某些需要的列是存储位置是连续的，那么一次读操作就可以把多个列的数据读取到内存。 谓词下推(Predicate PushDown)在数据库之类的查询系统中最常用的优化手段就是谓词下推了，通过将一些过滤条件尽可能的在最底层执行可以减少每一层交互的数据量，从而提升性能，例如”select count(1) from A Join B on A.id = B.id where A.a &gt; 10 and B.b &lt; 100”SQL查询中，在处理Join操作之前需要首先对A和B执行TableScan操作，然后再进行Join，再执行过滤，最后计算聚合函数返回，但是如果把过滤条件A.a &gt; 10和B.b &lt; 100分别移到A表的TableScan和B表的TableScan的时候执行，可以大大降低Join操作的输入数据。 无论是行式存储还是列式存储，都可以在将过滤条件在读取一条记录之后执行以判断该记录是否需要返回给调用者，在Parquet做了更进一步的优化，优化的方法时对每一个Row Group的每一个Column Chunk在存储的时候都计算对应的统计信息，包括该Column Chunk的最大值、最小值和空值个数。通过这些统计值和该列的过滤条件可以判断该Row Group是否需要扫描。另外Parquet未来还会增加诸如Bloom Filter和Index等优化数据，更加有效的完成谓词下推。 在使用Parquet的时候可以通过如下两种策略提升查询性能：1、类似于关系数据库的主键，对需要频繁过滤的列设置为有序的，这样在导入数据的时候会根据该列的顺序存储数据，这样可以最大化的利用最大值、最小值实现谓词下推。2、减小行组大小和页大小，这样增加跳过整个行组的可能性，但是此时需要权衡由于压缩和编码效率下降带来的I/O负载。 性能相比传统的行式存储，Hadoop生态圈近年来也涌现出诸如RC、ORC、Parquet的列式存储格式，它们的性能优势主要体现在两个方面：1、更高的压缩比，由于相同类型的数据更容易针对不同类型的列使用高效的编码和压缩方式。2、更小的I/O操作，由于映射下推和谓词下推的使用，可以减少一大部分不必要的数据扫描，尤其是表结构比较庞大的时候更加明显，由此也能够带来更好的查询性能 上图是展示了使用不同格式存储TPC-H和TPC-DS数据集中两个表数据的文件大小对比，可以看出Parquet较之于其他的二进制文件存储格式能够更有效的利用存储空间，而新版本的Parquet(2.0版本)使用了更加高效的页存储方式，进一步的提升存储空间 上图展示了Twitter在Impala中使用不同格式文件执行TPC-DS基准测试的结果，测试结果可以看出Parquet较之于其他的行式存储格式有较明显的性能提升。 上图展示了criteo公司在Hive中使用ORC和Parquet两种列式存储格式执行TPC-DS基准测试的结果，测试结果可以看出在数据存储方面，两种存储格式在都是用snappy压缩的情况下量中存储格式占用的空间相差并不大，查询的结果显示Parquet格式稍好于ORC格式，两者在功能上也都有优缺点，Parquet原生支持嵌套式数据结构，而ORC对此支持的较差，这种复杂的Schema查询也相对较差；而Parquet不支持数据的修改和ACID，但是ORC对此提供支持，但是在OLAP环境下很少会对单条数据修改，更多的则是批量导入。 项目发展自从2012年由Twitter和Cloudera共同研发Parquet开始，该项目一直处于高速发展之中，并且在项目之初就将其贡献给开源社区，2013年，Criteo公司加入开发并且向Hive社区提交了向hive集成Parquet的patch(HIVE-5783)，在Hive 0.13版本之后正式加入了Parquet的支持；之后越来越多的查询引擎对此进行支持，也进一步带动了Parquet的发展。 目前Parquet正处于向2.0版本迈进的阶段，在新的版本中实现了新的Page存储格式，针对不同的类型优化编码算法，另外丰富了支持的原始类型，增加了Decimal、Timestamp等类型的支持，增加更加丰富的统计信息，例如Bloon Filter，能够尽可能得将谓词下推在元数据层完成。 总结本文介绍了一种支持嵌套数据模型对的列式存储系统Parquet，作为大数据系统中OLAP查询的优化方案，它已经被多种查询引擎原生支持，并且部分高性能引擎将其作为默认的文件存储格式。通过数据编码和压缩，以及映射下推和谓词下推功能，Parquet的性能也较之其它文件格式有所提升，可以预见，随着数据模型的丰富和Ad hoc查询的需求，Parquet将会被更广泛的使用。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"数据集网站汇总","slug":"2019-09-17-数据集网站汇总","date":"2019-09-17T02:30:04.000Z","updated":"2019-09-17T01:29:13.803Z","comments":true,"path":"2019-09-17-数据集网站汇总.html","link":"","permalink":"http://zhangfuxin.cn/2019-09-17-数据集网站汇总.html","excerpt":"** 数据集网站汇总：** &lt;Excerpt in index | 首页摘要&gt; ​ 如果用一个句子总结学习数据科学的本质，那就是： 学习数据科学的最佳方法就是应用数据科学。 如果你是一个初学者，你每完成一个新项目后自身能力都会有极大的提高，如果你是一个有经验的数据科学专家，你已经知道这里所蕴含的价值。","text":"** 数据集网站汇总：** &lt;Excerpt in index | 首页摘要&gt; ​ 如果用一个句子总结学习数据科学的本质，那就是： 学习数据科学的最佳方法就是应用数据科学。 如果你是一个初学者，你每完成一个新项目后自身能力都会有极大的提高，如果你是一个有经验的数据科学专家，你已经知道这里所蕴含的价值。 &lt;The rest of contents | 余下全文&gt; 一.如何使用这些资源? 如何使用这些数据源是没有限制的，应用和使用只受到您的创造力和实际应用。使用它们最简单的方法是进行数据项目并在网站上发布它们。这不仅能提高你的数据和可视化技能，还能改善你的结构化思维。 另一方面，如果你正在考虑/处理基于数据的产品，这些数据集可以通过提供额外的/新的输入数据来增加您的产品的功能。所以，继续在这些项目上工作吧，与更大的世界分享它们，以展示你的数据能力!我们已经在不同的部分中划分了这些数据源，以帮助你根据应用程序对数据源进行分类。 我们从简单、通用和易于处理数据集开始，然后转向大型/行业相关数据集。然后，我们为特定的目的——文本挖掘、图像分类、推荐引擎等提供数据集的链接。这将为您提供一个完整的数据资源列表。如果你能想到这些数据集的任何应用，或者知道我们漏掉了什么流行的资源，请在下面的评论中与我们分享。(部分可能需要翻墙) 二.由简单和通用的数据集开始 1.data.gov ( https://www.data.gov/ ) 这是美国政府公开数据的所在地，该站点包含了超过19万的数据点。这些数据集不同于气候、教育、能源、金融和更多领域的数据。 2.data.gov.in ( https://data.gov.in/ ) 这是印度政府公开数据的所在地，通过各种行业、气候、医疗保健等来寻找数据，你可以在这里找到一些灵感。根据你居住的国家的不同，你也可以从其他一些网站上浏览类似的网站。 3.World Bank( http://data.worldbank.org/ ) 世界银行的开放数据。该平台提供 Open Data Catalog，世界发展指数，教育指数等几个工具。 4.RBI ( https://rbi.org.in/Scripts/Statistics.aspx ) 印度储备银行提供的数据。这包括了货币市场操作、收支平衡、银行使用和一些产品的几个指标。 5.Five Thirty Eight Datasets ( https://github.com/fivethirtyeight/data ) Five Thirty Eight，亦称作 538，专注与民意调查分析，政治，经济与体育的博客。该数据集为 Five Thirty Eight Datasets 使用的数据集。每个数据集包括数据，解释数据的字典和Five Thirty Eight 文章的链接。如果你想学习如何创建数据故事，没有比这个更好。 三.大型数据集 1.Amazon Web Services(AWS)datasets ( https://aws.amazon.com/cn/datasets/ )Amazon提供了一些大数据集，可以在他们的平台上使用，也可以在本地计算机上使用。您还可以通过EMR使用EC2和Hadoop来分析云中的数据。在亚马逊上流行的数据集包括完整的安然电子邮件数据集，Google Books n-gram，NASA NEX 数据集，百万歌曲数据集等。 2.Google datasets ( https://cloud.google.com/bigquery/public-data/ ) Google 提供了一些数据集作为其 Big Query 工具的一部分。包括 GitHub 公共资料库的数据，Hacker News 的所有故事和评论。 3.Youtube labeled Video Dataset ( https://research.google.com/youtube8m/ ) 几个月前，谷歌研究小组发布了YouTube上的“数据集”，它由800万个YouTube视频id和4800个视觉实体的相关标签组成。它来自数十亿帧的预先计算的，最先进的视觉特征。 四.预测建模与机器学习数据集 1.UCI Machine Learning Repository ( https://archive.ics.uci.edu/ml/datasets.html )UCI机器学习库显然是最著名的数据存储库。如果您正在寻找与机器学习存储库相关的数据集，通常是首选的地方。这些数据集包括了各种各样的数据集，从像Iris和泰坦尼克这样的流行数据集到最近的贡献，比如空气质量和GPS轨迹。存储库包含超过350个与域名类似的数据集(分类/回归)。您可以使用这些过滤器来确定您需要的数据集。 2.Kaggle ( https://www.kaggle.com/datasets ) Kaggle提出了一个平台，人们可以贡献数据集，其他社区成员可以投票并运行内核/脚本。他们总共有超过350个数据集——有超过200个特征数据集。虽然一些最初的数据集通常出现在其他地方，但我在平台上看到了一些有趣的数据集，而不是在其他地方出现。与新的数据集一起，界面的另一个好处是，您可以在相同的界面上看到来自社区成员的脚本和问题。 3.Analytics Vidhya (https://datahack.analyticsvidhya.com/contest/all/ ) 您可以从我们的实践问题和黑客马拉松问题中参与和下载数据集。问题数据集基于真实的行业问题，并且相对较小，因为它们意味着2 - 7天的黑客马拉松。 4.Quandl ( https://www.quandl.com/ ) Quandl 通过起网站、API 或一些工具的直接集成提供了不同来源的财务、经济和替代数据。他们的数据集分为开放和付费。所有开放数据集为免费，但高级数据集需要付费。通过搜索仍然可以在平台上找到优质数据集。例如，来自印度的证券交易所数据是免费的。 5.Past KDD Cups ( http://www.kdd.org/kdd-cup ) KDD Cup 是 ACM Special Interest Group 组织的年度数据挖掘和知识发现竞赛。 6.Driven Data ( https://www.drivendata.org/ ) Driven Data 发现运用数据科学带来积极社会影响的现实问题。然后，他们为数据科学家组织在线模拟竞赛，从而开发出最好的模型来解决这些问题。 五.图像分类数据集 1.The MNIST Database ( http://yann.lecun.com/exdb/mnist/ ) 最流行的图像识别数据集，使用手写数字。它包括6万个示例和1万个示例的测试集。这通常是第一个进行图像识别的数据集。 2.Chars74K (http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/ ) 这里是下一阶段的进化，如果你已经通过了手写的数字。该数据集包括自然图像中的字符识别。数据集包含74,000个图像，因此数据集的名称。 3.Frontal Face Images (http://vasc.ri.cmu.edu//idb/html/face/frontal_images/index.html ) 如果你已经完成了前两个项目，并且能够识别数字和字符，这是图像识别中的下一个挑战级别——正面人脸图像。这些图像是由CMU &amp; MIT收集的，排列在四个文件夹中。 4.ImageNet ( http://image-net.org/ ) 现在是时候构建一些通用的东西了。根据WordNet层次结构组织的图像数据库(目前仅为名词)。层次结构的每个节点都由数百个图像描述。目前，该集合平均每个节点有超过500个图像(而且还在增加)。 六.文本分类数据集 1.Spam – Non Spam (http://www.esp.uem.es/jmgomez/smsspamcorpus/) 区分短信是否为垃圾邮件是一个有趣的问题。你需要构建一个分类器将短信进行分类。 2.Twitter Sentiment Analysis (http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/) 该数据集包含 1578627 个分类推文，每行被标记为1的积极情绪，0位负面情绪。数据依次基于 Kaggle 比赛和 Nick Sanders 的分析。 3.Movie Review Data (http://www.cs.cornell.edu/People/pabo/movie-review-data/) 这个网站提供了一系列的电影评论文件，这些文件标注了他们的总体情绪极性(正面或负面)或主观评价(例如，“两个半明星”)和对其主观性地位(主观或客观)或极性的标签。 七.推荐引擎数据集 1.MovieLens ( https://grouplens.org/ ) MovieLens 是一个帮助人们查找电影的网站。它有成千上万的注册用户。他们进行自动内容推荐，推荐界面，基于标签的推荐页面等在线实验。这些数据集可供下载，可用于创建自己的推荐系统。 2.Jester (http://www.ieor.berkeley.edu/~goldberg/jester-data/) 在线笑话推荐系统。 八.各种来源的数据集网站 1.KDNuggets (http://www.kdnuggets.com/datasets/index.html) KDNuggets 的数据集页面一直是人们搜索数据集的参考。列表全面，但是某些来源不再提供数据集。因此，需要谨慎选择数据集和来源。 2.Awesome Public Datasets (https://github.com/caesar0301/awesome-public-datasets) 一个GitHub存储库，它包含一个由域分类的完整的数据集列表。数据集被整齐地分类在不同的领域，这是非常有用的。但是，对于存储库本身的数据集没有描述，这可能使它非常有用。 3.Reddit Datasets Subreddit (https://www.reddit.com/r/datasets/) 由于这是一个社区驱动的论坛，它可能会遇到一些麻烦(与之前的两个来源相比)。但是，您可以通过流行/投票来对数据集进行排序，以查看最流行的数据集。另外，它还有一些有趣的数据集和讨论。 九.结尾的话 我们希望这一资源清单对于那些想项目的人来说是非常有用的。这绝对是一个金矿，好好加以利用吧! 转自：https://mp.weixin.qq.com/s?__biz=MzI2MjM2MDEzNQ==&amp;mid=2247489072&amp;idx=1&amp;sn=2ac46ef358be4eef43f3de8670086746&amp;chksm=ea4d0b18dd3a820ef82122648806c8516970e8e7323efb5475aa0db1da1752d22ee8c38ec604&amp;mpshare=1&amp;scene=23&amp;srcid=042625ULmfK6xU66wcmkCf1G#rd","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Vim常用按键大全","slug":"2019-09-16-Vim常用按键大全","date":"2019-09-16T02:30:04.000Z","updated":"2019-09-18T16:07:02.309Z","comments":true,"path":"2019-09-16-Vim常用按键大全.html","link":"","permalink":"http://zhangfuxin.cn/2019-09-16-Vim常用按键大全.html","excerpt":"** Vim常用按键大全：** &lt;Excerpt in index | 首页摘要&gt; ​ Vim常用按键大全","text":"** Vim常用按键大全：** &lt;Excerpt in index | 首页摘要&gt; ​ Vim常用按键大全 &lt;The rest of contents | 余下全文&gt; Vim完全可以用键盘进行操作。本文将常用的按键归纳总结。 第一部分：一般模式可用的按钮，如光标移动、复制粘贴、查找替换等移动光标的方法 h, j, k, l 光标向左，下，上，右移动 Ctrl + f / b 屏幕向下/上移动 Ctrl + d / u 屏幕向下/上移动半页 0 移动到一行的最前面 $ 移动到一行的最后面字符 H / M / L 移动到屏幕最上方/中央/最下方那一行的第一个字符 G 移动到文件的最后一行 nG / ngg 移动到文件的第n行 gg 移动到文件的第一行 n[Enter] 向下移动n行 查找与替换 /word 向下查找word字符串 ?word 向上查找word字符串 n 代表重复前一个查找动作 N 代表反向重复前一个查找动作 : s/old/new 将第一个old替换为new : s/old/new/g 将一行中所有的old替换为new :n1, n2s/word1/word2/g 将行n1与n2之间的word1替换为word2 :%s/old/new/g 将文件所有的old替换为new :%s/old/new/gc 替换前要求确认 删除、复制与粘贴 x/X 向后/前删除一个字符 nx 连续删除n个字符 dd 删除整行 ndd 删除n行 d1G 删除光标所在到第一行数据 dG 删除光标所在到最后一行数据 d$ 删除光标所在到该行最后一个字符 d0 删除光标所在到该行最前面一个字符 yy 复制光标所在的一行 nyy 向下复制n行 y1G 复制光标所在到第一行数据 yG 复制光标所在到最后一行数据 y$ 复制光标所在到该行最后一个字符 y0 复制光标所在到该行最前面一个字符 p/P 粘贴数据在光标下/上一行 J 将光标所在行与下一行数据结合成同一行 u 回撤前一操作 Ctrl + r 重做前一操作 . 重复前一个操作 第二部分：一般模式切换到编辑模式进入插入或替换的编辑模式 i, I 进入插入模式： i从当前光标所在处插入，I在目前所在行的第一个非空格符处插入 a, A 进入插入模式： a从当前光标所在的下一个字符插入，A从光标所在行的最后一个字符后插入 o, O 进入插入模式： o从当前光标所在行的下一行插入新的一行；O正好相反，从上一行插入新行 r, R 进入替换模式： r只会替换光标所在的那一个字符一次；R会一直替换光标所在文字，直到Esc 块选择 v 字符选择，将光标经过的地方反白选择 V 行选择，将光标经过的行反白选择 Ctrl + v 块选择，可以用长方形选择数据 y 将反白的地方复制 d 删除反白的地方 多窗口 ：sp filename 打开新窗口，如果有加filename,新窗口打开新文件，否则打开相同文件 Ctrl + w + s/v 水平/垂直分割打开新窗口 Ctrl + w + h/j/k/l 光标移动到左/下/上/右窗口 Ctrl + w + q 退出窗口 vim常用命令示意图","categories":[{"name":"Linux","slug":"Linux","permalink":"http://zhangfuxin.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://zhangfuxin.cn/tags/Linux/"}],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://zhangfuxin.cn/categories/Linux/"}]},{"title":"CM+CDH离线安装","slug":"CDH-hadoop","date":"2019-09-05T17:30:04.000Z","updated":"2019-09-06T00:12:24.698Z","comments":true,"path":"CDH-hadoop.html","link":"","permalink":"http://zhangfuxin.cn/CDH-hadoop.html","excerpt":"** CM+CDH离线安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Cloudera Manager可以轻松管理任何生产规模的Hadoop部署。通过直观的用户界面快速部署，配置和监控群集 - 完成滚动升级，备份和灾难恢复以及可定制警报。","text":"** CM+CDH离线安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Cloudera Manager可以轻松管理任何生产规模的Hadoop部署。通过直观的用户界面快速部署，配置和监控群集 - 完成滚动升级，备份和灾难恢复以及可定制警报。 &lt;The rest of contents | 余下全文&gt; 1.1 Cloudera 简介1.1.1Cloudera 简介官网：https://www.cloudera.com/ 文档：https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_intro.html ​ CDH是Apache Hadoop和相关项目中最完整，经过测试和最流行的发行版。CDH提供了Hadoop的核心元素 - 可扩展存储和分布式计算 - 以及基于Web的用户界面和重要的企业功能。CDH是Apache许可的开源软件，是唯一提供统一批处理，交互式SQL和交互式搜索以及基于角色的访问控制的Hadoop解决方案。 CDH提供： 灵活性 - 存储任何类型的数据并使用各种不同的计算框架对其进行操作，包括批处理，交互式SQL，自由文本搜索，机器学习和统计计算。 集成 - 在完整的Hadoop平台上快速启动和运行，该平台可与各种硬件和软件解决方案配合使用。 安全 - 处理和控制敏感数据。 可扩展性 - 支持广泛的应用程序，并扩展和扩展它们以满足您的要求。 高可用性 - 充满信心地执行任务关键型业务任务。 兼容性 - 利用您现有的IT基础架构和投资。 1.1.2Hadoop起源​ 2004年，Cutting和同为程序员出身的Mike Cafarella决定开发一款可以代替当时的主流搜索产品的开源搜索引擎，这个项目被命名为Nutch。Nutch基于Lucence实现的搜索引擎，能够从互联网上抓取网页数据。抓取来的海量数据的存储问题。但是，这些海量数据都是非结构化数据，不能存在关系型数据库里。如果连数据的存储和管理都解决不了化，就谈不上后续为用户提供搜索服务，包括通过算法去优化检索速度。那时的Cutting正处在Freelancer的生涯 中，所以他希望自己的项目能通过一种低开销的方式来构建网页中的大量算法。幸运的是，Google这时正好发布了一项研究报告，报告中介绍了两款Google为支持自家的搜索引擎而开发的软件平台。这两个平台一个是GFS（Google File System），用于存储不同设备所产生的海量数据；另一个是MapReduce，它运行在GFS之上，负责分布式大规模数据。 ​ 2006年cutting根据《Google File System》设计了Nutch的HDFS,hadoop distributed file system。 Hadoop最开始是nutch的子项目，目的是解决nutch的海量数据存储问题。在nutch 0.8版本之后，Hadoop独立处理，成为一个独立的项目。后来，又根据《Google MapReduce》设计了基于HDFS的MapRedce计算框架。 2006年之后，cutting带着Hadoop去了雅虎，当时有100多人的团队共同帮cutting完善hadoop。后来yahoo把Hadoop贡献了Apache。所以，现在Hadoop是Apache的顶级项目。 2.1Hadoop搭建Hadoop的三种运行模式 ： 独立（本地）运行模式：无需任何守护进程，所有的程序都运行在同一个JVM上执行。在独立模式下调试MR程序非常高效方便。所以一般该模式主要是在学习或者开发阶段调试使用 。 伪分布式模式： Hadoop守护进程运行在本地机器上，模拟一个小规模的集群，换句话说，可以配置一台机器的Hadoop集群,伪分布式是完全分布式的一个特例。 完全分布式模式：Hadoop守护进程运行在一个集群上。 3.1 单机伪分布模式​ 只支持MapReduce，不支持HDFS。这种模式一般用于调试MapReduce任务用的。 3.1.1 准备Linux环境，最低的工作内存1G内容详见：Vmware安装Centos6.9文档 3.1.2 关闭防火墙临时关闭防火墙：service iptables stop 1service iptables stop 永久关闭防火墙：chkconfig iptables off 1chkconfig iptables off 注意：永久修改防火墙需要重启，永久和临时同时执行，可以不用重启。 3.1.3 配置主机名查询主机名称：hostname 1hostname 临时修改主机名：hostname 1hostname &lt;name&gt; 永久修改主机名：vim /etc/sysconfig/network 1vim /etc/sysconfig/network 注意： 1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。 2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。 注意： 1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。 2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。 3.1.4 配置hosts文件执行: vim /etc/hosts 1vim /etc/hosts 注意： 不要删除前两行内容。 IP在前，主机名在后。 3.1.5 配置免密码登录3.1.5.1 免密登陆原理 A机器生成公钥和私钥 机器A发送自己的公钥到机器B，这个时候机器B有了机器A的公钥 机器B发送一个随机的字符串向机器A 机器A利用自己的私钥把字符串加密 机器A把加密后的字符串再次发送给机器B 机器B利用公钥解密字符串，如果和原来的一样，则OK。 3.1.5.1 免密登陆实现 生成自己的公钥和私钥 ssh-keygen 1ssh-keygen 把生成的公钥copy到远程机器上 ssh-copy-id root@hadoop01 1ssh-copy-id root@hadoop01 注意：如果是单机的伪分布式环境，自己节点也需要配置免密登录。 3.1.6 安装和配置jdk 执行： 1vim /etc/profile 在尾行添加 12345#Set Java ENVJAVA_HOME=/home/jdk1.8.0_65PATH=$JAVA_HOME/bin:$PATHCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JAVA_HOME PATH CLASS_PATH 保存退出 :wq 命令行执行： 1source /etc/profile java -version 查看JDK版本信息。 1java -version 3.1.7 上传和安装hadoop下载地址：http://hadoop.apache.org/releases.html 注意： source表示源码 binary表示二级制包（安装包） 3.1.7.1 解压Hadoop文件包执行：tar -zxvf hadoop-2.7.1_64bit.tar.gz 1tar -zxvf hadoop-2.7.1_64bit.tar.gz 3.1.7.2 Hadoop目录说明bin目录：命令脚本 etc/hadoop:存放hadoop的配置文件 lib目录：hadoop运行的依赖jar包 sbin目录：启动和关闭hadoop等命令都在这里 libexec目录：存放的也是hadoop命令，但一般不常用 注意：最常用的就是bin和etc目录。 3.1.8 配置hadoop配置文件Hadoop目录下/home/hadoop-2.7.1/etc/hadoop/目录下6个文件 3.1.8.1 hadoop-env.sh执行：vim hadoop-env.sh 1vim hadoop-env.sh 修改：修改java_home路径和hadoop_conf_dir 路径 25行 33行 1234#25行export JAVA_HOME=/home/jdk1.8.0_65#33行export HADOOP_CONF_DIR=/home/hadoop-2.7.1/etc/hadoop 然后执行：source hadoop-env.sh编译文件。 1source hadoop-env.sh 3.1.8.2 core-site.xml命令行执行：vim core-site.xml 1vim core-site.xml 123456789101112&lt;configuration&gt;&lt;!--用来指定hdfs的老大，namenode的地址--&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://tedu:9000&lt;/value&gt;&lt;/property&gt;&lt;!--用来指定hadoop运行时产生文件的存放目录--&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/home/hadoop-2.7.1/tmp&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.3 hdfs-site .xml命令行执行：vim hdfs-site.xml 1vim hdfs-site.xml 12345678910111213&lt;configuration&gt;&lt;!--指定hdfs保存数据副本的数量，包括自己，默认值是3--&gt;&lt;!--如果是伪分布模式，此值是1--&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;!--设置hdfs的操作权限，false表示任何用户都可以在hdfs上操作文件--&gt;&lt;property&gt;&lt;name&gt;dfs.permissions&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.4 mapred-site.xml命令行执行： 123cp mapred-site.xml.template mapred-site.xmlvim mapred-site.xml 1234567&lt;configuration&gt;&lt;property&gt;&lt;!--指定mapreduce运行在yarn上--&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.5 yarn-site.xml命令行执行：vim yarn-site.xml 1vim yarn-site.xml 12345678910111213&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt;&lt;property&gt;&lt;!--指定yarn的老大 resoucemanager的地址--&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;tedu&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!--NodeManager获取数据的方式--&gt;&lt;name&gt;yarn.nodemanager.aux- services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.6 slaves命令行执行： 1vim slaves 修改主机名 3.1.9 配置hadoop的环境变量 文件最后追加文件 HADOOP_HOME=/home/hadoop-2.7.1 export HADOOP_HOME source /etc/profile 使更改的配置立即生效。 123456#Set Java ENVJAVA_HOME=/home/jdk1.8.0_65HADOOP_HOME=/home/hadoop-2.7.1PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JAVA_HOME PATH CLASSPATH HADOOP_HOME 3.1.10 格式化Namenode执行：hdfs namenode -format 1hdfs namenode -format 如果不好使，可以重启linux 当出现：successfully，证明格式化成功。 3.1.11 启动Hadoop在/home/hadoop-2.7.1/sbin目录下 执行:./start-all.sh 1./start-all.sh 3.1.12 验证启动成功可以访问网址： http://192.168.220.128:50070","categories":[{"name":"CDH","slug":"CDH","permalink":"http://zhangfuxin.cn/categories/CDH/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"http://zhangfuxin.cn/tags/CDH/"}],"keywords":[{"name":"CDH","slug":"CDH","permalink":"http://zhangfuxin.cn/categories/CDH/"}]},{"title":"Hadoop伪分布式搭建","slug":"hadoop-single","date":"2019-08-29T17:30:04.000Z","updated":"2019-08-29T17:28:47.951Z","comments":true,"path":"hadoop-single.html","link":"","permalink":"http://zhangfuxin.cn/hadoop-single.html","excerpt":"** Hadoop伪分布式搭建：** &lt;Excerpt in index | 首页摘要&gt; ​ 大数据（Big Data），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。​ 大数据的定义是4Vs：数据量大、处理速度快、数据源多样、真实性。用中文简单描述就是大、快、多、真。","text":"** Hadoop伪分布式搭建：** &lt;Excerpt in index | 首页摘要&gt; ​ 大数据（Big Data），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。​ 大数据的定义是4Vs：数据量大、处理速度快、数据源多样、真实性。用中文简单描述就是大、快、多、真。 &lt;The rest of contents | 余下全文&gt; 1.1Hadoop简介1.1.1Hadoop创始人​ 1985年，Doug Cutting毕业于美国斯坦福大学。他并不是一开始就决心投身IT行业的，在大学时代的头两年，Cutting学习了诸如物理、地理等常规课程。因为学费的压力，Cutting开始意识到，自己必须学习一些更加实用、有趣的技能。这样，一方面可以帮助自己还清贷款，另一方面，也是为自己未来的生活做打算。因为斯坦福大学座落在IT行业的“圣地”硅谷，所以学习软件对年轻人来说是再自然不过的事情了。 1997年底，Cutting开始以每周两天的时间投入，在家里试着用Java把这个想法变成现实，不久之后，Lucene诞生了。作为第一个提供全文文本搜索的开源函数库，Lucene的伟大自不必多言。 Doug Cutting是Lucence,Nutch,Hadoop的创始人。 1.1.2Hadoop起源​ 2004年，Cutting和同为程序员出身的Mike Cafarella决定开发一款可以代替当时的主流搜索产品的开源搜索引擎，这个项目被命名为Nutch。Nutch基于Lucence实现的搜索引擎，能够从互联网上抓取网页数据。抓取来的海量数据的存储问题。但是，这些海量数据都是非结构化数据，不能存在关系型数据库里。如果连数据的存储和管理都解决不了化，就谈不上后续为用户提供搜索服务，包括通过算法去优化检索速度。那时的Cutting正处在Freelancer的生涯 中，所以他希望自己的项目能通过一种低开销的方式来构建网页中的大量算法。幸运的是，Google这时正好发布了一项研究报告，报告中介绍了两款Google为支持自家的搜索引擎而开发的软件平台。这两个平台一个是GFS（Google File System），用于存储不同设备所产生的海量数据；另一个是MapReduce，它运行在GFS之上，负责分布式大规模数据。 ​ 2006年cutting根据《Google File System》设计了Nutch的HDFS,hadoop distributed file system。 Hadoop最开始是nutch的子项目，目的是解决nutch的海量数据存储问题。在nutch 0.8版本之后，Hadoop独立处理，成为一个独立的项目。后来，又根据《Google MapReduce》设计了基于HDFS的MapRedce计算框架。 2006年之后，cutting带着Hadoop去了雅虎，当时有100多人的团队共同帮cutting完善hadoop。后来yahoo把Hadoop贡献了Apache。所以，现在Hadoop是Apache的顶级项目。 2.1Hadoop搭建Hadoop的三种运行模式 ： 独立（本地）运行模式：无需任何守护进程，所有的程序都运行在同一个JVM上执行。在独立模式下调试MR程序非常高效方便。所以一般该模式主要是在学习或者开发阶段调试使用 。 伪分布式模式： Hadoop守护进程运行在本地机器上，模拟一个小规模的集群，换句话说，可以配置一台机器的Hadoop集群,伪分布式是完全分布式的一个特例。 完全分布式模式：Hadoop守护进程运行在一个集群上。 3.1 单机伪分布模式​ 只支持MapReduce，不支持HDFS。这种模式一般用于调试MapReduce任务用的。 3.1.1 准备Linux环境，最低的工作内存1G内容详见：Vmware安装Centos6.9文档 3.1.2 关闭防火墙临时关闭防火墙：service iptables stop 1service iptables stop 永久关闭防火墙：chkconfig iptables off 1chkconfig iptables off 注意：永久修改防火墙需要重启，永久和临时同时执行，可以不用重启。 3.1.3 配置主机名查询主机名称：hostname 1hostname 临时修改主机名：hostname 1hostname &lt;name&gt; 永久修改主机名：vim /etc/sysconfig/network 1vim /etc/sysconfig/network 注意： 1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。 2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。 注意： 1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。 2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。 3.1.4 配置hosts文件执行: vim /etc/hosts 1vim /etc/hosts 注意： 不要删除前两行内容。 IP在前，主机名在后。 3.1.5 配置免密码登录3.1.5.1 免密登陆原理 A机器生成公钥和私钥 机器A发送自己的公钥到机器B，这个时候机器B有了机器A的公钥 机器B发送一个随机的字符串向机器A 机器A利用自己的私钥把字符串加密 机器A把加密后的字符串再次发送给机器B 机器B利用公钥解密字符串，如果和原来的一样，则OK。 3.1.5.1 免密登陆实现 生成自己的公钥和私钥 ssh-keygen 1ssh-keygen 把生成的公钥copy到远程机器上 ssh-copy-id root@hadoop01 1ssh-copy-id root@hadoop01 注意：如果是单机的伪分布式环境，自己节点也需要配置免密登录。 3.1.6 安装和配置jdk 执行： 1vim /etc/profile 在尾行添加 12345#Set Java ENVJAVA_HOME=/home/jdk1.8.0_65PATH=$JAVA_HOME/bin:$PATHCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JAVA_HOME PATH CLASS_PATH 保存退出 :wq 命令行执行： 1source /etc/profile java -version 查看JDK版本信息。 1java -version 3.1.7 上传和安装hadoop下载地址：http://hadoop.apache.org/releases.html 注意： source表示源码 binary表示二级制包（安装包） 3.1.7.1 解压Hadoop文件包执行：tar -zxvf hadoop-2.7.1_64bit.tar.gz 1tar -zxvf hadoop-2.7.1_64bit.tar.gz 3.1.7.2 Hadoop目录说明bin目录：命令脚本 etc/hadoop:存放hadoop的配置文件 lib目录：hadoop运行的依赖jar包 sbin目录：启动和关闭hadoop等命令都在这里 libexec目录：存放的也是hadoop命令，但一般不常用 注意：最常用的就是bin和etc目录。 3.1.8 配置hadoop配置文件Hadoop目录下/home/hadoop-2.7.1/etc/hadoop/目录下6个文件 3.1.8.1 hadoop-env.sh执行：vim hadoop-env.sh 1vim hadoop-env.sh 修改：修改java_home路径和hadoop_conf_dir 路径 25行 33行 1234#25行export JAVA_HOME=/home/jdk1.8.0_65#33行export HADOOP_CONF_DIR=/home/hadoop-2.7.1/etc/hadoop 然后执行：source hadoop-env.sh编译文件。 1source hadoop-env.sh 3.1.8.2 core-site.xml命令行执行：vim core-site.xml 1vim core-site.xml 123456789101112&lt;configuration&gt;&lt;!--用来指定hdfs的老大，namenode的地址--&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://tedu:9000&lt;/value&gt;&lt;/property&gt;&lt;!--用来指定hadoop运行时产生文件的存放目录--&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/home/hadoop-2.7.1/tmp&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.3 hdfs-site .xml命令行执行：vim hdfs-site.xml 1vim hdfs-site.xml 12345678910111213&lt;configuration&gt;&lt;!--指定hdfs保存数据副本的数量，包括自己，默认值是3--&gt;&lt;!--如果是伪分布模式，此值是1--&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;!--设置hdfs的操作权限，false表示任何用户都可以在hdfs上操作文件--&gt;&lt;property&gt;&lt;name&gt;dfs.permissions&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.4 mapred-site.xml命令行执行： 123cp mapred-site.xml.template mapred-site.xmlvim mapred-site.xml 1234567&lt;configuration&gt;&lt;property&gt;&lt;!--指定mapreduce运行在yarn上--&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.5 yarn-site.xml命令行执行：vim yarn-site.xml 1vim yarn-site.xml 12345678910111213&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt;&lt;property&gt;&lt;!--指定yarn的老大 resoucemanager的地址--&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;tedu&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!--NodeManager获取数据的方式--&gt;&lt;name&gt;yarn.nodemanager.aux- services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.6 slaves命令行执行： 1vim slaves 修改主机名 3.1.9 配置hadoop的环境变量 文件最后追加文件 HADOOP_HOME=/home/hadoop-2.7.1 export HADOOP_HOME source /etc/profile 使更改的配置立即生效。 123456#Set Java ENVJAVA_HOME=/home/jdk1.8.0_65HADOOP_HOME=/home/hadoop-2.7.1PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JAVA_HOME PATH CLASSPATH HADOOP_HOME 3.1.10 格式化Namenode执行：hdfs namenode -format 1hdfs namenode -format 如果不好使，可以重启linux 当出现：successfully，证明格式化成功。 3.1.11 启动Hadoop在/home/hadoop-2.7.1/sbin目录下 执行:./start-all.sh 1./start-all.sh 3.1.12 验证启动成功可以访问网址： http://192.168.220.128:50070","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"如何选购合适的电脑","slug":"buy-computer","date":"2019-08-28T16:30:04.000Z","updated":"2019-08-28T17:10:49.787Z","comments":true,"path":"buy-computer.html","link":"","permalink":"http://zhangfuxin.cn/buy-computer.html","excerpt":"** 购买合适的电脑：** &lt;Excerpt in index | 首页摘要&gt;随着时代的发展，人们生活水平的提高，计算机也成为了我们工作必不可少的生产力，办公，打游戏，看电影，购物等等。市面上电脑种类根据用途又可以分为很多类，台式机、笔记本、IPAD（平板）。购买一台适合自己的电脑工作时可以如虎添翼，电脑用着舒服，心情也会舒畅许多。","text":"** 购买合适的电脑：** &lt;Excerpt in index | 首页摘要&gt;随着时代的发展，人们生活水平的提高，计算机也成为了我们工作必不可少的生产力，办公，打游戏，看电影，购物等等。市面上电脑种类根据用途又可以分为很多类，台式机、笔记本、IPAD（平板）。购买一台适合自己的电脑工作时可以如虎添翼，电脑用着舒服，心情也会舒畅许多。 &lt;The rest of contents | 余下全文&gt; 买电脑主要需求 看电影，上网（购物） 打游戏 办公（移动办公） 平面设计（CAD） UI(影视剪辑) 编程 其他 电脑配置说明目前电脑配置的CPU（绝对过剩），内存Win10最低要8个G，显卡要根据自己需求一般显卡基本够用，电脑最大的瓶颈都是在硬盘上，所以现在买电脑带不带固态硬盘是我首选的配置（我对固态硬盘定义最低要128G,512G固态才是标配，毕竟固态大小会影响到一定的读写速率，还有为了保证固态寿命做系统时会留出10%的空间不划分到分区中），至于买笔记本还是台式机需要根据不同应用场景来定。台式机性能肯定远超同价位笔记本，这个是毋庸置疑的。 看电影，上网（购物）对于这方面需求的一般一女生居多，看电影上网，对电脑配置要求比较低的，一般普通双核CPU，AMD、酷睿i3都可以（最好是i5），内存8G就够了（win7的话4G就够，但是Win7现在不支持更新了）。要是女生最重要的是漂亮，这里推荐DELL或者HP相对性价比会比较合适。毕竟要是要以轻薄、美观为主。要是资金充足可以考虑各家品牌的超级本。要是父母的需求的话其实买笔记本或者台式机都可以。这里不推荐苹果笔记本，因为用苹果看电影会容易热，要是妹子是苹果控或者周边产品都是苹果产品，苹果笔记本也可在考虑之列。 打游戏游戏主机两个最主要的要求配置和扩展性，主要是CPU和显卡，我们又称之为“双烧”，建议买台式机。要是需要便携的话，外星人品牌是一个不错的考虑，笔记本显卡最好不要超过GTX2070以上，也许你会问为什么不买笔记本GTX2080的本子，一方面是贵，价格会差很多。还有就是散热问题。为了更好体验还是台式机加水冷。 一般的主流网游：i5或i7处理器，内存16G，中端显卡就可以了，硬盘128G固态+1T机械起 大型单机：i7或i9处理器（水冷），内存16-32G，，显卡中高端GTX1060起，要是玩刺客信条奥德赛GTX2080Ti不用犹豫，硬盘512三星固态+1T机械（最好在配置1T的固态，毕竟游戏不小）起 发烧友：i9处理器（水冷），内存32G-64G，显卡高端GTX2080或者是多显交火，硬盘512G（三星固态PRO系列）+1T固态 办公用于办公的大多是商务人士，对笔记本的性能要求一般，最主要的是便携性，各大品牌的超极本都很合适，还能衬托气质，最推荐的还是联想的thinkpad系列，没钱买个E系类（基本三年就会坏），要是有资金充裕T系列或者X系列是首选配置（尤其是X系列）。 平面设计（CAD）这个是专业领域的需求，对CPU、显卡和内存、显示器都较高，能好一点就好一点。 UI(影视剪辑)苹果的Macbookpro 16G，512SSD（固态太小用久了会后悔的），i7处理器 最为合适。没有比苹果更适合做平面设计的电脑。Windows系统和苹果系统没得比。 编程苹果的Macbookpro 16G、512SSD、i7处理器。个人推荐MAC的笔记本做编程，一用就停不下来，会上瘾。Windows系统用来打游戏就好了。推荐配置：Macbookpro 16G、i7处理器（i9也是阉割版没意义）、512SSD（固态真的不能太小，512G就不大，考虑到价格没办法）、最好是能带键盘灯、Air pods耳机还是要有一个的，用了就知道不亏。经济允许最好是配置一个IPAD PRO做分屏开发可以调高效率。 其他IPAD我对它的定义就是一台游戏机，不建议用IPAD看电影（用久了手会麻）。因为我不做UI我也没有体会到那只笔的好处。 还有一个设备一点光要说一下就是亚马逊的Kindle，要是你经常看小说，或者是看英文，建议有一个（前期是你不是必须要纸质书）还是很方便的，尤其是书多了的时候。IPAD优势在于pdf文档做笔记。用了就会知道两个不一样。Kindle看电子书是生活品质提升的表现。","categories":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/categories/others/"}],"tags":[{"name":"数码产品","slug":"数码产品","permalink":"http://zhangfuxin.cn/tags/数码产品/"}],"keywords":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/categories/others/"}]},{"title":"Spark学习之路 （十九）SparkSQL的自定义函数UDF","slug":"2019-06-19-Spark学习之路 （十九）SparkSQL的自定义函数UDF","date":"2019-06-19T02:30:04.000Z","updated":"2019-09-17T04:15:17.246Z","comments":true,"path":"2019-06-19-Spark学习之路 （十九）SparkSQL的自定义函数UDF.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-19-Spark学习之路 （十九）SparkSQL的自定义函数UDF.html","excerpt":"** Spark学习之路 （十九）SparkSQL的自定义函数UDF：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十九）SparkSQL的自定义函数UDF","text":"** Spark学习之路 （十九）SparkSQL的自定义函数UDF：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十九）SparkSQL的自定义函数UDF &lt;The rest of contents | 余下全文&gt; 在Spark中，也支持Hive中的自定义函数。自定义函数大致可以分为三种： UDF(User-Defined-Function)，即最基本的自定义函数，类似to_char,to_date等 UDAF（User- Defined Aggregation Funcation），用户自定义聚合函数，类似在group by之后使用的sum,avg等 UDTF(User-Defined Table-Generating Functions),用户自定义生成函数，有点像stream里面的flatMap 自定义一个UDF函数需要继承UserDefinedAggregateFunction类，并实现其中的8个方法 示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import org.apache.spark.sql.Rowimport org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;import org.apache.spark.sql.types.&#123;DataType, StringType, StructField, StructType&#125;object GetDistinctCityUDF extends UserDefinedAggregateFunction&#123; /** * 输入的数据类型 * */ override def inputSchema: StructType = StructType( StructField(\"status\",StringType,true) :: Nil ) /** * 缓存字段类型 * */ override def bufferSchema: StructType = &#123; StructType( Array( StructField(\"buffer_city_info\",StringType,true) ) ) &#125;/** * 输出结果类型 * */ override def dataType: DataType = StringType/** * 输入类型和输出类型是否一致 * */ override def deterministic: Boolean = true/** * 对辅助字段进行初始化 * */ override def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer.update(0,\"\") &#125;/** *修改辅助字段的值 * */ override def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; //获取最后一次的值 var last_str = buffer.getString(0) //获取当前的值 val current_str = input.getString(0) //判断最后一次的值是否包含当前的值 if(!last_str.contains(current_str))&#123; //判断是否是第一个值，是的话走if赋值，不是的话走else追加 if(last_str.equals(\"\"))&#123; last_str = current_str &#125;else&#123; last_str += \",\" + current_str &#125; &#125; buffer.update(0,last_str) &#125;/** *对分区结果进行合并 * buffer1是机器hadoop1上的结果 * buffer2是机器Hadoop2上的结果 * */ override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; var buf1 = buffer1.getString(0) val buf2 = buffer2.getString(0) //将buf2里面存在的数据而buf1里面没有的数据追加到buf1 //buf2的数据按照，进行切分 for(s &lt;- buf2.split(\",\"))&#123; if(!buf1.contains(s))&#123; if(buf1.equals(\"\"))&#123; buf1 = s &#125;else&#123; buf1 += s &#125; &#125; &#125; buffer1.update(0,buf1) &#125;/** * 最终的计算结果 * */ override def evaluate(buffer: Row): Any = &#123; buffer.getString(0) &#125;&#125; 注册自定义的UDF函数为临时函数 123456789101112131415161718def main(args: Array[String]): Unit = &#123; /** * 第一步 创建程序入口 */ val conf = new SparkConf().setAppName(\"AralHotProductSpark\") val sc = new SparkContext(conf) val hiveContext = new HiveContext(sc) //注册成为临时函数 hiveContext.udf.register(\"get_distinct_city\",GetDistinctCityUDF) //注册成为临时函数 hiveContext.udf.register(\"get_product_status\",(str:String) =&gt;&#123; var status = 0 for(s &lt;- str.split(\",\"))&#123; if(s.contains(\"product_status\"))&#123; status = s.split(\":\")(1).toInt &#125; &#125; &#125;)&#125;","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"RDD、DataFrame和DataSet的区别是什么","slug":"2019-06-18-DataSet和DataFrame区别和转换","date":"2019-06-18T03:30:04.000Z","updated":"2019-09-17T04:14:51.186Z","comments":true,"path":"2019-06-18-DataSet和DataFrame区别和转换.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-18-DataSet和DataFrame区别和转换.html","excerpt":"** RDD、DataFrame和DataSet的区别是什么：** &lt;Excerpt in index | 首页摘要&gt; ​ RDD、DataFrame和DataSet是容易产生混淆的概念，必须对其相互之间对比，才可以知道其中异同：DataFrame多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合。DataFrame是分布式的Row对象的集合。","text":"** RDD、DataFrame和DataSet的区别是什么：** &lt;Excerpt in index | 首页摘要&gt; ​ RDD、DataFrame和DataSet是容易产生混淆的概念，必须对其相互之间对比，才可以知道其中异同：DataFrame多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合。DataFrame是分布式的Row对象的集合。 &lt;The rest of contents | 余下全文&gt; RDD、DataFrame和DataSet是容易产生混淆的概念，必须对其相互之间对比，才可以知道其中异同。 RDD和DataFrame RDD-DataFrame 上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解 Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。 提升执行效率RDD API是函数式的，强调不变性，在大部分场景下倾向于创建新对象而不是修改老对象。这一特点虽然带来了干净整洁的API，却也使得Spark应用程序在运行期倾向于创建大量临时对象，对GC造成压力。在现有RDD API的基础之上，我们固然可以利用mapPartitions方法来重载RDD单个分片内的数据创建方式，用复用可变对象的方式来减小对象分配和GC的开销，但这牺牲了代码的可读性，而且要求开发者对Spark运行时机制有一定的了解，门槛较高。另一方面，Spark SQL在框架内部已经在各种可能的情况下尽量重用对象，这样做虽然在内部会打破了不变性，但在将数据返回给用户时，还会重新转为不可变数据。利用 DataFrame API进行开发，可以免费地享受到这些优化效果。 减少数据读取分析大数据，最快的方法就是 ——忽略它。这里的“忽略”并不是熟视无睹，而是根据查询条件进行恰当的剪枝。 上文讨论分区表时提到的分区剪 枝便是其中一种——当查询的过滤条件中涉及到分区列时，我们可以根据查询条件剪掉肯定不包含目标数据的分区目录，从而减少IO。 对于一些“智能”数据格 式，Spark SQL还可以根据数据文件中附带的统计信息来进行剪枝。简单来说，在这类数据格式中，数据是分段保存的，每段数据都带有最大值、最小值、null值数量等 一些基本的统计信息。当统计信息表名某一数据段肯定不包括符合查询条件的目标数据时，该数据段就可以直接跳过(例如某整数列a某段的最大值为100，而查询条件要求a &gt; 200)。 此外，Spark SQL也可以充分利用RCFile、ORC、Parquet等列式存储格式的优势，仅扫描查询真正涉及的列，忽略其余列的数据。 执行优化 人口数据分析示例 为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter 下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。 得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。 对于普通开发者而言，查询优化 器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。 RDD和DataSetDataSet以Catalyst逻辑执行计划表示，并且数据以编码的二进制形式被存储，不需要反序列化就可以执行sorting、shuffle等操作。 DataSet创立需要一个显式的Encoder，把对象序列化为二进制，可以把对象的scheme映射为SparkSQl类型，然而RDD依赖于运行时反射机制。 通过上面两点，DataSet的性能比RDD的要好很多。 DataFrame和DataSetDataSet跟DataFrame还是有挺大区别的，DataFrame开发都是写sql，但是DataSet是使用类似RDD的API。主要区别是Dataset每一个record存储的是一个强类型值而不是一个Row。 (1)相同点：都是分布式数据集 DataFrame底层是RDD，但是DataSet不是，不过他们最后都是转换成RDD运行 DataSet和DataFrame的相同点都是有数据特征、数据类型的分布式数据集(schema) (2)不同点：(a)schema信息： RDD中的数据是没有数据类型的 DataFrame中的数据是弱数据类型，不会做数据类型检查 虽然有schema规定了数据类型，但是编译时是不会报错的，运行时才会报错 DataSet中的数据类型是强数据类型 (b)序列化机制： RDD和DataFrame默认的序列化机制是java的序列化，可以修改为Kyro的机制 DataSet使用自定义的数据编码器进行序列化和反序列化 创建方式：(1)要使用toDS之前1import sqlContext.implicits._ (2)将内存中的数据转换成DataSet123456// Encoders for most common types are automatically provided by importingsqlContext.implicits._val ds = Seq(1, 2, 3).toDS()ds.map(_ + 1).collect() // Returns: Array(2, 3, 4) 其中： collect()：返回一个Array，包含所有行信息 Returns an array that contains all rows in this Dataset. (3)可以直接把case class对象转化成DataSet1234// Encoders are also created for case classes.case class Person(name: String, age: Long)val ds = Seq(Person(\"Andy\", 32)).toDS() (4)将DataFrame转换成DataSet，不过要求是DataFrame的数据类型必须是case class并且要求DataFrame的数据类型必须和case class一致(顺序也必须一致) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package _0729DFimport org.apache.sparkimport org.apache.spark.sql.&#123;Dataset, SparkSession&#125;//import org.apache.sparkobject Dataset extends App&#123;// import spark.implicits._// val ds = Seq(1, 2, 3).toDS()// ds.map(_ + 1).collect() // Returns: Array(2, 3, 4)// // Encoders are also created for case classes.// case class Person(name: String, age: Long)// val ds = Seq(Person(\"Andy\", 32)).toDS()// ds.showval session = SparkSession.builder().appName(\"app\").master(\"local\").getOrCreate()val sqlContext = session.sqlContextval wcDs = sqlContext.read.textFile(\"datas/halibote.txt\")// 导入隐式转换import session.implicits._val wordData=wcDs.flatMap(_.split(\" \"))wordData.createTempView(\"t_word\")wordData.show() //wordData.printSchema()// Encoders for most common types are automatically provided by importing sqlContext.implicits._val ds=Seq(1,2,3).toDS()ds.map(_ + 1).collect() // Returns: Array(2, 3, 4)// // Encoders are also created for case classes.// case class Person(name: String, age: Long)// val ds = Seq(Person(\"Andy\", 32)).toDS()// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name.case class Person(age:Long,name:String)val path = \"datas/people.json\"val people: Dataset[Person] = sqlContext.read.json(path).as[Person]people.show()&#125; 用wordcount举例： 12345678910// DataFrame// Load a text file and interpret each line as a java.lang.Stringval ds = sqlContext.read.text(\"/home/spark/1.6/lines\").as[String]val result = ds .flatMap(_.split(\" \")) // Split on whitespace .filter(_ != \"\") // Filter empty words .toDF() // Convert to DataFrame to perform aggregation / sorting .groupBy($\"value\") // Count number of occurences of each word .agg(count(\"*\") as \"numOccurances\") .orderBy($\"numOccurances\" desc) // Show most common words first 后面版本DataFrame会继承DataSet，DataFrame是面向Spark SQL的接口。 123456// DataSet,完全使用scala编程，不要切换到DataFrameval wordCount = ds.flatMap(.split(\" \")) .filter( != \"\") .groupBy(_.toLowerCase()) // Instead of grouping on a column expression (i.e. $\"value\") we pass a lambda function .count() DataFrame和DataSet可以相互转化， df.as[ElementType] 这样可以把DataFrame转化为DataSet， ds.toDF() 这样可以把DataSet转化为DataFrame。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十八）SparkSQL简单使用","slug":"2019-06-18-Spark学习之路 （十八）SparkSQL简单使用","date":"2019-06-18T02:30:04.000Z","updated":"2019-09-17T03:39:50.782Z","comments":true,"path":"2019-06-18-Spark学习之路 （十八）SparkSQL简单使用.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-18-Spark学习之路 （十八）SparkSQL简单使用.html","excerpt":"** Spark学习之路 （十八）SparkSQL简单使用：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十八）SparkSQL简单使用","text":"** Spark学习之路 （十八）SparkSQL简单使用：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十八）SparkSQL简单使用 &lt;The rest of contents | 余下全文&gt; 一、SparkSQL的进化之路1.0以前： Shark 1.1.x： SparkSQL(只是测试性的) SQL 1.3.x: SparkSQL(正式版本)+Dataframe 1.5.x: SparkSQL 钨丝计划 1.6.x： SparkSQL+DataFrame+DataSet(测试版本) 2.x : SparkSQL+DataFrame+DataSet(正式版本) ​ SparkSQL:还有其他的优化 ​ StructuredStreaming(DataSet) 二、认识SparkSQL2.1 什么是SparkSQL?spark SQL是spark的一个模块，主要用于进行结构化数据的处理。它提供的最核心的编程抽象就是DataFrame。 2.2 SparkSQL的作用提供一个编程抽象（DataFrame） 并且作为分布式 SQL 查询引擎 DataFrame：它可以根据很多源进行构建，包括：结构化的数据文件，hive中的表，外部的关系型数据库，以及RDD 2.3 运行原理将 Spark SQL 转化为 RDD， 然后提交到集群执行 2.4 特点（1）容易整合 （2）统一的数据访问方式 （3）兼容 Hive （4）标准的数据连接 2.5 SparkSessionSparkSession是Spark 2.0引如的新概念。SparkSession为用户提供了统一的切入点，来让用户学习spark的各项功能。 在spark的早期版本中，SparkContext是spark的主要切入点，由于RDD是主要的API，我们通过sparkcontext来创建和操作RDD。对于每个其他的API，我们需要使用不同的context。例如，对于Streming，我们需要使用StreamingContext；对于sql，使用sqlContext；对于Hive，使用hiveContext。但是随着DataSet和DataFrame的API逐渐成为标准的API，就需要为他们建立接入点。所以在spark2.0中，引入SparkSession作为DataSet和DataFrame API的切入点，SparkSession封装了SparkConf、SparkContext和SQLContext。为了向后兼容，SQLContext和HiveContext也被保存下来。 SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。 特点： —- 为用户提供一个统一的切入点使用Spark 各项功能 ​ —- 允许用户通过它调用 DataFrame 和 Dataset 相关 API 来编写程序 ​ —- 减少了用户需要了解的一些概念，可以很容易的与 Spark 进行交互 ​ —- 与 Spark 交互之时不需要显示的创建 SparkConf, SparkContext 以及 SQlContext，这些对象已经封闭在 SparkSession 中 2.7 DataFrames在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。 三、RDD转换成为DataFrame使用spark1.x版本的方式 测试数据目录：/home/hadoop/apps/spark/examples/src/main/resources（spark的安装目录里面） people.txt 3.1 方式一：通过 case class 创建 DataFrames（反射）12345678910111213141516171819//定义case class，相当于表结构case class People(var name:String,var age:Int)object TestDataFrame1 &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"RDDToDataFrame\").setMaster(\"local\") val sc = new SparkContext(conf) val context = new SQLContext(sc) // 将本地的数据读入 RDD， 并将 RDD 与 case class 关联 val peopleRDD = sc.textFile(\"E:\\\\666\\\\people.txt\") .map(line =&gt; People(line.split(\",\")(0), line.split(\",\")(1).trim.toInt)) import context.implicits._ // 将RDD 转换成 DataFrames val df = peopleRDD.toDF //将DataFrames创建成一个临时的视图 df.createOrReplaceTempView(\"people\") //使用SQL语句进行查询 context.sql(\"select * from people\").show() &#125;&#125; 运行结果 3.2 方式二：通过 structType 创建 DataFrames（编程接口）1234567891011121314151617181920212223242526object TestDataFrame2 &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"TestDataFrame2\").setMaster(\"local\") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val fileRDD = sc.textFile(\"E:\\\\666\\\\people.txt\") // 将 RDD 数据映射成 Row，需要 import org.apache.spark.sql.Row val rowRDD: RDD[Row] = fileRDD.map(line =&gt; &#123; val fields = line.split(\",\") Row(fields(0), fields(1).trim.toInt) &#125;) // 创建 StructType 来定义结构 val structType: StructType = StructType( //字段名，字段类型，是否可以为空 StructField(\"name\", StringType, true) :: StructField(\"age\", IntegerType, true) :: Nil ) /** * rows: java.util.List[Row], * schema: StructType * */ val df: DataFrame = sqlContext.createDataFrame(rowRDD,structType) df.createOrReplaceTempView(\"people\") sqlContext.sql(\"select * from people\").show() &#125;&#125; 运行结果 3.3 方式三：通过 json 文件创建 DataFrames12345678910object TestDataFrame3 &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"TestDataFrame2\").setMaster(\"local\") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val df: DataFrame = sqlContext.read.json(\"E:\\\\666\\\\people.json\") df.createOrReplaceTempView(\"people\") sqlContext.sql(\"select * from people\").show() &#125;&#125; 四、DataFrame的read和save和savemode4.1 数据的读取123456789101112131415object TestRead &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"TestDataFrame2\").setMaster(\"local\") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) //方式一 val df1 = sqlContext.read.json(\"E:\\\\666\\\\people.json\") val df2 = sqlContext.read.parquet(\"E:\\\\666\\\\users.parquet\") //方式二 val df3 = sqlContext.read.format(\"json\").load(\"E:\\\\666\\\\people.json\") val df4 = sqlContext.read.format(\"parquet\").load(\"E:\\\\666\\\\users.parquet\") //方式三，默认是parquet格式 val df5 = sqlContext.load(\"E:\\\\666\\\\users.parquet\") &#125;&#125; 4.2 数据的保存1234567891011121314151617object TestSave &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"TestDataFrame2\").setMaster(\"local\") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val df1 = sqlContext.read.json(\"E:\\\\666\\\\people.json\") //方式一 df1.write.json(\"E:\\\\111\") df1.write.parquet(\"E:\\\\222\") //方式二 df1.write.format(\"json\").save(\"E:\\\\333\") df1.write.format(\"parquet\").save(\"E:\\\\444\") //方式三 df1.write.save(\"E:\\\\555\") &#125;&#125; 4.3 数据的保存模式使用mode 1df1.write.format(&quot;parquet&quot;).mode(SaveMode.Ignore).save(&quot;E:\\\\444&quot;) 五、数据源5.1 数据源只json参考4.1 5.2 数据源之parquet参考4.1 5.3 数据源之Mysql123456789101112131415161718object TestMysql &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"TestMysql\").setMaster(\"local\") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val url = \"jdbc:mysql://192.168.123.102:3306/hivedb\" val table = \"dbs\" val properties = new Properties() properties.setProperty(\"user\",\"root\") properties.setProperty(\"password\",\"root\") //需要传入Mysql的URL、表明、properties（连接数据库的用户名密码） val df = sqlContext.read.jdbc(url,table,properties) df.createOrReplaceTempView(\"dbs\") sqlContext.sql(\"select * from dbs\").show() &#125;&#125; 运行结果 5.4 数据源之Hive（1）准备工作在pom.xml文件中添加依赖 123456&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; 开发环境则把resource文件夹下添加hive-site.xml文件，集群环境把hive的配置文件要发到$SPARK_HOME/conf目录下 12345678910111213141516171819202122232425262728&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hivedb?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;!-- 如果 mysql 和 hive 在同一个服务器节点，那么请更改 hadoop02 为 localhost --&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/hive/warehouse&lt;/value&gt; &lt;description&gt;hive default warehouse, if nessecory, change it&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; （2）测试代码12345678object TestHive &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setMaster(\"local\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(conf) val sqlContext = new HiveContext(sc) sqlContext.sql(\"select * from myhive.student\").show() &#125;&#125; 运行结果","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十七）Spark分区","slug":"2019-06-17-Spark学习之路 （十七）Spark分区","date":"2019-06-17T02:30:04.000Z","updated":"2019-09-17T03:30:58.175Z","comments":true,"path":"2019-06-17-Spark学习之路 （十七）Spark分区.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-17-Spark学习之路 （十七）Spark分区.html","excerpt":"** Spark学习之路 （十七）Spark分区：** &lt;Excerpt in index | 首页摘要&gt; 分区是RDD内部并行计算的一个计算单元，RDD的数据集在逻辑上被划分为多个分片，每一个分片称为分区，分区的格式决定了并行计算的粒度，而每个分区的数值计算都是在一个任务中进行的，因此任务的个数，也是由RDD(准确来说是作业最后一个RDD)的分区数决定。","text":"** Spark学习之路 （十七）Spark分区：** &lt;Excerpt in index | 首页摘要&gt; 分区是RDD内部并行计算的一个计算单元，RDD的数据集在逻辑上被划分为多个分片，每一个分片称为分区，分区的格式决定了并行计算的粒度，而每个分区的数值计算都是在一个任务中进行的，因此任务的个数，也是由RDD(准确来说是作业最后一个RDD)的分区数决定。 &lt;The rest of contents | 余下全文&gt; 一、为什么要进行分区 数据分区，在分布式集群里，网络通信的代价很大，减少网络传输可以极大提升性能。mapreduce框架的性能开支主要在io和网络传输，io因为要大量读写文件，它是不可避免的，但是网络传输是可以避免的，把大文件压缩变小文件， 从而减少网络传输，但是增加了cpu的计算负载。 Spark里面io也是不可避免的，但是网络传输spark里面进行了优化： Spark把rdd进行分区（分片），放在集群上并行计算。同一个rdd分片100个，10个节点，平均一个节点10个分区，当进行sum型的计算的时候，先进行每个分区的sum，然后把sum值shuffle传输到主程序进行全局sum，所以进行sum型计算对网络传输非常小。但对于进行join型的计算的时候，需要把数据本身进行shuffle，网络开销很大。 spark是如何优化这个问题的呢？ Spark把key－value rdd通过key的hashcode进行分区，而且保证相同的key存储在同一个节点上，这样对改rdd进行key聚合时，就不需要shuffle过程，我们进行mapreduce计算的时候为什么要进行shuffle？，就是说mapreduce里面网络传输主要在shuffle阶段，shuffle的根本原因是相同的key存在不同的节点上，按key进行聚合的时候不得不进行shuffle。shuffle是非常影响网络的，它要把所有的数据混在一起走网络，然后它才能把相同的key走到一起。要进行shuffle是存储决定的。 Spark从这个教训中得到启发，spark会把key进行分区，也就是key的hashcode进行分区，相同的key，hashcode肯定是一样的，所以它进行分区的时候100t的数据分成10分，每部分10个t，它能确保相同的key肯定在一个分区里面，而且它能保证存储的时候相同的key能够存在同一个节点上。比如一个rdd分成了100份，集群有10个节点，所以每个节点存10份，每一分称为每个分区，spark能保证相同的key存在同一个节点上，实际上相同的key存在同一个分区。 key的分布不均决定了有的分区大有的分区小。没法分区保证完全相等，但它会保证在一个接近的范围。所以mapreduce里面做的某些工作里边，spark就不需要shuffle了，spark解决网络传输这块的根本原理就是这个。 进行join的时候是两个表，不可能把两个表都分区好，通常情况下是把用的频繁的大表事先进行分区，小表进行关联它的时候小表进行shuffle过程。 大表不需要shuffle。 需要在工作节点间进行数据混洗的转换极大地受益于分区。这样的转换是 cogroup，groupWith，join，leftOuterJoin，rightOuterJoin，groupByKey，reduceByKey，combineByKey 和lookup。 分区是可配置的，只要RDD是基于键值对的即可。 二、Spark分区原则及方法RDD分区的一个分区原则：尽可能是得分区的个数等于集群核心数目 无论是本地模式、Standalone模式、YARN模式或Mesos模式，我们都可以通过spark.default.parallelism来配置其默认分区个数，若没有设置该值，则根据不同的集群环境确定该值 2.1 本地模式（1）默认方式以下这种默认方式就一个分区 结果 （2）手动设置设置了几个分区就是几个分区 结果 （3）跟local[n] 有关n等于几默认就是几个分区 如果n=* 那么分区个数就等于cpu core的个数 结果 本机电脑查看cpu core，我的电脑–》右键管理–》设备管理器–》处理器 （4）参数控制 结果 2.2 YARN模式 进入defaultParallelism方法 继续进入defaultParallelism方法 这个一个trait，其实现类是（Ctrl+h） 进入TaskSchedulerImpl类找到defaultParallelism方法 继续进入defaultParallelism方法，又是一个trait，看其实现类 Ctrl+h看SchedulerBackend类的实现类 进入CoarseGrainedSchedulerBackend找到defaultParallelism totalCoreCount.get()是所有executor使用的core总数，和2比较去较大值 如果正常的情况下，那你设置了多少就是多少 四、分区器（1）如果是从HDFS里面读取出来的数据，不需要分区器。因为HDFS本来就分好区了。 分区数我们是可以控制的，但是没必要有分区器。 （2）非key-value RDD分区，没必要设置分区器 1234al testRDD = sc.textFile(\"C:\\\\Users\\\\Administrator\\\\IdeaProjects\\\\myspark\\\\src\\\\main\\\\hello.txt\") .flatMap(line =&gt; line.split(\",\")) .map(word =&gt; (word, 1)).partitionBy(new HashPartitioner(2)) 没必要设置，但是非要设置也行。 （3）Key-value形式的时候，我们就有必要了。 HashPartitioner 1234val resultRDD = testRDD.reduceByKey(new HashPartitioner(2),(x:Int,y:Int) =&gt; x+ y)//如果不设置默认也是HashPartitoiner，分区数跟spark.default.parallelism一样println(resultRDD.partitioner)println(\"resultRDD\"+resultRDD.getNumPartitions) RangePartitioner 12345val resultRDD = testRDD.reduceByKey((x:Int,y:Int) =&gt; x+ y)val newresultRDD=resultRDD.partitionBy(new RangePartitioner[String,Int](3,resultRDD))println(newresultRDD.partitioner)println(\"newresultRDD\"+newresultRDD.getNumPartitions)注：按照范围进行分区的，如果是字符串，那么就按字典顺序的范围划分。如果是数字，就按数据自的范围划分。 自定义分区 需要实现2个方法 123456789101112131415161718192021222324252627282930313233343536class MyPartitoiner(val numParts:Int) extends Partitioner&#123; override def numPartitions: Int = numParts override def getPartition(key: Any): Int = &#123; val domain = new URL(key.toString).getHost val code = (domain.hashCode % numParts) if (code &lt; 0) &#123; code + numParts &#125; else &#123; code &#125; &#125;&#125;object DomainNamePartitioner &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"word count\").setMaster(\"local\") val sc = new SparkContext(conf) val urlRDD = sc.makeRDD(Seq((\"http://baidu.com/test\", 2), (\"http://baidu.com/index\", 2), (\"http://ali.com\", 3), (\"http://baidu.com/tmmmm\", 4), (\"http://baidu.com/test\", 4))) //Array[Array[(String, Int)]] // = Array(Array(), // Array((http://baidu.com/index,2), (http://baidu.com/tmmmm,4), // (http://baidu.com/test,4), (http://baidu.com/test,2), (http://ali.com,3))) val hashPartitionedRDD = urlRDD.partitionBy(new HashPartitioner(2)) hashPartitionedRDD.glom().collect() //使用spark-shell --jar的方式将这个partitioner所在的jar包引进去，然后测试下面的代码 // spark-shell --master spark://master:7077 --jars spark-rdd-1.0-SNAPSHOT.jar val partitionedRDD = urlRDD.partitionBy(new MyPartitoiner(2)) val array = partitionedRDD.glom().collect() &#125;&#125;","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本","slug":"2019-06-16-Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本","date":"2019-06-16T02:30:04.000Z","updated":"2019-09-17T03:17:31.537Z","comments":true,"path":"2019-06-16-Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-16-Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本.html","excerpt":"** Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本","text":"** Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本 &lt;The rest of contents | 余下全文&gt; 一、概述上一篇主要是介绍了spark启动的一些脚本，这篇主要分析一下Spark源码中提交任务脚本的处理逻辑，从spark-submit一步步深入进去看看任务提交的整体流程,首先看一下整体的流程概要图： 二、源码解读2.1 spark-submit 12345678910111213141516# -z是检查后面变量是否为空（空则真） shell可以在双引号之内引用变量，单引号不可#这一步作用是检查SPARK_HOME变量是否为空，为空则执行then后面程序#source命令： source filename作用在当前bash环境下读取并执行filename中的命令#$0代表shell脚本文件本身的文件名，这里即使spark-submit#dirname用于取得脚本文件所在目录 dirname $0取得当前脚本文件所在目录#$(命令)表示返回该命令的结果#故整个if语句的含义是：如果SPARK_HOME变量没有设置值，则执行当前目录下的find-spark-home脚本文件，设置SPARK_HOME值if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then source \"$(dirname \"$0\")\"/find-spark-homefi# disable randomized hash for string in Python 3.3+export PYTHONHASHSEED=0#执行spark-class脚本，传递参数org.apache.spark.deploy.SparkSubmit 和\"$@\"#这里$@表示之前spark-submit接收到的全部参数exec \"$&#123;SPARK_HOME&#125;\"/bin/spark-class org.apache.spark.deploy.SparkSubmit \"$@\" 所以spark-submit脚本的整体逻辑就是：首先 检查SPARK_HOME是否设置；if 已经设置 执行spark-class文件 否则加载执行find-spark-home文件 2.2 find-spark-home1234567891011121314151617181920212223#定义一个变量用于后续判断是否存在定义SPARK_HOME的python脚本文件FIND_SPARK_HOME_PYTHON_SCRIPT=\"$(cd \"$(dirname \"$0\")\"; pwd)/find_spark_home.py\"# Short cirtuit if the user already has this set.##如果SPARK_HOME为不为空值，成功退出程序if [ ! -z \"$&#123;SPARK_HOME&#125;\" ]; then exit 0# -f用于判断这个文件是否存在并且是否为常规文件，是的话为真，这里不存在为假，执行下面语句，给SPARK_HOME变量赋值elif [ ! -f \"$FIND_SPARK_HOME_PYTHON_SCRIPT\" ]; then # If we are not in the same directory as find_spark_home.py we are not pip installed so we don't # need to search the different Python directories for a Spark installation. # Note only that, if the user has pip installed PySpark but is directly calling pyspark-shell or # spark-submit in another directory we want to use that version of PySpark rather than the # pip installed version of PySpark. export SPARK_HOME=\"$(cd \"$(dirname \"$0\")\"/..; pwd)\"else # We are pip installed, use the Python script to resolve a reasonable SPARK_HOME # Default to standard python interpreter unless told otherwise if [[ -z \"$PYSPARK_DRIVER_PYTHON\" ]]; then PYSPARK_DRIVER_PYTHON=\"$&#123;PYSPARK_PYTHON:-\"python\"&#125;\" fi export SPARK_HOME=$($PYSPARK_DRIVER_PYTHON \"$FIND_SPARK_HOME_PYTHON_SCRIPT\")fi 可以看到，如果事先用户没有设定SPARK_HOME的值，这里程序也会自动设置并且将其注册为环境变量，供后面程序使用 当SPARK_HOME的值设定完成之后，就会执行Spark-class文件，这也是我们分析的重要部分，源码如下： 2.3 spark-class1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#!/usr/bin/env bash#依旧是检查设置SPARK_HOME的值if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then source \"$(dirname \"$0\")\"/find-spark-homefi#执行load-spark-env.sh脚本文件，主要目的在于加载设定一些变量值#设定spark-env.sh中的变量值到环境变量中，供后续使用#设定scala版本变量值. \"$&#123;SPARK_HOME&#125;\"/bin/load-spark-env.sh# Find the java binary#检查设定java环境值#-n代表检测变量长度是否为0，不为0时候为真#如果已经安装Java没有设置JAVA_HOME,command -v java返回的值为$&#123;JAVA_HOME&#125;/bin/javaif [ -n \"$&#123;JAVA_HOME&#125;\" ]; then RUNNER=\"$&#123;JAVA_HOME&#125;/bin/java\"else if [ \"$(command -v java)\" ]; then RUNNER=\"java\" else echo \"JAVA_HOME is not set\" &gt;&amp;2 exit 1 fifi# Find Spark jars.#-d检测文件是否为目录，若为目录则为真#设置一些关联Class文件if [ -d \"$&#123;SPARK_HOME&#125;/jars\" ]; then SPARK_JARS_DIR=\"$&#123;SPARK_HOME&#125;/jars\"else SPARK_JARS_DIR=\"$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars\"fiif [ ! -d \"$SPARK_JARS_DIR\" ] &amp;&amp; [ -z \"$SPARK_TESTING$SPARK_SQL_TESTING\" ]; then echo \"Failed to find Spark jars directory ($SPARK_JARS_DIR).\" 1&gt;&amp;2 echo \"You need to build Spark with the target \\\"package\\\" before running this program.\" 1&gt;&amp;2 exit 1else LAUNCH_CLASSPATH=\"$SPARK_JARS_DIR/*\"fi# Add the launcher build dir to the classpath if requested.if [ -n \"$SPARK_PREPEND_CLASSES\" ]; then LAUNCH_CLASSPATH=\"$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH\"fi# For testsif [[ -n \"$SPARK_TESTING\" ]]; then unset YARN_CONF_DIR unset HADOOP_CONF_DIRfi# The launcher library will print arguments separated by a NULL character, to allow arguments with# characters that would be otherwise interpreted by the shell. Read that in a while loop, populating# an array that will be used to exec the final command.## The exit code of the launcher is appended to the output, so the parent shell removes it from the# command array and checks the value to see if the launcher succeeded.#执行类文件org.apache.spark.launcher.Main，返回解析后的参数build_command() &#123; \"$RUNNER\" -Xmx128m -cp \"$LAUNCH_CLASSPATH\" org.apache.spark.launcher.Main \"$@\" printf \"%d\\0\" $?&#125;# Turn off posix mode since it does not allow process substitution#将build_command方法解析后的参数赋给CMDset +o posixCMD=()while IFS= read -d '' -r ARG; do CMD+=(\"$ARG\")done &lt; &lt;(build_command \"$@\")COUNT=$&#123;#CMD[@]&#125;LAST=$((COUNT - 1))LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;# Certain JVM failures result in errors being printed to stdout (instead of stderr), which causes# the code that parses the output of the launcher to get confused. In those cases, check if the# exit code is an integer, and if it's not, handle it as a special error case.if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then echo \"$&#123;CMD[@]&#125;\" | head -n-1 1&gt;&amp;2 exit 1fiif [ $LAUNCHER_EXIT_CODE != 0 ]; then exit $LAUNCHER_EXIT_CODEfiCMD=(\"$&#123;CMD[@]:0:$LAST&#125;\")#执行CMD中的某个参数类org.apache.spark.deploy.SparkSubmitexec \"$&#123;CMD[@]&#125;\" spark-class文件的执行逻辑稍显复杂，总体上应该是这样的： 检查SPARK_HOME的值—-》执行load-spark-env.sh文件，设定一些需要用到的环境变量，如scala环境值，这其中也加载了spark-env.sh文件——-》检查设定java的执行路径变量值——-》寻找spark jars,设定一些引用相关类的位置变量——》执行类文件org.apache.spark.launcher.Main，返回解析后的参数给CMD——-》判断解析参数是否正确（代表了用户设置的参数是否正确）——–》正确的话执行org.apache.spark.deploy.SparkSubmit这个类 2.4 SparkSubmit2.1最后提交语句，D:\\src\\spark-2.3.0\\core\\src\\main\\scala\\org\\apache\\spark\\deploy\\SparkSubmit.scala 1exec \"$&#123;SPARK_HOME&#125;\"/bin/spark-class org.apache.spark.deploy.SparkSubmit \"$@\" 123456789101112131415161718192021override def main(args: Array[String]): Unit = &#123; // Initialize logging if it hasn't been done yet. Keep track of whether logging needs to // be reset before the application starts. val uninitLog = initializeLogIfNecessary(true, silent = true) //拿到submit脚本传入的参数 val appArgs = new SparkSubmitArguments(args) if (appArgs.verbose) &#123; // scalastyle:off println printStream.println(appArgs) // scalastyle:on println &#125; //根据传入的参数匹配对应的执行方法 appArgs.action match &#123; //根据传入的参数提交命令 case SparkSubmitAction.SUBMIT =&gt; submit(appArgs, uninitLog) //只有standalone和mesos集群模式才触发 case SparkSubmitAction.KILL =&gt; kill(appArgs) //只有standalone和mesos集群模式才触发 case SparkSubmitAction.REQUEST_STATUS =&gt; requestStatus(appArgs) &#125; &#125; 2.4.1 submit十分关键，主要分为两步骤（1）调用prepareSubmitEnvironment （2）调用doRunMain","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本","slug":"2019-06-15-Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本","date":"2019-06-15T02:30:04.000Z","updated":"2019-09-17T04:21:07.772Z","comments":true,"path":"2019-06-15-Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-15-Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本.html","excerpt":"** Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本","text":"** Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本 &lt;The rest of contents | 余下全文&gt; 一、启动脚本分析独立部署模式下，主要由master和slaves组成，master可以利用zk实现高可用性，其driver，work，app等信息可以持久化到zk上；slaves由一台至多台主机构成。Driver通过向Master申请资源获取运行环境。 启动master和slaves主要是执行/usr/dahua/spark/sbin目录下的start-master.sh和start-slaves.sh，或者执行 start-all.sh，其中star-all.sh本质上就是调用start-master.sh和start-slaves.sh 1.1 start-all.sh12345678910111213#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi#2.执行$&#123;SPARK_HOME&#125;/sbin/spark-config.sh，见以下分析. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"#3.执行\"$&#123;SPARK_HOME&#125;/sbin\"/start-master.sh，见以下分析\"$&#123;SPARK_HOME&#125;/sbin\"/start-master.sh#4.执行\"$&#123;SPARK_HOME&#125;/sbin\"/start-slaves.sh，见以下分析\"$&#123;SPARK_HOME&#125;/sbin\"/star` t-slaves.sh 其中start-master.sh和start-slave.sh分别调用的是 org.apache.spark.deploy.master.Master和org.apache.spark.deploy.worker.Worker 1.2 start-master.shstart-master.sh调用了spark-daemon.sh，注意这里指定了启动的类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi# NOTE: This exact class name is matched downstream by SparkSubmit.# Any changes need to be reflected there.#2.设置CLASS=\"org.apache.spark.deploy.master.Master\"CLASS=\"org.apache.spark.deploy.master.Master\"#3.如果参数结尾包含--help或者-h则打印帮助信息，并退出if [[ \"$@\" = *--help ]] || [[ \"$@\" = *-h ]]; then echo \"Usage: ./sbin/start-master.sh [options]\" pattern=\"Usage:\" pattern+=\"\\|Using Spark's default log4j profile:\" pattern+=\"\\|Registered signal handlers for\" \"$&#123;SPARK_HOME&#125;\"/bin/spark-class $CLASS --help 2&gt;&amp;1 | grep -v \"$pattern\" 1&gt;&amp;2 exit 1fi#4.设置ORIGINAL_ARGS为所有参数ORIGINAL_ARGS=\"$@\"#5.执行$&#123;SPARK_HOME&#125;/sbin/spark-config.sh. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"#6.执行$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh. \"$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh\"#7.SPARK_MASTER_PORT为空则赋值7077if [ \"$SPARK_MASTER_PORT\" = \"\" ]; then SPARK_MASTER_PORT=7077fi#8.SPARK_MASTER_HOST为空则赋值本主机名(hostname)if [ \"$SPARK_MASTER_HOST\" = \"\" ]; then case `uname` in (SunOS) SPARK_MASTER_HOST=\"`/usr/sbin/check-hostname | awk '&#123;print $NF&#125;'`\" ;; (*) SPARK_MASTER_HOST=\"`hostname -f`\" ;; esacfi#9.SPARK_MASTER_WEBUI_PORT为空则赋值8080if [ \"$SPARK_MASTER_WEBUI_PORT\" = \"\" ]; then SPARK_MASTER_WEBUI_PORT=8080fi#10.执行脚本\"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start $CLASS 1 \\ --host $SPARK_MASTER_HOST --port $SPARK_MASTER_PORT --webui-port $SPARK_MASTER_WEBUI_PORT \\ $ORIGINAL_ARGS 其中10肯定是重点，分析之前我们看看5，6都干了些啥，最后直译出最后一个脚本 1.3 spark-config.sh(1.2的第5步)123456789101112#判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi#SPARK_CONF_DIR存在就用此目录，不存在用$&#123;SPARK_HOME&#125;/confexport SPARK_CONF_DIR=\"$&#123;SPARK_CONF_DIR:-\"$&#123;SPARK_HOME&#125;/conf\"&#125;\"# Add the PySpark classes to the PYTHONPATH:if [ -z \"$&#123;PYSPARK_PYTHONPATH_SET&#125;\" ]; then export PYTHONPATH=\"$&#123;SPARK_HOME&#125;/python:$&#123;PYTHONPATH&#125;\" export PYTHONPATH=\"$&#123;SPARK_HOME&#125;/python/lib/py4j-0.10.6-src.zip:$&#123;PYTHONPATH&#125;\" export PYSPARK_PYTHONPATH_SET=1fi 1.4 load-spark-env.sh(1.2的第6步)12345678910111213141516171819202122232425262728293031323334353637#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then source \"$(dirname \"$0\")\"/find-spark-homefi#2.判断SPARK_ENV_LOADED是否有值，没有将其设置为1if [ -z \"$SPARK_ENV_LOADED\" ]; then export SPARK_ENV_LOADED=1#3.设置user_conf_dir为SPARK_CONF_DIR或SPARK_HOME/conf export SPARK_CONF_DIR=\"$&#123;SPARK_CONF_DIR:-\"$&#123;SPARK_HOME&#125;\"/conf&#125;\"#4.执行\"$&#123;user_conf_dir&#125;/spark-env.sh\" [注：set -/+a含义再做研究] if [ -f \"$&#123;SPARK_CONF_DIR&#125;/spark-env.sh\" ]; then # Promote all variable declarations to environment (exported) variables set -a . \"$&#123;SPARK_CONF_DIR&#125;/spark-env.sh\" set +a fifi# Setting SPARK_SCALA_VERSION if not already set.#5.选择scala版本，2.11和2.12都存在的情况下，优先选择2.11if [ -z \"$SPARK_SCALA_VERSION\" ]; then ASSEMBLY_DIR2=\"$&#123;SPARK_HOME&#125;/assembly/target/scala-2.11\" ASSEMBLY_DIR1=\"$&#123;SPARK_HOME&#125;/assembly/target/scala-2.12\" if [[ -d \"$ASSEMBLY_DIR2\" &amp;&amp; -d \"$ASSEMBLY_DIR1\" ]]; then echo -e \"Presence of build for multiple Scala versions detected.\" 1&gt;&amp;2 echo -e 'Either clean one of them or, export SPARK_SCALA_VERSION in spark-env.sh.' 1&gt;&amp;2 exit 1 fi if [ -d \"$ASSEMBLY_DIR2\" ]; then export SPARK_SCALA_VERSION=\"2.11\" else export SPARK_SCALA_VERSION=\"2.12\" fifi 1.5 spark-env.sh列举很多种模式的选项配置 1.6 spark-daemon.sh回过头来看看1.2第10步中需要直译出的最后一个脚本,如下： 1sbin/spark-daemon.sh start org.apache.spark.deploy.master.Master 1 --host hostname --port 7077 --webui-port 8080 上面搞了半天只是设置了变量，最终才进入主角，继续分析spark-daemon.sh脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223#1.参数个数小于等于1，打印帮助if [ $# -le 1 ]; then echo $usage exit 1fi#2.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi#3.执行$&#123;SPARK_HOME&#125;/sbin/spark-config.sh，见上述分析 [类似脚本是否有重复？原因是有的人是直接用spark-daemon.sh启动的服务，反正重复设置下变量不需要什么代价]. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"# get arguments# Check if --config is passed as an argument. It is an optional parameter.# Exit if the argument is not a directory.#4.判断第一个参数是否是--config,如果是取空格后一个字符串，然后判断该目录是否存在，不存在则打印错误信息并退出，存在设置SPARK_CONF_DIR为该目录,shift到下一个参数#[注：--config只能用在第一参数上]if [ \"$1\" == \"--config\" ]then shift conf_dir=\"$1\" if [ ! -d \"$conf_dir\" ] then echo \"ERROR : $conf_dir is not a directory\" echo $usage exit 1 else export SPARK_CONF_DIR=\"$conf_dir\" fi shiftfi#5.分别设置option、command、instance为后面的三个参数(如：option=start,command=org.apache.spark.deploy.master.Master,instance=1)#[注：很多人用spark-daemon.sh启动服务不成功的原因是名字不全]option=$1shiftcommand=$1shiftinstance=$1shift#6.日志回滚函数，主要用于更改日志名，如log--&gt;log.1等，略过spark_rotate_log ()&#123; log=$1; num=5; if [ -n \"$2\" ]; then num=$2 fi if [ -f \"$log\" ]; then # rotate logs while [ $num -gt 1 ]; do prev=`expr $num - 1` [ -f \"$log.$prev\" ] &amp;&amp; mv \"$log.$prev\" \"$log.$num\" num=$prev done mv \"$log\" \"$log.$num\"; fi&#125;#7.执行$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh\"#8.判断SPARK_IDENT_STRING是否有值，没有将其设置为$USER(linux用户)if [ \"$SPARK_IDENT_STRING\" = \"\" ]; then export SPARK_IDENT_STRING=\"$USER\"fi#9.设置SPARK_PRINT_LAUNCH_COMMAND=1export SPARK_PRINT_LAUNCH_COMMAND=\"1\"# get log directory#10.判断SPARK_LOG_DIR是否有值，没有将其设置为$&#123;SPARK_HOME&#125;/logs，并创建改目录，测试创建文件，修改权限if [ \"$SPARK_LOG_DIR\" = \"\" ]; then export SPARK_LOG_DIR=\"$&#123;SPARK_HOME&#125;/logs\"fimkdir -p \"$SPARK_LOG_DIR\"touch \"$SPARK_LOG_DIR\"/.spark_test &gt; /dev/null 2&gt;&amp;1TEST_LOG_DIR=$?if [ \"$&#123;TEST_LOG_DIR&#125;\" = \"0\" ]; then rm -f \"$SPARK_LOG_DIR\"/.spark_testelse chown \"$SPARK_IDENT_STRING\" \"$SPARK_LOG_DIR\"fi#11.判断SPARK_PID_DIR是否有值，没有将其设置为/tmpif [ \"$SPARK_PID_DIR\" = \"\" ]; then SPARK_PID_DIR=/tmpfi# some variables#12.设置log和pidlog=\"$SPARK_LOG_DIR/spark-$SPARK_IDENT_STRING-$command-$instance-$HOSTNAME.out\"pid=\"$SPARK_PID_DIR/spark-$SPARK_IDENT_STRING-$command-$instance.pid\"# Set default scheduling priority#13.判断SPARK_NICENESS是否有值，没有将其设置为0 [注：调度优先级，见后面]if [ \"$SPARK_NICENESS\" = \"\" ]; then export SPARK_NICENESS=0fi#14.execute_command()函数，暂且略过，调用时再作分析execute_command() &#123; if [ -z $&#123;SPARK_NO_DAEMONIZE+set&#125; ]; then nohup -- \"$@\" &gt;&gt; $log 2&gt;&amp;1 &lt; /dev/null &amp; newpid=\"$!\" echo \"$newpid\" &gt; \"$pid\" # Poll for up to 5 seconds for the java process to start for i in &#123;1..10&#125; do if [[ $(ps -p \"$newpid\" -o comm=) =~ \"java\" ]]; then break fi sleep 0.5 done sleep 2 # Check if the process has died; in that case we'll tail the log so the user can see if [[ ! $(ps -p \"$newpid\" -o comm=) =~ \"java\" ]]; then echo \"failed to launch: $@\" tail -10 \"$log\" | sed 's/^/ /' echo \"full log in $log\" fi else \"$@\" fi&#125;#15.进入case语句，判断option值，进入该分支，我们以start为例# 执行run_command class \"$@\"，其中$@此时为空，经验证，启动带上此参数后，关闭也需，不然关闭不了，后面再分析此参数作用# 我们正式进入run_command()函数，分析# I.设置mode=class,创建SPARK_PID_DIR，上面的pid文件是否存在，# II.SPARK_MASTER不为空，同步删除某些文件# III.回滚log日志# IV.进入case，command=org.apache.spark.deploy.master.Master，最终执行# nohup nice -n \"$SPARK_NICENESS\" \"$&#123;SPARK_HOME&#125;\"/bin/spark-class $command \"$@\" &gt;&gt; \"$log\" 2&gt;&amp;1 &lt; /dev/null &amp;# newpid=\"$!\"# echo \"$newpid\" &gt; \"$pid\"# 重点转向bin/spark-class org.apache.spark.deploy.master.Masterrun_command() &#123; mode=\"$1\" shift mkdir -p \"$SPARK_PID_DIR\" if [ -f \"$pid\" ]; then TARGET_ID=\"$(cat \"$pid\")\" if [[ $(ps -p \"$TARGET_ID\" -o comm=) =~ \"java\" ]]; then echo \"$command running as process $TARGET_ID. Stop it first.\" exit 1 fi fi if [ \"$SPARK_MASTER\" != \"\" ]; then echo rsync from \"$SPARK_MASTER\" rsync -a -e ssh --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' \"$SPARK_MASTER/\" \"$&#123;SPARK_HOME&#125;\" fi spark_rotate_log \"$log\" echo \"starting $command, logging to $log\" case \"$mode\" in (class) execute_command nice -n \"$SPARK_NICENESS\" \"$&#123;SPARK_HOME&#125;\"/bin/spark-class \"$command\" \"$@\" ;; (submit) execute_command nice -n \"$SPARK_NICENESS\" bash \"$&#123;SPARK_HOME&#125;\"/bin/spark-submit --class \"$command\" \"$@\" ;; (*) echo \"unknown mode: $mode\" exit 1 ;; esac&#125;case $option in (submit) run_command submit \"$@\" ;; (start) run_command class \"$@\" ;; (stop) if [ -f $pid ]; then TARGET_ID=\"$(cat \"$pid\")\" if [[ $(ps -p \"$TARGET_ID\" -o comm=) =~ \"java\" ]]; then echo \"stopping $command\" kill \"$TARGET_ID\" &amp;&amp; rm -f \"$pid\" else echo \"no $command to stop\" fi else echo \"no $command to stop\" fi ;; (status) if [ -f $pid ]; then TARGET_ID=\"$(cat \"$pid\")\" if [[ $(ps -p \"$TARGET_ID\" -o comm=) =~ \"java\" ]]; then echo $command is running. exit 0 else echo $pid file is present but $command not running exit 1 fi else echo $command not running. exit 2 fi ;; (*) echo $usage exit 1 ;;esac 1.7 spark-class1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then source \"$(dirname \"$0\")\"/find-spark-homefi#2.执行$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh，见上述分析. \"$&#123;SPARK_HOME&#125;\"/bin/load-spark-env.sh# Find the java binary#3.判断JAVA_HOME是否为NULL，不是则设置RUNNER=\"$&#123;JAVA_HOME&#125;/bin/java\"，否则找系统自带，在没有则报未设置，并退出if [ -n \"$&#123;JAVA_HOME&#125;\" ]; then RUNNER=\"$&#123;JAVA_HOME&#125;/bin/java\"else if [ \"$(command -v java)\" ]; then RUNNER=\"java\" else echo \"JAVA_HOME is not set\" &gt;&amp;2 exit 1 fifi# Find Spark jars.#4.查找SPARK_JARS_DIR，若$&#123;SPARK_HOME&#125;/RELEASE文件存在，则SPARK_JARS_DIR=\"$&#123;SPARK_HOME&#125;/jars\"，否则#SPARK_JARS_DIR=\"$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars\"if [ -d \"$&#123;SPARK_HOME&#125;/jars\" ]; then SPARK_JARS_DIR=\"$&#123;SPARK_HOME&#125;/jars\"else SPARK_JARS_DIR=\"$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars\"fi#5.若SPARK_JARS_DIR不存在且$SPARK_TESTING$SPARK_SQL_TESTING有值[注：一般我们不设置这两变量]，报错退出，否则LAUNCH_CLASSPATH=\"$SPARK_JARS_DIR/*\"if [ ! -d \"$SPARK_JARS_DIR\" ] &amp;&amp; [ -z \"$SPARK_TESTING$SPARK_SQL_TESTING\" ]; then echo \"Failed to find Spark jars directory ($SPARK_JARS_DIR).\" 1&gt;&amp;2 echo \"You need to build Spark with the target \\\"package\\\" before running this program.\" 1&gt;&amp;2 exit 1else LAUNCH_CLASSPATH=\"$SPARK_JARS_DIR/*\"fi# Add the launcher build dir to the classpath if requested.#6.SPARK_PREPEND_CLASSES不是NULL，则LAUNCH_CLASSPATH=\"$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH\"，#添加编译相关至LAUNCH_CLASSPATHif [ -n \"$SPARK_PREPEND_CLASSES\" ]; then LAUNCH_CLASSPATH=\"$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH\"fi# For tests#7.SPARK_TESTING不是NULL，则unset YARN_CONF_DIR和unset HADOOP_CONF_DIR，暂且当做是为了某种测试if [[ -n \"$SPARK_TESTING\" ]]; then unset YARN_CONF_DIR unset HADOOP_CONF_DIRfi#8.build_command函数，略过build_command() &#123; \"$RUNNER\" -Xmx128m -cp \"$LAUNCH_CLASSPATH\" org.apache.spark.launcher.Main \"$@\" printf \"%d\\0\" $?&#125;# Turn off posix mode since it does not allow process substitutionset +o posixCMD=()while IFS= read -d '' -r ARG; do CMD+=(\"$ARG\") #9.最终调用\"$RUNNER\" -Xmx128m -cp \"$LAUNCH_CLASSPATH\" org.apache.spark.launcher.Main \"$@\"， #直译：java -Xmx128m -cp \"$LAUNCH_CLASSPATH\" org.apache.spark.launcher.Main \"$@\" #转向java类org.apache.spark.launcher.Main，这就是java入口类done &lt; &lt;(build_command \"$@\")COUNT=$&#123;#CMD[@]&#125;LAST=$((COUNT - 1))LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;# Certain JVM failures result in errors being printed to stdout (instead of stderr), which causes# the code that parses the output of the launcher to get confused. In those cases, check if the# exit code is an integer, and if it's not, handle it as a special error case.if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then echo \"$&#123;CMD[@]&#125;\" | head -n-1 1&gt;&amp;2 exit 1fiif [ $LAUNCHER_EXIT_CODE != 0 ]; then exit $LAUNCHER_EXIT_CODEfiCMD=(\"$&#123;CMD[@]:0:$LAST&#125;\")exec \"$&#123;CMD[@]&#125;\" 1.8 start-slaves.sh12345678910111213141516171819202122232425262728293031323334#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi#2.执行$&#123;SPARK_HOME&#125;/sbin/spark-config.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"#3.执行$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh\"# Find the port number for the master#4.SPARK_MASTER_PORT为空则设置为7077if [ \"$SPARK_MASTER_PORT\" = \"\" ]; then SPARK_MASTER_PORT=7077fi#5.SPARK_MASTER_HOST为空则设置为`hostname`if [ \"$SPARK_MASTER_HOST\" = \"\" ]; then case `uname` in (SunOS) SPARK_MASTER_HOST=\"`/usr/sbin/check-hostname | awk '&#123;print $NF&#125;'`\" ;; (*) SPARK_MASTER_HOST=\"`hostname -f`\" ;; esacfi# Launch the slaves#6.启动slaves，# \"$&#123;SPARK_HOME&#125;/sbin/slaves.sh\" cd \"$&#123;SPARK_HOME&#125;\" \\; \"$&#123;SPARK_HOME&#125;/sbin/start-slave.sh\" \"spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT\"# 遍历conf/slaves中主机，其中有设置SPARK_SSH_OPTS，ssh每一台机器执行\"$&#123;SPARK_HOME&#125;/sbin/start-slave.sh\" \"spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT\"\"$&#123;SPARK_HOME&#125;/sbin/slaves.sh\" cd \"$&#123;SPARK_HOME&#125;\" \\; \"$&#123;SPARK_HOME&#125;/sbin/start-slave.sh\" \"spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT\" 1.9 转向start-slave.sh1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi#2.设置CLASS=\"org.apache.spark.deploy.worker.Worker\"CLASS=\"org.apache.spark.deploy.worker.Worker\"#3.如果参数结尾包含--help或者-h则打印帮助信息，并退出if [[ $# -lt 1 ]] || [[ \"$@\" = *--help ]] || [[ \"$@\" = *-h ]]; then echo \"Usage: ./sbin/start-slave.sh [options] &lt;master&gt;\" pattern=\"Usage:\" pattern+=\"\\|Using Spark's default log4j profile:\" pattern+=\"\\|Registered signal handlers for\" \"$&#123;SPARK_HOME&#125;\"/bin/spark-class $CLASS --help 2&gt;&amp;1 | grep -v \"$pattern\" 1&gt;&amp;2 exit 1fi#4.执行$&#123;SPARK_HOME&#125;/sbin/spark-config.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"#5.执行$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh\"#6.MASTER=$1,这里MASTER=spark://hostname:7077，然后shift，也就是说单独启动单个slave使用start-slave.sh spark://hostname:7077MASTER=$1shift#7.SPARK_WORKER_WEBUI_PORT为空则设置为8081if [ \"$SPARK_WORKER_WEBUI_PORT\" = \"\" ]; then SPARK_WORKER_WEBUI_PORT=8081fi#8.函数start_instance，略过function start_instance &#123;#设置WORKER_NUM=$1 WORKER_NUM=$1 shift if [ \"$SPARK_WORKER_PORT\" = \"\" ]; then PORT_FLAG= PORT_NUM= else PORT_FLAG=\"--port\" PORT_NUM=$(( $SPARK_WORKER_PORT + $WORKER_NUM - 1 )) fi WEBUI_PORT=$(( $SPARK_WORKER_WEBUI_PORT + $WORKER_NUM - 1 )) #直译：spark-daemon.sh start org.apache.spark.deploy.worker.Worker 1 --webui-port 7077 spark://hostname:7077 #代码再次转向spark-daemon.sh，见上诉分析 \"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start $CLASS $WORKER_NUM \\ --webui-port \"$WEBUI_PORT\" $PORT_FLAG $PORT_NUM $MASTER \"$@\"&#125;#9.判断SPARK_WORKER_INSTANCES(可以认为是单节点Worker进程数)是否为空# 为空，则start_instance 1 \"$@\"# 不为空，则循环# for ((i=0; i&lt;$SPARK_WORKER_INSTANCES; i++)); do# start_instance $(( 1 + $i )) \"$@\"# doneif [ \"$SPARK_WORKER_INSTANCES\" = \"\" ]; then start_instance 1 \"$@\"else for ((i=0; i&lt;$SPARK_WORKER_INSTANCES; i++)); do #10.转向start_instance函数 start_instance $(( 1 + $i )) \"$@\" donefi 二、其他脚本2.1 start-history-server.sh1234567891011#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi#2.执行$&#123;SPARK_HOME&#125;/sbin/spark-config.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"#3.执行$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh\"#4.exec \"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start org.apache.spark.deploy.history.HistoryServer 1 $@ ，见上诉分析exec \"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start org.apache.spark.deploy.history.HistoryServer 1 \"$@\" 2.2 start-shuffle-service.sh1234567891011#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi#2.执行$&#123;SPARK_HOME&#125;/sbin/spark-config.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"#3.执行$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh\"#4.exec \"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start org.apache.spark.deploy.ExternalShuffleService 1 ，见上诉分析exec \"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start org.apache.spark.deploy.ExternalShuffleService 1","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器","slug":"2019-06-14-Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器","date":"2019-06-14T02:30:04.000Z","updated":"2019-09-17T02:15:20.277Z","comments":true,"path":"2019-06-14-Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-14-Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器.html","excerpt":"** Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器：** &lt;Excerpt in index | 首页摘要&gt; ​ 垃圾收集 Garbage Collection 通常被称为“GC”，它诞生于1960年 MIT 的 Lisp 语言，经过半个多世纪，目前已经十分成熟了。 jvm 中，程序计数器、虚拟机栈、本地方法栈都是随线程而生随线程而灭，栈帧随着方法的进入和退出做入栈和出栈操作，实现了自动的内存清理，因此，我们的内存垃圾回收主要集中于 java 堆和方法区中，在程序运行期间，这部分内存的分配和使用都是动态的。","text":"** Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器：** &lt;Excerpt in index | 首页摘要&gt; ​ 垃圾收集 Garbage Collection 通常被称为“GC”，它诞生于1960年 MIT 的 Lisp 语言，经过半个多世纪，目前已经十分成熟了。 jvm 中，程序计数器、虚拟机栈、本地方法栈都是随线程而生随线程而灭，栈帧随着方法的进入和退出做入栈和出栈操作，实现了自动的内存清理，因此，我们的内存垃圾回收主要集中于 java 堆和方法区中，在程序运行期间，这部分内存的分配和使用都是动态的。 &lt;The rest of contents | 余下全文&gt; 一、垃圾收集器(garbage collector (GC)) 是什么？GC其实是一种自动的内存管理工具，其行为主要包括2步 在Java堆中，为新创建的对象分配空间 在Java堆中，回收没用的对象占用的空间 二、为什么需要GC？**释放开发人员的生产力 三、为什么需要多种GC？**首先，Java平台被部署在各种各样的硬件资源上，其次，在Java平台上部署和运行着各种各样的应用，并且用户对不同的应用的 性能指标 (吞吐率和延迟) 预期也不同，为了满足不同应用的对内存管理的不同需求，JVM提供了多种GC以供选择 性能指标最大停顿时长：垃圾回收导致的应用停顿时间的最大值吞吐率：垃圾回收停顿时长和应用运行总时长的比例 不同的GC能满足不同应用不同的性能需求，现有的GC包括： 序列化GC(serial garbage collector)：适合占用内存少的应用 并行GC 或 吞吐率GC(parallel or throughput garbage collector)：适合占用内存较多，多CPU，追求高吞吐率的应用 并发GC：适合占用内存较多，多CPU的应用，对延迟有要求的应用 四、对象存活的判断判断对象是否存活一般有两种方式： 引用计数：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，缺点是无法解决对象相互循环引用的问题。 可达性分析（Reachability Analysis）：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。不可达对象。 在Java语言中，GC Roots包括： 虚拟机栈中引用的对象。 方法区中类静态属性实体引用的对象。 方法区中常量引用的对象。 本地方法栈中JNI引用的对象。 由于循环引用的问题，一般采用跟踪（可达性分析）方法 五、垃圾回收算法5.1 标记 -清除算法“标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。 它的主要缺点有两个：一个是效率问题，标记和清除过程的效率都不高；另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 5.2 复制算法“复制”（Copying）的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，持续复制长生存期的对象则导致效率降低。 5.3 标记-整理算法复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。 根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存 5.4 分代收集算法GC分代的基本假设：绝大部分对象的生命周期都非常短暂，存活时间短。 “分代收集”（Generational Collection）算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或“标记-整理”算法来进行回收。 六、垃圾收集器如果说收集算法是内存回收的方法论，垃圾收集器就是内存回收的具体实现，不同厂商、不同版本的虚拟机实现差别很大，HotSpot中包含的收集器如下： 6.1 Serial收集器串行收集器是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。新生代、老年代使用串行回收；新生代复制算法、老年代标记-压缩；垃圾收集的过程中会Stop The World（服务暂停） 参数控制：-XX:+UseSerialGC 串行收集器 6.2 ParNew收集器ParNew收集器其实就是Serial收集器的多线程版本。新生代并行，老年代串行；新生代复制算法、老年代标记-压缩 参数控制：-XX:+UseParNewGC ParNew收集器 -XX:ParallelGCThreads 限制线程数量 6.3 Parallel收集器Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。可以通过参数来打开自适应调节策略，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或最大的吞吐量；也可以通过参数控制GC的时间不大于多少毫秒或者比例；新生代复制算法、老年代标记-压缩 参数控制：-XX:+UseParallelGC 使用Parallel收集器+ 老年代串行 6.4 CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用都集中在互联网站或B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。 从名字（包含“Mark Sweep”）上就可以看出CMS收集器是基于“标记-清除”算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为4个步骤，包括： 初始标记（CMS initial mark） 并发标记（CMS concurrent mark） 重新标记（CMS remark） 并发清除（CMS concurrent sweep） 其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。 由于整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，所以总体上来说，CMS收集器的内存回收过程是与用户线程一起并发地执行。老年代收集器（新生代使用ParNew） 优点:并发收集、低停顿 缺点：产生大量空间碎片、并发阶段会降低吞吐量 参数控制：-XX:+UseConcMarkSweepGC 使用CMS收集器 ​ -XX:+ UseCMSCompactAtFullCollection Full GC后，进行一次碎片整理；整理过程是独占的，会引起停顿时间变长 ​ -XX:+CMSFullGCsBeforeCompaction 设置进行几次Full GC后，进行一次碎片整理 ​ -XX:ParallelCMSThreads 设定CMS的线程数量（一般情况约等于可用CPU数量） 6.5 G1收集器G1是目前技术发展的最前沿成果之一，HotSpot开发团队赋予它的使命是未来可以替换掉JDK1.5中发布的CMS收集器。与CMS收集器相比G1收集器有以下特点： 空间整合，G1收集器采用标记整理算法，不会产生内存空间碎片。分配大对象时不会因为无法找到连续空间而提前触发下一次GC。 可预测停顿，这是G1的另一大优势，降低停顿时间是G1和CMS的共同关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为N毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了。 上面提到的垃圾收集器，收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔阂了，它们都是一部分（可以不连续）Region的集合。 G1对Heap的划分 G1的新生代收集跟ParNew类似，当新生代占用达到一定比例的时候，开始出发收集。和CMS类似，G1收集器收集老年代对象会有短暂停顿。 收集步骤1、标记阶段，首先初始标记(Initial-Mark),这个阶段是停顿的(Stop the World Event)，并且会触发一次普通Mintor GC。对应GC log:GC pause (young) (inital-mark) 2、Root Region Scanning，程序运行过程中会回收survivor区(存活到老年代)，这一过程必须在young GC之前完成。 3、Concurrent Marking，在整个堆中进行并发标记(和应用程序并发执行)，此过程可能被young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那个这个区域会被立即回收(图中打X)。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。 4、Remark, 再标记，会有短暂停顿(STW)。再标记阶段是用来收集 并发标记阶段 产生新的垃圾(并发阶段和应用程序一同运行)；G1中采用了比CMS更快的初始快照算法:snapshot-at-the-beginning (SATB)。 5、Copy/Clean up，多线程清除失活对象，会有STW。G1将回收区域的存活对象拷贝到新区域，清除Remember Sets，并发清空回收区域并把它返回到空闲区域链表中。 6、复制/清除过程后。回收区域的活性对象已经被集中回收到深蓝色和深绿色区域。 八、常用的收集器组合 新生代GC策略 年老代GC策略 说明 组合1 Serial Serial Old Serial和Serial Old都是单线程进行GC，特点就是GC时暂停所有应用线程。 组合2 Serial CMS+Serial Old CMS（Concurrent Mark Sweep）是并发GC，实现GC线程和应用线程并发工作，不需要暂停所有应用线程。另外，当CMS进行GC失败时，会自动使用Serial Old策略进行GC。 组合3 ParNew CMS 使用-XX:+UseParNewGC选项来开启。ParNew是Serial的并行版本，可以指定GC线程数，默认GC线程数为CPU的数量。可以使用-XX:ParallelGCThreads选项指定GC的线程数。如果指定了选项-XX:+UseConcMarkSweepGC选项，则新生代默认使用ParNew GC策略。 组合4 ParNew Serial Old 使用-XX:+UseParNewGC选项来开启。新生代使用ParNew GC策略，年老代默认使用Serial Old GC策略。 组合5 Parallel Scavenge Serial Old Parallel Scavenge策略主要是关注一个可控的吞吐量：应用程序运行时间 / (应用程序运行时间 + GC时间)，可见这会使得CPU的利用率尽可能的高，适用于后台持久运行的应用程序，而不适用于交互较多的应用程序。 组合6 Parallel Scavenge Parallel Old Parallel Old是Serial Old的并行版本 组合7 G1GC G1GC -XX:+UnlockExperimentalVMOptions -XX:+UseG1GC #开启 -XX:MaxGCPauseMillis =50 #暂停时间目标 -XX:GCPauseIntervalMillis =200 #暂停间隔目标 -XX:+G1YoungGenSize=512m #年轻代大小 -XX:SurvivorRatio=6 #幸存区比例","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构","slug":"2019-06-13-Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构","date":"2019-06-13T02:30:04.000Z","updated":"2019-09-17T01:42:22.631Z","comments":true,"path":"2019-06-13-Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-13-Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构.html","excerpt":"** Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt; 一、JVM的结构图1.1 Java内存结构 JVM内存结构主要有三大块：堆内存、方法区和栈。 堆内存是JVM中最大的一块由年轻代和老年代组成，而年轻代内存又被分成三部分，Eden空间、From Survivor空间、To Survivor空间,默认情况下年轻代按照8:1:1的比例来分配； 方法区存储类信息、常量、静态变量等数据，是线程共享的区域，为与Java堆区分，方法区还有一个别名Non-Heap(非堆)； 栈又分为java虚拟机栈和本地方法栈主要用于方法的执行。 1.2 如何通过参数来控制各区域的内存大小 1.3 控制参数-Xms设置堆的最小空间大小。 -Xmx设置堆的最大空间大小。 -XX:NewSize设置新生代最小空间大小。 -XX:MaxNewSize设置新生代最大空间大小。 -XX:PermSize设置永久代最小空间大小。 -XX:MaxPermSize设置永久代最大空间大小。 -Xss设置每个线程的堆栈大小。 没有直接设置老年代的参数，但是可以设置堆空间大小和新生代空间大小两个参数来间接控制。 老年代空间大小=堆空间大小-年轻代大空间大小 1.4 JVM和系统调用之间的关系 方法区和堆是所有线程共享的内存区域；而java栈、本地方法栈和程序员计数器是运行是线程私有的内存区域。 二、JVM各区域的作用2.1 Java堆（Heap）​ 对于大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 ​ Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”。如果从内存回收的角度看，由于现在收集器基本都是采用的分代收集算法，所以Java堆中还可以细分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。 根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx和-Xms控制）。 如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 2.2 方法区（Method Area） 方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。 对于习惯在HotSpot虚拟机上开发和部署程序的开发者来说，很多人愿意把方法区称为“永久代”（Permanent Generation），本质上两者并不等价，仅仅是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已。 Java虚拟机规范对这个区域的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是有必要的。 根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 2.3 程序计数器（Program Counter Register）程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie方法，这个计数器值则为空（Undefined）。 此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 2.4 JVM栈（JVM Stacks）与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，根据不同的虚拟机实现，它可能是一个指向对象起始地址的引用指针，也可能指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。 其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。 2.5 本地方法栈（Native Method Stacks）本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十二）SparkCore的调优之资源调优","slug":"2019-06-12-Spark学习之路 （十二）SparkCore的调优之资源调优","date":"2019-06-12T02:30:04.000Z","updated":"2019-09-17T01:35:58.907Z","comments":true,"path":"2019-06-12-Spark学习之路 （十二）SparkCore的调优之资源调优.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-12-Spark学习之路 （十二）SparkCore的调优之资源调优.html","excerpt":"** Spark学习之路 （十二）SparkCore的调优之资源调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。","text":"** Spark学习之路 （十二）SparkCore的调优之资源调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。 &lt;The rest of contents | 余下全文&gt; 一、Spark作业基本运行原理 详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。 在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。 Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。 当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。 因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。 task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。 以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。 二、资源参数调优了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。 2.1 num-executors 参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。 参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。 2.2 executor-memory 参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。 参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。 2.3 executor-cores 参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。 参数调优建议：Executor的CPU core数量设置为24个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/31/2左右比较合适，也是避免影响其他同学的作业运行。最好的应该就是一个cpu core对应两到三个task 2.4 driver-memory 参数说明：该参数用于设置Driver进程的内存。 参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。 2.5 spark.default.parallelism 参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。一个分区对应一个task，也就是这个参数其实就是设置task的数量 参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。 2.6 spark.storage.memoryFraction 参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。 参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 2.7 spark.shuffle.memoryFraction 参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。 参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十一）SparkCore的调优之Spark内存模型","slug":"2019-06-11-Spark学习之路 （十一）SparkCore的调优之Spark内存模型","date":"2019-06-11T02:30:04.000Z","updated":"2019-09-17T01:21:32.711Z","comments":true,"path":"2019-06-11-Spark学习之路 （十一）SparkCore的调优之Spark内存模型.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-11-Spark学习之路 （十一）SparkCore的调优之Spark内存模型.html","excerpt":"** Spark学习之路 （十一）SparkCore的调优之Spark内存模型：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark 作为一个基于内存的分布式计算引擎，其内存管理模块在整个系统中扮演着非常重要的角色。理解 Spark 内存管理的基本原理，有助于更好地开发 Spark 应用程序和进行性能调优。本文旨在梳理出 Spark 内存管理的脉络，抛砖引玉，引出读者对这个话题的深入探讨。本文中阐述的原理基于 Spark 2.1 版本，阅读本文需要读者有一定的 Spark 和 Java 基础，了解 RDD、Shuffle、JVM 等相关概念。 在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能[1]。由于 Driver 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。","text":"** Spark学习之路 （十一）SparkCore的调优之Spark内存模型：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark 作为一个基于内存的分布式计算引擎，其内存管理模块在整个系统中扮演着非常重要的角色。理解 Spark 内存管理的基本原理，有助于更好地开发 Spark 应用程序和进行性能调优。本文旨在梳理出 Spark 内存管理的脉络，抛砖引玉，引出读者对这个话题的深入探讨。本文中阐述的原理基于 Spark 2.1 版本，阅读本文需要读者有一定的 Spark 和 Java 基础，了解 RDD、Shuffle、JVM 等相关概念。 在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能[1]。由于 Driver 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。 &lt;The rest of contents | 余下全文&gt; 1. 堆内和堆外内存规划​ 作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内（On-heap）空间进行了更为详细的分配，以充分利用内存。同时，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。 图 1 . 堆内和堆外内存示意图 1.1 堆内内存堆内内存的大小，由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置。Executor 内运行的并发任务共享 JVM 堆内内存，这些任务在缓存 RDD 数据和广播（Broadcast）数据时占用的内存被规划为存储（Storage）内存，而这些任务在执行 Shuffle 时占用的内存被规划为执行（Execution）内存，剩余的部分不做特殊规划，那些 Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间。不同的管理模式下，这三部分占用的空间大小各不相同（下面第 2 小节会进行介绍）。 Spark 对堆内内存的管理是一种逻辑上的”规划式”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前记录这些内存，我们来看其具体流程： 申请内存： Spark 在代码中 new 一个对象实例 JVM 从堆内内存分配空间，创建对象并返回对象引用 Spark 保存该对象的引用，记录该对象占用的内存 释放内存： Spark 记录该对象释放的内存，删除该对象的引用 等待 JVM 的垃圾回收机制释放该对象占用的堆内内存 我们知道，JVM 的对象可以以序列化的方式存储，序列化的过程是将对象转换为二进制字节流，本质上可以理解为将非连续空间的链式存储转化为连续空间或块存储，在访问时则需要进行序列化的逆过程——反序列化，将字节流转化为对象，序列化的方式可以节省存储空间，但增加了存储和读取时候的计算开销。 对于 Spark 中序列化的对象，由于是字节流的形式，其占用的内存大小可直接计算，而对于非序列化的对象，其占用的内存是通过周期性地采样近似估算而得，即并不是每次新增的数据项都会计算一次占用的内存大小，这种方法降低了时间开销但是有可能误差较大，导致某一时刻的实际内存有可能远远超出预期[2]。此外，在被 Spark 标记为释放的对象实例，很有可能在实际上并没有被 JVM 回收，导致实际可用的内存小于 Spark 记录的可用内存。所以 Spark 并不能准确记录实际可用的堆内内存，从而也就无法完全避免内存溢出（OOM, Out of Memory）的异常。 虽然不能精准控制堆内内存的申请和释放，但 Spark 通过对存储内存和执行内存各自独立的规划管理，可以决定是否要在存储内存里缓存新的 RDD，以及是否为新的任务分配执行内存，在一定程度上可以提升内存的利用率，减少异常的出现。 1.2 堆外内存为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。利用 JDK Unsafe API（从 Spark 2.0 开始，在管理堆外的存储内存时不再基于 Tachyon，而是与堆外的执行内存一样，基于 JDK Unsafe API 实现[3]），Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。堆外内存可以被精确地申请和释放，而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。 在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存。 1.3 内存管理接口Spark 为存储内存和执行内存的管理提供了统一的接口——MemoryManager，同一个 Executor 内的任务都调用这个接口的方法来申请或释放内存: 清单 1 . 内存管理接口的主要方法123456789101112//申请存储内存def acquireStorageMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean//申请展开内存def acquireUnrollMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean//申请执行内存def acquireExecutionMemory(numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long//释放存储内存def releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit//释放执行内存def releaseExecutionMemory(numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Unit//释放展开内存def releaseUnrollMemory(numBytes: Long, memoryMode: MemoryMode): Unit 我们看到，在调用这些方法时都需要指定其内存模式（MemoryMode），这个参数决定了是在堆内还是堆外完成这次操作。 MemoryManager 的具体实现上，Spark 1.6 之后默认为统一管理（Unified Memory Manager）方式，1.6 之前采用的静态管理（Static Memory Manager）方式仍被保留，可通过配置 spark.memory.useLegacyMode 参数启用。两种方式的区别在于对空间分配的方式，下面的第 2 小节会分别对这两种方式进行介绍。 2 . 内存空间分配2.1 静态内存管理在 Spark 最初采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置，堆内内存的分配如图 2 所示： 图 2 . 静态内存管理图示——堆内 可以看到，可用的堆内内存的大小需要按照下面的方式计算： 清单 2 . 可用堆内内存空间12可用的存储内存 = systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction可用的执行内存 = systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction 其中 systemMaxMemory 取决于当前 JVM 堆内内存的大小，最后可用的执行内存或者存储内存要在此基础上与各自的 memoryFraction 参数和 safetyFraction 参数相乘得出。上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 1-safetyFraction 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险（上文提到，对于非序列化对象的内存采样估算会产生误差）。值得注意的是，这个预留的保险区域仅仅是一种逻辑上的规划，在具体使用时 Spark 并没有区别对待，和”其它内存”一样交给了 JVM 去管理。 堆外的空间分配较为简单，只有存储内存和执行内存，如图 3 所示。可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域。 图 3 . 静态内存管理图示——堆外 静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成”一半海水，一半火焰”的局面，即存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。 2.2 统一内存管理Spark 1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域，如图 4 和图 5 所示 图 4 . 统一内存管理图示——堆内 图 5 . 统一内存管理图示——堆外 其中最重要的优化在于动态占用机制，其规则如下： 设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围 双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block） 执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间 存储内存的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂[4] 图 6 . 动态占用机制图示 凭借统一内存管理机制，Spark 在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护 Spark 内存的难度，但并不意味着开发者可以高枕无忧。譬如，所以如果存储内存的空间太大或者说缓存的数据过多，反而会导致频繁的全量垃圾回收，降低任务执行时的性能，因为缓存的 RDD 数据通常都是长期驻留内存的 [5] 。所以要想充分发挥 Spark 的性能，需要开发者进一步了解存储内存和执行内存各自的管理方式和实现原理。 3. 存储内存管理3.1 RDD 的持久化机制弹性分布式数据集（RDD）作为 Spark 最根本的数据抽象，是只读的分区记录（Partition）的集合，只能基于在稳定物理存储中的数据集上创建，或者在其他已有的 RDD 上执行转换（Transformation）操作产生一个新的 RDD。转换后的 RDD 与原始的 RDD 之间产生的依赖关系，构成了血统（Lineage）。凭借血统，Spark 保证了每一个 RDD 都可以被重新恢复。但 RDD 的所有转换都是惰性的，即只有当一个返回结果给 Driver 的行动（Action）发生时，Spark 才会创建任务读取 RDD，然后真正触发转换的执行。Task 在启动之初读取一个分区时，会先判断这个分区是否已经被持久化，如果没有则需要检查 Checkpoint 或按照血统重新计算。所以如果一个 RDD 上要执行多次行动，可以在第一次行动中使用 persist 或 cache 方法，在内存或磁盘中持久化或缓存这个 RDD，从而在后面的行动时提升计算速度。事实上，cache 方法是使用默认的 MEMORY_ONLY 的存储级别将 RDD 持久化到内存，故缓存是一种特殊的持久化。 堆内和堆外存储内存的设计，便可以对缓存 RDD 时使用的内存做统一的规划和管 理 （存储内存的其他应用场景，如缓存 broadcast 数据，暂时不在本文的讨论范围之内）。 RDD 的持久化由 Spark 的 Storage 模块 [7] 负责，实现了 RDD 与物理存储的解耦合。Storage 模块负责管理 Spark 在计算过程中产生的数据，将那些在内存或磁盘、在本地或远程存取数据的功能封装了起来。在具体实现时 Driver 端和 Executor 端的 Storage 模块构成了主从式的架构，即 Driver 端的 BlockManager 为 Master，Executor 端的 BlockManager 为 Slave。Storage 模块在逻辑上以 Block 为基本存储单位，RDD 的每个 Partition 经过处理后唯一对应一个 Block（BlockId 的格式为 rdd_RDD-ID_PARTITION-ID ）。Master 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 Slave 需要将 Block 的更新等状态上报到 Master，同时接收 Master 的命令，例如新增或删除一个 RDD。 图 7 . Storage 模块示意图 在对 RDD 持久化时，Spark 规定了 MEMORY_ONLY、MEMORY_AND_DISK 等12 种不同的 存储级别 ，而存储级别是以下 5 个变量的组合： 清单 3 . 存储级别1234567class StorageLevel private( private var _useDisk: Boolean, //磁盘 private var _useMemory: Boolean, //这里其实是指堆内内存 private var _useOffHeap: Boolean, //堆外内存 private var _deserialized: Boolean, //是否为非序列化 private var _replication: Int = 1 //副本个数) 通过对数据结构的分析，可以看出存储级别从三个维度定义了 RDD 的 Partition（同时也就是 Block）的存储方式： 存储位置：磁盘／堆内内存／堆外内存。如 MEMORY_AND_DISK 是同时在磁盘和堆内内存上存储，实现了冗余备份。OFF_HEAP 则是只在堆外内存存储，目前选择堆外内存时不能同时存储到其他位置。 存储形式：Block 缓存到存储内存后，是否为非序列化的形式。如 MEMORY_ONLY 是非序列化方式存储，OFF_HEAP 是序列化方式存储。 副本数量：大于 1 时需要远程冗余备份到其他节点。如 DISK_ONLY_2 需要远程备份 1 个副本。 3.2 RDD 缓存的过程RDD 在缓存到存储内存之前，Partition 中的数据一般以迭代器（Iterator）的数据结构来访问，这是 Scala 语言中一种遍历数据集合的方法。通过 Iterator 可以获取分区中每一条序列化或者非序列化的数据项(Record)，这些 Record 的对象实例在逻辑上占用了 JVM 堆内内存的 other 部分的空间，同一 Partition 的不同 Record 的空间并不连续。 RDD 在缓存到存储内存之后，Partition 被转换成 Block，Record 在堆内或堆外存储内存中占用一块连续的空间。将Partition由不连续的存储空间转换为连续存储空间的过程，Spark称之为”展开”（Unroll）。Block 有序列化和非序列化两种存储格式，具体以哪种方式取决于该 RDD 的存储级别。非序列化的 Block 以一种 DeserializedMemoryEntry 的数据结构定义，用一个数组存储所有的对象实例，序列化的 Block 则以 SerializedMemoryEntry的数据结构定义，用字节缓冲区（ByteBuffer）来存储二进制数据。每个 Executor 的 Storage 模块用一个链式 Map 结构（LinkedHashMap）来管理堆内和堆外存储内存中所有的 Block 对象的实例[6]，对这个 LinkedHashMap 新增和删除间接记录了内存的申请和释放。 因为不能保证存储空间可以一次容纳 Iterator 中的所有数据，当前的计算任务在 Unroll 时要向 MemoryManager 申请足够的 Unroll 空间来临时占位，空间不足则 Unroll 失败，空间足够时可以继续进行。对于序列化的 Partition，其所需的 Unroll 空间可以直接累加计算，一次申请。而非序列化的 Partition 则要在遍历 Record 的过程中依次申请，即每读取一条 Record，采样估算其所需的 Unroll 空间并进行申请，空间不足时可以中断，释放已占用的 Unroll 空间。如果最终 Unroll 成功，当前 Partition 所占用的 Unroll 空间被转换为正常的缓存 RDD 的存储空间，如下图 8 所示。 图 8. Spark Unroll 示意图 在图 3 和图 5 中可以看到，在静态内存管理时，Spark 在存储内存中专门划分了一块 Unroll 空间，其大小是固定的，统一内存管理时则没有对 Unroll 空间进行特别区分，当存储空间不足时会根据动态占用机制进行处理。 3.3 淘汰和落盘由于同一个 Executor 的所有的计算任务共享有限的存储内存空间，当有新的 Block 需要缓存但是剩余空间不足且无法动态占用时，就要对 LinkedHashMap 中的旧 Block 进行淘汰（Eviction），而被淘汰的 Block 如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘（Drop），否则直接删除该 Block。 存储内存的淘汰规则为： 被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存 新旧 Block 不能属于同一个 RDD，避免循环淘汰 旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题 遍历 LinkedHashMap 中 Block，按照最近最少使用（LRU）的顺序淘汰，直到满足新 Block 所需的空间。其中 LRU 是 LinkedHashMap 的特性。 落盘的流程则比较简单，如果其存储级别符合_useDisk 为 true 的条件，再根据其_deserialized 判断是否是非序列化的形式，若是则对其进行序列化，最后将数据存储到磁盘，在 Storage 模块中更新其信息。 4. 执行内存管理4.1 多任务间内存分配Executor 内运行的任务同样共享执行内存，Spark 用一个 HashMap 结构保存了任务到内存耗费的映射。每个任务可占用的执行内存大小的范围为 1/2N ~ 1/N，其中 N 为当前 Executor 内正在运行的任务的个数。每个任务在启动之时，要向 MemoryManager 请求申请最少为 1/2N 的执行内存，如果不能被满足要求则该任务被阻塞，直到有其他任务释放了足够的执行内存，该任务才可以被唤醒。 4.2 Shuffle 的内存占用执行内存主要用来存储任务在执行 Shuffle 时占用的内存，Shuffle 是按照一定规则对 RDD 数据重新分区的过程，我们来看 Shuffle 的 Write 和 Read 两阶段对执行内存的使用： Shuffle Write 若在 map 端选择普通的排序方式，会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间。 若在 map 端选择 Tungsten 的排序方式，则采用 ShuffleExternalSorter 直接对以序列化形式存储的数据排序，在内存中存储数据时可以占用堆外或堆内执行空间，取决于用户是否开启了堆外内存以及堆外执行内存是否足够。 Shuffle Read 在对 reduce 端的数据进行聚合时，要将数据交给 Aggregator 处理，在内存中存储数据时占用堆内执行空间。 如果需要进行最终结果排序，则要将再次将数据交给 ExternalSorter 处理，占用堆内执行空间。 在 ExternalSorter 和 Aggregator 中，Spark 会使用一种叫 AppendOnlyMap 的哈希表在堆内执行内存中存储数据，但在 Shuffle 过程中所有数据并不能都保存到该哈希表中，当这个哈希表占用的内存会进行周期性地采样估算，当其大到一定程度，无法再从 MemoryManager 申请到新的执行内存时，Spark 就会将其全部内容存储到磁盘文件中，这个过程被称为溢存(Spill)，溢存到磁盘的文件最后会被归并(Merge)。 Shuffle Write 阶段中用到的 Tungsten 是 Databricks 公司提出的对 Spark 优化内存和 CPU 使用的计划[9]，解决了一些 JVM 在性能上的限制和弊端。Spark 会根据 Shuffle 的情况来自动选择是否采用 Tungsten 排序。Tungsten 采用的页式内存管理机制建立在 MemoryManager 之上，即 Tungsten 对执行内存的使用进行了一步的抽象，这样在 Shuffle 过程中无需关心数据具体存储在堆内还是堆外。每个内存页用一个 MemoryBlock 来定义，并用 Object obj 和 long offset 这两个变量统一标识一个内存页在系统内存中的地址。堆内的 MemoryBlock 是以 long 型数组的形式分配的内存，其 obj 的值为是这个数组的对象引用，offset 是 long 型数组的在 JVM 中的初始偏移地址，两者配合使用可以定位这个数组在堆内的绝对地址；堆外的 MemoryBlock 是直接申请到的内存块，其 obj 为 null，offset 是这个内存块在系统内存中的 64 位绝对地址。Spark 用 MemoryBlock 巧妙地将堆内和堆外内存页统一抽象封装，并用页表(pageTable)管理每个 Task 申请到的内存页。 Tungsten 页式管理下的所有内存用 64 位的逻辑地址表示，由页号和页内偏移量组成： 页号：占 13 位，唯一标识一个内存页，Spark 在申请内存页之前要先申请空闲页号。 页内偏移量：占 51 位，是在使用内存页存储数据时，数据在页内的偏移地址。 有了统一的寻址方式，Spark 可以用 64 位逻辑地址的指针定位到堆内或堆外的内存，整个 Shuffle Write 排序的过程只需要对指针进行排序，并且无需反序列化，整个过程非常高效，对于内存访问效率和 CPU 使用效率带来了明显的提升[10]。 Spark 的存储内存和执行内存有着截然不同的管理方式：对于存储内存来说，Spark 用一个 LinkedHashMap 来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成；而对于执行内存，Spark 用 AppendOnlyMap 来存储 Shuffle 过程中的数据，在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制。 转自：https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十）SparkCore的调优之Shuffle调优","slug":"2019-06-10-Spark学习之路 （十）SparkCore的调优之Shuffle调优","date":"2019-06-10T02:30:04.000Z","updated":"2019-09-17T00:42:16.880Z","comments":true,"path":"2019-06-10-Spark学习之路 （十）SparkCore的调优之Shuffle调优.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-10-Spark学习之路 （十）SparkCore的调优之Shuffle调优.html","excerpt":"** Spark学习之路 （十）SparkCore的调优之Shuffle调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。","text":"** Spark学习之路 （十）SparkCore的调优之Shuffle调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。 &lt;The rest of contents | 余下全文&gt; 一、shuffle的定义Spark的运行主要分为2部分： 一部分是驱动程序，其核心是SparkContext； 另一部分是Worker节点上Task,它是运行实际任务的。程序运行的时候，Driver和Executor进程相互交互：运行什么任务，即Driver会分配Task到Executor，Driver 跟 Executor 进行网络传输; 任务数据从哪儿获取，即Task要从 Driver 抓取其他上游的 Task 的数据结果，所以有这个过程中就不断的产生网络结果。其中，下一个 Stage 向上一个 Stage 要数据这个过程，我们就称之为 Shuffle。 二、ShuffleManager发展概述​ 在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。 在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。 因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。 下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。 三、HashShuffleManager的运行原理在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。 在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。 因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。 下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。 3.1 未经优化的HashShuffleManager图解说明 文字说明上图说明了未经优化的HashShuffleManager的原理。这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。 我们先从shuffle write开始说起。shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。 那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢？很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。 接着我们来说说shuffle read。shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。 shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。 3.2 优化后的HashShuffleManager图解说明 文字说明上图说明了优化后的HashShuffleManager的原理。这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。 开启consolidate机制之后，在shuffle write过程中，task就不是为下游stage的每个task创建一个磁盘文件了。此时会出现shuffleFileGroup的概念，每个shuffleFileGroup会对应一批磁盘文件，磁盘文件的数量与下游stage的task数量是相同的。一个Executor上有多少个CPU core，就可以并行执行多少个task。而第一批并行执行的每个task都会创建一个shuffleFileGroup，并将数据写入对应的磁盘文件内。 当Executor的CPU core执行完一批task，接着执行下一批task时，下一批task就会复用之前已有的shuffleFileGroup，包括其中的磁盘文件。也就是说，此时task会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。 假设第二个stage有100个task，第一个stage有50个task，总共还是有10个Executor，每个Executor执行5个task。那么原本使用未经优化的HashShuffleManager时，每个Executor会产生500个磁盘文件，所有Executor会产生5000个磁盘文件的。但是此时经过优化之后，每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。也就是说，每个Executor此时只会创建100个磁盘文件，所有Executor只会创建1000个磁盘文件。 四、SortShuffleManager运行原理SortShuffleManager的运行机制主要分成两种，一种是普通运行机制，另一种是bypass运行机制。当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。 4.1 普通运行机制图解说明 文字说明上图说明了普通的SortShuffleManager的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。 在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。 一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。 SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于每个task最终只有一个磁盘文件，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。 4.2 bypass运行机制图解说明 文字说明上图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下： shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。 不是聚合类的shuffle算子（比如reduceByKey）。 此时task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。 该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。 而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。 五、shuffle相关参数调优以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。 Spark各个版本的参数默认值可能会有不同，具体使用请参考官方网站的说明： （1）先选择对应的Spark版本：http://spark.apache.org/documentation.html （2）再查看对应的文档说明 spark.shuffle.file.buffer 默认值：32k 参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 spark.reducer.maxSizeInFlight 默认值：48m 参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 spark.shuffle.io.maxRetries 默认值：3 参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。 调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。 spark.shuffle.io.retryWait 默认值：5s 参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。 调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。 spark.shuffle.memoryFraction（已经弃用） 默认值：0.2 参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。 调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。 spark.shuffle.manager（已经弃用） 默认值：sort 参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。 调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。 spark.shuffle.sort.bypassMergeThreshold 默认值：200 参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。 调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。 spark.shuffle.consolidateFiles（已经弃用） 默认值：false 参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。 调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （九）SparkCore的调优之数据倾斜调优","slug":"2019-06-09-Spark学习之路 （九）SparkCore的调优之数据倾斜调优","date":"2019-06-09T02:30:04.000Z","updated":"2019-09-17T00:27:40.324Z","comments":true,"path":"2019-06-09-Spark学习之路 （九）SparkCore的调优之数据倾斜调优.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-09-Spark学习之路 （九）SparkCore的调优之数据倾斜调优.html","excerpt":"** Spark学习之路 （九）SparkCore的调优之数据倾斜调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。","text":"** Spark学习之路 （九）SparkCore的调优之数据倾斜调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。 &lt;The rest of contents | 余下全文&gt; 数据倾斜发生时的现象 绝大多数task执行得都非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。 原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。 数据倾斜发生的原理数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。 因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。 下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。 如何定位导致数据倾斜的代码数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。 某个task执行特别慢的情况首先要看的，就是数据倾斜发生在第几个stage中。 如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。 比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。 知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。 这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。 stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。 stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。 123456789val conf = new SparkConf()val sc = new SparkContext(conf)val lines = sc.textFile(\"hdfs://...\")val words = lines.flatMap(_.split(\" \"))val pairs = words.map((_, 1))val wordCounts = pairs.reduceByKey(_ + _)wordCounts.collect().foreach(println(_)) ​ 通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由reduceByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。 某个task莫名其妙内存溢出的情况​ 这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。 但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。 查看导致数据倾斜的key的数据分布情况​ 知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。 此时根据你执行操作的情况不同，可以有很多种查看key分布的方式： 如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。 如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。 举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。 123val sampledPairs = pairs.sample(false, 0.1)val sampledWordCounts = sampledPairs.countByKey()sampledWordCounts.foreach(println(_)) 数据倾斜的解决方案解决方案一：使用Hive ETL预处理数据方案适用场景：导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。 方案实现思路：此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。 方案实现原理：这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。 方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。 方案缺点：治标不治本，Hive ETL中还是会发生数据倾斜。 方案实践经验：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。 项目实践经验：在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。 解决方案二：过滤少数导致倾斜的key方案适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。 方案实现思路：如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。 方案实现原理：将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。 方案优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。 方案缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。 方案实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。 解决方案三：提高shuffle操作的并行度方案适用场景：如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。 方案实现思路：在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。 方案实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。 方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。 方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。 方案实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。 解决方案四：两阶段聚合（局部聚合+全局聚合）方案适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。 方案实现思路：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。 方案实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。 方案优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。 方案缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 第一步，给RDD中的每个key都打上一个随机前缀。JavaPairRDD&lt;String, Long&gt; randomPrefixRdd = rdd.mapToPair( new PairFunction&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(10); return new Tuple2&lt;String, Long&gt;(prefix + \"_\" + tuple._1, tuple._2); &#125; &#125;);// 第二步，对打上随机前缀的key进行局部聚合。JavaPairRDD&lt;String, Long&gt; localAggrRdd = randomPrefixRdd.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;);// 第三步，去除RDD中每个key的随机前缀。JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair( new PairFunction&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;String, Long&gt; tuple) throws Exception &#123; long originalKey = Long.valueOf(tuple._1.split(\"_\")[1]); return new Tuple2&lt;Long, Long&gt;(originalKey, tuple._2); &#125; &#125;);// 第四步，对去除了随机前缀的RDD进行全局聚合。JavaPairRDD&lt;Long, Long&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;); 解决方案五：将reduce join转为map join方案适用场景：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。 方案实现思路：不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。 方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。 方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。 方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。 123456789101112131415161718192021222324252627282930313233// 首先将数据量比较小的RDD的数据，collect到Driver中来。List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。// 可以尽可能节省内存空间，并且减少网络传输性能开销。final Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);// 对另外一个RDD执行map类操作，而不再是join类操作。JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; // 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。 List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value(); // 可以将rdd1的数据转换为一个Map，便于后面进行join操作。 Map&lt;Long, Row&gt; rdd1DataMap = new HashMap&lt;Long, Row&gt;(); for(Tuple2&lt;Long, Row&gt; data : rdd1Data) &#123; rdd1DataMap.put(data._1, data._2); &#125; // 获取当前RDD数据的key以及value。 String key = tuple._1; String value = tuple._2; // 从rdd1数据Map中，根据key获取到可以join到的数据。 Row rdd1Value = rdd1DataMap.get(key); return new Tuple2&lt;String, String&gt;(key, new Tuple2&lt;String, Row&gt;(value, rdd1Value)); &#125; &#125;);// 这里得提示一下。// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。// rdd2中每条数据都可能会返回多条join后的数据。 解决方案六：采样倾斜key并分拆join操作方案适用场景：两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。 方案实现思路： 对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。 然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。 接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。 再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。 而另外两个普通的RDD就照常join即可。 最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。 方案实现原理：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。 方案优点：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。 方案缺点：如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。JavaPairRDD&lt;Long, String&gt; sampledRDD = rdd1.sample(false, 0.1);// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。JavaPairRDD&lt;Long, Long&gt; mappedSampledRDD = sampledRDD.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return new Tuple2&lt;Long, Long&gt;(tuple._1, 1L); &#125; &#125;);JavaPairRDD&lt;Long, Long&gt; countedSampledRDD = mappedSampledRDD.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;);JavaPairRDD&lt;Long, Long&gt; reversedSampledRDD = countedSampledRDD.mapToPair( new PairFunction&lt;Tuple2&lt;Long,Long&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple) throws Exception &#123; return new Tuple2&lt;Long, Long&gt;(tuple._2, tuple._1); &#125; &#125;);final Long skewedUserid = reversedSampledRDD.sortByKey(false).take(1).get(0)._2;// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。JavaPairRDD&lt;Long, String&gt; skewedRDD = rdd1.filter( new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return tuple._1.equals(skewedUserid); &#125; &#125;);// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。JavaPairRDD&lt;Long, String&gt; commonRDD = rdd1.filter( new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return !tuple._1.equals(skewedUserid); &#125; &#125;);// rdd2，就是那个所有key的分布相对较为均匀的rdd。// 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。// 对扩容的每条数据，都打上0～100的前缀。JavaPairRDD&lt;String, Row&gt; skewedRdd2 = rdd2.filter( new Function&lt;Tuple2&lt;Long,Row&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; return tuple._1.equals(skewedUserid); &#125; &#125;).flatMapToPair(new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call( Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; Random random = new Random(); List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;(); for(int i = 0; i &lt; 100; i++) &#123; list.add(new Tuple2&lt;String, Row&gt;(i + \"_\" + tuple._1, tuple._2)); &#125; return list; &#125; &#125;);// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD1 = skewedRDD.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(100); return new Tuple2&lt;String, String&gt;(prefix + \"_\" + tuple._1, tuple._2); &#125; &#125;) .join(skewedUserid2infoRDD) .mapToPair(new PairFunction&lt;Tuple2&lt;String,Tuple2&lt;String,Row&gt;&gt;, Long, Tuple2&lt;String, Row&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt; call( Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; tuple) throws Exception &#123; long key = Long.valueOf(tuple._1.split(\"_\")[1]); return new Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt;(key, tuple._2); &#125; &#125;);// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);// 将倾斜key join后的结果与普通key join后的结果，uinon起来。// 就是最终的join结果。JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2); 解决方案七：使用随机前缀和扩容RDD进行join方案适用场景：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。 方案实现思路： 该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。 然后将该RDD的每条数据都打上一个n以内的随机前缀。 同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。 最后将两个处理后的RDD进行join即可。 方案实现原理：将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。 方案优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。 方案缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。 方案实践经验：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。 123456789101112131415161718192021222324252627282930// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。JavaPairRDD&lt;String, Row&gt; expandedRDD = rdd1.flatMapToPair( new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;(); for(int i = 0; i &lt; 100; i++) &#123; list.add(new Tuple2&lt;String, Row&gt;(0 + \"_\" + tuple._1, tuple._2)); &#125; return list; &#125; &#125;);// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。JavaPairRDD&lt;String, String&gt; mappedRDD = rdd2.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(100); return new Tuple2&lt;String, String&gt;(prefix + \"_\" + tuple._1, tuple._2); &#125; &#125;);// 将两个处理后的RDD进行join即可。JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD); 解决方案八：多种方案组合使用​ 在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，预处理一部分数据，并过滤一部分数据来缓解；其次可以对某些shuffle操作提升并行度，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"cache和persist的区别","slug":"2019-06-08-cache和persist的区别","date":"2019-06-08T03:30:04.000Z","updated":"2019-09-16T17:31:38.817Z","comments":true,"path":"2019-06-08-cache和persist的区别.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-08-cache和persist的区别.html","excerpt":"** cache和persist的区别：** &lt;Excerpt in index | 首页摘要&gt; cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间。","text":"** cache和persist的区别：** &lt;Excerpt in index | 首页摘要&gt; cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间。 &lt;The rest of contents | 余下全文&gt; cache和persist的区别基于Spark 1.6.1 的源码，可以看到 12/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */def cache(): this.type = persist() 说明是cache()调用了persist(), 想要知道二者的不同还需要看一下persist函数： 12/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) 可以看到persist()内部调用了persist(StorageLevel.MEMORY_ONLY)，继续深入： 1234567891011121314151617/** * Set this RDD's storage level to persist its values across operations after the first time * it is computed. This can only be used to assign a new storage level if the RDD does not * have a storage level set yet.. */def persist(newLevel: StorageLevel): this.type = &#123; // TODO: Handle changes of StorageLevel if (storageLevel != StorageLevel.NONE &amp;&amp; newLevel != storageLevel) &#123; throw new UnsupportedOperationException( \"Cannot change storage level of an RDD after it was already assigned a level\") &#125; sc.persistRDD(this) // Register the RDD with the ContextCleaner for automatic GC-based cleanup sc.cleaner.foreach(_.registerRDDForCleanup(this)) storageLevel = newLevel this&#125; 可以看出来persist有一个 StorageLevel 类型的参数，这个表示的是RDD的缓存级别。 至此便可得出cache和persist的区别了：cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。 RDD的缓存级别顺便看一下RDD都有哪些缓存级别，查看 StorageLevel 类的源码： 123456789101112131415object StorageLevel &#123; val NONE = new StorageLevel(false, false, false, false) val DISK_ONLY = new StorageLevel(true, false, false, false) val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) val MEMORY_ONLY = new StorageLevel(false, true, false, true) val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) val OFF_HEAP = new StorageLevel(false, false, true, false) ......&#125; 可以看到这里列出了12种缓存级别，但这些有什么区别呢？可以看到每个缓存级别后面都跟了一个StorageLevel的构造函数，里面包含了4个或5个参数，如下 1val MEMORY_ONLY = new StorageLevel(false, true, false, true) 查看其构造函数 123456789101112131415class StorageLevel private( private var _useDisk: Boolean, private var _useMemory: Boolean, private var _useOffHeap: Boolean, private var _deserialized: Boolean, private var _replication: Int = 1) extends Externalizable &#123; ...... def useDisk: Boolean = _useDisk def useMemory: Boolean = _useMemory def useOffHeap: Boolean = _useOffHeap def deserialized: Boolean = _deserialized def replication: Int = _replication ......&#125; 可以看到StorageLevel类的主构造器包含了5个参数： useDisk：使用硬盘（外存） useMemory：使用内存 useOffHeap：使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。 deserialized：反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象 replication：备份数（在多个节点上备份） 理解了这5个参数，StorageLevel 的12种缓存级别就不难理解了。 val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) 就表示使用这种缓存级别的RDD将存储在硬盘以及内存中，使用序列化（在硬盘中），并且在多个节点上备份2份（正常的RDD只有一份） 另外还注意到有一种特殊的缓存级别 1val OFF_HEAP = new StorageLevel(false, false, true, false) 使用了堆外内存，StorageLevel 类的源码中有一段代码可以看出这个的特殊性，它不能和其它几个参数共存。 123456if (useOffHeap) &#123; require(!useDisk, \"Off-heap storage level does not support using disk\") require(!useMemory, \"Off-heap storage level does not support using heap memory\") require(!deserialized, \"Off-heap storage level does not support deserialized storage\") require(replication == 1, \"Off-heap storage level does not support multiple replication\")&#125;","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （八）SparkCore的调优之开发调优","slug":"2019-06-08-Spark学习之路 （八）SparkCore的调优之开发调优","date":"2019-06-08T02:30:04.000Z","updated":"2019-09-16T15:05:55.685Z","comments":true,"path":"2019-06-08-Spark学习之路 （八）SparkCore的调优之开发调优.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-08-Spark学习之路 （八）SparkCore的调优之开发调优.html","excerpt":"** Spark学习之路 （八）SparkCore的调优之开发调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。 ​ 然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。","text":"** Spark学习之路 （八）SparkCore的调优之开发调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。 ​ 然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。 &lt;The rest of contents | 余下全文&gt; 前言​ Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。 ​ 笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。 本文作为Spark性能优化指南的基础篇，主要讲解开发调优以及资源调优。 调优概述​ Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。 原则一：避免创建重复的RDD​ 通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。 我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。 一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。 一个简单的例子1234567891011121314151617// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。val rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\")rdd1.map(...)val rdd2 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\")rdd2.reduce(...)// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。val rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\")rdd1.map(...)rdd1.reduce(...) 原则二：尽可能复用同一个RDD​ 除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。 一个简单的例子1234567891011121314151617181920212223242526// 错误的做法。// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。JavaPairRDD&lt;Long, String&gt; rdd1 = ...JavaRDD&lt;String&gt; rdd2 = rdd1.map(...)// 分别对rdd1和rdd2执行了不同的算子操作。rdd1.reduceByKey(...)rdd2.map(...)// 正确的做法。// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。// 其实在这种情况下完全可以复用同一个RDD。// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。JavaPairRDD&lt;Long, String&gt; rdd1 = ...rdd1.reduceByKey(...)rdd1.map(tuple._2...)// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。 原则三：对多次使用的RDD进行持久化​ 当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。 ​ Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。 ​ 因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。 对多次使用的RDD进行持久化的代码示例1234567891011121314151617// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。// 正确的做法。// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。val rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\").cache()rdd1.map(...)rdd1.reduce(...)// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。val rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\").persist(StorageLevel.MEMORY_AND_DISK_SER)rdd1.map(...)rdd1.reduce(...) 对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。 Spark的持久化级别 持久化级别 含义解释 MEMORY_ONLY 使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。 MEMORY_AND_DISK 使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。 MEMORY_ONLY_SER 基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 MEMORY_AND_DISK_SER 基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 DISK_ONLY 使用未序列化的Java对象格式，将数据全部写入磁盘文件中。 MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等. 对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。 如何选择一种最合适的持久化策略 默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。 如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。 如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。 通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。 原则四：尽量避免使用shuffle类算子​ 如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。 shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。 因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。 Broadcast与map进行join代码示例12345678910111213141516// 传统的join操作会导致shuffle操作。// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。val rdd3 = rdd1.join(rdd2)// Broadcast+map的join操作，不会导致shuffle操作。// 使用Broadcast将一个数据量较小的RDD作为广播变量。val rdd2Data = rdd2.collect()val rdd2DataBroadcast = sc.broadcast(rdd2Data)// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。val rdd3 = rdd1.map(rdd2DataBroadcast...)// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。 原则五：使用map-side预聚合的shuffle操作如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。 ​ 所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。 比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。 原则六：使用高性能的算子除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。 使用reduceByKey/aggregateByKey替代groupByKey详情见“原则五：使用map-side预聚合的shuffle操作”。 使用mapPartitions替代普通map​ mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！ 使用foreachPartitions替代foreach原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。 使用filter之后进行coalesce操作通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。 使用repartitionAndSortWithinPartitions替代repartition与sort类操作repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。 原则七：广播大变量有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。 在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。 因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。 广播大变量的代码示例123456789101112// 以下代码在算子函数中，使用了外部的变量。// 此时没有做任何特殊操作，每个task都会有一份list1的副本。val list1 = ...rdd1.map(list1...)// 以下代码将list1封装成了Broadcast类型的广播变量。// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。// 每个Executor内存中，就只会驻留一份广播变量副本。val list1 = ...val list1Broadcast = sc.broadcast(list1)rdd1.map(list1Broadcast...) 原则八：使用Kryo优化序列化性能在Spark中，主要有三个地方涉及到了序列化： 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。 对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。 以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）： 123456// 创建SparkConf对象。val conf = new SparkConf().setMaster(...).setAppName(...)// 设置序列化器为KryoSerializer。conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")// 注册要序列化的自定义类型。conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2])) 原则九：优化数据结构Java中，有三种类型比较耗费内存： 对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。 字符串，每个字符串内部都有一个字符数组以及长度等额外信息。 集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。 因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。 但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。 原则十：Data Locality本地化级别PROCESS_LOCAL：进程本地化，代码和数据在同一个进程中，也就是在同一个executor中；计算数据的task由executor执行，数据在executor的BlockManager中；性能最好 NODE_LOCAL：节点本地化，代码和数据在同一个节点中；比如说，数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行；或者是，数据和task在一个节点上的不同executor中；数据需要在进程间进行传输NO_PREF：对于task来说，数据从哪里获取都一样，没有好坏之分RACK_LOCAL：机架本地化，数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输ANY：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差 spark.locality.wait，默认是3s Spark在Driver上，对Application的每一个stage的task，进行分配之前，都会计算出每个task要计算的是哪个分片数据，RDD的某个partition；Spark的task分配算法，优先，会希望每个task正好分配到它要计算的数据所在的节点，这样的话，就不用在网络间传输数据； 但是可能task没有机会分配到它的数据所在的节点，因为可能那个节点的计算资源和计算能力都满了；所以呢，这种时候，通常来说，Spark会等待一段时间，默认情况下是3s钟（不是绝对的，还有很多种情况，对不同的本地化级别，都会去等待），到最后，实在是等待不了了，就会选择一个比较差的本地化级别，比如说，将task分配到靠它要计算的数据所在节点，比较近的一个节点，然后进行计算。 但是对于第二种情况，通常来说，肯定是要发生数据传输，task会通过其所在节点的BlockManager来获取数据，BlockManager发现自己本地没有数据，会通过一个getRemote()方法，通过TransferService（网络数据传输组件）从数据所在节点的BlockManager中，获取数据，通过网络传输回task所在节点。 对于我们来说，当然不希望是类似于第二种情况的了。最好的，当然是task和数据在一个节点上，直接从本地executor的BlockManager中获取数据，纯内存，或者带一点磁盘IO；如果要通过网络传输数据的话，那么实在是，性能肯定会下降的，大量网络传输，以及磁盘IO，都是性能的杀手。 什么时候要调节这个参数？ 观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。日志里面会显示，starting task。。。，PROCESS LOCAL、NODE LOCAL，观察大部分task的数据本地化级别。 如果大多都是PROCESS_LOCAL，那就不用调节了如果是发现，好多的级别都是NODE_LOCAL、ANY，那么最好就去调节一下数据本地化的等待时长调节完，应该是要反复调节，每次调节完以后，再来运行，观察日志看看大部分的task的本地化级别有没有提升；看看，整个spark作业的运行时间有没有缩短 但是注意别本末倒置，本地化级别倒是提升了，但是因为大量的等待时长，spark作业的运行时间反而增加了，那就还是不要调节了。 spark.locality.wait，默认是3s；可以改成6s，10s 默认情况下，下面3个的等待时长，都是跟上面那个是一样的，都是3s 123spark.locality.wait.process//建议60sspark.locality.wait.node//建议30sspark.locality.wait.rack//建议20s","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （七）Spark 运行流程","slug":"2019-06-07-Spark学习之路 （七）Spark 运行流程","date":"2019-06-07T02:30:04.000Z","updated":"2019-09-16T15:05:27.815Z","comments":true,"path":"2019-06-07-Spark学习之路 （七）Spark 运行流程.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-07-Spark学习之路 （七）Spark 运行流程.html","excerpt":"** Spark学习之路 （七）Spark 运行流程：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （七）Spark 运行流程","text":"** Spark学习之路 （七）Spark 运行流程：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （七）Spark 运行流程 &lt;The rest of contents | 余下全文&gt; 一、Spark中的基本概念（1）Application：表示你的应用程序 （2）Driver：表示main()函数，创建SparkContext。由SparkContext负责与ClusterManager通信，进行资源的申请，任务的分配和监控等。程序执行完毕后关闭SparkContext （3）Executor：某个Application运行在Worker节点上的一个进程，该进程负责运行某些task，并且负责将数据存在内存或者磁盘上。在Spark on Yarn模式下，其进程名称为 CoarseGrainedExecutor Backend，一个CoarseGrainedExecutor Backend进程有且仅有一个executor对象，它负责将Task包装成taskRunner，并从线程池中抽取出一个空闲线程运行Task，这样，每个CoarseGrainedExecutorBackend能并行运行Task的数据就取决于分配给它的CPU的个数。 （4）Worker：集群中可以运行Application代码的节点。在Standalone模式中指的是通过slave文件配置的worker节点，在Spark on Yarn模式中指的就是NodeManager节点。 （5）Task：在Executor进程中执行任务的工作单元，多个Task组成一个Stage （6）Job：包含多个Task组成的并行计算，是由Action行为触发的 （7）Stage：每个Job会被拆分很多组Task，作为一个TaskSet，其名称为Stage （8）DAGScheduler：根据Job构建基于Stage的DAG，并提交Stage给TaskScheduler，其划分Stage的依据是RDD之间的依赖关系 （9）TaskScheduler：将TaskSet提交给Worker（集群）运行，每个Executor运行什么Task就是在此处分配的。 二、Spark的运行流程2.1 Spark的基本运行流程1、说明 (1)构建Spark Application的运行环境（启动SparkContext），SparkContext向资源管理器（可以是Standalone、Mesos或YARN）注册并申请运行Executor资源； (2)资源管理器分配Executor资源并启动StandaloneExecutorBackend，Executor运行情况将随着心跳发送到资源管理器上； (3)SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler。Executor向SparkContext申请Task (4)Task Scheduler将Task发放给Executor运行同时SparkContext将应用程序代码发放给Executor。 (5)Task在Executor上运行，运行完毕释放所有资源。 2、图解 3、Spark运行架构特点 （1）每个Application获取专属的executor进程，该进程在Application期间一直驻留，并以多线程方式运行tasks。这种Application隔离机制有其优势的，无论是从调度角度看（每个Driver调度它自己的任务），还是从运行角度看（来自不同Application的Task运行在不同的JVM中）。当然，这也意味着Spark Application不能跨应用程序共享数据，除非将数据写入到外部存储系统。 （2）Spark与资源管理器无关，只要能够获取executor进程，并能保持相互通信就可以了。 （3）提交SparkContext的Client应该靠近Worker节点（运行Executor的节点)，最好是在同一个Rack里，因为Spark Application运行过程中SparkContext和Executor之间有大量的信息交换；如果想在远程集群中运行，最好使用RPC将SparkContext提交给集群，不要远离Worker运行SparkContext。 （4）Task采用了数据本地性和推测执行的优化机制。 4、DAGSchedulerJob=多个stage，Stage=多个同种task, Task分为ShuffleMapTask和ResultTask，Dependency分为ShuffleDependency和NarrowDependency 面向stage的切分，切分依据为宽依赖 维护waiting jobs和active jobs，维护waiting stages、active stages和failed stages，以及与jobs的映射关系 主要职能： 1、接收提交Job的主入口，submitJob(rdd, ...)或runJob(rdd, ...)。在SparkContext里会调用这两个方法。 生成一个Stage并提交，接着判断Stage是否有父Stage未完成，若有，提交并等待父Stage，以此类推。结果是：DAGScheduler里增加了一些waiting stage和一个running stage。 running stage提交后，分析stage里Task的类型，生成一个Task描述，即TaskSet。 调用TaskScheduler.submitTask(taskSet, ...)方法，把Task描述提交给TaskScheduler。TaskScheduler依据资源量和触发分配条件，会为这个TaskSet分配资源并触发执行。 DAGScheduler提交job后，异步返回JobWaiter对象，能够返回job运行状态，能够cancel job，执行成功后会处理并返回结果 2、处理TaskCompletionEvent 如果task执行成功，对应的stage里减去这个task，做一些计数工作： 如果task是ResultTask，计数器Accumulator加一，在job里为该task置true，job finish总数加一。加完后如果finish数目与partition数目相等，说明这个stage完成了，标记stage完成，从running stages里减去这个stage，做一些stage移除的清理工作 如果task是ShuffleMapTask，计数器Accumulator加一，在stage里加上一个output location，里面是一个MapStatus类。MapStatus是ShuffleMapTask执行完成的返回，包含location信息和block size(可以选择压缩或未压缩)。同时检查该stage完成，向MapOutputTracker注册本stage里的shuffleId和location信息。然后检查stage的output location里是否存在空，若存在空，说明一些task失败了，整个stage重新提交；否则，继续从waiting stages里提交下一个需要做的stage 如果task是重提交，对应的stage里增加这个task 如果task是fetch失败，马上标记对应的stage完成，从running stages里减去。如果不允许retry，abort整个stage；否则，重新提交整个stage。另外，把这个fetch相关的location和map任务信息，从stage里剔除，从MapOutputTracker注销掉。最后，如果这次fetch的blockManagerId对象不为空，做一次ExecutorLost处理，下次shuffle会换在另一个executor上去执行。 其他task状态会由TaskScheduler处理，如Exception, TaskResultLost, commitDenied等。 3、其他与job相关的操作还包括：cancel job， cancel stage, resubmit failed stage等 其他职能： cacheLocations 和 preferLocation 5、TaskScheduler维护task和executor对应关系，executor和物理资源对应关系，在排队的task和正在跑的task。 内部维护一个任务队列，根据FIFO或Fair策略，调度任务。 TaskScheduler本身是个接口，spark里只实现了一个TaskSchedulerImpl，理论上任务调度可以定制。 主要功能： 1、submitTasks(taskSet)，接收DAGScheduler提交来的tasks 为tasks创建一个TaskSetManager，添加到任务队列里。TaskSetManager跟踪每个task的执行状况，维护了task的许多具体信息。 触发一次资源的索要。 首先，TaskScheduler对照手头的可用资源和Task队列，进行executor分配(考虑优先级、本地化等策略)，符合条件的executor会被分配给TaskSetManager。 然后，得到的Task描述交给SchedulerBackend，调用launchTask(tasks)，触发executor上task的执行。task描述被序列化后发给executor，executor提取task信息，调用task的run()方法执行计算。 2、cancelTasks(stageId)，取消一个stage的tasks 调用SchedulerBackend的killTask(taskId, executorId, ...)方法。taskId和executorId在TaskScheduler里一直维护着。 3、resourceOffer(offers: Seq[Workers])，这是非常重要的一个方法，调用者是SchedulerBacnend，用途是底层资源SchedulerBackend把空余的workers资源交给TaskScheduler，让其根据调度策略为排队的任务分配合理的cpu和内存资源，然后把任务描述列表传回给SchedulerBackend 从worker offers里，搜集executor和host的对应关系、active executors、机架信息等等 worker offers资源列表进行随机洗牌，任务队列里的任务列表依据调度策略进行一次排序 遍历每个taskSet，按照进程本地化、worker本地化、机器本地化、机架本地化的优先级顺序，为每个taskSet提供可用的cpu核数，看是否满足 默认一个task需要一个cpu，设置参数为&quot;spark.task.cpus=1&quot; 为taskSet分配资源，校验是否满足的逻辑，最终在TaskSetManager的resourceOffer(execId, host, maxLocality)方法里 满足的话，会生成最终的任务描述，并且调用DAGScheduler的taskStarted(task, info)方法，通知DAGScheduler，这时候每次会触发DAGScheduler做一次submitMissingStage的尝试，即stage的tasks都分配到了资源的话，马上会被提交执行 4、statusUpdate(taskId, taskState, data),另一个非常重要的方法，调用者是SchedulerBacnend，用途是SchedulerBacnend会将task执行的状态汇报给TaskScheduler做一些决定 若TaskLost，找到该task对应的executor，从active executor里移除，避免这个executor被分配到其他task继续失败下去。 task finish包括四种状态：finished, killed, failed, lost。只有finished是成功执行完成了。其他三种是失败。 task成功执行完，调用TaskResultGetter.enqueueSuccessfulTask(taskSet, tid, data)，否则调用TaskResultGetter.enqueueFailedTask(taskSet, tid, state, data)。TaskResultGetter内部维护了一个线程池，负责异步fetch task执行结果并反序列化。默认开四个线程做这件事，可配参数&quot;spark.resultGetter.threads&quot;=4。 TaskResultGetter取task result的逻辑 1、对于success task，如果taskResult里的数据是直接结果数据，直接把data反序列出来得到结果；如果不是，会调用blockManager.getRemoteBytes(blockId)从远程获取。如果远程取回的数据是空的，那么会调用TaskScheduler.handleFailedTask，告诉它这个任务是完成了的但是数据是丢失的。否则，取到数据之后会通知BlockManagerMaster移除这个block信息，调用TaskScheduler.handleSuccessfulTask，告诉它这个任务是执行成功的，并且把result data传回去。 2、对于failed task，从data里解析出fail的理由，调用TaskScheduler.handleFailedTask，告诉它这个任务失败了，理由是什么。 6、SchedulerBackend在TaskScheduler下层，用于对接不同的资源管理系统，SchedulerBackend是个接口，需要实现的主要方法如下： 12345def start(): Unitdef stop(): Unitdef reviveOffers(): Unit // 重要方法：SchedulerBackend把自己手头上的可用资源交给TaskScheduler，TaskScheduler根据调度策略分配给排队的任务吗，返回一批可执行的任务描述，SchedulerBackend负责launchTask，即最终把task塞到了executor模型上，executor里的线程池会执行task的run()def killTask(taskId: Long, executorId: String, interruptThread: Boolean): Unit = throw new UnsupportedOperationException 粗粒度：进程常驻的模式，典型代表是standalone模式，mesos粗粒度模式，yarn 细粒度：mesos细粒度模式 这里讨论粗粒度模式，更好理解：CoarseGrainedSchedulerBackend。 维护executor相关信息(包括executor的地址、通信端口、host、总核数，剩余核数)，手头上executor有多少被注册使用了，有多少剩余，总共还有多少核是空的等等。 主要职能 1、Driver端主要通过actor监听和处理下面这些事件： RegisterExecutor(executorId, hostPort, cores, logUrls)。这是executor添加的来源，通常worker拉起、重启会触发executor的注册。CoarseGrainedSchedulerBackend把这些executor维护起来，更新内部的资源信息，比如总核数增加。最后调用一次makeOffer()，即把手头资源丢给TaskScheduler去分配一次，返回任务描述回来，把任务launch起来。这个makeOffer()的调用会出现在任何与资源变化相关的事件中，下面会看到。 StatusUpdate(executorId, taskId, state, data)。task的状态回调。首先，调用TaskScheduler.statusUpdate上报上去。然后，判断这个task是否执行结束了，结束了的话把executor上的freeCore加回去，调用一次makeOffer()。 ReviveOffers。这个事件就是别人直接向SchedulerBackend请求资源，直接调用makeOffer()。 KillTask(taskId, executorId, interruptThread)。这个killTask的事件，会被发送给executor的actor，executor会处理KillTask这个事件。 StopExecutors。通知每一个executor，处理StopExecutor事件。 RemoveExecutor(executorId, reason)。从维护信息中，那这堆executor涉及的资源数减掉，然后调用TaskScheduler.executorLost()方法，通知上层我这边有一批资源不能用了，你处理下吧。TaskScheduler会继续把executorLost的事件上报给DAGScheduler，原因是DAGScheduler关心shuffle任务的output location。DAGScheduler会告诉BlockManager这个executor不可用了，移走它，然后把所有的stage的shuffleOutput信息都遍历一遍，移走这个executor，并且把更新后的shuffleOutput信息注册到MapOutputTracker上，最后清理下本地的CachedLocationsMap。 2、reviveOffers()方法的实现。直接调用了makeOffers()方法，得到一批可执行的任务描述，调用launchTasks。 3、launchTasks(tasks: Seq[Seq[TaskDescription]])方法。 遍历每个task描述，序列化成二进制，然后发送给每个对应的executor这个任务信息 如果这个二进制信息太大，超过了9.2M(默认的akkaFrameSize 10M 减去 默认 为akka留空的200K)，会出错，abort整个taskSet，并打印提醒增大akka frame size 如果二进制数据大小可接受，发送给executor的actor，处理LaunchTask(serializedTask)事件。 7、ExecutorExecutor是spark里的进程模型，可以套用到不同的资源管理系统上，与SchedulerBackend配合使用。 内部有个线程池，有个running tasks map，有个actor，接收上面提到的由SchedulerBackend发来的事件。 事件处理 launchTask。根据task描述，生成一个TaskRunner线程，丢尽running tasks map里，用线程池执行这个TaskRunner killTask。从running tasks map里拿出线程对象，调它的kill方法。 三、Spark在不同集群中的运行架构Spark注重建立良好的生态系统，它不仅支持多种外部文件存储系统，提供了多种多样的集群运行模式。部署在单台机器上时，既可以用本地（Local）模式运行，也可以使用伪分布式模式来运行；当以分布式集群部署的时候，可以根据自己集群的实际情况选择Standalone模式（Spark自带的模式）、YARN-Client模式或者YARN-Cluster模式。Spark的各种运行模式虽然在启动方式、运行位置、调度策略上各有不同，但它们的目的基本都是一致的，就是在合适的位置安全可靠的根据用户的配置和Job的需要运行和管理Task。 3.1 Spark on Standalone运行过程Standalone模式是Spark实现的资源调度框架，其主要的节点有Client节点、Master节点和Worker节点。其中Driver既可以运行在Master节点上中，也可以运行在本地Client端。当用spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行；当使用spark-submit工具提交Job或者在Eclips、IDEA等开发平台上使用”new SparkConf().setMaster(“spark://master:7077”)”方式运行Spark任务时，Driver是运行在本地Client端上的。 运行过程文字说明 1、我们提交一个任务，任务就叫Application2、初始化程序的入口SparkContext， 2.1 初始化DAG Scheduler 2.2 初始化Task Scheduler3、Task Scheduler向master去进行注册并申请资源（CPU Core和Memory）4、Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上获取资源，然后启动StandaloneExecutorBackend；顺便初 始化好了一个线程池5、StandaloneExecutorBackend向Driver(SparkContext)注册,这样Driver就知道哪些Executor为他进行服务了。 到这个时候其实我们的初始化过程基本完成了，我们开始执行transformation的代码，但是代码并不会真正的运行，直到我们遇到一个action操作。生产一个job任务，进行stage的划分6、SparkContext将Applicaiton代码发送给StandaloneExecutorBackend；并且SparkContext解析Applicaiton代码，构建DAG图，并提交给DAG Scheduler分解成Stage（当碰到Action操作 时，就会催生Job；每个Job中含有1个或多个Stage，Stage一般在获取外部数据和shuffle之前产生）。7、将Stage（或者称为TaskSet）提交给Task Scheduler。Task Scheduler负责将Task分配到相应的Worker，最后提交给StandaloneExecutorBackend执行；8、对task进行序列化，并根据task的分配算法，分配task9、对接收过来的task进行反序列化，把task封装成一个线程10、开始执行Task，并向SparkContext报告，直至Task完成。11、资源注销 运行过程图形说明 3.2 Spark on YARN运行过程YARN是一种统一资源管理机制，在其上面可以运行多套计算框架。目前的大数据技术世界，大多数公司除了使用Spark来进行数据计算，由于历史原因或者单方面业务处理的性能考虑而使用着其他的计算框架，比如MapReduce、Storm等计算框架。Spark基于此种情况开发了Spark on YARN的运行模式，由于借助了YARN良好的弹性资源管理机制，不仅部署Application更加方便，而且用户在YARN集群中运行的服务和Application的资源也完全隔离，更具实践应用价值的是YARN可以通过队列的方式，管理同时运行在集群中的多个服务。 Spark on YARN模式根据Driver在集群中的位置分为两种模式：一种是YARN-Client模式，另一种是YARN-Cluster（或称为YARN-Standalone模式）。 3.2.1 YARN框架流程任何框架与YARN的结合，都必须遵循YARN的开发模式。在分析Spark on YARN的实现细节之前，有必要先分析一下YARN框架的一些基本原理。 参考：http://www.cnblogs.com/qingyunzong/p/8615096.html 3.2.2 YARN-ClientYarn-Client模式中，Driver在客户端本地运行，这种模式可以使得Spark Application和客户端进行交互，因为Driver在客户端，所以可以通过webUI访问Driver的状态，默认是http://hadoop1:4040访问，而YARN通过http:// hadoop1:8088访问。 YARN-client的工作流程分为以下几个步骤： 文字说明 1.Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend； 2.ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派； 3.Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）； 4.一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task； 5.Client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务； 6.应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。 图片说明 3.2.3 YARN-Cluster在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动；第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成。 YARN-cluster的工作流程分为以下几个步骤： 文字说明 Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等； ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化； ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束； 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等； ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务； 应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己。 图片说明 3.2.4 YARN-Client 与 YARN-Cluster 区别理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别。 1、YARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业； 2、YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"map与flatMap的区别","slug":"2019-06-06-map与flatMap的区别","date":"2019-06-06T04:30:04.000Z","updated":"2019-09-16T09:39:14.905Z","comments":true,"path":"2019-06-06-map与flatMap的区别.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-06-map与flatMap的区别.html","excerpt":"** map与flatMap的区别：** &lt;Excerpt in index | 首页摘要&gt; ​ map与flatMap的区别","text":"** map与flatMap的区别：** &lt;Excerpt in index | 首页摘要&gt; ​ map与flatMap的区别 &lt;The rest of contents | 余下全文&gt; spark的转换算子中map和flatMap都十分常见，要了解清楚它们的区别，我们必须弄懂每执行一次的数据结构是什么。 we are superman torrow is good color green red 总结：map操作结果：Array[Array[String]] = Array(Array(we, are, superman), Array(torrow, is, good), Array(color, green, red)) flatmap操作结果：Array[String] = Array(we, are, superman, torrow, is, good, color, green, red) spark中map函数会对每一条输入进行指定操作，然后为每一条输入返回一个对象； 而flatmap函数则是两个操作的集合，最后将所有对象合并为一个对象。需要特别说明一下，flatmap适用于统计文件单词类的。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （五）Spark伪分布式安装","slug":"2019-06-05-Spark学习之路 （五）Spark伪分布式安装","date":"2019-06-05T02:30:04.000Z","updated":"2019-09-16T08:27:02.534Z","comments":true,"path":"2019-06-05-Spark学习之路 （五）Spark伪分布式安装.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-05-Spark学习之路 （五）Spark伪分布式安装.html","excerpt":"** Spark学习之路 （五）Spark伪分布式安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （五）Spark伪分布式安装","text":"** Spark学习之路 （五）Spark伪分布式安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （五）Spark伪分布式安装 &lt;The rest of contents | 余下全文&gt; Hadoop部分建议参考hadoop伪分布部署 一、JDK的安装​ LINUX系统安装jdk（最好是1.8版本） 1.1 上传安装包并解压1[root@hadoop1 soft]# tar -zxvf jdk-8u73-linux-x64.tar.gz -C /usr/local/ 1.2 配置环境变量12345[root@hadoop1 soft]# vi /etc/profile#JAVAexport JAVA_HOME=/usr/local/jdk1.8.0_73export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH:$HOME/bin 1.3 验证Java版本1[root@hadoop1 soft]# java -version 二、配置免密登陆2.1 检测正常情况下，本机通过ssh连接自己也是需要输入密码的 2.2 生成私钥和公钥秘钥对1[hadoop@hadoop1 ~]$ ssh-keygen -t rsa 2.3 将公钥添加到authorized_keys1[hadoop@hadoop1 ~]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 2.4 赋予authorized_keys文件600的权限1[hadoop@hadoop1 ~]$ chmod 600 ~/.ssh/authorized_keys 2.5 修改Linux映射文件(root用户)1[root@hadoop1 ~]$ vi /etc/hosts 2.6 验证1[hadoop@hadoop1 ~]$ ssh hadoop1 此时不需要输入密码，免密登录设置成功。 三、安装Hadoop-2.7.53.1 上传解压缩1[hadoop@hadoop1 ~]$ tar -zxvf hadoop-2.7.5-centos-6.7.tar.gz -C apps/ 3.2 创建安装包对应的软连接为解压的hadoop包创建软连接 12345[hadoop@hadoop1 ~]$ cd apps/[hadoop@hadoop1 apps]$ ll总用量 4drwxr-xr-x. 9 hadoop hadoop 4096 12月 24 13:43 hadoop-2.7.5[hadoop@hadoop1 apps]$ ln -s hadoop-2.7.5/ hadoop 3.3 修改配置文件进入/home/hadoop/apps/hadoop/etc/hadoop/目录下修改配置文件 （1）修改hadoop-env.sh12[hadoop@hadoop1 hadoop]$ vi hadoop-env.sh export JAVA_HOME=/usr/local/jdk1.8.0_73 （2）修改core-site.xml1[hadoop@hadoop1 hadoop]$ vi core-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/hadoopdata&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; （3）修改hdfs-site.xml1[hadoop@hadoop1 hadoop]$ vi hdfs-site.xml dfs的备份数目，单机用1份就行 1234567891011121314151617&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/hadoopdata/name&lt;/value&gt; &lt;description&gt;为了保证元数据的安全一般配置多个不同目录&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/hadoopdata/data&lt;/value&gt; &lt;description&gt;datanode 的数据存储目录&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;description&gt;HDFS 的数据块的副本存储个数, 默认是3&lt;/description&gt;&lt;/property&gt; （4）修改mapred-site.xml12[hadoop@hadoop1 hadoop]$ cp mapred-site.xml.template mapred-site.xml[hadoop@hadoop1 hadoop]$ vi mapred-site.xml mapreduce.framework.name：指定mr框架为yarn方式,Hadoop二代MP也基于资源管理系统Yarn来运行 。 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; （5）修改yarn-site.xml1[hadoop@hadoop1 hadoop]$ vi yarn-site.xml 123456&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;description&gt;YARN 集群为 MapReduce 程序提供的 shuffle 服务&lt;/description&gt;&lt;/property&gt; 3.4 配置环境变量千万注意： 1、如果你使用root用户进行安装。 vi /etc/profile 即可 系统变量 2、如果你使用普通用户进行安装。 vi ~/.bashrc 用户变量 123[hadoop@hadoop1 ~]$ vi .bashrc#HADOOP_HOMEexport HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin: 使环境变量生效 1[hadoop@hadoop1 bin]$ source ~/.bashrc 3.5 查看hadoop版本1[hadoop@hadoop1 ~]$ hadoop version 3.6 创建文件夹文件夹的路径参考配置文件hdfs-site.xml里面的路径 12[hadoop@hadoop1 ~]$ mkdir -p /home/hadoop/data/hadoopdata/name[hadoop@hadoop1 ~]$ mkdir -p /home/hadoop/data/hadoopdata/data 3.7 Hadoop的初始化1[hadoop@hadoop1 ~]$ hadoop namenode -format 3.8 启动HDFS和YARN1[hadoop@hadoop1 ~]$ start-dfs.sh[hadoop@hadoop1 ~]$ start-yarn.sh 3.9 检查WebUI浏览器打开端口50070：http://hadoop1:50070 其他端口说明：port 8088: cluster and all applicationsport 50070: Hadoop NameNodeport 50090: Secondary NameNodeport 50075: DataNode 四、Scala的安装（可选）使用root安装 4.1 下载Scala下载地址http://www.scala-lang.org/download/all.html 选择对应的版本，此处在Linux上安装，选择的版本是scala-2.11.8.tgz 4.2 上传解压缩1[root@hadoop1 hadoop]# tar -zxvf scala-2.11.8.tgz -C /usr/local/ 4.3 配置环境变量1234[root@hadoop1 hadoop]# vi /etc/profile#Scalaexport SCALA_HOME=/usr/local/scala-2.11.8export PATH=$SCALA_HOME/bin:$PATH 保存并使其立即生效 1[root@hadoop1 scala-2.11.8]# source /etc/profile 4.4 验证是否安装成功1[root@hadoop1 ~]# scala -version 五、Spark的安装5.1 下载安装包下载地址： http://spark.apache.org/downloads.html 5.2 上传解压缩1[hadoop@hadoop1 ~]$ tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz -C apps/ 5.3 为解压包创建一个软连接1234[hadoop@hadoop1 ~]$ cd apps/[hadoop@hadoop1 apps]$ lshadoop hadoop-2.7.5 spark-2.3.0-bin-hadoop2.7[hadoop@hadoop1 apps]$ ln -s spark-2.3.0-bin-hadoop2.7/ spark 5.4 进入spark/conf修改配置文件1[hadoop@hadoop1 apps]$ cd spark/conf/ 复制spark-env.sh.template并重命名为spark-env.sh，并在文件最后添加配置内容 12[hadoop@hadoop1 conf]$ cp spark-env.sh.template spark-env.sh[hadoop@hadoop1 conf]$ vi spark-env.sh 123456export JAVA_HOME=/usr/local/jdk1.8.0_73export SCALA_HOME=/usr/share/scala-2.11.8export HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5export HADOOP_CONF_DIR=/home/hadoop/apps/hadoop-2.7.5/etc/hadoopexport SPARK_MASTER_IP=hadoop1export SPARK_MASTER_PORT=7077 5.5 配置环境变量1234[hadoop@hadoop1 conf]$ vi ~/.bashrc #SPARK_HOMEexport SPARK_HOME=/home/hadoop/apps/sparkexport PATH=$PATH:$SPARK_HOME/bin 保存使其立即生效 1[hadoop@hadoop1 conf]$ source ~/.bashrc 5.6 启动Spark1[hadoop@hadoop1 ~]$ ~/apps/spark/sbin/start-all.sh 5.7 查看进程 5.8 查看web界面http://hadoop1:8080/","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （四）Spark的广播变量和累加器","slug":"2019-06-04-Spark学习之路 （四）Spark的广播变量和累加器","date":"2019-06-04T03:30:04.000Z","updated":"2019-09-16T08:06:46.996Z","comments":true,"path":"2019-06-04-Spark学习之路 （四）Spark的广播变量和累加器.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-04-Spark学习之路 （四）Spark的广播变量和累加器.html","excerpt":"** Spark学习之路 （四）Spark的广播变量和累加器：** &lt;Excerpt in index | 首页摘要&gt; ​ 在spark程序中，当一个传递给Spark操作(例如map和reduce)的函数在远程节点上面运行时，Spark操作实际上操作的是这个函数所用变量的一个独立副本。这些变量会被复制到每台机器上，并且这些变量在远程机器上的所有更新都不会传递回驱动程序。通常跨任务的读写变量是低效的，但是，Spark还是为两种常见的使用模式提供了两种有限的共享变量：广播变（broadcast variable）和累加器（accumulator）","text":"** Spark学习之路 （四）Spark的广播变量和累加器：** &lt;Excerpt in index | 首页摘要&gt; ​ 在spark程序中，当一个传递给Spark操作(例如map和reduce)的函数在远程节点上面运行时，Spark操作实际上操作的是这个函数所用变量的一个独立副本。这些变量会被复制到每台机器上，并且这些变量在远程机器上的所有更新都不会传递回驱动程序。通常跨任务的读写变量是低效的，但是，Spark还是为两种常见的使用模式提供了两种有限的共享变量：广播变（broadcast variable）和累加器（accumulator） &lt;The rest of contents | 余下全文&gt; 一、广播变量broadcast variable​ 广播变量允许程序员在每台机器上保留一个只读变量，而不是随副本一起发送它的副本。例如，它们可用于以有效的方式为每个节点提供大输入数据集的副本。Spark还尝试使用有效的广播算法来分发广播变量，以降低通信成本。 ​ Spark动作通过一组阶段执行，由分布式“shuffle”操作分隔。Spark自动广播每个阶段中任务所需的公共数据。以这种方式广播的数据以序列化形式缓存并在运行每个任务之前反序列化。这意味着显式创建广播变量仅在跨多个阶段的任务需要相同数据或以反序列化形式缓存数据很重要时才有用。 ​ 广播变量是v通过调用从变量创建的SparkContext.broadcast(v)。广播变量是一个包装器v，可以通过调用该value 方法来访问它的值。下面的代码显示了这个： 1.1 为什么要将变量定义成广播变量？如果我们要在分布式计算里面分发大对象，例如：字典，集合，黑白名单等，这个都会由Driver端进行分发，一般来讲，如果这个变量不是广播变量，那么每个task就会分发一份，这在task数目十分多的情况下Driver的带宽会成为系统的瓶颈，而且会大量消耗task服务器上的资源，如果将这个变量声明为广播变量，那么知识每个executor拥有一份，这个executor启动的task会共享这个变量，节省了通信的成本和服务器的资源。 1.2 广播变量图解错误的，不使用广播变量 正确的，使用广播变量的情况 2.3 如何定义一个广播变量？12val a = 3val broadcast = sc.broadcast(a) 2.4 如何还原一个广播变量？1val c = broadcast.value 2.5 定义广播变量需要的注意点？变量一旦被定义为一个广播变量，那么这个变量只能读，不能修改 2.6 注意事项1、能不能将一个RDD使用广播变量广播出去？ ​ 不能，因为RDD是不存储数据的。可以将RDD的结果广播出去。 2、 广播变量只能在Driver端定义，不能在Executor端定义。 3、 在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值。 4、如果executor端用到了Driver的变量，如果不使用广播变量在Executor有多少task就有多少Driver端的变量副本。 5、如果Executor端用到了Driver的变量，如果使用广播变量在每个Executor中只有一份Driver端的变量副本。 二、累加器​ 累加器是仅通过关联和交换操作“添加”的变量，因此可以并行有效地支持。它们可用于实现计数器（如MapReduce）或总和。Spark本身支持数值类型的累加器，程序员可以添加对新类型的支持。 作为用户，您可以创建命名或未命名的累加器。如下图所示，命名累加器（在此实例中counter）将显示在Web UI中，用于修改该累加器的阶段。Spark显示“任务”表中任务修改的每个累加器的值。 跟踪UI中的累加器对于理解运行阶段的进度非常有用（注意：Python中尚不支持）。 2.1 为什么要将一个变量定义为一个累加器？​ 在spark应用程序中，我们经常会有这样的需求，如异常监控，调试，记录符合某特性的数据的数目，这种需求都需要用到计数器，如果一个变量不被声明为一个累加器，那么它将在被改变时不会再driver端进行全局汇总，即在分布式运行时每个task运行的只是原始变量的一个副本，并不能改变原始变量的值，但是当这个变量被声明为累加器后，该变量就会有分布式计数的功能。 2.2 图解累加器错误的图解 正确的图解 2.3 如何定义一个累加器？1val a = sc.accumulator(0) 2.4 如何还原一个累加器？1val b = a.value 2.5 注意事项1、 累加器在Driver端定义赋初始值，累加器只能在Driver端读取最后的值，在Excutor端更新。 2、累加器不是一个调优的操作，因为如果不这样做，结果是错的","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （三）Spark之RDD","slug":"2019-06-03-Spark学习之路 （三）Spark之RDD","date":"2019-06-03T02:30:04.000Z","updated":"2019-09-16T04:12:59.674Z","comments":true,"path":"2019-06-03-Spark学习之路 （三）Spark之RDD.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-03-Spark学习之路 （三）Spark之RDD.html","excerpt":"** Spark学习之路 （三）Spark之RDD：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （三）Spark之RDD","text":"** Spark学习之路 （三）Spark之RDD：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （三）Spark之RDD &lt;The rest of contents | 余下全文&gt; 一、RDD的概述1.1 什么是RDD​ RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。 1.2 RDD的属性https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala A list of partitions A function for computing each split A list of dependencies on other RDDs Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned) Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file) （1）一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。 （2）一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。 （3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 （4）一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。 （5）一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 其中hello.txt 二、RDD的创建方式2.1 通过读取文件生成的由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等 1scala&gt; val file = sc.textFile(&quot;/spark/hello.txt&quot;) 2.2 通过并行化的方式创建RDD由一个已经存在的Scala集合创建。 1234567scala&gt; val array = Array(1,2,3,4,5)array: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt; val rdd = sc.parallelize(array)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[27] at parallelize at &lt;console&gt;:26scala&gt; 2.3 其他方式读取数据库等等其他的操作。也可以生成RDD。RDD转换为ParallelCollectionRDD。 三、RDD编程APISpark支持两个类型（算子）操作：Transformation和Action 3.1 Transformation​ 主要做的是就是将一个已有的RDD生成另外一个RDD。Transformation具有lazy**特性(延迟加载)**。Transformation算子的代码不会真正被执行。只有当我们的程序里面遇到一个action算子的时候，代码才会真正的被执行。这种设计让Spark更加有效率地运行。 http://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds 常用的Transformation： 转换 含义 map(func) 返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成 filter(func) 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成 flatMap(func) 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素） mapPartitions(func) 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U] mapPartitionsWithIndex(func) 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U] sample(withReplacement, fraction, seed) 根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子 union(otherDataset) 对源RDD和参数RDD求并集后返回一个新的RDD intersection(otherDataset) 对源RDD和参数RDD求交集后返回一个新的RDD distinct([numTasks])) 对源RDD进行去重后返回一个新的RDD groupByKey([numTasks]) 在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD reduceByKey(func, [numTasks]) 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置 aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) 先按分区聚合 再总的聚合 每次要跟初始值交流 例如：aggregateByKey(0)(+,+) 对k/y的RDD进行操作 sortByKey([ascending], [numTasks]) 在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD sortBy(func,[ascending], [numTasks]) 与sortByKey类似，但是更灵活 第一个参数是根据什么排序 第二个是怎么排序 false倒序 第三个排序后分区数 默认与原RDD一样 join(otherDataset, [numTasks]) 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD 相当于内连接（求交集） cogroup(otherDataset, [numTasks]) 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable,Iterable))类型的RDD cartesian(otherDataset) 两个RDD的笛卡尔积 的成很多个K/V pipe(command, [envVars]) 调用外部程序 coalesce(numPartitions) 重新分区 第一个参数是要分多少区，第二个参数是否shuffle 默认false 少分区变多分区 true 多分区变少分区 false repartition(numPartitions) 重新分区 必须shuffle 参数是要分多少区 少变多 repartitionAndSortWithinPartitions(partitioner) 重新分区+排序 比先分区再排序效率高 对K/V的RDD进行操作 foldByKey(zeroValue)(seqOp) 该函数用于K/V做折叠，合并处理 ，与aggregate类似 第一个括号的参数应用于每个V值 第二括号函数是聚合例如：+ combineByKey 合并相同的key的值 rdd1.combineByKey(x =&gt; x, (a: Int, b: Int) =&gt; a + b, (m: Int, n: Int) =&gt; m + n) partitionBy**（partitioner）** 对RDD进行分区 partitioner是分区器 例如new HashPartition(2 cache RDD缓存，可以避免重复计算从而减少时间，区别：cache内部调用了persist算子，cache默认就一个缓存级别MEMORY-ONLY ，而persist则可以选择缓存级别 persist Subtract**（rdd）** 返回前rdd元素不在后rdd的rdd leftOuterJoin leftOuterJoin类似于SQL中的左外关联left outer join，返回结果以前面的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可。 rightOuterJoin rightOuterJoin类似于SQL中的有外关联right outer join，返回结果以参数中的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可 subtractByKey substractByKey和基本转换操作中的subtract类似只不过这里是针对K的，返回在主RDD中出现，并且不在otherRDD中出现的元素 3.2 Action触发代码的运行，我们一段spark代码里面至少需要有一个action操作。 常用的Action: 动作 含义 reduce(func) 通过func函数聚集RDD中的所有元素，这个功能必须是课交换且可并联的 collect() 在驱动程序中，以数组的形式返回数据集的所有元素 count() 返回RDD的元素个数 first() 返回RDD的第一个元素（类似于take(1)） take(n) 返回一个由数据集的前n个元素组成的数组 takeSample(withReplacement,num, [seed]) 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子 takeOrdered(n, [ordering]) saveAsTextFile(path) 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本 saveAsSequenceFile(path) 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。 saveAsObjectFile(path) countByKey() 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。 foreach(func) 在数据集的每一个元素上，运行函数func进行更新。 aggregate 先对分区进行操作，在总体操作 reduceByKeyLocally lookup top fold foreachPartition 3.3 Spark WordCount代码编写使用maven进行项目构建 （1）使用scala进行编写查看官方网站，需要导入2个依赖包 详细代码 SparkWordCountWithScala.scala 12345678910111213141516171819202122232425262728293031import org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object SparkWordCountWithScala &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() /** * 如果这个参数不设置，默认认为你运行的是集群模式 * 如果设置成local代表运行的是local模式 */ conf.setMaster(\"local\") //设置任务名 conf.setAppName(\"WordCount\") //创建SparkCore的程序入口 val sc = new SparkContext(conf) //读取文件 生成RDD val file: RDD[String] = sc.textFile(\"E:\\\\hello.txt\") //把每一行数据按照，分割 val word: RDD[String] = file.flatMap(_.split(\",\")) //让每一个单词都出现一次 val wordOne: RDD[(String, Int)] = word.map((_,1)) //单词计数 val wordCount: RDD[(String, Int)] = wordOne.reduceByKey(_+_) //按照单词出现的次数 降序排序 val sortRdd: RDD[(String, Int)] = wordCount.sortBy(tuple =&gt; tuple._2,false) //将最终的结果进行保存 sortRdd.saveAsTextFile(\"E:\\\\result\") sc.stop() &#125; 运行结果 （2）使用java jdk7进行编写SparkWordCountWithJava7.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import scala.Tuple2;import java.util.Arrays;import java.util.Iterator;public class SparkWordCountWithJava7 &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf(); conf.setMaster(\"local\"); conf.setAppName(\"WordCount\"); JavaSparkContext sc = new JavaSparkContext(conf); JavaRDD&lt;String&gt; fileRdd = sc.textFile(\"E:\\\\hello.txt\"); JavaRDD&lt;String&gt; wordRDD = fileRdd.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterator&lt;String&gt; call(String line) throws Exception &#123; return Arrays.asList(line.split(\",\")).iterator(); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; wordOneRDD = wordRDD.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123; return new Tuple2&lt;&gt;(word, 1); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; wordCountRDD = wordOneRDD.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer i1, Integer i2) throws Exception &#123; return i1 + i2; &#125; &#125;); JavaPairRDD&lt;Integer, String&gt; count2WordRDD = wordCountRDD.mapToPair(new PairFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, String&gt;() &#123; @Override public Tuple2&lt;Integer, String&gt; call(Tuple2&lt;String, Integer&gt; tuple) throws Exception &#123; return new Tuple2&lt;&gt;(tuple._2, tuple._1); &#125; &#125;); JavaPairRDD&lt;Integer, String&gt; sortRDD = count2WordRDD.sortByKey(false); JavaPairRDD&lt;String, Integer&gt; resultRDD = sortRDD.mapToPair(new PairFunction&lt;Tuple2&lt;Integer, String&gt;, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(Tuple2&lt;Integer, String&gt; tuple) throws Exception &#123; return new Tuple2&lt;&gt;(tuple._2, tuple._1); &#125; &#125;); resultRDD.saveAsTextFile(\"E:\\\\result7\"); &#125;&#125; （3）使用java jdk8进行编写lambda表达式 SparkWordCountWithJava8.java 12345678910111213141516171819202122232425import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import scala.Tuple2;import java.util.Arrays;public class SparkWordCountWithJava8 &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf(); conf.setAppName(\"WortCount\"); conf.setMaster(\"local\"); JavaSparkContext sc = new JavaSparkContext(conf); JavaRDD&lt;String&gt; fileRDD = sc.textFile(\"E:\\\\hello.txt\"); JavaRDD&lt;String&gt; wordRdd = fileRDD.flatMap(line -&gt; Arrays.asList(line.split(\",\")).iterator()); JavaPairRDD&lt;String, Integer&gt; wordOneRDD = wordRdd.mapToPair(word -&gt; new Tuple2&lt;&gt;(word, 1)); JavaPairRDD&lt;String, Integer&gt; wordCountRDD = wordOneRDD.reduceByKey((x, y) -&gt; x + y); JavaPairRDD&lt;Integer, String&gt; count2WordRDD = wordCountRDD.mapToPair(tuple -&gt; new Tuple2&lt;&gt;(tuple._2, tuple._1)); JavaPairRDD&lt;Integer, String&gt; sortRDD = count2WordRDD.sortByKey(false); JavaPairRDD&lt;String, Integer&gt; resultRDD = sortRDD.mapToPair(tuple -&gt; new Tuple2&lt;&gt;(tuple._2, tuple._1)); resultRDD.saveAsTextFile(\"E:\\\\result8\"); &#125; 四、RDD的宽依赖和窄依赖4.1 RDD依赖关系的本质内幕由于RDD是粗粒度的操作数据集，每个Transformation操作都会生成一个新的RDD，所以RDD之间就会形成类似流水线的前后依赖关系；RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。如图所示显示了RDD之间的依赖关系。 从图中可知： 窄依赖：是指每个父RDD的一个Partition最多被子RDD的一个Partition所使用，例如map、filter、union等操作都会产生窄依赖；（独生子女） 宽依赖：是指一个父RDD的Partition会被多个子RDD的Partition所使用，例如groupByKey、reduceByKey、sortByKey等操作都会产生宽依赖；（超生） 需要特别说明的是对join操作有两种情况： （1）图中左半部分join：如果两个RDD在进行join操作时，一个RDD的partition仅仅和另一个RDD中已知个数的Partition进行join，那么这种类型的join操作就是窄依赖，例如图1中左半部分的join操作(join with inputs co-partitioned)； （2）图中右半部分join：其它情况的join操作就是宽依赖,例如图1中右半部分的join操作(join with inputs not co-partitioned)，由于是需要父RDD的所有partition进行join的转换，这就涉及到了shuffle，因此这种类型的join操作也是宽依赖。 总结： 在这里我们是从父RDD的partition被使用的个数来定义窄依赖和宽依赖，因此可以用一句话概括下：如果父RDD的一个Partition被子RDD的一个Partition所使用就是窄依赖，否则的话就是宽依赖。因为是确定的partition数量的依赖关系，所以RDD之间的依赖关系就是窄依赖；由此我们可以得出一个推论：即窄依赖不仅包含一对一的窄依赖，还包含一对固定个数的窄依赖。 一对固定个数的窄依赖的理解：即子RDD的partition对父RDD依赖的Partition的数量不会随着RDD数据规模的改变而改变；换句话说，无论是有100T的数据量还是1P的数据量，在窄依赖中，子RDD所依赖的父RDD的partition的个数是确定的，而宽依赖是shuffle级别的，数据量越大，那么子RDD所依赖的父RDD的个数就越多，从而子RDD所依赖的父RDD的partition的个数也会变得越来越多。 4.2 依赖关系下的数据流视图 在spark中，会根据RDD之间的依赖关系将DAG图（有向无环图）划分为不同的阶段，对于窄依赖，由于partition依赖关系的确定性，partition的转换处理就可以在同一个线程里完成，窄依赖就被spark划分到同一个stage中，而对于宽依赖，只能等父RDD shuffle处理完成后，下一个stage才能开始接下来的计算。 因此spark划分stage的整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。因此在图2中RDD C,RDD D,RDD E,RDDF被构建在一个stage中,RDD A被构建在一个单独的Stage中,而RDD B和RDD G又被构建在同一个stage中。 在spark中，Task的类型分为2种：ShuffleMapTask和ResultTask； 简单来说，DAG的最后一个阶段会为每个结果的partition生成一个ResultTask，即每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！而其余所有阶段都会生成ShuffleMapTask；之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中；也就是说上图中的stage1和stage2相当于mapreduce中的Mapper,而ResultTask所代表的stage3就相当于mapreduce中的reducer。 在之前动手操作了一个wordcount程序，因此可知，Hadoop中MapReduce操作中的Mapper和Reducer在spark中的基本等量算子是map和reduceByKey;不过区别在于：Hadoop中的MapReduce天生就是排序的；而reduceByKey只是根据Key进行reduce，但spark除了这两个算子还有其他的算子；因此从这个意义上来说，Spark比Hadoop的计算算子更为丰富。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler","slug":"2019-06-02-Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler","date":"2019-06-02T05:30:04.000Z","updated":"2019-09-16T02:59:42.540Z","comments":true,"path":"2019-06-02-Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler.html","excerpt":"** Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler","text":"** Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler &lt;The rest of contents | 余下全文&gt; 目前Hadoop有三种比较流行的资源调度器：FIFO 、Capacity Scheduler、Fair Scheduler。目前hadoop2.7默认使用的是Capacity Scheduler容量调度器。 一、FIFO（先入先出调度器）hadoop1.x使用的默认调度器就是FIFO。FIFO采用队列方式将一个一个job任务按照时间先后顺序进行服务。比如排在最前面的job需要若干maptask和若干reducetask，当发现有空闲的服务器节点就分配给这个job，直到job执行完毕。 二、Capacity Scheduler（容量调度器）hadoop2.x使用的默认调度器是Capacity Scheduler。 1、支持多个队列，每个队列可配置一定量的资源，每个采用FIFO的方式调度。 2、为了防止同一个用户的job任务独占队列中的资源，调度器会对同一用户提交的job任务所占资源进行限制。 3、分配新的job任务时，首先计算每个队列中正在运行task个数与其队列应该分配的资源量做比值，然后选择比值最小的队列。比如如图队列A15个task，20%资源量，那么就是15%0.2=70，队列B是25%0.5=50 ，队列C是25%0.3=80.33 。所以选择最小值队列B。 4、其次，按照job任务的优先级和时间顺序，同时要考虑到用户的资源量和内存的限制，对队列中的job任务进行排序执行。 5、多个队列同时按照任务队列内的先后顺序一次执行。例如下图中job11、job21、job31分别在各自队列中顺序比较靠前，三个任务就同时执行。 三、Fair Scheduler（公平调度器）1、支持多个队列，每个队列可以配置一定的资源，每个队列中的job任务公平共享其所在队列的所有资源。 2、队列中的job任务都是按照优先级分配资源，优先级越高分配的资源越多，但是为了确保公平每个job任务都会分配到资源。优先级是根据每个job任务的理想获取资源量减去实际获取资源量的差值决定的，差值越大优先级越高。 原文链接：https://blog.csdn.net/xiaomage510/article/details/82500067","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Spark调度模式-FIFO和FAIR","slug":"2019-06-02-Spark调度模式-FIFO和FAIR","date":"2019-06-02T05:20:04.000Z","updated":"2019-09-16T03:25:31.638Z","comments":true,"path":"2019-06-02-Spark调度模式-FIFO和FAIR.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-Spark调度模式-FIFO和FAIR.html","excerpt":"** Spark调度模式-FIFO和FAIR：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark调度模式-FIFO和FAIR","text":"** Spark调度模式-FIFO和FAIR：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark调度模式-FIFO和FAIR &lt;The rest of contents | 余下全文&gt; Spark中的调度模式主要有两种：FIFO和FAIR。默认情况下Spark的调度模式是FIFO（先进先出），谁先提交谁先执行，后面的任务需要等待前面的任务执行。而FAIR（公平调度）模式支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。对这两种调度模式的具体实现，接下来会根据spark-1.6.0的源码来进行详细的分析。使用哪种调度器由参数spark.scheduler.mode来设置，可选的参数有FAIR和FIFO，默认是FIFO。 一、源码入口 在Scheduler模块中，当Stage划分好，然后提交Task的过程中，会进入TaskSchedulerImpl#submitTasks方法。 schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties) //目前支持FIFO和FAIR两种调度策略。 在上面代码中有一个schedulableBuilder对象，这个对象在TaskSchedulerImpl类中的定义及实现可以参考下面这段源代码： 12345678910111213141516var schedulableBuilder: SchedulableBuilder = null... def initialize(backend: SchedulerBackend) &#123; this.backend = backend // temporarily set rootPool name to empty rootPool = new Pool(\"\", schedulingMode, 0, 0) schedulableBuilder = &#123; schedulingMode match &#123; case SchedulingMode.FIFO =&gt; new FIFOSchedulableBuilder(rootPool) //rootPool包含了一组TaskSetManager case SchedulingMode.FAIR =&gt; new FairSchedulableBuilder(rootPool, conf) //rootPool包含了一组Pool树，这棵树的叶子节点都是TaskSetManager &#125; &#125; schedulableBuilder.buildPools() //在FIFO中的实现是空 &#125; 根据用户配置的SchedulingMode决定是生成FIFOSchedulableBuilder还是生成FairSchedulableBuilder类型的schedulableBuilder对象。 在生成schedulableBuilder后，调用其buildPools方法生成调度池。 调度模式由配置参数spark.scheduler.mode（默认值为FIFO）来确定。 两种模式的调度逻辑图如下： 二、FIFOSchedulableBuilder FIFO的rootPool包含一组TaskSetManager。从上面的类继承图中看出在FIFOSchedulableBuilder中有两个方法： 1、buildPools实现为空: 123override def buildPools() &#123; // nothing &#125; 所以，对于FIFO模式，获取到schedulableBuilder对象后，在调用buildPools方法后，不做任何操作。 2、addTaskSetManager 该方法将TaskSetManager装载到rootPool中。直接调用的方法是Pool#addSchedulable()。 123override def addTaskSetManager(manager: Schedulable, properties: Properties) &#123; rootPool.addSchedulable(manager)&#125; Pool#addSchedulable()方法： 12345678val schedulableQueue = new ConcurrentLinkedQueue[Schedulable]... override def addSchedulable(schedulable: Schedulable) &#123; require(schedulable != null) schedulableQueue.add(schedulable) schedulableNameToSchedulable.put(schedulable.name, schedulable) schedulable.parent = this &#125; 将该TaskSetManager加入到调度队列schedulableQueue中。 三、FairSchedulableBuilder FAIR的rootPool中包含一组Pool，在Pool中包含了TaskSetManager。 1、buildPools 在该方法中，会读取配置文件，按照配置文件中的配置参数调用buildFairSchedulerPool生成配置的调度池，以及调用buildDefaultPool生成默认调度池。 默认情况下FAIR模式的配置文件是位于SPARK_HOME/conf/fairscheduler.xml文件，也可以通过参数spark.scheduler.allocation.file设置用户自定义配置文件。spark中提供的fairscheduler.xml模板如下所示： 123456789101112&lt;allocations&gt; &lt;pool name=\"production\"&gt; &lt;schedulingMode&gt;FAIR&lt;/schedulingMode&gt; &lt;weight&gt;1&lt;/weight&gt; &lt;minShare&gt;2&lt;/minShare&gt; &lt;/pool&gt; &lt;pool name=\"test\"&gt; &lt;schedulingMode&gt;FIFO&lt;/schedulingMode&gt; &lt;weight&gt;2&lt;/weight&gt; &lt;minShare&gt;3&lt;/minShare&gt; &lt;/pool&gt;&lt;/allocations&gt; 参数含义：（1）name: 该调度池的名称，可根据该参数使用指定pool，入sc.setLocalProperty(“spark.scheduler.pool”, “test”)（2）weight: 该调度池的权重，各调度池根据该参数分配系统资源。每个调度池得到的资源数为weight / sum(weight)，weight为2的分配到的资源为weight为1的两倍。（3）minShare: 该调度池需要的最小资源数（CPU核数）。fair调度器首先会尝试为每个调度池分配最少minShare资源，然后剩余资源才会按照weight大小继续分配。（4）schedulingMode: 该调度池内的调度模式。 2、buildFairSchedulerPool 从上面的配置文件可以看到，每一个调度池有一个name属性指定名字，然后在该pool中可以设置其schedulingMode(可为空，默认为FIFO), weight(可为空，默认值是1), 以及minShare(可为空，默认值是0)参数。然后使用这些参数生成一个Pool对象，把该pool对象放入rootPool中。入下所示： 12val pool = new Pool(poolName, schedulingMode, minShare, weight)rootPool.addSchedulable(pool) 3、buildDefaultPool 如果如果配置文件中没有设置一个name为default的pool，系统才会自动生成一个使用默认参数生成的pool对象。各项参数的默认值在buildFairSchedulerPool中有提到。 4、addTaskSetManager 这一段逻辑中是把配置文件中的pool，或者default pool放入rootPool中，然后把TaskSetManager存入rootPool对应的子pool。 12345678910111213141516171819override def addTaskSetManager(manager: Schedulable, properties: Properties) &#123; var poolName = DEFAULT_POOL_NAME var parentPool = rootPool.getSchedulableByName(poolName) if (properties != null) &#123; poolName = properties.getProperty(FAIR_SCHEDULER_PROPERTIES, DEFAULT_POOL_NAME) parentPool = rootPool.getSchedulableByName(poolName) if (parentPool == null) &#123; // we will create a new pool that user has configured in app // instead of being defined in xml file parentPool = new Pool(poolName, DEFAULT_SCHEDULING_MODE, DEFAULT_MINIMUM_SHARE, DEFAULT_WEIGHT) rootPool.addSchedulable(parentPool) logInfo(\"Created pool %s, schedulingMode: %s, minShare: %d, weight: %d\".format( poolName, DEFAULT_SCHEDULING_MODE, DEFAULT_MINIMUM_SHARE, DEFAULT_WEIGHT)) &#125; &#125; parentPool.addSchedulable(manager) logInfo(\"Added task set \" + manager.name + \" tasks to pool \" + poolName) &#125; 5、FAIR调度池使用方法 在Spark-1.6.1官方文档中写道： 如果不加设置，jobs会提交到default调度池中。由于调度池的使用是Thread级别的，只能通过具体的SparkContext来设置local属性（即无法在配置文件中通过参数spark.scheduler.pool来设置，因为配置文件中的参数会被加载到SparkConf对象中）。所以需要使用指定调度池的话，需要在具体代码中通过SparkContext对象sc来按照如下方法进行设置：sc.setLocalProperty(“spark.scheduler.pool”, “test”)设置该参数后，在该thread中提交的所有job都会提交到test Pool中。如果接下来不再需要使用到该test调度池，sc.setLocalProperty(“spark.scheduler.pool”, null) 四、FIFO和FAIR的调度顺序这里必须提到的一个类是上面提到的Pool，在这个类中实现了不同调度模式的调度算法。 12345678var taskSetSchedulingAlgorithm: SchedulingAlgorithm = &#123; schedulingMode match &#123; case SchedulingMode.FAIR =&gt; new FairSchedulingAlgorithm() case SchedulingMode.FIFO =&gt; new FIFOSchedulingAlgorithm() &#125;&#125; FIFO模式的算法类是FIFOSchedulingAlgorithm，FAIR模式的算法实现类是FairSchedulingAlgorithm。 接下来的两节中comparator方法传入参数Schedulable类型是一个trait，具体实现主要有两个：1，Pool；2，TaskSetManager。与最前面那个调度模式的逻辑图相对应。 1、FIFO模式的调度算法FIFOSchedulingAlgorithm在这个类里面，主要逻辑是一个comparator方法。 123456789101112131415override def comparator(s1: Schedulable, s2: Schedulable): Boolean = &#123; val priority1 = s1.priority //实际上是Job ID val priority2 = s2.priority var res = math.signum(priority1 - priority2) if (res == 0) &#123; //如果Job ID相同，就比较Stage ID val stageId1 = s1.stageId val stageId2 = s2.stageId res = math.signum(stageId1 - stageId2) &#125; if (res &lt; 0) &#123; true &#125; else &#123; false &#125;&#125; 如果有两个调度任务s1和s2，首先获得两个任务的priority，在FIFO中该优先级实际上是Job ID。首先比较两个任务的Job ID，如果priority1比priority2小，那么返回true，表示s1的优先级比s2的高。我们知道Job ID是顺序生成的，先生成的Job ID比较小，所以先提交的job肯定比后提交的job先执行。但是如果是同一个job的不同任务，接下来就比较各自的Stage ID，类似于比较Job ID，Stage ID小的优先级高。 2、FAIR模式的调度算法FairSchedulingAlgorithm 这个类中的comparator方法源代码如下： 123456789101112131415161718192021222324252627282930override def comparator(s1: Schedulable, s2: Schedulable): Boolean = &#123; val minShare1 = s1.minShare //在这里share理解成份额，即每个调度池要求的最少cpu核数 val minShare2 = s2.minShare val runningTasks1 = s1.runningTasks // 该Pool或者TaskSetManager中正在运行的任务数 val runningTasks2 = s2.runningTasks val s1Needy = runningTasks1 &lt; minShare1 // 如果正在运行任务数比该调度池最小cpu核数要小 val s2Needy = runningTasks2 &lt; minShare2 val minShareRatio1 = runningTasks1.toDouble / math.max(minShare1, 1.0).toDouble val minShareRatio2 = runningTasks2.toDouble / math.max(minShare2, 1.0).toDouble val taskToWeightRatio1 = runningTasks1.toDouble / s1.weight.toDouble val taskToWeightRatio2 = runningTasks2.toDouble / s2.weight.toDouble var compare: Int = 0 if (s1Needy &amp;&amp; !s2Needy) &#123; return true &#125; else if (!s1Needy &amp;&amp; s2Needy) &#123; return false &#125; else if (s1Needy &amp;&amp; s2Needy) &#123; compare = minShareRatio1.compareTo(minShareRatio2) &#125; else &#123; compare = taskToWeightRatio1.compareTo(taskToWeightRatio2) &#125; if (compare &lt; 0) &#123; true &#125; else if (compare &gt; 0) &#123; false &#125; else &#123; s1.name &lt; s2.name &#125; &#125; minShare对应fairscheduler.xml配置文件中的minShare属性。（1）如果s1所在Pool或者TaskSetManager中运行状态的task数量比minShare小，s2所在Pool或者TaskSetManager中运行状态的task数量比minShare大，那么s1会优先调度。反之，s2优先调度。（2）如果s1和s2所在Pool或者TaskSetManager中运行状态的task数量都比各自minShare小，那么minShareRatio小的优先被调度。minShareRatio是运行状态task数与minShare的比值，即相对来说minShare使用较少的先被调度。（3）如果minShareRatio相同，那么最后比较各自Pool的名字。 原文链接：https://blog.csdn.net/dabokele/article/details/51526048","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark中parallelize函数和makeRDD函数的区别","slug":"2019-06-02-Spark中parallelize函数和makeRDD函数的区别","date":"2019-06-02T03:30:04.000Z","updated":"2019-09-16T01:39:52.469Z","comments":true,"path":"2019-06-02-Spark中parallelize函数和makeRDD函数的区别.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-Spark中parallelize函数和makeRDD函数的区别.html","excerpt":"** Spark中parallelize函数和makeRDD函数的区别：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark中parallelize函数和makeRDD函数的区别","text":"** Spark中parallelize函数和makeRDD函数的区别：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark中parallelize函数和makeRDD函数的区别 &lt;The rest of contents | 余下全文&gt; 我们知道，在Spark中创建RDD的创建方式大概可以分为三种： （1）、从集合中创建RDD； （2）、从外部存储创建RDD； （3）、从其他RDD创建。 而从集合中创建RDD，Spark主要提供了两中函数：parallelize和makeRDD。我们可以先看看这两个函数的声明： 123456789def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] 我们可以从上面看出makeRDD有两种实现，而且第一个makeRDD函数接收的参数和parallelize完全一致。其实第一种makeRDD函数实现是依赖了parallelize函数的实现，来看看Spark中是怎么实现这个makeRDD函数的： 12345def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; parallelize(seq, numSlices)&#125; ​ 我们可以看出，这个makeRDD函数完全和parallelize函数一致。但是我们得看看第二种makeRDD函数函数实现了，它接收的参数类型是Seq[(T, Seq[String])]，Spark文档的说明是 Distribute a local Scala collection to form an RDD, with one or more location preferences (hostnames of Spark nodes) for each object. Create a new partition for each collection item. 分发本地scala集合以形成RDD，每个对象具有一个或多个位置首选项（spark节点的主机名）。为每个集合项创建一个新分区。 原来，这个函数还为数据提供了位置信息，来看看我们怎么使用： 12345678910111213141516171819202122scala&gt; val iteblog1 = sc.parallelize(List(1,2,3))iteblog1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:21 scala&gt; val iteblog2 = sc.makeRDD(List(1,2,3))iteblog2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at makeRDD at &lt;console&gt;:21 scala&gt; val seq = List((1, List(\"iteblog.com\", \"sparkhost1.com\", \"sparkhost2.com\")), | (2, List(\"iteblog.com\", \"sparkhost2.com\")))seq: List[(Int, List[String])] = List((1,List(iteblog.com, sparkhost1.com, sparkhost2.com)), (2,List(iteblog.com, sparkhost2.com))) scala&gt; val iteblog3 = sc.makeRDD(seq)iteblog3: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at makeRDD at &lt;console&gt;:23 scala&gt; iteblog3.preferredLocations(iteblog3.partitions(1))res26: Seq[String] = List(iteblog.com, sparkhost2.com) scala&gt; iteblog3.preferredLocations(iteblog3.partitions(0))res27: Seq[String] = List(iteblog.com, sparkhost1.com, sparkhost2.com) scala&gt; iteblog1.preferredLocations(iteblog1.partitions(0))res28: Seq[String] = List() 我们可以看到，makeRDD函数有两种实现，第一种实现其实完全和parallelize一致；而第二种实现可以为数据提供位置信息，而除此之外的实现和parallelize函数也是一致的，如下： 123456789101112def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())&#125; def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope &#123; assertNotStopped() val indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap new ParallelCollectionRDD[T](this, seq.map(_._1), seq.size, indexToPrefs)&#125; 都是返回ParallelCollectionRDD，而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。 转载自过往记忆（https://www.iteblog.com/）","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （二）Spark2.3 HA集群的分布式安装","slug":"2019-06-02-Spark学习之路 （二）Spark2.3 HA集群的分布式安装","date":"2019-06-02T02:31:04.000Z","updated":"2019-09-16T02:02:45.545Z","comments":true,"path":"2019-06-02-Spark学习之路 （二）Spark2.3 HA集群的分布式安装.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-Spark学习之路 （二）Spark2.3 HA集群的分布式安装.html","excerpt":"** Spark学习之路 （二）Spark2.3 HA集群的分布式安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （二）Spark2.3 HA集群的分布式安装","text":"** Spark学习之路 （二）Spark2.3 HA集群的分布式安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （二）Spark2.3 HA集群的分布式安装 &lt;The rest of contents | 余下全文&gt; 一、下载Spark安装包1、从官网下载http://spark.apache.org/downloads.html 二、安装基础1、Java8安装成功 2、Zookeeper安装成功 3、hadoop2.7.5 HA安装成功 4、Scala安装成功（不安装进程也可以启动） 三、Spark安装过程1、上传并解压缩12345[hadoop@hadoop1 ~]$ lsapps data exam inithive.conf movie spark-2.3.0-bin-hadoop2.7.tgz udf.jarcookies data.txt executions json.txt projects student zookeeper.outcourse emp hive.sql log sougou temp[hadoop@hadoop1 ~]$ tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz -C apps/ 2、为安装包创建一个软连接12345678910111213[hadoop@hadoop1 ~]$ cd apps/[hadoop@hadoop1 apps]$ lshadoop-2.7.5 hbase-1.2.6 spark-2.3.0-bin-hadoop2.7 zookeeper-3.4.10 zookeeper.out[hadoop@hadoop1 apps]$ ln -s spark-2.3.0-bin-hadoop2.7/ spark[hadoop@hadoop1 apps]$ ll总用量 36drwxr-xr-x. 10 hadoop hadoop 4096 3月 23 20:29 hadoop-2.7.5drwxrwxr-x. 7 hadoop hadoop 4096 3月 29 13:15 hbase-1.2.6lrwxrwxrwx. 1 hadoop hadoop 26 4月 20 13:48 spark -&gt; spark-2.3.0-bin-hadoop2.7/drwxr-xr-x. 13 hadoop hadoop 4096 2月 23 03:42 spark-2.3.0-bin-hadoop2.7drwxr-xr-x. 10 hadoop hadoop 4096 3月 23 2017 zookeeper-3.4.10-rw-rw-r--. 1 hadoop hadoop 17559 3月 29 13:37 zookeeper.out[hadoop@hadoop1 apps]$ 3、进入spark/conf修改配置文件（1）进入配置文件所在目录 1234567891011[hadoop@hadoop1 ~]$ cd apps/spark/conf/[hadoop@hadoop1 conf]$ ll总用量 36-rw-r--r--. 1 hadoop hadoop 996 2月 23 03:42 docker.properties.template-rw-r--r--. 1 hadoop hadoop 1105 2月 23 03:42 fairscheduler.xml.template-rw-r--r--. 1 hadoop hadoop 2025 2月 23 03:42 log4j.properties.template-rw-r--r--. 1 hadoop hadoop 7801 2月 23 03:42 metrics.properties.template-rw-r--r--. 1 hadoop hadoop 865 2月 23 03:42 slaves.template-rw-r--r--. 1 hadoop hadoop 1292 2月 23 03:42 spark-defaults.conf.template-rwxr-xr-x. 1 hadoop hadoop 4221 2月 23 03:42 spark-env.sh.template[hadoop@hadoop1 conf]$ （2）复制spark-env.sh.template并重命名为spark-env.sh，并在文件最后添加配置内容 12[hadoop@hadoop1 conf]$ cp spark-env.sh.template spark-env.sh[hadoop@hadoop1 conf]$ vi spark-env.sh 1234567export JAVA_HOME=/usr/local/jdk1.8.0_73#export SCALA_HOME=/usr/share/scalaexport HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5export HADOOP_CONF_DIR=/home/hadoop/apps/hadoop-2.7.5/etc/hadoopexport SPARK_WORKER_MEMORY=500mexport SPARK_WORKER_CORES=1export SPARK_DAEMON_JAVA_OPTS=\"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop1:2181,hadoop2:2181,hadoop3:2181,hadoop4:2181 -Dspark.deploy.zookeeper.dir=/spark\" 注： #export SPARK_MASTER_IP=hadoop1 这个配置要注释掉。集群搭建时配置的spark参数可能和现在的不一样，主要是考虑个人电脑配置问题，如果memory配置太大，机器运行很慢。说明：-Dspark.deploy.recoveryMode=ZOOKEEPER #说明整个集群状态是通过zookeeper来维护的，整个集群状态的恢复也是通过zookeeper来维护的。就是说用zookeeper做了spark的HA配置，Master(Active)挂掉的话，Master(standby)要想变成Master（Active）的话，Master(Standby)就要像zookeeper读取整个集群状态信息，然后进行恢复所有Worker和Driver的状态信息，和所有的Application状态信息；-Dspark.deploy.zookeeper.url=hadoop1:2181,hadoop2:2181,hadoop3:2181,hadoop4:2181#将所有配置了zookeeper，并且在这台机器上有可能做master(Active)的机器都配置进来；（我用了4台，就配置了4台） -Dspark.deploy.zookeeper.dir=/spark这里的dir和zookeeper配置文件zoo.cfg中的dataDir的区别？？？-Dspark.deploy.zookeeper.dir是保存spark的元数据，保存了spark的作业运行状态；zookeeper会保存spark集群的所有的状态信息，包括所有的Workers信息，所有的Applactions信息，所有的Driver信息,如果集群 （3）复制slaves.template成slaves 12[hadoop@hadoop1 conf]$ cp slaves.template slaves[hadoop@hadoop1 conf]$ vi slaves 添加如下内容 1234hadoop1hadoop2hadoop3hadoop4 （4）将安装包分发给其他节点 1234[hadoop@hadoop1 ~]$ cd apps/[hadoop@hadoop1 apps]$ scp -r spark-2.3.0-bin-hadoop2.7/ hadoop2:$PWD[hadoop@hadoop1 apps]$ scp -r spark-2.3.0-bin-hadoop2.7/ hadoop3:$PWD[hadoop@hadoop1 apps]$ scp -r spark-2.3.0-bin-hadoop2.7/ hadoop4:$PWD 创建软连接 123456789101112[hadoop@hadoop2 ~]$ cd apps/[hadoop@hadoop2 apps]$ lshadoop-2.7.5 hbase-1.2.6 spark-2.3.0-bin-hadoop2.7 zookeeper-3.4.10[hadoop@hadoop2 apps]$ ln -s spark-2.3.0-bin-hadoop2.7/ spark[hadoop@hadoop2 apps]$ ll总用量 16drwxr-xr-x 10 hadoop hadoop 4096 3月 23 20:29 hadoop-2.7.5drwxrwxr-x 7 hadoop hadoop 4096 3月 29 13:15 hbase-1.2.6lrwxrwxrwx 1 hadoop hadoop 26 4月 20 19:26 spark -&gt; spark-2.3.0-bin-hadoop2.7/drwxr-xr-x 13 hadoop hadoop 4096 4月 20 19:24 spark-2.3.0-bin-hadoop2.7drwxr-xr-x 10 hadoop hadoop 4096 3月 21 19:31 zookeeper-3.4.10[hadoop@hadoop2 apps]$ 4、配置环境变量所有节点均要配置 1234[hadoop@hadoop1 spark]$ vi ~/.bashrc #Sparkexport SPARK_HOME=/home/hadoop/apps/sparkexport PATH=$PATH:$SPARK_HOME/bin 保存并使其立即生效 1[hadoop@hadoop1 spark]$ source ~/.bashrc 四、启动1、先启动zookeeper集群所有节点均要执行 123456789[hadoop@hadoop1 ~]$ zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /home/hadoop/apps/zookeeper-3.4.10/bin/../conf/zoo.cfgStarting zookeeper ... STARTED[hadoop@hadoop1 ~]$ zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /home/hadoop/apps/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower[hadoop@hadoop1 ~]$ 2、在启动HDFS集群任意一个节点执行即可 1[hadoop@hadoop1 ~]$ start-dfs.sh 3、在启动Spark集群在一个节点上执行 12[hadoop@hadoop1 ~]$ cd apps/spark/sbin/[hadoop@hadoop1 sbin]$ start-all.sh 4、查看进程 5、问题查看进程发现spark集群只有hadoop1成功启动了Master进程，其他3个节点均没有启动成功，需要手动启动，进入到/home/hadoop/apps/spark/sbin目录下执行以下命令，3个节点都要执行 12[hadoop@hadoop2 ~]$ cd ~/apps/spark/sbin/[hadoop@hadoop2 sbin]$ start-master.sh 6、执行之后再次查看进程Master进程和Worker进程都以启动成功 五、验证1、查看Web界面Master状态hadoop1是ALIVE状态，hadoop2、hadoop3和hadoop4均是STANDBY状态 hadoop1节点 hadoop2节点 hadoop3 hadoop4 2、验证HA的高可用手动干掉hadoop1上面的Master进程，观察是否会自动进行切换 干掉hadoop1上的Master进程之后，再次查看web界面 hadoo1节点，由于Master进程被干掉，所以界面无法访问 hadoop2节点，Master被干掉之后，hadoop2节点上的Master成功篡位成功，成为ALIVE状态 hadoop3节点 hadoop4节点 1、执行第一个Spark程序1234567[hadoop@hadoop3 ~]$ /home/hadoop/apps/spark/bin/spark-submit \\&gt; --class org.apache.spark.examples.SparkPi \\&gt; --master spark://hadoop1:7077 \\&gt; --executor-memory 500m \\&gt; --total-executor-cores 1 \\&gt; /home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar \\&gt; 100 其中的spark://hadoop1:7077是下图中的地址 运行结果 2、启动spark shell1234[hadoop@hadoop1 ~]$ /home/hadoop/apps/spark/bin/spark-shell \\&gt; --master spark://hadoop1:7077 \\&gt; --executor-memory 500m \\&gt; --total-executor-cores 1 参数说明： –master spark://hadoop1:7077 指定Master的地址 –executor-memory 500m:指定每个worker可用内存为500m –total-executor-cores 1: 指定整个集群使用的cup核数为1个 注意： 如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。 Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可 Spark Shell中已经默认将SparkSQl类初始化为对象spark。用户代码如果需要用到，则直接应用spark即可 3、 在spark shell中编写WordCount程序（1）编写一个hello.txt文件并上传到HDFS上的spark目录下 123[hadoop@hadoop1 ~]$ vi hello.txt[hadoop@hadoop1 ~]$ hadoop fs -mkdir -p /spark[hadoop@hadoop1 ~]$ hadoop fs -put hello.txt /spark hello.txt的内容如下 12345you,jumpi,jumpyou,jumpi,jumpjump （2）在spark shell中用scala语言编写spark程序 1scala&gt; sc.textFile(&quot;/spark/hello.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;/spark/out&quot;) 说明： sc是SparkContext对象，该对象是提交spark程序的入口 textFile(“/spark/hello.txt”)是hdfs中读取数据 flatMap(_.split(“ “))先map再压平 map((_,1))将单词和1构成元组 reduceByKey(+)按照key进行reduce，并将value累加 saveAsTextFile(“/spark/out”)将结果写入到hdfs中 （3）使用hdfs命令查看结果 12345[hadoop@hadoop2 ~]$ hadoop fs -cat /spark/out/p*(jump,5)(you,2)(i,2)[hadoop@hadoop2 ~]$ 七、 执行Spark程序on YARN1、前提成功启动zookeeper集群、HDFS集群、YARN集群 2、启动Spark on YARN1[hadoop@hadoop1 bin]$ spark-shell --master yarn --deploy-mode client 报错如下： 报错原因：内存资源给的过小，yarn直接kill掉进程，则报rpc连接失败、ClosedChannelException等错误。 解决方法： 先停止YARN服务，然后修改yarn-site.xml，增加如下内容 12345678910&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;Whether virtual memory limits will be enforced for containers&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt; &lt;value&gt;4&lt;/value&gt; &lt;description&gt;Ratio between virtual memory to physical memory when setting memory limits for containers&lt;/description&gt;&lt;/property&gt; 将新的yarn-site.xml文件分发到其他Hadoop节点对应的目录下，最后在重新启动YARN。 重新执行以下命令启动spark on yarn 1[hadoop@hadoop1 hadoop]$ spark-shell --master yarn --deploy-mode client 启动成功 3、打开YARN的web界面打开YARN WEB页面：http://hadoop4:8088可以看到Spark shell应用程序正在运行 单击ID号链接，可以看到该应用程序的详细信息 单击“ApplicationMaster”链接 4、运行程序12345678910scala&gt; val array = Array(1,2,3,4,5)array: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt; val rdd = sc.makeRDD(array)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at makeRDD at &lt;console&gt;:26scala&gt; rdd.countres0: Long = 5 scala&gt; 再次查看YARN的web界面 查看executors 5、执行Spark自带的示例程序PI 12345678[hadoop@hadoop1 ~]$ spark-submit --class org.apache.spark.examples.SparkPi \\&gt; --master yarn \\&gt; --deploy-mode cluster \\&gt; --driver-memory 500m \\&gt; --executor-memory 500m \\&gt; --executor-cores 1 \\&gt; /home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar &gt; 10 执行过程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687[hadoop@hadoop1 ~]$ spark-submit --class org.apache.spark.examples.SparkPi \\&gt; --master yarn \\&gt; --deploy-mode cluster \\&gt; --driver-memory 500m \\&gt; --executor-memory 500m \\&gt; --executor-cores 1 \\&gt; /home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar \\&gt; 102018-04-21 17:57:32 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable2018-04-21 17:57:34 INFO ConfiguredRMFailoverProxyProvider:100 - Failing over to rm22018-04-21 17:57:34 INFO Client:54 - Requesting a new application from cluster with 4 NodeManagers2018-04-21 17:57:34 INFO Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)2018-04-21 17:57:34 INFO Client:54 - Will allocate AM container, with 884 MB memory including 384 MB overhead2018-04-21 17:57:34 INFO Client:54 - Setting up container launch context for our AM2018-04-21 17:57:34 INFO Client:54 - Setting up the launch environment for our AM container2018-04-21 17:57:34 INFO Client:54 - Preparing resources for our AM container2018-04-21 17:57:36 WARN Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.2018-04-21 17:57:39 INFO Client:54 - Uploading resource file:/tmp/spark-93bd68c9-85de-482e-bbd7-cd2cee60e720/__spark_libs__8262081479435245591.zip -&gt; hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005/__spark_libs__8262081479435245591.zip2018-04-21 17:57:44 INFO Client:54 - Uploading resource file:/home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar -&gt; hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005/spark-examples_2.11-2.3.0.jar2018-04-21 17:57:44 INFO Client:54 - Uploading resource file:/tmp/spark-93bd68c9-85de-482e-bbd7-cd2cee60e720/__spark_conf__2498510663663992254.zip -&gt; hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005/__spark_conf__.zip2018-04-21 17:57:44 INFO SecurityManager:54 - Changing view acls to: hadoop2018-04-21 17:57:44 INFO SecurityManager:54 - Changing modify acls to: hadoop2018-04-21 17:57:44 INFO SecurityManager:54 - Changing view acls groups to: 2018-04-21 17:57:44 INFO SecurityManager:54 - Changing modify acls groups to: 2018-04-21 17:57:44 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set()2018-04-21 17:57:44 INFO Client:54 - Submitting application application_1524303370510_0005 to ResourceManager2018-04-21 17:57:44 INFO YarnClientImpl:273 - Submitted application application_1524303370510_00052018-04-21 17:57:45 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:45 INFO Client:54 - client token: N/A diagnostics: N/A ApplicationMaster host: N/A ApplicationMaster RPC port: -1 queue: default start time: 1524304664749 final status: UNDEFINED tracking URL: http://hadoop4:8088/proxy/application_1524303370510_0005/ user: hadoop2018-04-21 17:57:46 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:47 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:48 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:49 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:50 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:51 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:52 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:53 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:54 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:57:54 INFO Client:54 - client token: N/A diagnostics: N/A ApplicationMaster host: 192.168.123.104 ApplicationMaster RPC port: 0 queue: default start time: 1524304664749 final status: UNDEFINED tracking URL: http://hadoop4:8088/proxy/application_1524303370510_0005/ user: hadoop2018-04-21 17:57:55 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:57:56 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:57:57 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:57:58 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:57:59 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:00 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:01 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:02 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:03 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:04 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:05 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:06 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:07 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:08 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:09 INFO Client:54 - Application report for application_1524303370510_0005 (state: FINISHED)2018-04-21 17:58:09 INFO Client:54 - client token: N/A diagnostics: N/A ApplicationMaster host: 192.168.123.104 ApplicationMaster RPC port: 0 queue: default start time: 1524304664749 final status: SUCCEEDED tracking URL: http://hadoop4:8088/proxy/application_1524303370510_0005/ user: hadoop2018-04-21 17:58:09 INFO Client:54 - Deleted staging directory hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_00052018-04-21 17:58:09 INFO ShutdownHookManager:54 - Shutdown hook called2018-04-21 17:58:09 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-93bd68c9-85de-482e-bbd7-cd2cee60e7202018-04-21 17:58:09 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-06de6905-8067-4f1e-a0a0-bc8a51daf535[hadoop@hadoop1 ~]$","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （一）Spark初识","slug":"2019-06-01-Spark学习之路 （一）Spark初识","date":"2019-06-01T02:30:04.000Z","updated":"2019-09-16T02:07:51.317Z","comments":true,"path":"2019-06-01-Spark学习之路 （一）Spark初识.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-01-Spark学习之路 （一）Spark初识.html","excerpt":"** Spark学习之路 （一）Spark初识：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** Spark学习之路 （一）Spark初识：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt; 1.1官网官网地址：http://spark.apache.org 1、什么是Spark Apache Spark™是用于大规模数据处理的统一分析引擎。 spark是一个实现快速通用的集群计算平台。它是由加州大学伯克利分校AMP实验室 开发的通用内存并行计算框架，用来构建大型的、低延迟的数据分析应用程序。它扩展了广泛使用的MapReduce计算 模型。高效的支撑更多计算模式，包括交互式查询和流处理。spark的一个主要特点是能够在内存中进行计算，及时依赖磁盘进行复杂的运算，Spark依然比MapReduce更加高效。 2、为什么要学Spark中间结果输出：基于MapReduce的计算引擎通常会将中间结果输出到磁盘上，进行存储和容错。出于任务管道承接的，考虑，当一些查询翻译到MapReduce任务时，往往会产生多个Stage，而这些串联的Stage又依赖于底层文件系统（如HDFS）来存储每一个Stage的输出结果。 Spark是MapReduce的替代方案，而且兼容HDFS、Hive，可融入Hadoop的生态系统，以弥补MapReduce的不足。 二、Spark的四大特性1、高效性运行速度提高100倍。 ​ Apache Spark使用最先进的DAG调度程序，查询优化程序和物理执行引擎，实现批量和流式数据的高性能。 2、易用性​ Spark提供80多个高级算法，可以轻松构建并行应用程序。您可以 从Scala，Python，R和SQL shell中以交互方式使用它。 3、通用性​ Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。 Spark组成(BDAS)：全称伯克利数据分析栈，通过大规模集成算法、机器、人之间展现大数据应用的一个平台。也是处理大数据、云计算、通信的技术解决方案。 它的主要组件有： SparkCore：将分布式数据抽象为弹性分布式数据集（RDD），实现了应用任务调度、RPC、序列化和压缩，并为运行在其上的上层组件提供API。 SparkSQL：Spark Sql 是Spark来操作结构化数据的程序包，可以让我使用SQL语句的方式来查询数据，Spark支持 多种数据源，包含Hive表，parquest以及JSON等内容。 SparkStreaming： 是Spark提供的实时数据进行流式计算的组件。 MLlib：提供常用机器学习算法的实现库。 GraphX：提供一个分布式图计算框架，能高效进行图计算。 4、兼容性​ Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。 Mesos：Spark可以运行在Mesos里面（Mesos 类似于yarn的一个资源调度框架） standalone：Spark自己可以给自己分配资源（master，worker） YARN：Spark可以运行在yarn上面 Kubernetes：Spark接收 Kubernetes的资源调度 三、应用场景Yahoo将Spark用在Audience Expansion中的应用，进行点击预测和即席查询等 淘宝技术团队使用了Spark来解决多次迭代的机器学习算法、高计算复杂度的算法等。应用于内容推荐、社区发现等腾讯大数据精准推荐借助Spark快速迭代的优势，实现了在“数据实时采集、算法实时训练、系统实时预测”的全流程实时并行高维算法，最终成功应用于广点通pCTR投放系统上。优酷土豆将Spark应用于视频推荐(图计算)、广告业务，主要实现机器学习、图计算等迭代计算。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Sqoop学习之路","slug":"2019-04-22-Sqoop学习之路","date":"2019-05-22T02:30:04.000Z","updated":"2019-09-18T15:03:52.425Z","comments":true,"path":"2019-04-22-Sqoop学习之路.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-22-Sqoop学习之路.html","excerpt":"** Sqoop学习之路：** &lt;Excerpt in index | 首页摘要&gt; ​ Sqoop学习之路 （一）","text":"** Sqoop学习之路：** &lt;Excerpt in index | 首页摘要&gt; ​ Sqoop学习之路 （一） &lt;The rest of contents | 余下全文&gt; 一、概述sqoop 是 apache 旗下一款“Hadoop 和关系数据库服务器之间传送数据”的工具。 核心的功能有两个： 导入、迁入 导出、迁出 导入数据：MySQL，Oracle 导入数据到 Hadoop 的 HDFS、HIVE、HBASE 等数据存储系统 导出数据：从 Hadoop 的文件系统中导出数据到关系数据库 mysql 等 Sqoop 的本质还是一个命令行工具，和 HDFS，Hive 相比，并没有什么高深的理论。 sqoop： 工具：本质就是迁移数据， 迁移的方式：就是把sqoop的迁移命令转换成MR程序 hive 工具，本质就是执行计算，依赖于HDFS存储数据，把SQL转换成MR程序 二、工作机制将导入或导出命令翻译成 MapReduce 程序来实现 在翻译出的 MapReduce 中主要是对 InputFormat 和 OutputFormat 进行定制 三、安装1、前提概述将来sqoop在使用的时候有可能会跟那些系统或者组件打交道？ HDFS， MapReduce， YARN， ZooKeeper， Hive， HBase， MySQL sqoop就是一个工具， 只需要在一个节点上进行安装即可。 补充一点： 如果你的sqoop工具将来要进行hive或者hbase等等的系统和MySQL之间的交互 你安装的SQOOP软件的节点一定要包含以上你要使用的集群或者软件系统的安装包 补充一点： 将来要使用的azakban这个软件 除了会调度 hadoop的任务或者hbase或者hive的任务之外， 还会调度sqoop的任务 azkaban这个软件的安装节点也必须包含以上这些软件系统的客户端/2、 2、软件下载下载地址http://mirrors.hust.edu.cn/apache/ sqoop版本说明 绝大部分企业所使用的sqoop的版本都是 sqoop1 sqoop-1.4.6 或者 sqoop-1.4.7 它是 sqoop1 sqoop-1.99.4—-都是 sqoop2 此处使用sqoop-1.4.6版本sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz 3、安装步骤（1）上传解压缩安装包到指定目录因为之前hive只是安装在hadoop3机器上，所以sqoop也同样安装在hadoop3机器上 1[hadoop@hadoop3 ~]$ tar -zxvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C apps/ （2）进入到 conf 文件夹，找到 sqoop-env-template.sh，修改其名称为 sqoop-env.sh cd conf123456789[hadoop@hadoop3 ~]$ cd apps/[hadoop@hadoop3 apps]$ lsapache-hive-2.3.3-bin hadoop-2.7.5 hbase-1.2.6 sqoop-1.4.6.bin__hadoop-2.0.4-alpha zookeeper-3.4.10[hadoop@hadoop3 apps]$ mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ sqoop-1.4.6[hadoop@hadoop3 apps]$ cd sqoop-1.4.6/conf/[hadoop@hadoop3 conf]$ lsoraoop-site-template.xml sqoop-env-template.sh sqoop-site.xmlsqoop-env-template.cmd sqoop-site-template.xml[hadoop@hadoop3 conf]$ mv sqoop-env-template.sh sqoop-env.sh （3）修改 sqoop-env.sh1[hadoop@hadoop3 conf]$ vi sqoop-env.sh 12345678910111213export HADOOP_COMMON_HOME=/home/hadoop/apps/hadoop-2.7.5#Set path to where hadoop-*-core.jar is availableexport HADOOP_MAPRED_HOME=/home/hadoop/apps/hadoop-2.7.5#set the path to where bin/hbase is availableexport HBASE_HOME=/home/hadoop/apps/hbase-1.2.6#Set the path to where bin/hive is availableexport HIVE_HOME=/home/hadoop/apps/apache-hive-2.3.3-bin#Set the path for where zookeper config dir isexport ZOOCFGDIR=/home/hadoop/apps/zookeeper-3.4.10/conf 为什么在sqoop-env.sh 文件中会要求分别进行 common和mapreduce的配置呢？？？ 在apache的hadoop的安装中；四大组件都是安装在同一个hadoop_home中的 但是在CDH, HDP中， 这些组件都是可选的。 在安装hadoop的时候，可以选择性的只安装HDFS或者YARN， CDH,HDP在安装hadoop的时候，会把HDFS和MapReduce有可能分别安装在不同的地方。 （4）加入 mysql 驱动包到 sqoop1.4.6/lib 目录下1[hadoop@hadoop3 ~]$ cp mysql-connector-java-5.1.40-bin.jar apps/sqoop-1.4.6/lib/ （5）配置系统环境变量1234[hadoop@hadoop3 ~]$ vi .bashrc #Sqoopexport SQOOP_HOME=/home/hadoop/apps/sqoop-1.4.6export PATH=$PATH:$SQOOP_HOME/bin 保存退出使其立即生效 1[hadoop@hadoop3 ~]$ source .bashrc （6）验证安装是否成功 sqoop-version 或者 sqoop version 四、Sqoop的基本命令基本操作首先，我们可以使用 sqoop help 来查看，sqoop 支持哪些命令 1234567891011121314151617181920212223242526[hadoop@hadoop3 ~]$ sqoop helpWarning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.18/04/12 13:37:19 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6usage: sqoop COMMAND [ARGS]Available commands: codegen Generate code to interact with database records create-hive-table Import a table definition into Hive eval Evaluate a SQL statement and display the results export Export an HDFS directory to a database table help List available commands import Import a table from a database to HDFS import-all-tables Import tables from a database to HDFS import-mainframe Import datasets from a mainframe server to HDFS job Work with saved jobs list-databases List available databases on a server list-tables List available tables in a database merge Merge results of incremental imports metastore Run a standalone Sqoop metastore version Display version informationSee 'sqoop help COMMAND' for information on a specific command.[hadoop@hadoop3 ~]$ 然后得到这些支持了的命令之后，如果不知道使用方式，可以使用 sqoop command 的方式 来查看某条具体命令的使用方式，比如： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307[hadoop@hadoop3 ~]$ sqoop help importWarning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.18/04/12 13:38:29 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6usage: sqoop import [GENERIC-ARGS] [TOOL-ARGS]Common arguments: --connect &lt;jdbc-uri&gt; Specify JDBC connect string --connection-manager &lt;class-name&gt; Specify connection manager class name --connection-param-file &lt;properties-file&gt; Specify connection parameters file --driver &lt;class-name&gt; Manually specify JDBC driver class to use --hadoop-home &lt;hdir&gt; Override $HADOOP_MAPRED_HOME_ARG --hadoop-mapred-home &lt;dir&gt; Override $HADOOP_MAPRED_HOME_ARG --help Print usage instructions-P Read password from console --password &lt;password&gt; Set authentication password --password-alias &lt;password-alias&gt; Credential provider password alias --password-file &lt;password-file&gt; Set authentication password file path --relaxed-isolation Use read-uncommitted isolation for imports --skip-dist-cache Skip copying jars to distributed cache --username &lt;username&gt; Set authentication username --verbose Print more information while workingImport control arguments: --append Imports data in append mode --as-avrodatafile Imports data to Avro data files --as-parquetfile Imports data to Parquet files --as-sequencefile Imports data to SequenceFile s --as-textfile Imports data as plain text (default) --autoreset-to-one-mapper Reset the number of mappers to one mapper if no split key available --boundary-query &lt;statement&gt; Set boundary query for retrieving max and min value of the primary key --columns &lt;col,col,col...&gt; Columns to import from table --compression-codec &lt;codec&gt; Compression codec to use for import --delete-target-dir Imports data in delete mode --direct Use direct import fast path --direct-split-size &lt;n&gt; Split the input stream every 'n' bytes when importing in direct mode-e,--query &lt;statement&gt; Import results of SQL 'statement' --fetch-size &lt;n&gt; Set number 'n' of rows to fetch from the database when more rows are needed --inline-lob-limit &lt;n&gt; Set the maximum size for an inline LOB-m,--num-mappers &lt;n&gt; Use 'n' map tasks to import in parallel --mapreduce-job-name &lt;name&gt; Set name for generated mapreduce job --merge-key &lt;column&gt; Key column to use to join results --split-by &lt;column-name&gt; Column of the table used to split work units --table &lt;table-name&gt; Table to read --target-dir &lt;dir&gt; HDFS plain table destination --validate Validate the copy using the configured validator --validation-failurehandler &lt;validation-failurehandler&gt; Fully qualified class name for ValidationFa ilureHandler --validation-threshold &lt;validation-threshold&gt; Fully qualified class name for ValidationTh reshold --validator &lt;validator&gt; Fully qualified class name for the Validator --warehouse-dir &lt;dir&gt; HDFS parent for table destination --where &lt;where clause&gt; WHERE clause to use during import-z,--compress Enable compressionIncremental import arguments: --check-column &lt;column&gt; Source column to check for incremental change --incremental &lt;import-type&gt; Define an incremental import of type 'append' or 'lastmodified' --last-value &lt;value&gt; Last imported value in the incremental check columnOutput line formatting arguments: --enclosed-by &lt;char&gt; Sets a required field enclosing character --escaped-by &lt;char&gt; Sets the escape character --fields-terminated-by &lt;char&gt; Sets the field separator character --lines-terminated-by &lt;char&gt; Sets the end-of-line character --mysql-delimiters Uses MySQL's default delimiter set: fields: , lines: \\n escaped-by: \\ optionally-enclosed-by: ' --optionally-enclosed-by &lt;char&gt; Sets a field enclosing characterInput parsing arguments: --input-enclosed-by &lt;char&gt; Sets a required field encloser --input-escaped-by &lt;char&gt; Sets the input escape character --input-fields-terminated-by &lt;char&gt; Sets the input field separator --input-lines-terminated-by &lt;char&gt; Sets the input end-of-line char --input-optionally-enclosed-by &lt;char&gt; Sets a field enclosing characterHive arguments: --create-hive-table Fail if the target hive table exists --hive-database &lt;database-name&gt; Sets the database name to use when importing to hive --hive-delims-replacement &lt;arg&gt; Replace Hive record \\0x01 and row delimiters (\\n\\r) from imported string fields with user-defined string --hive-drop-import-delims Drop Hive record \\0x01 and row delimiters (\\n\\r) from imported string fields --hive-home &lt;dir&gt; Override $HIVE_HOME --hive-import Import tables into Hive (Uses Hive's default delimiters if none are set.) --hive-overwrite Overwrite existing data in the Hive table --hive-partition-key &lt;partition-key&gt; Sets the partition key to use when importing to hive --hive-partition-value &lt;partition-value&gt; Sets the partition value to use when importing to hive --hive-table &lt;table-name&gt; Sets the table name to use when importing to hive --map-column-hive &lt;arg&gt; Override mapping for specific column to hive types.HBase arguments: --column-family &lt;family&gt; Sets the target column family for the import --hbase-bulkload Enables HBase bulk loading --hbase-create-table If specified, create missing HBase tables --hbase-row-key &lt;col&gt; Specifies which input column to use as the row key --hbase-table &lt;table&gt; Import to &lt;table&gt; in HBaseHCatalog arguments: --hcatalog-database &lt;arg&gt; HCatalog database name --hcatalog-home &lt;hdir&gt; Override $HCAT_HOME --hcatalog-partition-keys &lt;partition-key&gt; Sets the partition keys to use when importing to hive --hcatalog-partition-values &lt;partition-value&gt; Sets the partition values to use when importing to hive --hcatalog-table &lt;arg&gt; HCatalog table name --hive-home &lt;dir&gt; Override $HIVE_HOME --hive-partition-key &lt;partition-key&gt; Sets the partition key to use when importing to hive --hive-partition-value &lt;partition-value&gt; Sets the partition value to use when importing to hive --map-column-hive &lt;arg&gt; Override mapping for specific column to hive types.HCatalog import specific options: --create-hcatalog-table Create HCatalog before import --hcatalog-storage-stanza &lt;arg&gt; HCatalog storage stanza for table creationAccumulo arguments: --accumulo-batch-size &lt;size&gt; Batch size in bytes --accumulo-column-family &lt;family&gt; Sets the target column family for the import --accumulo-create-table If specified, create missing Accumulo tables --accumulo-instance &lt;instance&gt; Accumulo instance name. --accumulo-max-latency &lt;latency&gt; Max write latency in milliseconds --accumulo-password &lt;password&gt; Accumulo password. --accumulo-row-key &lt;col&gt; Specifies which input column to use as the row key --accumulo-table &lt;table&gt; Import to &lt;table&gt; in Accumulo --accumulo-user &lt;user&gt; Accumulo user name. --accumulo-visibility &lt;vis&gt; Visibility token to be applied to all rows imported --accumulo-zookeepers &lt;zookeepers&gt; Comma-separated list of zookeepers (host:port)Code generation arguments: --bindir &lt;dir&gt; Output directory for compiled objects --class-name &lt;name&gt; Sets the generated class name. This overrides --package-name. When combined with --jar-file, sets the input class. --input-null-non-string &lt;null-str&gt; Input null non-string representation --input-null-string &lt;null-str&gt; Input null string representation --jar-file &lt;file&gt; Disable code generation; use specified jar --map-column-java &lt;arg&gt; Override mapping for specific columns to java types --null-non-string &lt;null-str&gt; Null non-string representation --null-string &lt;null-str&gt; Null string representation --outdir &lt;dir&gt; Output directory for generated code --package-name &lt;name&gt; Put auto-generated classes in this packageGeneric Hadoop command-line arguments:(must preceed any tool-specific arguments)Generic options supported are-conf &lt;configuration file&gt; specify an application configuration file-D &lt;property=value&gt; use value for given property-fs &lt;local|namenode:port&gt; specify a namenode-jt &lt;local|resourcemanager:port&gt; specify a ResourceManager-files &lt;comma separated list of files&gt; specify comma separated files to be copied to the map reduce cluster-libjars &lt;comma separated list of jars&gt; specify comma separated jar files to include in the classpath.-archives &lt;comma separated list of archives&gt; specify comma separated archives to be unarchived on the compute machines.The general command line syntax isbin/hadoop command [genericOptions] [commandOptions]At minimum, you must specify --connect and --tableArguments to mysqldump and other subprograms may be suppliedafter a '--' on the command line.[hadoop@hadoop3 ~]$ 示例列出MySQL数据有哪些数据库1234567891011121314151617[hadoop@hadoop3 ~]$ sqoop list-databases \\&gt; --connect jdbc:mysql://hadoop1:3306/ \\&gt; --username root \\&gt; --password rootWarning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.18/04/12 13:43:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.618/04/12 13:43:51 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.18/04/12 13:43:51 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.information_schemahivedbmysqlperformance_schematest[hadoop@hadoop3 ~]$ 列出MySQL中的某个数据库有哪些数据表：1234[hadoop@hadoop3 ~]$ sqoop list-tables **&gt; --connect jdbc:mysql://hadoop1:3306/mysql **&gt; --username root **&gt; --password root** 12345678910111213141516171819202122232425262728293031323334353637383940[hadoop@hadoop3 ~]$ sqoop list-tables \\&gt; --connect jdbc:mysql://hadoop1:3306/mysql \\&gt; --username root \\&gt; --password rootWarning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.18/04/12 13:46:21 INFO sqoop.Sqoop: Running Sqoop version: 1.4.618/04/12 13:46:21 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.18/04/12 13:46:21 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.columns_privdbeventfuncgeneral_loghelp_categoryhelp_keywordhelp_relationhelp_topicinnodb_index_statsinnodb_table_statsndb_binlog_indexpluginprocprocs_privproxies_privserversslave_master_infoslave_relay_log_infoslave_worker_infoslow_logtables_privtime_zonetime_zone_leap_secondtime_zone_nametime_zone_transitiontime_zone_transition_typeuser[hadoop@hadoop3 ~]$ 创建一张跟mysql中的help_keyword表一样的hive表hk：123456sqoop create-hive-table \\--connect jdbc:mysql://hadoop1:3306/mysql \\--username root \\--password root \\--table help_keyword \\--hive-table hk 1234567891011121314151617181920212223242526272829303132333435[hadoop@hadoop3 ~]$ sqoop create-hive-table \\&gt; --connect jdbc:mysql://hadoop1:3306/mysql \\&gt; --username root \\&gt; --password root \\&gt; --table help_keyword \\&gt; --hive-table hkWarning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.18/04/12 13:50:20 INFO sqoop.Sqoop: Running Sqoop version: 1.4.618/04/12 13:50:20 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.18/04/12 13:50:20 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override18/04/12 13:50:20 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.18/04/12 13:50:20 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.18/04/12 13:50:21 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 118/04/12 13:50:21 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 1SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/hadoop/apps/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]18/04/12 13:50:23 INFO hive.HiveImport: Loading uploaded data into Hive18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/home/hadoop/apps/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]18/04/12 13:50:36 INFO hive.HiveImport: 18/04/12 13:50:36 INFO hive.HiveImport: Logging initialized using configuration in jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/hive-common-2.3.3.jar!/hive-log4j2.properties Async: true18/04/12 13:50:50 INFO hive.HiveImport: OK18/04/12 13:50:50 INFO hive.HiveImport: Time taken: 11.651 seconds18/04/12 13:50:51 INFO hive.HiveImport: Hive import complete.[hadoop@hadoop3 ~]$ 五、Sqoop的数据导入“导入工具”导入单个表从 RDBMS 到 HDFS。表中的每一行被视为 HDFS 的记录。所有记录 都存储为文本文件的文本数据（或者 Avro、sequence 文件等二进制数据） 1、从RDBMS导入到HDFS中语法格式1sqoop import (generic-args) (import-args) 常用参数 12345678910--connect &lt;jdbc-uri&gt; jdbc 连接地址--connection-manager &lt;class-name&gt; 连接管理者--driver &lt;class-name&gt; 驱动类--hadoop-mapred-home &lt;dir&gt; $HADOOP_MAPRED_HOME--help help 信息-P 从命令行输入密码--password &lt;password&gt; 密码--username &lt;username&gt; 账号--verbose 打印流程信息--connection-param-file &lt;filename&gt; 可选参数 示例普通导入：导入mysql库中的help_keyword的数据到HDFS上 导入的默认路径：/user/hadoop/help_keyword 123456sqoop import \\--connect jdbc:mysql://hadoop1:3306/mysql \\--username root \\--password root \\--table help_keyword \\-m 1 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081[hadoop@hadoop3 ~]$ sqoop import \\&gt; --connect jdbc:mysql://hadoop1:3306/mysql \\&gt; --username root \\&gt; --password root \\&gt; --table help_keyword \\&gt; -m 1Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.18/04/12 13:53:48 INFO sqoop.Sqoop: Running Sqoop version: 1.4.618/04/12 13:53:48 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.18/04/12 13:53:48 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.18/04/12 13:53:48 INFO tool.CodeGenTool: Beginning code generation18/04/12 13:53:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 118/04/12 13:53:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 118/04/12 13:53:49 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /home/hadoop/apps/hadoop-2.7.5注: /tmp/sqoop-hadoop/compile/979d87b9521d0a09ee6620060a112d60/help_keyword.java使用或覆盖了已过时的 API。注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。18/04/12 13:53:51 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/979d87b9521d0a09ee6620060a112d60/help_keyword.jar18/04/12 13:53:51 WARN manager.MySQLManager: It looks like you are importing from mysql.18/04/12 13:53:51 WARN manager.MySQLManager: This transfer can be faster! Use the --direct18/04/12 13:53:51 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.18/04/12 13:53:51 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)18/04/12 13:53:51 INFO mapreduce.ImportJobBase: Beginning import of help_keywordSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/hadoop/apps/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]18/04/12 13:53:52 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar18/04/12 13:53:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps18/04/12 13:53:58 INFO db.DBInputFormat: Using read commited transaction isolation18/04/12 13:53:58 INFO mapreduce.JobSubmitter: number of splits:118/04/12 13:53:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1523510178850_000118/04/12 13:54:00 INFO impl.YarnClientImpl: Submitted application application_1523510178850_000118/04/12 13:54:00 INFO mapreduce.Job: The url to track the job: http://hadoop3:8088/proxy/application_1523510178850_0001/18/04/12 13:54:00 INFO mapreduce.Job: Running job: job_1523510178850_000118/04/12 13:54:17 INFO mapreduce.Job: Job job_1523510178850_0001 running in uber mode : false18/04/12 13:54:17 INFO mapreduce.Job: map 0% reduce 0%18/04/12 13:54:33 INFO mapreduce.Job: map 100% reduce 0%18/04/12 13:54:34 INFO mapreduce.Job: Job job_1523510178850_0001 completed successfully18/04/12 13:54:35 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=142965 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=87 HDFS: Number of bytes written=8264 HDFS: Number of read operations=4 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched map tasks=1 Other local map tasks=1 Total time spent by all maps in occupied slots (ms)=12142 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=12142 Total vcore-milliseconds taken by all map tasks=12142 Total megabyte-milliseconds taken by all map tasks=12433408 Map-Reduce Framework Map input records=619 Map output records=619 Input split bytes=87 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=123 CPU time spent (ms)=1310 Physical memory (bytes) snapshot=93212672 Virtual memory (bytes) snapshot=2068234240 Total committed heap usage (bytes)=17567744 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=826418/04/12 13:54:35 INFO mapreduce.ImportJobBase: Transferred 8.0703 KB in 41.8111 seconds (197.6507 bytes/sec)18/04/12 13:54:35 INFO mapreduce.ImportJobBase: Retrieved 619 records.[hadoop@hadoop3 ~]$ 查看导入的文件 1[hadoop@hadoop4 ~]$ hadoop fs -cat /user/hadoop/help_keyword/part-m-00000 导入： 指定分隔符和导入路径 12345678sqoop import \\--connect jdbc:mysql://hadoop1:3306/mysql \\--username root \\--password root \\--table help_keyword \\--target-dir /user/hadoop11/my_help_keyword1 \\--fields-terminated-by &apos;\\t&apos; \\-m 2 导入数据：带where条件 12345678sqoop import \\--connect jdbc:mysql://hadoop1:3306/mysql \\--username root \\--password root \\--where &quot;name=&apos;STRING&apos; &quot; \\--table help_keyword \\--target-dir /sqoop/hadoop11/myoutport1 \\-m 1 查询指定列 12345678910sqoop import \\--connect jdbc:mysql://hadoop1:3306/mysql \\--username root \\--password root \\--columns &quot;name&quot; \\--where &quot;name=&apos;STRING&apos; &quot; \\--table help_keyword \\--target-dir /sqoop/hadoop11/myoutport22 \\-m 1selct name from help_keyword where name = &quot;string&quot; 导入：指定自定义查询SQL 123456789sqoop import \\--connect jdbc:mysql://hadoop1:3306/ \\--username root \\--password root \\--target-dir /user/hadoop/myimport33_1 \\--query &apos;select help_keyword_id,name from mysql.help_keyword where $CONDITIONS and name = &quot;STRING&quot;&apos; \\--split-by help_keyword_id \\--fields-terminated-by &apos;\\t&apos; \\-m 4 在以上需要按照自定义SQL语句导出数据到HDFS的情况下：1、引号问题，要么外层使用单引号，内层使用双引号，$CONDITIONS的$符号不用转义， 要么外层使用双引号，那么内层使用单引号，然后$CONDITIONS的$符号需要转义2、自定义的SQL语句中必须带有WHERE $CONDITIONS 2、把MySQL数据库中的表数据导入到Hive中Sqoop 导入关系型数据到 hive 的过程是先导入到 hdfs，然后再 load 进入 hive 普通导入：数据存储在默认的default hive库中，表名就是对应的mysql的表名： 1234567sqoop import \\--connect jdbc:mysql://hadoop1:3306/mysql \\--username root \\--password root \\--table help_keyword \\--hive-import \\-m 1 导入过程 第一步：导入mysql.help_keyword的数据到hdfs的默认路径第二步：自动仿造mysql.help_keyword去创建一张hive表, 创建在默认的default库中第三步：把临时目录中的数据导入到hive表中 查看数据 1[hadoop@hadoop3 ~]$ hadoop fs -cat /user/hive/warehouse/help_keyword/part-m-00000 指定行分隔符和列分隔符，指定hive-import，指定覆盖导入，指定自动创建hive表，指定表名，指定删除中间结果数据目录 12345678910111213sqoop import \\--connect jdbc:mysql://hadoop1:3306/mysql \\--username root \\--password root \\--table help_keyword \\--fields-terminated-by \"\\t\" \\--lines-terminated-by \"\\n\" \\--hive-import \\--hive-overwrite \\--create-hive-table \\--delete-target-dir \\--hive-database mydb_test \\--hive-table new_help_keyword 报错原因是hive-import 当前这个导入命令。 sqoop会自动给创建hive的表。 但是不会自动创建不存在的库 手动创建mydb_test数据块 1234hive&gt; create database mydb_test;OKTime taken: 6.147 secondshive&gt; 之后再执行上面的语句没有报错 查询一下 1select * from new_help_keyword limit 10; 上面的导入语句等价于 123456789101112sqoop import \\--connect jdbc:mysql://hadoop1:3306/mysql \\--username root \\--password root \\--table help_keyword \\--fields-terminated-by \"\\t\" \\--lines-terminated-by \"\\n\" \\--hive-import \\--hive-overwrite \\--create-hive-table \\ --hive-table mydb_test.new_help_keyword \\--delete-target-dir 增量导入 执行增量导入之前，先清空hive数据库中的help_keyword表中的数据 1truncate table help_keyword; 12345678910sqoop import \\--connect jdbc:mysql://hadoop1:3306/mysql \\--username root \\--password root \\--table help_keyword \\--target-dir /user/hadoop/myimport_add \\--incremental append \\--check-column help_keyword_id \\--last-value 500 \\-m 1 语句执行成功 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495[hadoop@hadoop3 ~]$ sqoop import \\&gt; --connect jdbc:mysql://hadoop1:3306/mysql \\&gt; --username root \\&gt; --password root \\&gt; --table help_keyword \\&gt; --target-dir /user/hadoop/myimport_add \\&gt; --incremental append \\&gt; --check-column help_keyword_id \\&gt; --last-value 500 \\&gt; -m 1Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.18/04/12 22:01:07 INFO sqoop.Sqoop: Running Sqoop version: 1.4.618/04/12 22:01:08 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.18/04/12 22:01:08 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.18/04/12 22:01:08 INFO tool.CodeGenTool: Beginning code generation18/04/12 22:01:08 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 118/04/12 22:01:08 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 118/04/12 22:01:08 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /home/hadoop/apps/hadoop-2.7.5注: /tmp/sqoop-hadoop/compile/a51619d1ef8c6e4b112a209326ed9e0f/help_keyword.java使用或覆盖了已过时的 API。注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。18/04/12 22:01:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/a51619d1ef8c6e4b112a209326ed9e0f/help_keyword.jarSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/hadoop/apps/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]18/04/12 22:01:12 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(`help_keyword_id`) FROM `help_keyword`18/04/12 22:01:12 INFO tool.ImportTool: Incremental import based on column `help_keyword_id`18/04/12 22:01:12 INFO tool.ImportTool: Lower bound value: 50018/04/12 22:01:12 INFO tool.ImportTool: Upper bound value: 61818/04/12 22:01:12 WARN manager.MySQLManager: It looks like you are importing from mysql.18/04/12 22:01:12 WARN manager.MySQLManager: This transfer can be faster! Use the --direct18/04/12 22:01:12 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.18/04/12 22:01:12 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)18/04/12 22:01:12 INFO mapreduce.ImportJobBase: Beginning import of help_keyword18/04/12 22:01:12 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar18/04/12 22:01:12 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps18/04/12 22:01:17 INFO db.DBInputFormat: Using read commited transaction isolation18/04/12 22:01:17 INFO mapreduce.JobSubmitter: number of splits:118/04/12 22:01:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1523510178850_001018/04/12 22:01:19 INFO impl.YarnClientImpl: Submitted application application_1523510178850_001018/04/12 22:01:19 INFO mapreduce.Job: The url to track the job: http://hadoop3:8088/proxy/application_1523510178850_0010/18/04/12 22:01:19 INFO mapreduce.Job: Running job: job_1523510178850_001018/04/12 22:01:30 INFO mapreduce.Job: Job job_1523510178850_0010 running in uber mode : false18/04/12 22:01:30 INFO mapreduce.Job: map 0% reduce 0%18/04/12 22:01:40 INFO mapreduce.Job: map 100% reduce 0%18/04/12 22:01:40 INFO mapreduce.Job: Job job_1523510178850_0010 completed successfully18/04/12 22:01:41 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=143200 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=87 HDFS: Number of bytes written=1576 HDFS: Number of read operations=4 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched map tasks=1 Other local map tasks=1 Total time spent by all maps in occupied slots (ms)=7188 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=7188 Total vcore-milliseconds taken by all map tasks=7188 Total megabyte-milliseconds taken by all map tasks=7360512 Map-Reduce Framework Map input records=118 Map output records=118 Input split bytes=87 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=86 CPU time spent (ms)=870 Physical memory (bytes) snapshot=95576064 Virtual memory (bytes) snapshot=2068234240 Total committed heap usage (bytes)=18608128 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=157618/04/12 22:01:41 INFO mapreduce.ImportJobBase: Transferred 1.5391 KB in 28.3008 seconds (55.6875 bytes/sec)18/04/12 22:01:41 INFO mapreduce.ImportJobBase: Retrieved 118 records.18/04/12 22:01:41 INFO util.AppendUtils: Creating missing output directory - myimport_add18/04/12 22:01:41 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:18/04/12 22:01:41 INFO tool.ImportTool: --incremental append18/04/12 22:01:41 INFO tool.ImportTool: --check-column help_keyword_id18/04/12 22:01:41 INFO tool.ImportTool: --last-value 61818/04/12 22:01:41 INFO tool.ImportTool: (Consider saving this with &apos;sqoop job --create&apos;)[hadoop@hadoop3 ~]$ 查看结果 3、把MySQL数据库中的表数据导入到hbase 普通导入 12345678sqoop import \\--connect jdbc:mysql://hadoop1:3306/mysql \\--username root \\--password root \\--table help_keyword \\--hbase-table new_help_keyword \\--column-family person \\--hbase-row-key help_keyword_id 此时会报错，因为需要先创建Hbase里面的表，再执行导入的语句 12345hbase(main):001:0&gt; create 'new_help_keyword', 'base_info'0 row(s) in 3.6280 seconds=&gt; Hbase::Table - new_help_keywordhbase(main):002:0&gt;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Sqoop","slug":"Sqoop","permalink":"http://zhangfuxin.cn/tags/Sqoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Scala学习之路 （十）Scala的Actor","slug":"2019-05-10-Scala学习之路 （十）Scala的Actor","date":"2019-05-10T02:30:04.000Z","updated":"2019-09-18T15:53:38.539Z","comments":true,"path":"2019-05-10-Scala学习之路 （十）Scala的Actor.html","link":"","permalink":"http://zhangfuxin.cn/2019-05-10-Scala学习之路 （十）Scala的Actor.html","excerpt":"** Scala学习之路 （十）Scala的Actor：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （十）Scala的Actor","text":"** Scala学习之路 （十）Scala的Actor：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （十）Scala的Actor &lt;The rest of contents | 余下全文&gt; 一、Scala中的并发编程1、Java中的并发编程①Java中的并发编程基本上满足了事件之间相互独立，但是事件能够同时发生的场景的需要。 ②Java中的并发编程是基于共享数据和加锁的一种机制，即会有一个共享的数据，然后有若干个线程去访问这个共享的数据(主要是对这个共享的数据进行修改)，同时Java利用加锁的机制(即synchronized)来确保同一时间只有一个线程对我们的共享数据进行访问,进而保证共享数据的一致性。 ③Java中的并发编程存在资源争夺和死锁等多种问题，因此程序越大问题越麻烦。 2、Scala中的并发编程①Scala中的并发编程思想与Java中的并发编程思想完全不一样，Scala中的Actor是一种不共享数据，依赖于消息传递的一种并发编程模式， 避免了死锁、资源争夺等情况。在具体实现的过程中，Scala中的Actor会不断的循环自己的邮箱，并通过receive偏函数进行消息的模式匹配并进行相应的处理。 ②如果Actor A和 Actor B要相互沟通的话，首先A要给B传递一个消息，B会有一个收件箱，然后B会不断的循环自己的收件箱， 若看见A发过来的消息，B就会解析A的消息并执行，处理完之后就有可能将处理的结果通过邮件的方式发送给A。 二、Scala中的Actor1、什么是Actor一个actor是一个容器，它包含 状态， 行为，信箱，子Actor 和 监管策略，所有这些包含在一个ActorReference（Actor引用）里。一个actor需要与外界隔离才能从actor模型中获益，所以actor是以actor引用的形式展现给外界的。 2、ActorSystem的层次结构如果一个Actor中的业务逻辑非常复杂，为了降低代码的复杂度，可以将其拆分成多个子任务（在一个actor的内部可以创建一个或多个actor，actor的创建者也是该actor的监控者） 一个ActorSystem应该被正确规划，例如哪一个Actor负责监控，监控什么等等： 负责分发的actor管理接受任务的actor 拥有重要数据的actor，找出所有可能丢失数据的子actor，并且处理他们的错误。 3、ActorPathActorPath是通过字符串描述Actor的层级关系，并唯一标识一个Actor的方法。 ActorPath包含协议，位置 和 Actor层级关系 12345678//本地path&quot;akka://my-sys/user/service-a/worker1&quot; //远程path akka.tcp://（ActorSystem的名称）@（远程地址的IP）：（远程地址的端口）/user/（Actor的名称）&quot;akka.tcp://my-sys@host.example.com:5678/user/service-b&quot; //akka集群&quot;cluster://my-cluster/service-c&quot; 远程地址不清楚是多少的话，可以在远程的服务启动的时候查看 4、获取Actor Reference获取Actor引用的方式有两种：创建 和 查找。 要创建Actor，可以调用ActorSystem.actorOf(..)，它创建的actor在guardian actor之下，接着可以调用ActorContext的actorOf(…) 在刚才创建的Actor内生成一个actor树。这些方法会返回新创建的actor的引用，每一个actor都可以通过访问ActorContext来获得自己（self），子Actor（children，child）和父actor（parent）。 要查找Actor Reference，可以调用ActorSystem或ActorContext的actorSelection(“path”)，在查找ActorRef时，可以使用相对路径或绝对路径，如果是相对路径，可以用 .. 来表示parent actor。 actorOf / actorSelection / actorFor的区别 actorOf 创建一个新的actor，创建的actor为调用该方法所属的context的直接子actor。 actorSelection 查找现有actor，并不会创建新的actor。 actorFor 查找现有actor，不创建新的actor，已过时。 5、Actor和ActorSystemActor：就是用来做消息传递的用来接收和发送消息的，一个actor就相当于是一个老师或者是学生。如果我们想要多个老师，或者学生，就需要创建多个actor实例。ActorSystem:用来创建和管理actor，并且还需要监控Actor。ActorSystem是单例的（object）在同一个进程里面，只需要一个ActorSystem就可以了 三、Actor的示例1、示例说明 2、代码实现MyResourceManager.scala（服务端） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package com.rpcimport akka.actor._import com.typesafe.config.&#123;Config, ConfigFactory&#125;import scala.collection.mutableclass MyResourceManager(var resourceManagerHostName:String, var resourceManagerPort:Int) extends Actor &#123; /** * 定义一个Map,接受MyNodeManager的注册信息，key是主机名， * value是NodeManagerInfo对象，里面存储主机名、CPU和内存信息 * */ var registerMap = new mutable.HashMap[String,NodeManagerInfo]() /** * 定义一个Set,接受MyNodeManager的注册信息，key是主机名， * value是NodeManagerInfo对象，里面存储主机名、CPU和内存信息 * 实际上和上面的Map里面存档内容一样，容易变历，可以不用写，主要是模仿后面Spark里面的内容 * 方便到时理解Spark源码 * */ var registerSet = new mutable.HashSet[NodeManagerInfo]() override def preStart(): Unit = &#123; import scala.concurrent.duration._ import context.dispatcher context.system.scheduler.schedule(0 millis, 5000 millis, self,CheckTimeOut) &#125; //对MyNodeManager传过来的信息进行匹配 override def receive: Receive = &#123; //匹配到NodeManager的注册信息进行对应处理 case NodeManagerRegisterMsg(nodeManagerID,cpu,memory) =&gt; &#123; //将注册信息实例化为一个NodeManagerInfo对象 val registerMsg = new NodeManagerInfo(nodeManagerID,cpu,memory) //将注册信息存储到registerMap和registerSet里面，key是主机名，value是NodeManagerInfo对象 registerMap.put(nodeManagerID,registerMsg) registerSet += registerMsg //注册成功之后，反馈个MyNodeManager一个成功的信息 sender() ! new RegisterFeedbackMsg(\"注册成功!\" + resourceManagerHostName+\":\"+resourceManagerPort) &#125; //匹配到心跳信息做相应处理 case HeartBeat(nodeManagerID) =&gt; &#123; //获取当前时间 val time:Long = System.currentTimeMillis() //根据nodeManagerID获取NodeManagerInfo对象 val info = registerMap(nodeManagerID) info.lastHeartBeatTime = time //更新registerMap和registerSet里面nodeManagerID对应的NodeManagerInfo对象信息(最后一次心跳时间) registerMap(nodeManagerID) = info registerSet += info &#125; //检测超时，对超时的数据从集合中删除 case CheckTimeOut =&gt; &#123; var time = System.currentTimeMillis() registerSet .filter( nm =&gt; time - nm.lastHeartBeatTime &gt; 10000) .foreach(deadnm =&gt; &#123; registerSet -= deadnm registerMap.remove(deadnm.nodeManagerID) &#125;) println(\"当前注册成功的节点数：\" + registerMap.size) &#125; &#125;&#125;object MyResourceManager &#123; def main(args: Array[String]): Unit = &#123; /** * 传参： * ResourceManager的主机地址、端口号 * */ val RM_HOSTNAME = args(0) val RM_PORT = args(1).toInt val str:String = \"\"\" |akka.actor.provider = \"akka.remote.RemoteActorRefProvider\" |akka.remote.netty.tcp.hostname =localhost |akka.remote.netty.tcp.port=19888 \"\"\".stripMargin val conf: Config = ConfigFactory.parseString(str) val actorSystem = ActorSystem(Conf.RMAS,conf) actorSystem.actorOf(Props(new MyResourceManager(RM_HOSTNAME,RM_PORT)),Conf.RMA) &#125;&#125; MyNodeManager.scala（客户端） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package com.rpcimport java.util.UUIDimport akka.actor._import com.typesafe.config.&#123;Config, ConfigFactory&#125;class MyNodeManager(resourceManagerHostName:String,resourceManagerPort:Int,cpu:Int,memory:Int) extends Actor&#123; //MyNodeManager的UUID var nodeManagerID:String = _ var rmref:ActorSelection = _ override def preStart(): Unit = &#123; //获取MyResourceManager的Actor的引用 rmref = context.actorSelection(s\"akka.tcp://$&#123;Conf.RMAS&#125;@$&#123;resourceManagerHostName&#125;:$&#123;resourceManagerPort&#125;/user/$&#123;Conf.RMA&#125;\") //生成随机的UUID nodeManagerID = UUID.randomUUID().toString /** * 向MyResourceManager发送注册信息 * */ rmref ! NodeManagerRegisterMsg(nodeManagerID,cpu,memory) &#125; //进行信息匹配 override def receive: Receive = &#123; //匹配到注册成功之后MyResourceManager反馈回的信息，进行相应处理 case RegisterFeedbackMsg(feedbackMsg) =&gt; &#123; /** * initialDelay: FiniteDuration, 多久以后开始执行 * interval: FiniteDuration, 每隔多长时间执行一次 * receiver: ActorRef, 给谁发送这个消息 * message: Any 发送的消息是啥 */ //定时任务需要导入的工具包 import scala.concurrent.duration._ import context.dispatcher //定时向自己发送信息 context.system.scheduler.schedule(0 millis, 3000 millis, self, SendMessage) &#125; //匹配到SendMessage信息之后做相应处理 case SendMessage =&gt; &#123; //向MyResourceManager发送心跳信息 rmref ! HeartBeat(nodeManagerID) println(Thread.currentThread().getId + \":\" + System.currentTimeMillis()) &#125; &#125;&#125;object MyNodeManager &#123; def main(args: Array[String]): Unit = &#123; /** * 传参： * NodeManager的主机地址、端口号、CPU、内存 * ResourceManager的主机地址、端口号 * */ val NM_HOSTNAME = args(0) val NM_PORT = args(1) val NM_CPU:Int = args(2).toInt val NM_MEMORY:Int = args(3).toInt val RM_HOSTNAME = args(4) val RM_PORT = args(5).toInt val str:String = s\"\"\" |akka.actor.provider = \"akka.remote.RemoteActorRefProvider\" |akka.remote.netty.tcp.hostname = $&#123;NM_HOSTNAME&#125; |akka.remote.netty.tcp.port = $&#123;NM_PORT&#125; \"\"\".stripMargin val conf: Config = ConfigFactory.parseString(str) val actorSystem = ActorSystem(Conf.NMAS,conf) actorSystem.actorOf(Props(new MyNodeManager(RM_HOSTNAME,RM_PORT,NM_CPU,NM_MEMORY)),Conf.NMA) &#125;&#125; Conf.scala（配置文件） 12345678910111213package com.rpc//避免硬编码object Conf &#123; //ResourceManagerActorSystem val RMAS = \"MyRMActorSystem\" //ResourceManagerActor val RMA = \"MyRMActor\" //NodeManagerActorSystem val NMAS = \"MyNMActorSystem\" //NodeManagerActor val NMA = \"MyNMactor\"&#125; Message.scala 123456789101112131415package com.rpc//NodeManager注册信息case class NodeManagerRegisterMsg(val nodeManagerID:String, var cpu:Int, var memory:Int)//ResourceManager接收到注册信息成功之后的返回信息case class RegisterFeedbackMsg(val feedbackMsg: String)//NodeManager的心跳信息case class HeartBeat(val nodeManagerID:String)//NodeManager注册信息class NodeManagerInfo(val nodeManagerID:String, var cpu:Int, var memory:Int)&#123; //定义一个属性，存储上一次的心跳时间 var lastHeartBeatTime:Long = _&#125;case object SendMessagecase object CheckTimeOut 3、运行（1）运行MyResourceManager运行结果 发现报错数组越界，原因是在启动时需要传入2个参数 重新启动，启动成功 2、运行MyNodeManager 报相同的错误，不过此处需要传入6个参数 重新启动，启动成功 3、观察MyResourceManager发现有一个节点连接成功 4、再启动一个MyNodeManager观察情况先修改MyNodeManager配置里面的端口 再启动 启动成功之后观察MyResourceManager，此时有2个节点连接成功 5、关闭一个节点，观察情况集合中连接超时的成功删除","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://zhangfuxin.cn/tags/Scala/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Scala学习之路 （九）Scala的上界和下届","slug":"2019-05-09-Scala学习之路 （九）Scala的上界和下届","date":"2019-05-09T02:30:04.000Z","updated":"2019-09-18T15:49:32.557Z","comments":true,"path":"2019-05-09-Scala学习之路 （九）Scala的上界和下届.html","link":"","permalink":"http://zhangfuxin.cn/2019-05-09-Scala学习之路 （九）Scala的上界和下届.html","excerpt":"** Scala学习之路 （九）Scala的上界和下届：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （九）Scala的上界和下届","text":"** Scala学习之路 （九）Scala的上界和下届：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （九）Scala的上界和下届 &lt;The rest of contents | 余下全文&gt; 一、泛型1、泛型的介绍泛型用于指定方法或类可以接受任意类型参数，参数在实际使用时才被确定，泛型可以有效地增强程序的适用性，使用泛型可以使得类或方法具有更强的通用性。泛型的典型应用场景是集合及集合中的方法参数，可以说同java一样，scala中泛型无处不在，具体可以查看scala的api。 2、泛型类、泛型方法泛型类：指定类可以接受任意类型参数。 泛型方法：指定方法可以接受任意类型参数。 3、示例1、定义泛型类1234567891011121314151617181920212223242526/* 下面的意思就是表示只要是Comparable就可以传递,下面是类上定义的泛型 */class GenericTest1[T &lt;: Comparable[T]] &#123; def choose(one:T,two:T): T =&#123; //定义一个选择的方法 if(one.compareTo(two) &gt; 0) one else two &#125;&#125;class Boy(val name:String,var age:Int) extends Comparable[Boy]&#123; override def compareTo(o: Boy): Int = &#123; this.age - o.age &#125;&#125;object GenericTestOne&#123; def main(args: Array[String]): Unit = &#123; val gt = new GenericTest1[Boy] val huangbo = new Boy(\"huangbo\",60) val xuzheng = new Boy(\"xuzheng\",66) val boy = gt.choose(huangbo,xuzheng) println(boy.name) &#125;&#125; 2、定义泛型方法1234567891011121314151617181920212223class GenericTest2&#123; //在方法上定义泛型 def choose[T &lt;: Comparable[T]](one:T,two:T): T =&#123; if(one.compareTo(two) &gt; 0) one else two &#125;&#125;class Boy(val name:String,var age:Int) extends Comparable[Boy]&#123; override def compareTo(o: Boy): Int = &#123; this.age - o.age &#125;&#125;object GenericTestTwo&#123; def main(args: Array[String]): Unit = &#123; val gt = new GenericTest2 val huangbo = new Boy(\"huangbo\",60) val xuzheng = new Boy(\"xuzheng\",66) val boy = gt.choose(huangbo,xuzheng) println(boy) &#125;&#125; 二、上界和下届1、介绍在指定泛型类型时，有时需要界定泛型类型的范围，而不是接收任意类型。比如，要求某个泛型类型，必须是某个类的子类，这样在程序中就可以放心的调用父类的方法，程序才能正常的使用与运行。此时，就可以使用上下边界Bounds的特性；Scala的上下边界特性允许泛型类型是某个类的子类，或者是某个类的父类； (1) U &gt;: T 这是类型下界的定义，也就是U必须是类型T的父类(或本身，自己也可以认为是自己的父类)。 (2) S &lt;: T 这是类型上界的定义，也就是S必须是类型T的子类（或本身，自己也可以认为是自己的子类)。 2、示例（1）上界示例参照上面的泛型方法 （2）下界示例1234567891011121314151617class GranderFatherclass Father extends GranderFatherclass Son extends Fatherclass Tongxueobject Card&#123; def getIDCard[T &gt;: Son](person:T): Unit =&#123; println(\"OK,交给你了\") &#125; def main(args: Array[String]): Unit = &#123; getIDCard[GranderFather](new Father) getIDCard[GranderFather](new GranderFather) getIDCard[GranderFather](new Son) //getIDCard[GranderFather](new Tongxue)//报错，所以注释 &#125;&#125; 三、协变和逆变对于一个带类型参数的类型，比如 List[T]： 如果对A及其子类型B，满足 List[B]也符合 List[A]的子类型，那么就称为covariance(协变)； 如果 List[A]是 List[B]的子类型，即与原来的父子关系正相反，则称为contravariance(逆变)。 协变 12345678910 ____ _____________ | | | || A | | List[ A ] ||_____| |_____________| ^ ^ | | _____ _____________ | | | || B | | List[ B ] ||_____| |_____________| 逆变 12345678910 ____ _____________ | | | || A | | List[ B ] ||_____| |_____________| ^ ^ | | _____ _____________ | | | || B | | List[ A ] ||_____| |_____________| 在声明Scala的泛型类型时，“+”表示协变，而“-”表示逆变。 C[+T]：如果A是B的子类，那么C[A]是C[B]的子类。 C[-T]：如果A是B的子类，那么C[B]是C[A]的子类。 C[T]：无论A和B是什么关系，C[A]和C[B]没有从属关系。 根据Liskov替换原则，如果A是B的子类，那么能适用于B的所有操作，都适用于A。让我们看看这边Function1的定义，是否满足这样的条件。假设Bird是Animal的子类，那么看看下面两个函数之间是什么关系： 12def f1(x: Bird): Animal // instance of Function1[Bird, Animal]def f2(x: Animal): Bird // instance of Function1[Animal, Bird] 在这里f2的类型是f1的类型的子类。为什么？ 我们先看一下参数类型，根据Liskov替换原则，f1能够接受的参数，f2也能接受。在这里f1接受的Bird类型，f2显然可以接受，因为Bird对象可以被当做其父类Animal的对象来使用。 再看返回类型，f1的返回值可以被当做Animal的实例使用，f2的返回值可以被当做Bird的实例使用，当然也可以被当做Animal的实例使用。 所以我们说，函数的参数类型是逆变的，而函数的返回类型是协变的。 那么我们在定义Scala类的时候，是不是可以随便指定泛型类型为协变或者逆变呢？答案是否定的。通过上面的例子可以看出，如果将Function1的参数类型定义为协变，或者返回类型定义为逆变，都会违反Liskov替换原则，因此，Scala规定，协变类型只能作为方法的返回类型，而逆变类型只能作为方法的参数类型。类比函数的行为，结合Liskov替换原则，就能发现这样的规定是非常合理的。 总结：参数是逆变的或者不变的，返回值是协变的或者不变的。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://zhangfuxin.cn/tags/Scala/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Scala学习之路 （八）Scala的隐式转换和隐式参数","slug":"2019-05-08-Scala学习之路 （八）Scala的隐式转换和隐式参数","date":"2019-05-08T02:30:04.000Z","updated":"2019-09-18T15:47:13.417Z","comments":true,"path":"2019-05-08-Scala学习之路 （八）Scala的隐式转换和隐式参数.html","link":"","permalink":"http://zhangfuxin.cn/2019-05-08-Scala学习之路 （八）Scala的隐式转换和隐式参数.html","excerpt":"** Scala学习之路 （八）Scala的隐式转换和隐式参数：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （八）Scala的隐式转换和隐式参数","text":"** Scala学习之路 （八）Scala的隐式转换和隐式参数：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （八）Scala的隐式转换和隐式参数 &lt;The rest of contents | 余下全文&gt; 一、概念Scala 2.10引入了一种叫做隐式类的新特性。隐式类指的是用implicit关键字修饰的类。在对应的作用域内，带有这个关键字的类的主构造函数可用于隐式转换。 隐式转换和隐式参数是Scala中两个非常强大的功能，利用隐式转换和隐式参数，你可以提供优雅的类库，对类库的使用者隐匿掉那些枯燥乏味的细节。 二、作用隐式的对类的方法进行增强，丰富现有类库的功能 三、隐式参数1）关键字：implicat2）隐士的东西只能在object里面才能使用3）作用域 四、隐式转换函数是指那种以implicit关键字声明的带有单个参数的函数。 可以通过：:implicit –v这个命令显示所有做隐式转换的类。 四、隐士转换的发生的时机1、当一个对象去调用某个方法，但是这个对象并不具备这个方法（1）scala源码示例 1是Int类型，从to方法看，Int应该有to方法 打开Int类的源码查看，并没有Int本身并没有to方法，发现Int继承了AnyVal类 查看AnyVal类，发现AnyVal类同样没有to方法，而AnyVal类继承了Any类 Any类里面没有to方法，而在RichInt里面有to方法 而在上面查看scala自动导入隐式转换函数时可以看到有Predef类的intWrapper方法，传入的参数是Int，返回的结果类型是RichInt （2）快学Scala示例值File和RichFile示例不使用隐式转换时，使用装饰模式进行读取 12345678910111213141516171819202122232425import java.io.Fileimport scala.io.Sourceclass RichFile(val file : File) &#123; //定义一个read方法，返回String类型 def read():String = Source.fromFile(file.getPath).mkString&#125;object RichFile&#123; //隐式转换方法（将原有的File类型转成了file类型，在用的时候需要导入相应的包） //implicit def file2RichFile(file:File) = new RichFile(file)&#125;object MainApp&#123; def main(args: Array[String]): Unit = &#123; val file = new File(\"D:\\\\student.txt\") //装饰模式，显示的增强(本来想实现：val contents = file.read()，但是却使用RichFile的方式，所以是显示的增强) val rf = new RichFile(file) val str = rf.read() print(str) &#125;&#125; 使用隐式转换方式 12345678910111213141516171819202122232425262728import java.io.Fileimport scala.io.Sourceclass RichFile(val file : File) &#123; //定义一个read方法，返回String类型 def read():String = Source.fromFile(file.getPath).mkString&#125;object RichFile&#123; //隐式转换方法（将原有的File类型转成了file类型，在用的时候需要导入相应的包） implicit def file2RichFile(file:File) = new RichFile(file)&#125;object MainApp&#123; def main(args: Array[String]): Unit = &#123; //目的是使用File的时候不知不觉的时候直接使用file.read()方法，所以这里就要做隐式转换 val file = new File(\"D:\\\\student.txt\") //导入隐式转换，._将它下满的所有的方法都导入进去了。 import RichFile._ //这里没有的read()方法的时候，它就到上面的这一行中的找带有implicit的定义方法 val str = file.read() //打印读取的内容 println(str) &#125;&#125; （3）超人示例1234567891011121314class Man(val name:String)class SuperMan &#123; def fly(): Unit =&#123; println(&quot;我要上天&quot;) &#125;&#125;object SuperMan&#123; //隐式转换，将Man转换为SuperMan implicit def man2SuperMan(man:Man)=new SuperMan def main(args: Array[String]): Unit = &#123; new Man(&quot;灰太狼&quot;).fly &#125;&#125; 2、调用某个方法的时候，这个方法确实也存在，存入的参数类型不匹配售票厅卖票老人和小孩是特殊人群，有单独的买票窗口 1234567891011121314151617181920212223242526272829303132333435363738394041//特殊人群（儿童和老人）class SpecialPerson(var name:String)//儿童class Children(var name:String)//老人class Older(var name:String)//青年工作者class Worker(var name:String)//特殊人群买票窗口class TicketHouse&#123; def buyTicket(p:SpecialPerson): Unit =&#123; println(p.name + \"买到票了\") &#125;&#125;object MyPredef&#123; //隐式转换，将儿童转换为特殊人群 implicit def children2SpecialPerson(c:Children)=new SpecialPerson(c.name) //隐式转换，将老人转换为特殊人群 implicit def older2SpecialPerson(o:Older)=new SpecialPerson(o.name)&#125;object TestBuyTicket&#123; def main(args: Array[String]): Unit = &#123; //导入MyPredef类中的所有隐式转换 import MyPredef._ val house = new TicketHouse //测试儿童买票 val children = new Children(\"wangbaoqiang\") house.buyTicket(children) //测试老人买票 val older = new Older(\"xuzheng\") house.buyTicket(older) //测试青年工作者买票 val worker = new Worker(\"huangbo\") //house.buyTicket(worker)//放开的话会报错 &#125;&#125; 3、视图边界 人狗之恋 123456789101112131415161718192021222324252627282930313233343536class Person(val name : String) &#123; def sayHello: Unit =&#123; println(\"Hello, my name is \" + name) &#125; //2个人交朋友 def mkFridens(p:Person): Unit =&#123; sayHello p.sayHello &#125;&#125;class Student(name : String) extends Person(name)class Dog(val name : String)//聚会时2个人交朋友class Party[T &lt;% Person](p1:Person,p2:Person)&#123; p1.mkFridens(p2)&#125;object Test&#123; //隐式转换，将狗转换成人 implicit def dog2Person(dog:Dog):Person=&#123; new Person(dog.name) &#125; def main(args: Array[String]): Unit = &#123; val huangxiaoming = new Person(\"huangxiaoming\") val angelababy = new Student(\"angelababy\") new Party[Person](huangxiaoming,angelababy) println(\"------------------------------------------------\") val erlangshen = new Person(\"erlangshen\") val xiaotianquan = new Dog(\"xiaotianquan\") new Party[Person](erlangshen,xiaotianquan) &#125;&#125;","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://zhangfuxin.cn/tags/Scala/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Scala学习之路 （七）Scala的柯里化及其应用","slug":"2019-05-07-Scala学习之路 （七）Scala的柯里化及其应用","date":"2019-05-07T02:30:04.000Z","updated":"2019-09-18T15:44:41.309Z","comments":true,"path":"2019-05-07-Scala学习之路 （七）Scala的柯里化及其应用.html","link":"","permalink":"http://zhangfuxin.cn/2019-05-07-Scala学习之路 （七）Scala的柯里化及其应用.html","excerpt":"** Scala学习之路 （七）Scala的柯里化及其应用：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （七）Scala的柯里化及其应用","text":"** Scala学习之路 （七）Scala的柯里化及其应用：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （七）Scala的柯里化及其应用 &lt;The rest of contents | 余下全文&gt; 一、概念柯里化(currying, 以逻辑学家Haskell Brooks Curry的名字命名)指的是将原来接受两个参数的函数变成新的接受一个参数的函数的过程。新的函数返回一个以原有第二个参数作为参数的函数。 在Scala中方法和函数有细微的差别，通常编译器会自动完成方法到函数的转换。 二、Scala中柯里化的形式Scala中柯里化方法的定义形式和普通方法类似，区别在于柯里化方法拥有多组参数列表，每组参数用圆括号括起来，例如： mysum方法拥有两组参数，分别是(x: Int)和(y: Int)。mysum方法对应的柯里化函数类型是： 1Int =&gt; Int = &gt;Int 柯里化函数的类型声明是右结合的，即上面的类型等价于： 1Int =&gt; (Int = &gt;Int) 表明该函数若只接受一个Int参数，则返回一个Int =&gt; Int类型的函数，这也和柯里化的过程相吻合。 三、示例上面的代码定义了一个柯里化方法，在Scala中可以直接操纵函数，但是不能直接操纵方法，所以在使用柯里化方法前，需要将其转换成柯里化函数。最简单的方式是使用编译器提供的语法糖： 使用Scala中的部分应用函数(partially applied function)技巧也可以实现转换，但是请注意转后后得到的并不是柯里化函数，而是一个接受两个（而不是两组）参数的普通函数 传入一个参数 即一个接受一个Int参数返回Int类型的函数。 继续传入第2个参数： 两组参数都已经传入，返回一个Int类型结果","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://zhangfuxin.cn/tags/Scala/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Scala学习之路 （六）Scala的类、对象、继承、特质","slug":"2019-05-06-Scala学习之路 （六）Scala的类、对象、继承、特质","date":"2019-05-06T02:30:04.000Z","updated":"2019-09-18T15:43:28.998Z","comments":true,"path":"2019-05-06-Scala学习之路 （六）Scala的类、对象、继承、特质.html","link":"","permalink":"http://zhangfuxin.cn/2019-05-06-Scala学习之路 （六）Scala的类、对象、继承、特质.html","excerpt":"** Scala学习之路 （六）Scala的类、对象、继承、特质：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （六）Scala的类、对象、继承、特质","text":"** Scala学习之路 （六）Scala的类、对象、继承、特质：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （六）Scala的类、对象、继承、特质 &lt;The rest of contents | 余下全文&gt; 一、类1、类的定义scala语言中没有static成员存在，但是scala允许以某种方式去使用static成员这个就是伴生机制，所谓伴生，就是在语言层面上，把static成员和非static成员用不同的表达方式，class和object，但双方具有相同的package和name，但是最终编译器会把他们编译到一起，这是纯粹从语法层面上的约定。通过javap可以反编译看到。另外一个小魔法就是单例，单例本质上是通过伴生机制完成的，直接由编译器生成一个class对象，这样至少在底层能够统一。 12345678910111213//在Scala中，类并不用声明为public。//Scala源文件中可以包含多个类，所有这些类都具有公有可见性。class ClassDemo &#123; //用val修饰的变量是只读属性，有getter但没有setter //（相当与Java中用final修饰的变量） val id = 666 //用var修饰的变量既有getter又有setter var name = \"huangbo\" //类私有字段,只能在类的内部使用 var age = 24 //对象私有字段,访问权限更加严格的，ClassDemo类的方法只能访问到当前对象的字段 private[this] val address = \"三里屯\"&#125; 2、构造器注意：主构造器会执行类定义中的所有语句 123456789101112131415161718192021222324252627282930313233/** *每个类都有主构造器，主构造器的参数直接放置类名后面，与类交织在一起 */class Person(val name:String,val age:Int) &#123; //主构造器会执行类定义中的所有语句 println(\"Hello Spark\") val x = 1 if(x &gt; 1)&#123; println(\"666\") &#125;else if(x &lt; 1)&#123; println(\"哈哈。。。\") &#125;else&#123; println(\"呵呵。。。\") &#125; private var address = \"BJ\" //用this关键字定义辅助构造器 def this(name:String,age:Int,address:String)&#123; //每个辅助构造器必须以主构造器或其他的辅助构造器的调用开始 this(name,age) println(\"执行辅助构造器\") this.address = address &#125;&#125;object Person&#123; def main(args: Array[String]): Unit = &#123; val p = new Person(\"dengchao\",33,\"SH\") &#125;&#125; 总结： 主构造方法： 1）与类名交织在一起 2）主构造方法运行，导致类名后面的大括号里面的代码都会运行 辅助构造方法： 1）必须名字叫this 2) 必须以调用主构造方法或者是其他辅助构造方法开始。 3）里面的属性不能写修饰符 对象单例对象在Scala中没有静态方法和静态字段，但是可以使用object这个语法结构来达到同样的目的 1.存放工具方法和常量 2.高效共享单个不可变的实例=单例模式 12345678910111213141516171819202122232425import scala.collection.mutable.ArrayBufferobject SingletonDemo &#123; def main(args: Array[String]): Unit = &#123; val session = SessionFactory.getSession() println(session) &#125;&#125;class Session&#123;&#125;object SessionFactory&#123; //该部分相当于java中的静态块 var counts = 5 val sessions = new ArrayBuffer[Session]() while (counts &gt; 0)&#123; sessions += new Session counts -= 1 &#125; //在object中的方法相当于java中的静态方法 def getSession(): Session =&#123; sessions.remove(0) &#125;&#125; 总结：1）object里面的方法都是静态方法 2）Object里面的字段都是静态字段 3）它本身就是一个单例，(因为不需要去new) 伴生对象在同一个文件中，在Scala的类中，与类名相同的对象叫做伴生对象，类和伴生对象之间可以相互访问私有的方法和属性 123456789101112131415161718192021222324class Dog &#123; val id = 666 private var name = &quot;道哥&quot; def printName(): Unit =&#123; //在Dog类中可以访问伴生对象Dog的私有属性 println(Dog.CONSTANT + name) &#125;&#125;/** * 伴生对象 */object Dog&#123; //伴生对象中的私有属性 private var CONSTANT = &quot;汪汪汪。。。&quot; //主方法 def main(args: Array[String]): Unit = &#123; val dog = new Dog //访问私有的字段name dog.name = &quot;道哥666&quot; dog.printName() &#125;&#125; 总结：伴生对象和伴生类可以互相访问私有属性和私有方法。 apply方法通常我们会在类的伴生对象中定义apply方法，当遇到类名(参数1,…参数n)时apply方法会被调用 12345678910object ApplyDemo &#123; def main(args: Array[String]): Unit = &#123; //调用Array伴生对象的apply方法 val array = Array(1,2,3,4,5) println(array.toBuffer) //new了一个长度为9的数组，数组里面包含了9个null var arr = new Array(9) println(arr) &#125;&#125; 继承Scala中，让子类继承父类，与Java一样，也是使用extends关键字继承就代表，子类可以从父类继承父类的field和method；然后子类可以在自己内部放入父类所没有，子类特有的field和method；使用继承可以有效复用代码子类可以覆盖父类的field和method；但是如果父类用final修饰，field和method用final修饰，则该类是无法被继承的，field和method是无法被覆盖的 12345678910111213141516class People &#123; private var name = &quot;始皇帝&quot; def getName = name&#125;class Student extends People&#123; private var score = 59 def getScore = score&#125;object Test&#123; def main(args: Array[String]): Unit = &#123; val student = new Student println(student.getName) &#125;&#125; Scala中，如果子类要覆盖一个父类中的非抽象方法，则必须使用override关键字override关键字可以帮助我们尽早地发现代码里的错误，比如：override修饰的父类方法的方法名我们拼写错了；比如要覆盖的父类方法的参数我们写错了；等等此外，在子类覆盖父类方法之后，如果我们在子类中就是要调用父类的被覆盖的方法呢？那就可以使用super关键字，显式地指定要调用父类的方法 123456789101112131415161718class People &#123; private var name = \"始皇帝\" def getName = name&#125;class Student extends People&#123; private var score = 59 def getScore = score override def getName: String = super.getName + \":嬴政\"&#125;object Test&#123; def main(args: Array[String]): Unit = &#123; val student = new Student println(student.getName) &#125;&#125; 抽象类如果在父类中，有某些方法无法立即实现，而需要依赖不同的子来来覆盖，重写实现自己不同的方法实现。此时可以将父类中的这些方法不给出具体的实现，只有方法签名，这种方法就是抽象方法。 而一个类中如果有一个抽象方法，那么类就必须用abstract来声明为抽象类，此时抽象类是不可以实例化的 在子类中覆盖抽象类的抽象方法时，不需要使用override**关键字** 1234567891011121314abstract class AbstractDemo(name:String) &#123; def sayHello:Unit&#125;class StudentDemo(name:String) extends AbstractDemo(name)&#123; def sayHello: Unit = println(\"Hello \" + name)&#125;object StudentDemo&#123; def main(args: Array[String]): Unit = &#123; val li = new StudentDemo(\"Li\") li.sayHello &#125;&#125; 扩展类在Scala中扩展类的方式和Java一样都是使用extends关键字 重写方法在Scala中重写一个非抽象的方法必须使用override修饰符 特质(trait)1将特质作为接口使用12345678910111213141516171819202122232425262728/** * // Scala中的Triat是一种特殊的概念// 首先我们可以将Trait作为接口来使用，此时的Triat就与Java中的接口非常类似// 在triat中可以定义抽象方法，就与抽象类中的抽象方法一样，只要不给出方法的具体实现即可// 类可以使用extends关键字继承trait，注意，这里不是implement，而是extends，在scala中没有implement的概念，无论继承类还是trait，统一都是extends// 类继承trait后，必须实现其中的抽象方法，实现时不需要使用override关键字// scala不支持对类进行多继承，但是支持多重继承trait，使用with关键字即可 */trait HelloTrait &#123; def sayHello(name:String)&#125;trait MakeFriendsTrait&#123; def makeFriends(w:Worker)&#125;class Worker(var name:String) extends HelloTrait with MakeFriendsTrait&#123; def sayHello(name:String) = println(\"hello ,\"+name) def makeFriends(w:Worker)=println(\"hello, my name is\"+name+\" you name is\"+w.name)&#125;object Test&#123; def main(args: Array[String]) &#123; val p1=new Worker(\"xiaoma\"); val p2=new Worker(\"linghuchong\") p1.sayHello(\"lihuchong\") p1.makeFriends(p2) &#125;&#125; 2、在trait中定义具体方法123456789101112131415161718192021/** // Scala中的Triat可以不是只定义抽象方法，还可以定义具体方法，此时trait更像是包含了通用工具方法的东西// 有一个专有的名词来形容这种情况，就是说trait的功能混入了类// 举例来说，trait中可以包含一些很多类都通用的功能方法，比如打印日志等等，spark中就使用了trait来定义了通用的日志打印方法 */trait Logger &#123; def log(message:String) = println(message)&#125;class Person(val name:String) extends Logger&#123; def makeFridends(p:Person): Unit =&#123; println(\"I'm\"+name+\" i'm glade to make friends with you\"+p.name); log(\"makeFridends method invoked!!\") &#125;&#125;object Test&#123; def main(args: Array[String]) &#123; val p1=new Person(\"linpingzhi\") val p2=new Person(\"yuelingshan\"); p1.makeFridends(p2) &#125;&#125; 3、在trait中定义具体字段1234567891011121314151617/** * // Scala中的Triat可以定义具体field，此时继承trait的类就自动获得了trait中定义的field// 但是这种获取field的方式与继承class是不同的：如果是继承class获取的field，实际是定义在父类中的； 而继承trait获取的field，就直接被添加到了类中 */trait Person &#123; val eyeNum:Int=2&#125;class Student(val name:String) extends Person&#123; def sayHello()=println(\"Hi,I'm \"+name +\"I have \"+eyeNum+\"eyes !\" )&#125;object Test&#123; def main(args: Array[String]) &#123; val s=new Student(\"zhangsanfeng\") s.sayHello(); &#125;&#125; 4、 在trait中定义抽象字段1234567891011121314151617181920212223/** * // Scala中的Triat可以定义抽象field，而trait中的具体方法则可以基于抽象field来编写// 但是继承trait的类，则必须覆盖抽象field，提供具体的值 */trait sayHello &#123; val msg:String def sayHello(name:String)=println(msg +\" , \"+name)&#125;class Person(val name:String) extends sayHello&#123; val msg:String = \"hello\" def makeFriends(p:Person): Unit =&#123; sayHello(p.name) println(\"I'm\"+name +\" I want to make frieds with you\") &#125;&#125;object Test&#123; def main(args: Array[String]) &#123; val p1=new Person(\"zhangwuji\") val p2=new Person(\"zhangsanfeng\") p1.makeFriends(p2) &#125;&#125; 5、为实例对象混入trait123456789101112131415161718192021222324252627282930313233/** * // 有时我们可以在创建类的对象时，指定该对象混入某个trait，这样，就只有这个对象混入该trait的方法，而类的其他对象则没有 */trait Logged &#123; def log(msg:String)&#123;&#125;&#125;trait AMyLogger extends Logged&#123; override def log(msg:String): Unit =&#123; println(\"test:\"+msg) &#125;&#125;trait BMyLogger extends Logged&#123; override def log(msg:String): Unit =&#123; println(\"log:\"+msg) &#125;&#125;class Person(val name:String) extends AMyLogger&#123; def sayHello(): Unit =&#123; println(\"Hi ,i'm name\") log(\"sayHello is invoked!\") &#125;&#125;object Test&#123; def main(args: Array[String]) &#123; val p1=new Person(\"liudehua\") p1.sayHello() val p2=new Person(\"zhangxueyou\") with BMyLogger p2.sayHello() &#125;&#125; 6、trait调用链123456789101112131415161718192021222324252627282930313233/** * // Scala中支持让类继承多个trait后，依次调用多个trait中的同一个方法，只要让多个trait的同一个方法中，在最后都执行super.方法 即可// 类中调用多个trait中都有的这个方法时，首先会从最右边的trait的方法开始执行，然后依次往左执行，形成一个调用链条// 这种特性非常强大，其实就相当于设计模式中的责任链模式的一种具体实现依赖 */trait Handler &#123; def handler(data:String)&#123;&#125;&#125;trait DataValidHandler extends Handler&#123; override def handler(data:String): Unit =&#123; println(\"check data:\"+data) super.handler(data) &#125;&#125;trait SignatureValidHandler extends Handler&#123; override def handler(data:String): Unit =&#123; println(\"check signatrue:\"+data) super.handler(data) &#125;&#125;class Person(val name:String) extends SignatureValidHandler with DataValidHandler&#123; def sayHello=&#123; println(\"Hello \"+name) handler(name) &#125;&#125;object Test&#123; def main(args: Array[String]) &#123; val p=new Person(\"lixiaolong\"); p.sayHello &#125;&#125; 模式匹配Scala有一个十分强大的模式匹配机制，可以应用到很多场合：如switch语句、类型检查等。 并且Scala还提供了样例类，对模式匹配进行了优化，可以快速进行匹配 1、匹配字符串123456789101112import scala.util.Randomobject CaseDemo01 extends App&#123; val arr = Array(\"Hadoop\", \"HBase\", \"Spark\") val name = arr(Random.nextInt(arr.length)) name match &#123; case \"Hadoop\" =&gt; println(\"哈肚普...\") case \"HBase\" =&gt; println(\"H贝斯...\") case _ =&gt; println(\"真不知道你们在说什么...\") &#125;&#125;&#125; 2、匹配类型1234567891011121314import scala.util.Randomobject CaseDemo01 extends App&#123; //val v = if(x &gt;= 5) 1 else if(x &lt; 2) 2.0 else \"hello\" val arr = Array(\"hello\", 1, 2.0, CaseDemo01) val v = arr(Random.nextInt(4)) println(v) v match &#123; case x: Int =&gt; println(\"Int \" + x) case y: Double if(y &gt;= 0) =&gt; println(\"Double \"+ y) case z: String =&gt; println(\"String \" + z) case _ =&gt; throw new Exception(\"not match exception\") &#125;&#125; 注意：case y: Doubleif(y &gt;= 0) =&gt; … 模式匹配的时候还可以添加守卫条件。如不符合守卫条件，将掉入case _中 3、匹配数组、元组、集合12345678910111213141516171819202122232425object CaseDemo03 extends App&#123; val arr = Array(1, 3, 5) arr match &#123; case Array(1, x, y) =&gt; println(x + \" \" + y) case Array(0) =&gt; println(\"only 0\") case Array(0, _*) =&gt; println(\"0 ...\") case _ =&gt; println(\"something else\") &#125; val lst = List(3, -1) lst match &#123; case 0 :: Nil =&gt; println(\"only 0\") case x :: y :: Nil =&gt; println(s\"x: $x y: $y\") case 0 :: tail =&gt; println(\"0 ...\") case _ =&gt; println(\"something else\") &#125; val tup = (2, 3, 5) tup match &#123; case (2, x, y) =&gt; println(s\"1, $x , $y\") case (_, z, 5) =&gt; println(z) case _ =&gt; println(\"else\") &#125;&#125; 注意：在Scala中列表要么为空（Nil表示空列表）要么是一个head元素加上一个tail列表。 9 :: List(5, 2) :: 操作符是将给定的头和尾创建一个新的列表 注意：:: 操作符是右结合的，如9 :: 5 :: 2 :: Nil相当于 9 :: (5 :: (2 :: Nil)) 4、样例类在Scala中样例类是一中特殊的类，可用于模式匹配。case class是多例的，后面要跟构造参数，case object是单例的 12345678910111213141516171819202122import scala.util.Randomcase class SubmitTask(id: String, name: String)case class HeartBeat(time: Long)case object CheckTimeOutTaskobject CaseDemo04 extends App&#123; val arr = Array(CheckTimeOutTask, HeartBeat(12333), SubmitTask(\"0001\", \"task-0001\")) arr(Random.nextInt(arr.length)) match &#123; case SubmitTask(id, name) =&gt; &#123; println(s\"$id, $name\") &#125; case HeartBeat(time) =&gt; &#123; println(time) &#125; case CheckTimeOutTask =&gt; &#123; println(\"check\") &#125; &#125;&#125; 总结 123456789&gt; 本质上来讲，class case class用起来就是一样的：&gt; 最不一样的一个东西：如果我们scala要做模式匹配，去匹配类型的话，建议使用&gt; case case 因为scala的底层对它做了优化，匹配起来性能较好。&gt; * 1:case class 自动生成伴生对象，自动实现了apply方法&gt; * 2:case class 用于做匹配，性能较好（scala的底层做过优化）&gt; * 3：case class 默认实现了序列化 Serializable&gt; * 4: case class 默认实现了toString equals等方法&gt; * 5：case class 主构造函数 里面没有修饰符，默认的是val&gt; 偏函数被包在花括号内没有match的一组case语句是一个偏函数，它是PartialFunction[A, B]的一个实例，A代表参数类型，B代表返回类型，常用作输入模式匹配 12345678910111213141516171819object PartialFuncDemo &#123; def func1: PartialFunction[String, Int] = &#123; case \"one\" =&gt; 1 case \"two\" =&gt; 2 case _ =&gt; -1 &#125; def func2(num: String) : Int = num match &#123; case \"one\" =&gt; 1 case \"two\" =&gt; 2 case _ =&gt; -1 &#125; def main(args: Array[String]) &#123; println(func1(\"one\")) println(func2(\"one\")) &#125;&#125; 总结： 偏函数就是用来做模式匹配的。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://zhangfuxin.cn/tags/Scala/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Scala学习之路 （五）Scala的关键字Lazy","slug":"2019-05-05-Scala学习之路 （五）Scala的关键字Lazy","date":"2019-05-05T02:30:04.000Z","updated":"2019-09-18T15:34:16.181Z","comments":true,"path":"2019-05-05-Scala学习之路 （五）Scala的关键字Lazy.html","link":"","permalink":"http://zhangfuxin.cn/2019-05-05-Scala学习之路 （五）Scala的关键字Lazy.html","excerpt":"** Scala学习之路 （五）Scala的关键字Lazy：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （五）Scala的关键字Lazy","text":"** Scala学习之路 （五）Scala的关键字Lazy：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （五）Scala的关键字Lazy &lt;The rest of contents | 余下全文&gt; Scala中使用关键字lazy来定义惰性变量，实现延迟加载(懒加载)。惰性变量只能是不可变变量，并且只有在调用惰性变量时，才会去实例化这个变量。 在Java中，要实现延迟加载(懒加载)，需要自己手动实现。一般的做法是这样的: 1234567891011121314151617public class JavaLazyDemo &#123; private String name; //初始化姓名为huangbo private String initName()&#123; return \"huangbo\"; &#125; public String getName()&#123; //如果name为空，进行初始化 if(name == null)&#123; name = initName(); &#125; return name; &#125;&#125; 在Scala中对延迟加载这一特性提供了语法级别的支持: 1lazy val name = initName() 使用lazy关键字修饰变量后，只有在使用该变量时，才会调用其实例化方法。也就是说在定义property=initProperty()时并不会调用initProperty()方法，只有在后面的代码中使用变量property时才会调用initProperty()方法。 如果不使用lazy关键字对变量修饰，那么变量property是立即实例化的: 123456789101112object ScalaLazyDemo &#123; def init():String = &#123; println(\"huangbo 666\") return \"huangbo\" &#125; def main(args: Array[String]): Unit = &#123; val name = init(); println(\"666\") println(name) &#125;&#125; 上面的property没有使用lazy关键字进行修饰，所以property是立即实例化的，调用了initName()方法进行实例化。 使用Lazy进行修饰 123456789101112object ScalaLazyDemo &#123; def init():String = &#123; println(\"huangbo 666\") return \"huangbo\" &#125; def main(args: Array[String]): Unit = &#123; lazy val name = init(); println(\"666\") println(name) &#125;&#125; 在声明name时，并没有立即调用实例化方法initName(),而是在使用name时，才会调用实例化方法,并且无论缩少次调用，实例化方法只会执行一次。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://zhangfuxin.cn/tags/Scala/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Scala学习之路 （三）Scala的基本使用","slug":"2019-05-03-Scala学习之路 （三）Scala的基本使用","date":"2019-05-03T02:30:04.000Z","updated":"2019-09-18T15:54:38.557Z","comments":true,"path":"2019-05-03-Scala学习之路 （三）Scala的基本使用.html","link":"","permalink":"http://zhangfuxin.cn/2019-05-03-Scala学习之路 （三）Scala的基本使用.html","excerpt":"** Scala学习之路 （三）Scala的基本使用：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （三）Scala的基本使用","text":"** Scala学习之路 （三）Scala的基本使用：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （三）Scala的基本使用 &lt;The rest of contents | 余下全文&gt; 一、Scala概述scala是一门多范式编程语言，集成了面向对象编程和函数式编程等多种特性。scala运行在虚拟机上，并兼容现有的Java程序。Scala源代码被编译成java字节码，所以运行在JVM上，并可以调用现有的Java类库。 二、第一个Scala程序Scala语句末尾的分号可写可不写 HelloSpark.scala 12345object HelloSpark&#123; def main(args:Array[String]):Unit = &#123; println(&quot;Hello Spark!&quot;) &#125;&#125; 运行过程需要先进行编译 编译之后生成2个文件 运行HelloSpark.class 输出结果Hello Spark 三、Scala的基本语法1、概述123456789101112131415161718192021222324252627/** * Scala基本语法: * 区分大小写 * 类名首字母大写（MyFirstScalaClass） * 方法名称第一个字母小写（myMethodName()） * 程序文件名应该与对象名称完全匹配 * def main(args:Array[String]):scala程序从main方法开始处理，程序的入口。 * * Scala注释：分为多行/**/和单行// * * 换行符：Scala是面向行的语言，语句可以用分号（；）结束或换行符（println()） * * 定义包有两种方法： * 1、package com.ahu * class HelloScala * 2、package com.ahu&#123; * class HelloScala * &#125; * * 引用：import java.awt.Color * 如果想要引入包中的几个成员，可以用selector（选取器）: * import java.awt.&#123;Color,Font&#125; * // 重命名成员 * import java.util.&#123;HashMap =&gt; JavaHashMap&#125; * // 隐藏成员 默认情况下，Scala 总会引入 java.lang._ 、 scala._ 和 Predef._，所以在使用时都是省去scala.的 * import java.util.&#123;HashMap =&gt; _, _&#125; //引入了util包所有成员，但HashMap被隐藏了 */ 2、Scala的数据类型Scala 与 Java有着相同的数据类型，下表列出了 Scala 支持的数据类型： 数据类型 描述 Byte 8位有符号补码整数。数值区间为 -128 到 127 Short 16位有符号补码整数。数值区间为 -32768 到 32767 Int 32位有符号补码整数。数值区间为 -2147483648 到 2147483647 Long 64位有符号补码整数。数值区间为 -9223372036854775808 到 9223372036854775807 Float 32位IEEE754单精度浮点数 Double 64位IEEE754单精度浮点数 Char 16位无符号Unicode字符, 区间值为 U+0000 到 U+FFFF String 字符序列 Boolean true或false Unit 表示无值，和其他语言中void等同。用作不返回任何结果的方法的结果类型。Unit只有一个实例值，写成()。 Null null 或空引用 Nothing Nothing类型在Scala的类层级的最低端；它是任何其他类型的子类型。 Any Any是所有其他类的超类 AnyRef AnyRef类是Scala里所有引用类(reference class)的基类 Scala多行字符串的表示方式 1234567var str = &quot;&quot;&quot; |第一行 |第二行 |第三行 &quot;&quot;&quot;.stripMargin println(str) 3、Scala的变量1234567891011object VariableTest &#123; def main(args: Array[String]) &#123; //使用val定义的变量值是不可变的，相当于java里用final修饰的变量 val i = 1 //使用var定义的变量是可变的，在Scala中鼓励使用val var s = \"hello\" //Scala编译器会自动推断变量的类型，必要的时候可以指定类型 //变量名在前，类型在后 val str: String = \"world\" &#125;&#125; 总结： 1）数据类型可以指定，也可以不指定，如果不指定，那么就会进行数据类型的推断。 2）如果指定数据类型，数据类型的执行 方式是 在变量名后面写一个冒号，然后写上数据类型。 3）我们的scala里面变量的修饰符一共有两个，一个是var 一个是val，如果是var修饰的变量，那么这个变量的值是可以修改的。如果是val修饰的变量，那么这个变量的值是不可以修改的。 4、Scala访问修饰符Scala访问修饰符和Java基本一样，分别有private、protected、public。 默认情况下，Scala对象的访问级别是public。 （1）私有成员用private关键字修饰的成员仅在包含了成员定义的类或对象内部可见。 123456789101112class Outer &#123; class Inner&#123; def start() = println(\"start\") def end() = println(\"end\") private def pause() = println(\"pause\") &#125; new Inner().start() new Inner().end() new Inner().pause()&#125; 在上面的代码里start和end两个方法被定义为public类型，可以通过任意Inner实例访问；pause被显示定义为private，这样就不能在Inner类外部访问它。执行这段代码，就会如注释处声明的一样，会在该处报错： （2）protected和私有成员类似,Scala的访问控制比Java来说也是稍显严格些。在 Scala中,由protected定义的成员只能由定义该成员和其派生类型访问。而在 Java中,由protected定义的成员可以由同一个包中的其它类型访问。在Scala中,可以通过其它方式来实现这种功能。 1234567891011121314151617package p&#123; class Super &#123; protected def f()&#123;println(\"f\")&#125; &#125; class Sub extends Super &#123; f() //OK &#125; class Other &#123; (new Super).f() //Error:f不可访问 &#125;&#125; （3）publicpublic访问控制为Scala定义的缺省方式,所有没有使用private和 protected修饰的成员(定义的类和方法)都是“公开的”,这样的成员可以在任何地方被访问。Scala不需要使用public来指定“公开访问”修饰符。 注意：Scala中定义的类和方法默认都是public的，但在类中声明的属性默认是private的。 （4）作用保护域作用域保护：Scala中，访问修饰符可以通过使用限定词强调。private[x] 或者 protected[x]private[x]：这个成员除了对[…]中的类或[…]中的包中的类及他们的伴生对象可见外，对其他的类都是private。 1234567891011121314151617181920212223242526272829package bobsrockets&#123; package navigation &#123; //如果为private class Navigator,则类Navigator只会对当前包navigation中所有类型可见。 //即private默认省略了[X],X为当前包或者当前类或者当前单例对象。 //private[bobsrockets]则表示将类Navigator从当前包扩展到对bobsrockets包中的所有类型可见。 private[bobsrockets] class Navigator &#123; protected[navigation] def useStarChart() &#123;&#125; class LegOfJourney &#123; private[Navigator] val distance = 100 &#125; private[this] var speed = 200 &#125; &#125; package launch &#123; import navigation._ object Vehicle &#123; //private val guide：表示guide默认被当前单例对象可见。 //private[launch] val guide：表示guide由默认对当前单例对象可见扩展到对launch包中的所有类型可见。 private[launch] val guide = new Navigator &#125; &#125;&#125; 在这个例子中,类Navigator使用 private[bobsrockets] 来修饰,这表示这个类可以被bobsrockets包中所有类型访问,比如通常情况下 Vehicle无法访问私有类型Navigator,但使用包作用域之后,Vechile 中可以访问Navigator。 5、Scala运算符与Java一样，不过Scala的运算符实际上是一种方法。 6、条件表达式Scala的的条件表达式比较简洁，例如： 12345678910111213141516171819202122232425def main(args: Array[String]): Unit = &#123; val x = 1 //判断x的值，将结果赋给y val y = if (x &gt; 0) 1 else -1 //打印y的值 println(\"y=\" + y) //支持混合类型表达式 val z = if (x &gt; 1) 1 else \"error\" //打印z的值 println(\"z=\" + z) //如果缺失else，相当于if (x &gt; 2) 1 else () val m = if (x &gt; 2) 1 println(\"m=\" + m) //在scala中每个表达式都有值，scala中有个Unit类，写做(),相当于Java中的void val n = if (x &gt; 2) 1 else () println(\"n=\" + n) //if和else if val k = if (x &lt; 0) 0 else if (x &gt;= 1) 1 else -1 println(\"k=\" + k) &#125; 运行结果 总结： 1）**if**条件表达式它是有返回值的 2）返回值会根据条件表达式的情况会进行自动的数据类型的推断。 7、块表达式123456789101112def main(args: Array[String]): Unit = &#123; val x = 0 val result = &#123; if(x &lt; 0) 1 else if(x &gt;= 1) -1 else \"error\" &#125; println(result) &#125; 运行结果 8、循环（1）while循环123456789def main(args: Array[String]): Unit = &#123; var n = 10 while ( &#123; n &gt; 0 &#125;) &#123; println(n) n -= 1 &#125;&#125; 总结： 1）while使用跟java一模一样 2）**注意点：在scala**里面不支持 i++ i– 等操作 统一写成 i-=1 （2）for循环在scala中有for循环和while循环，用for循环比较多 for循环语法结构： for (i &lt;- 表达式/数组/集合) 123456789101112131415161718192021222324def main(args: Array[String]): Unit = &#123; //for(i &lt;- 表达式),表达式1 to 10返回一个Range（区间） //每次循环将区间中的一个值赋给i for (i &lt;- 1 to 10) print(i+\"\\t\") //for(i &lt;- 数组) println() val arr = Array(\"a\", \"b\", \"c\") for (i &lt;- arr) println(i) //高级for循环 //每个生成器都可以带一个条件，注意：if前面没有分号 for(i &lt;- 1 to 3; j &lt;- 1 to 3 if i != j) print((10 * i + j) + \" \") println() //for推导式：如果for循环的循环体以yield开始，则该循环会构建出一个集合 //每次迭代生成集合中的一个值 val v = for (i &lt;- 1 to 10) yield i * 10 println(v) &#125; 运行结果 总结： 1）在scala里面没有运算符，都有的符号其实都是方法。 2）在scala里面没有++ – 的用法 3）for( i &lt;- 表达式/数组/集合) 4）在for循环里面我们是可以添加if表达式 5）有两个特殊表达式需要了解： To 1 to 3 1 2 3 Until 1 until 3 12 6）如果在使用for循环的时候，for循环的时候我们需要获取，我们可以是使用yield关键字。 9、方法和函数（1）定义方法 方法的返回值类型可以不写，编译器可以自动推断出来，但是对于递归函数，必须指定返回类型。 如果不写等号，代表没有返回值。 （2）定义函数 （3）方法和函数的区别在函数式编程语言中，函数是“头等公民”，它可以像任何其他数据类型一样被传递和操作 案例：首先定义一个方法，再定义一个函数，然后将函数传递到方法里面 123456789101112131415161718192021object TestScala &#123; //定义一个方法 //方法m2参数要求是一个函数，函数的参数必须是两个Int类型 //返回值类型也是Int类型 def m1(f:(Int,Int) =&gt; Int) : Int = &#123; f(2,6) &#125; //定义一个函数f1，参数是两个Int类型，返回值是一个Int类型 val f1 = (x:Int,y:Int) =&gt; x+y //再定义一个函数f2 val f2 = (m:Int,n:Int) =&gt; m*n //main方法 def main(args: Array[String]): Unit = &#123; //调用m1方法，并传入f1函数 val r1 = m1(f1) println(\"r1=\"+r1) //调用m1方法，并传入f2函数 val r2 = m1(f2) println(\"r2=\"+r2) &#125;&#125; （4）将方法转换成函数","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://zhangfuxin.cn/tags/Scala/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Scala学习之路 （二）使用IDEA开发Scala","slug":"2019-05-02-Scala学习之路 （二）使用IDEA开发Scala","date":"2019-05-02T02:30:04.000Z","updated":"2019-09-18T15:28:31.906Z","comments":true,"path":"2019-05-02-Scala学习之路 （二）使用IDEA开发Scala.html","link":"","permalink":"http://zhangfuxin.cn/2019-05-02-Scala学习之路 （二）使用IDEA开发Scala.html","excerpt":"** Scala学习之路 （二）使用IDEA开发Scala：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （二）使用IDEA开发Scala","text":"** Scala学习之路 （二）使用IDEA开发Scala：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （二）使用IDEA开发Scala &lt;The rest of contents | 余下全文&gt; 目前Scala的开发工具主要有两种：Eclipse和IDEA，这两个开发工具都有相应的Scala插件，如果使用Eclipse，直接到Scala官网下载即可http://scala-ide.org/download/sdk.html。 由于IDEA的Scala插件更优秀，大多数Scala程序员都选择IDEA，可以到http://www.jetbrains.com/idea/download/下载，点击下一步安装即可，安装时如果有网络可以选择在线安装Scala插件。这里我们使用离线安装Scala插件： 1.安装IDEA，点击下一步即可。由于我们离线安装插件，所以点击Skip All and Set Defaul 2.下载IEDA的scala插件，地址http://plugins.jetbrains.com/?idea_ce 3.安装Scala插件：Configure -&gt; Plugins -&gt; Install plugin from disk -&gt; 选择Scala插件 -&gt; OK -&gt; 重启IDEA 4、创建项目","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://zhangfuxin.cn/tags/Scala/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Scala学习之路 （一）Scala的安装","slug":"2019-05-01-Scala学习之路 （一）Scala的安装","date":"2019-05-01T02:30:04.000Z","updated":"2019-09-18T15:27:27.405Z","comments":true,"path":"2019-05-01-Scala学习之路 （一）Scala的安装.html","link":"","permalink":"http://zhangfuxin.cn/2019-05-01-Scala学习之路 （一）Scala的安装.html","excerpt":"** Scala学习之路 （一）Scala的安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （一）Scala的安装","text":"** Scala学习之路 （一）Scala的安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Scala学习之路 （一）Scala的安装 &lt;The rest of contents | 余下全文&gt; 1、Scala下载版本选择，看spark官网 http://spark.apache.org/docs/latest/ spark2.3.0版本是用2.11版本的Scala进行开发的，所以此处下载Scala2.11的版本 Scala下载地址http://www.scala-lang.org/download/all.html 2、双击安装即可默认安装即可 3、检查Scala版本","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://zhangfuxin.cn/tags/Scala/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Hive学习之路 （二十一）Hive 优化策略","slug":"2019-04-21-Hive学习之路 （二十一）Hive 优化策略","date":"2019-04-21T02:30:04.000Z","updated":"2019-09-18T15:20:48.419Z","comments":true,"path":"2019-04-21-Hive学习之路 （二十一）Hive 优化策略.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-21-Hive学习之路 （二十一）Hive 优化策略.html","excerpt":"** Hive学习之路 （二十一）Hive 优化策略：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （二十一）Hive 优化策略","text":"** Hive学习之路 （二十一）Hive 优化策略：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （二十一）Hive 优化策略 &lt;The rest of contents | 余下全文&gt; 一、Hadoop 框架计算特性1、数据量大不是问题，数据倾斜是个问题 2、jobs 数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联多次 汇总，产生十几个 jobs，耗时很长。原因是 map reduce 作业初始化的时间是比较长的 3、sum,count,max,min 等 UDAF，不怕数据倾斜问题，hadoop 在 map 端的汇总合并优化，使 数据倾斜不成问题 4、count(distinct userid)，在数据量大的情况下，效率较低，如果是多 count(distinct userid,month)效率更低，因为 count(distinct)是按 group by 字段分组，按 distinct 字段排序， 一般这种分布方式是很 倾斜的，比如 PV 数据，淘宝一天 30 亿的 pv，如果按性别分组，分 配 2 个 reduce，每个 reduce 期望处理 15 亿数据，但现实必定是男少女多 二、优化常用手段1、好的模型设计事半功倍 2、解决数据倾斜问题 3、减少 job 数 4、设置合理的 MapReduce 的 task 数，能有效提升性能。(比如，10w+级别的计算，用 160个 reduce，那是相当的浪费，1 个足够) 5、了解数据分布，自己动手解决数据倾斜问题是个不错的选择。这是通用的算法优化，但 算法优化有时不能适应特定业务背景，开发人员了解业务，了解数据，可以通过业务逻辑精 确有效的解决数据倾斜问题 6、数据量较大的情况下，慎用 count(distinct)，group by 容易产生倾斜问题 7、对小文件进行合并，是行之有效的提高调度效率的方法，假如所有的作业设置合理的文 件数，对云梯的整体调度效率也会产生积极的正向影响 8、优化时把握整体，单个作业最优不如整体最优 三、排序选择cluster by：对同一字段分桶并排序，不能和 sort by 连用 distribute by + sort by：分桶，保证同一字段值只存在一个结果文件当中，结合 sort by 保证 每个 reduceTask 结果有序 sort by：单机排序，单个 reduce 结果有序 order by：全局排序，缺陷是只能使用一个 reduce 一定要区分这四种排序的使用方式和适用场景 四、怎样做笛卡尔积当 Hive 设定为严格模式（hive.mapred.mode=strict）时，不允许在 HQL 语句中出现笛卡尔积， 这实际说明了 Hive 对笛卡尔积支持较弱。因为找不到 Join key，Hive 只能使用 1 个 reducer 来完成笛卡尔积。 当然也可以使用 limit 的办法来减少某个表参与 join 的数据量，但对于需要笛卡尔积语义的 需求来说，经常是一个大表和一个小表的 Join 操作，结果仍然很大（以至于无法用单机处 理），这时 MapJoin才是最好的解决办法。MapJoin，顾名思义，会在 Map 端完成 Join 操作。 这需要将 Join 操作的一个或多个表完全读入内存。 PS：MapJoin 在子查询中可能出现未知 BUG。在大表和小表做笛卡尔积时，规避笛卡尔积的 方法是，给 Join 添加一个 Join key，原理很简单：将小表扩充一列 join key，并将小表的条 目复制数倍，join key 各不相同；将大表扩充一列 join key 为随机数。 精髓就在于复制几倍，最后就有几个 reduce 来做，而且大表的数据是前面小表扩张 key 值 范围里面随机出来的，所以复制了几倍 n，就相当于这个随机范围就有多大 n，那么相应的， 大表的数据就被随机的分为了 n 份。并且最后处理所用的 reduce 数量也是 n，而且也不会 出现数据倾斜。 五、怎样写 in/exists 语句虽然经过测验，hive1.2.1 也支持 in/exists 操作，但还是推荐使用 hive 的一个高效替代方案：left semi join 比如说： 12select a.id, a.name from a where a.id in (select b.id from b);select a.id, a.name from a where exists (select id from b where a.id = b.id); 应该转换成： 1select a.id, a.name from a left semi join b on a.id = b.id; 六、设置合理的 maptask 数量Map 数过大 Map 阶段输出文件太小，产生大量小文件 初始化和创建 Map 的开销很大 Map 数太小 文件处理或查询并发度小，Job 执行时间过长 大量作业时，容易堵塞集群 在 MapReduce 的编程案例中，我们得知，一个MR Job的 MapTask 数量是由输入分片 InputSplit 决定的。而输入分片是由 FileInputFormat.getSplit()决定的。一个输入分片对应一个 MapTask， 而输入分片是由三个参数决定的： 输入分片大小的计算是这么计算出来的： long splitSize = Math.max(minSize, Math.min(maxSize, blockSize)) 默认情况下，输入分片大小和 HDFS 集群默认数据块大小一致，也就是默认一个数据块，启 用一个 MapTask 进行处理，这样做的好处是避免了服务器节点之间的数据传输，提高 job 处 理效率 两种经典的控制 MapTask 的个数方案：减少 MapTask 数或者增加 MapTask 数 1、 减少 MapTask 数是通过合并小文件来实现，这一点主要是针对数据源 2、 增加 MapTask 数可以通过控制上一个 job 的 reduceTask 个数 因为 Hive 语句最终要转换为一系列的 MapReduce Job 的，而每一个 MapReduce Job 是由一 系列的 MapTask 和 ReduceTask 组成的，默认情况下， MapReduce 中一个 MapTask 或者一个 ReduceTask 就会启动一个 JVM 进程，一个 Task 执行完毕后， JVM 进程就退出。这样如果任 务花费时间很短，又要多次启动 JVM 的情况下，JVM 的启动时间会变成一个比较大的消耗， 这个时候，就可以通过重用 JVM 来解决： set mapred.job.reuse.jvm.num.tasks=5 七、小文件合并文件数目过多，会给 HDFS 带来压力，并且会影响处理效率，可以通过合并 Map 和 Reduce 的 结果文件来消除这样的影响： set hive.merge.mapfiles = true ##在 map only 的任务结束时合并小文件 set hive.merge.mapredfiles = false ## true 时在 MapReduce 的任务结束时合并小文件 set hive.merge.size.per.task = 256*1000*1000 ##合并文件的大小 set mapred.max.split.size=256000000; ##每个 Map 最大分割大小 set mapred.min.split.size.per.node=1; ##一个节点上 split 的最少值 set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; ##执行 Map 前进行小文件合并 八、设置合理的 reduceTask 的数量Hadoop MapReduce 程序中，reducer 个数的设定极大影响执行效率，这使得 Hive 怎样决定 reducer 个数成为一个关键问题。遗憾的是 Hive 的估计机制很弱，不指定 reducer 个数的情 况下，Hive 会猜测确定一个 reducer 个数，基于以下两个设定： 1、hive.exec.reducers.bytes.per.reducer（默认为 256000000） 2、hive.exec.reducers.max（默认为 1009） 3、mapreduce.job.reduces=-1（设置一个常量 reducetask 数量） 计算 reducer 数的公式很简单： N=min(参数 2，总输入数据量/参数 1) 通常情况下，有必要手动指定 reducer 个数。考虑到 map 阶段的输出数据量通常会比输入有 大幅减少，因此即使不设定 reducer 个数，重设参数 2 还是必要的。 依据 Hadoop 的经验，可以将参数 2 设定为 0.95*(集群中 datanode 个数)。 九、合并 MapReduce 操作Multi-group by 是 Hive 的一个非常好的特性，它使得 Hive 中利用中间结果变得非常方便。 例如： 123456FROM (SELECT a.status, b.school, b.gender FROM status_updates a JOIN profiles b ON (a.userid =b.userid and a.ds='2009-03-20' ) ) subq1INSERT OVERWRITE TABLE gender_summary PARTITION(ds='2009-03-20')SELECT subq1.gender, COUNT(1) GROUP BY subq1.genderINSERT OVERWRITE TABLE school_summary PARTITION(ds='2009-03-20')SELECT subq1.school, COUNT(1) GROUP BY subq1.school 上述查询语句使用了 multi-group by 特性连续 group by 了 2 次数据，使用不同的 group by key。 这一特性可以减少一次 MapReduce 操作 十、合理利用分桶：Bucketing 和 SamplingBucket 是指将数据以指定列的值为 key 进行 hash，hash 到指定数目的桶中。这样就可以支 持高效采样了。如下例就是以 userid 这一列为 bucket 的依据，共设置 32 个 buckets 1234567891011CREATE TABLE page_view(viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User') COMMENT 'This is the page view table' PARTITIONED BY(dt STRING, country STRING) CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY '1' COLLECTION ITEMS TERMINATED BY '2' MAP KEYS TERMINATED BY '3' STORED AS SEQUENCEFILE; 通常情况下，Sampling 在全体数据上进行采样，这样效率自然就低，它要去访问所有数据。 而如果一个表已经对某一列制作了 bucket，就可以采样所有桶中指定序号的某个桶，这就 减少了访问量。 如下例所示就是采样了 page_view 中 32 个桶中的第三个桶的全部数据： 1SELECT * FROM page_view TABLESAMPLE(BUCKET 3 OUT OF 32); 如下例所示就是采样了 page_view 中 32 个桶中的第三个桶的一半数据： 1SELECT * FROM page_view TABLESAMPLE(BUCKET 3 OUT OF 64); 十一、合理利用分区：Partition Partition 就是分区。分区通过在创建表时启用 partitioned by 实现，用来 partition 的维度并不 是实际数据的某一列，具体分区的标志是由插入内容时给定的。当要查询某一分区的内容时 可以采用 where 语句，形似 where tablename.partition_column = a 来实现。 创建含分区的表 123456CREATE TABLE page_view(viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User')PARTITIONED BY(date STRING, country STRING)ROW FORMAT DELIMITED FIELDS TERMINATED BY '1'STORED AS TEXTFILE; 载入内容，并指定分区标志 12load data local inpath '/home/hadoop/pv_2008-06-08_us.txt' into table page_viewpartition(date='2008-06-08', country='US'); 查询指定标志的分区内容 123SELECT page_views.* FROM page_views WHERE page_views.date &gt;= '2008-03-01' AND page_views.date &lt;= '2008-03-31' ANDpage_views.referrer_url like '%xyz.com'; 十二、Join 优化总体原则： 1、 优先过滤后再进行 Join 操作，最大限度的减少参与 join 的数据量 2、 小表 join 大表，最好启动 mapjoin 3、 Join on 的条件相同的话，最好放入同一个 job，并且 join 表的排列顺序从小到大 在使用写有 Join 操作的查询语句时有一条原则：应该将条目少的表/子查询放在 Join 操作 符的左边。原因是在 Join 操作的 Reduce 阶段，位于 Join 操作符左边的表的内容会被加 载进内存，将条目少的表放在左边，可以有效减少发生 OOM 错误的几率。对于一条语句 中有多个 Join 的情况，如果 Join 的条件相同，比如查询 1234INSERT OVERWRITE TABLE pv_usersSELECT pv.pageid, u.age FROM page_view pJOIN user u ON (pv.userid = u.userid)JOIN newuser x ON (u.userid = x.userid); 如果 Join 的 key 相同，不管有多少个表，都会则会合并为一个 Map-Reduce 任务，而不 是”n”个，在做 OUTER JOIN 的时候也是一样 如果 join 的条件不相同，比如： 1234INSERT OVERWRITE TABLE pv_users SELECT pv.pageid, u.age FROM page_view p JOIN user u ON (pv.userid = u.userid) JOIN newuser x on (u.age = x.age); Map-Reduce 的任务数目和 Join 操作的数目是对应的，上述查询和以下查询是等价的 123456--先 page_view 表和 user 表做链接INSERT OVERWRITE TABLE tmptable SELECT * FROM page_view p JOIN user u ON (pv.userid = u.userid);-- 然后结果表 temptable 和 newuser 表做链接INSERT OVERWRITE TABLE pv_users SELECT x.pageid, x.age FROM tmptable x JOIN newuser y ON (x.age = y.age); 在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置： set hive.skewjoin.key=100000; // 这个是 join 的键对应的记录条数超过这个值则会进行 分拆，值根据具体数据量设置 set hive.optimize.skewjoin=true; // 如果是 join 过程出现倾斜应该设置为 true 十三、Group By 优化1、Map 端部分聚合并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进 行部分聚合，最后在 Reduce 端得出最终结果。 MapReduce 的 combiner 组件参数包括： set hive.map.aggr = true 是否在 Map 端进行聚合，默认为 True set hive.groupby.mapaggr.checkinterval = 100000 在 Map 端进行聚合操作的条目数目 2、使用 Group By 有数据倾斜的时候进行负载均衡 set hive.groupby.skewindata = true 当 sql 语句使用 groupby 时数据出现倾斜时，如果该变量设置为 true，那么 Hive 会自动进行 负载均衡。策略就是把 MR 任务拆分成两个：第一个先做预汇总，第二个再做最终汇总 在 MR 的第一个阶段中，Map 的输出结果集合会缓存到 maptaks 中，每个 Reduce 做部分聚 合操作，并输出结果，这样处理的结果是相同 Group By Key 有可能被分发到不同的 Reduce 中， 从而达到负载均衡的目的；第二个阶段 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成 最终的聚合操作。 十四、合理利用文件存储格式创建表时，尽量使用 orc、parquet 这些列式存储格式，因为列式存储的表，每一列的数据在 物理上是存储在一起的，Hive 查询时会只遍历需要列数据，大大减少处理的数据量。 十五、本地模式执行 MapReduceHive 在集群上查询时，默认是在集群上 N 台机器上运行， 需要多个机器进行协调运行，这 个方式很好地解决了大数据量的查询问题。但是当 Hive 查询处理的数据量比较小时，其实 没有必要启动分布式模式去执行，因为以分布式方式执行就涉及到跨网络传输、多节点协调 等，并且消耗资源。这个时间可以只使用本地模式来执行 mapreduce job，只在一台机器上 执行，速度会很快。启动本地模式涉及到三个参数： set hive.exec.mode.local.auto=true 是打开 hive 自动判断是否启动本地模式的开关，但是只 是打开这个参数并不能保证启动本地模式，要当 map 任务数不超过 hive.exec.mode.local.auto.input.files.max 的个数并且 map 输入文件大小不超过 hive.exec.mode.local.auto.inputbytes.max 所指定的大小时，才能启动本地模式。 十六、并行化处理一个 hive sql 语句可能会转为多个 mapreduce Job，每一个 job 就是一个 stage，这些 job 顺序 执行，这个在 cli 的运行日志中也可以看到。但是有时候这些任务之间并不是是相互依赖的， 如果集群资源允许的话，可以让多个并不相互依赖 stage 并发执行，这样就节约了时间，提 高了执行速度，但是如果集群资源匮乏时，启用并行化反倒是会导致各个 job 相互抢占资源 而导致整体执行性能的下降。启用并行化： set hive.exec.parallel=true; set hive.exec.parallel.thread.number=8; //同一个 sql 允许并行任务的最大线程数 十七、设置压缩存储1、压缩的原因Hive 最终是转为 MapReduce 程序来执行的，而 MapReduce 的性能瓶颈在于网络 IO 和 磁盘 IO，要解决性能瓶颈，最主要的是减少数据量，对数据进行压缩是个好的方式。压缩 虽然是减少了数据量，但是压缩过程要消耗 CPU 的，但是在 Hadoop 中， 往往性能瓶颈不 在于 CPU，CPU 压力并不大，所以压缩充分利用了比较空闲的 CPU 2、常用压缩方法对比 各个压缩方式所对应的 Class 类： 3、压缩方式的选择 压缩比率 压缩解压缩速度 是否支持 Split 4、压缩使用Job 输出文件按照 block 以 GZip 的方式进行压缩： set mapreduce.output.fileoutputformat.compress=true // 默认值是 false set mapreduce.output.fileoutputformat.compress.type=BLOCK // 默认值是 Record set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec // 默认值是 org.apache.hadoop.io.compress.DefaultCodec Map 输出结果也以 Gzip 进行压缩： set mapred.map.output.compress=true set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.GzipCodec // 默认值是 org.apache.hadoop.io.compress.DefaultCodec 对 Hive 输出结果和中间都进行压缩： set hive.exec.compress.output=true // 默认值是 false，不压缩 set hive.exec.compress.intermediate=true // 默认值是 false，为 true 时 MR 设置的压缩才启用","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （二十）Hive 执行过程实例分析","slug":"2019-04-20-Hive学习之路 （二十）Hive 执行过程实例分析","date":"2019-04-20T02:30:04.000Z","updated":"2019-09-18T15:20:31.496Z","comments":true,"path":"2019-04-20-Hive学习之路 （二十）Hive 执行过程实例分析.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-20-Hive学习之路 （二十）Hive 执行过程实例分析.html","excerpt":"** Hive学习之路 （二十）Hive 执行过程实例分析：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （二十）Hive 执行过程实例分析","text":"** Hive学习之路 （二十）Hive 执行过程实例分析：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （二十）Hive 执行过程实例分析 &lt;The rest of contents | 余下全文&gt; 一、Hive 执行过程概述1、概述（1） Hive 将 HQL 转换成一组操作符（Operator），比如 GroupByOperator, JoinOperator 等 （2）操作符 Operator 是 Hive 的最小处理单元 （3）每个操作符代表一个 HDFS 操作或者 MapReduce 作业 （4）Hive 通过 ExecMapper 和 ExecReducer 执行 MapReduce 程序，执行模式有本地模式和分 布式两种模式 2、Hive 操作符列表 3、Hive 编译器的工作职责（1）Parser：将 HQL 语句转换成抽象语法树（AST：Abstract Syntax Tree） （2）Semantic Analyzer：将抽象语法树转换成查询块 （3）Logic Plan Generator：将查询块转换成逻辑查询计划 （4）Logic Optimizer：重写逻辑查询计划，优化逻辑执行计划 （5）Physical Plan Gernerator：将逻辑计划转化成物理计划（MapReduce Jobs） （6）Physical Optimizer：选择最佳的 Join 策略，优化物理执行计划 4、优化器类型 上表中带①符号的，优化目的都是尽量将任务合并到一个 Job 中，以减少 Job 数量，带②的 优化目的是尽量减少 shuffle 数据量 二、join1、对于 join 操作1SELECT pv.pageid, u.age FROM page_view pv JOIN user u ON pv.userid = u.userid; 2、实现过程 Map： 1、以 JOIN ON 条件中的列作为 Key，如果有多个列，则 Key 是这些列的组合 2、以 JOIN 之后所关心的列作为 Value，当有多个列时，Value 是这些列的组合。在 Value 中还会包含表的 Tag 信息，用于标明此 Value 对应于哪个表 3、按照 Key 进行排序 Shuffle： 1、根据 Key 的值进行 Hash，并将 Key/Value 对按照 Hash 值推至不同对 Reduce 中 Reduce： 1、 Reducer 根据 Key 值进行 Join 操作，并且通过 Tag 来识别不同的表中的数据 3、具体实现过程 三、Group By1、对于 group by操作1SELECT pageid, age, count(1) FROM pv_users GROUP BY pageid, age; 2、实现过程 四、Distinct1、对于 distinct的操作按照 age 分组，然后统计每个分组里面的不重复的 pageid 有多少个 1SELECT age, count(distinct pageid) FROM pv_users GROUP BY age; 2、实现过程 3、详细过程解释该 SQL 语句会按照 age 和 pageid 预先分组，进行 distinct 操作。然后会再按 照 age 进行分组，再进行一次 distinct 操作","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （十九）Hive的数据倾斜","slug":"2019-04-19-Hive学习之路 （十九）Hive的数据倾斜","date":"2019-04-19T02:30:04.000Z","updated":"2019-09-18T15:20:27.275Z","comments":true,"path":"2019-04-19-Hive学习之路 （十九）Hive的数据倾斜.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-19-Hive学习之路 （十九）Hive的数据倾斜.html","excerpt":"** Hive学习之路 （十九）Hive的数据倾斜：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十九）Hive的数据倾斜","text":"** Hive学习之路 （十九）Hive的数据倾斜：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十九）Hive的数据倾斜 &lt;The rest of contents | 余下全文&gt; 1、什么是数据倾斜？由于数据分布不均匀，造成数据大量的集中到一点，造成数据热点 2、Hadoop 框架的特性 A、不怕数据大，怕数据倾斜 B、Jobs 数比较多的作业运行效率相对比较低，如子查询比较多 C、 sum,count,max,min 等聚集函数，通常不会有数据倾斜问题 3、主要表现任务进度长时间维持在 99%或者 100%的附近，查看任务监控页面，发现只有少量 reduce 子任务未完成，因为其处理的数据量和其他的 reduce 差异过大。 单一 reduce 处理的记录数和平均记录数相差太大，通常达到好几倍之多，最长时间远大 于平均时长。 4、容易数据倾斜情况 A、group by 不和聚集函数搭配使用的时候 B、count(distinct)，在数据量大的情况下，容易数据倾斜，因为 count(distinct)是按 group by 字段分组，按 distinct 字段排序 C、 小表关联超大表 join 5、产生数据倾斜的原因 A：key 分布不均匀 B：业务数据本身的特性 C：建表考虑不周全 D：某些 HQL 语句本身就存在数据倾斜 6、业务场景（1）空值产生的数据倾斜场景说明在日志中，常会有信息丢失的问题，比如日志中的 user_id，如果取其中的 user_id 和用户表中的 user_id 相关联，就会碰到数据倾斜的问题。 解决方案解决方案 1：user_id 为空的不参与关联 123select * from log a join user b on a.user_id is not null and a.user_id = b.user_idunion allselect * from log c where c.user_id is null; 解决方案 2：赋予空值新的 key 值 12select * from log a left outer join user b oncase when a.user_id is null then concat(&apos;hive&apos;,rand()) else a.user_id end = b.user_id 总结方法 2 比方法 1 效率更好，不但 IO 少了，而且作业数也少了，方案 1 中，log 表 读了两次，jobs 肯定是 2，而方案 2 是 1。这个优化适合无效 id（比如-99，’’，null）产 生的数据倾斜，把空值的 key 变 成一个字符串加上一个随机数，就能把造成数据倾斜的 数据分到不同的 reduce 上解决数据倾斜的问题。 改变之处：使本身为 null 的所有记录不会拥挤在同一个 reduceTask 了，会由于有替代的 随机字符串值，而分散到了多个 reduceTask 中了，由于 null 值关联不上，处理后并不影响最终结果。 （2）不同数据类型关联产生数据倾斜场景说明用户表中 user_id 字段为 int，log 表中 user_id 为既有 string 也有 int 的类型， 当按照两个表的 user_id 进行 join 操作的时候，默认的 hash 操作会按照 int 类型的 id 进 行分配，这样就会导致所有的 string 类型的 id 就被分到同一个 reducer 当中 解决方案把数字类型 id 转换成 string 类型的 id 1select * from user a left outer join log b on b.user_id = cast(a.user_id as string) （3）大小表关联查询产生数据倾斜 注意：使用map join解决小表关联大表造成的数据倾斜问题。这个方法使用的频率很高。 map join 概念：将其中做连接的小表（全量数据）分发到所有 MapTask 端进行 Join，从 而避免了 reduceTask，前提要求是内存足以装下该全量数据 以大表 a 和小表 b 为例，所有的 maptask 节点都装载小表 b 的所有数据，然后大表 a 的 一个数据块数据比如说是 a1 去跟 b 全量数据做链接，就省去了 reduce 做汇总的过程。 所以相对来说，在内存允许的条件下使用 map join 比直接使用 MapReduce 效率还高些， 当然这只限于做 join 查询的时候。 在 hive 中，直接提供了能够在 HQL 语句指定该次查询使用 map join，map join 的用法是 在查询/子查询的SELECT关键字后面添加/*+ MAPJOIN(tablelist) */提示优化器转化为map join（早期的 Hive 版本的优化器是不能自动优化 map join 的）。其中 tablelist 可以是一个 表，或以逗号连接的表的列表。tablelist 中的表将会读入内存，通常应该是将小表写在 这里。 MapJoin 具体用法： 123select /* +mapjoin(a) */ a.id aid, name, age from a join b on a.id = b.id;select /* +mapjoin(movies) */ a.title, b.rating from movies a join ratings b on a.movieid =b.movieid; 在 hive0.11 版本以后会自动开启 map join 优化，由两个参数控制： set hive.auto.convert.join=true; //设置 MapJoin 优化自动开启 set hive.mapjoin.smalltable.filesize=25000000 //设置小表不超过多大时开启 mapjoin 优化 如果是大大表关联呢？那就大事化小，小事化了。把大表切分成小表，然后分别 map join 那么如果小表不大不小，那该如何处理呢？？？ 使用 map join 解决小表(记录数少)关联大表的数据倾斜问题，这个方法使用的频率非常 高，但如果小表很大，大到 map join 会出现 bug 或异常，这时就需要特别的处理 举一例：日志表和用户表做链接 1select * from log a left outer join users b on a.user_id = b.user_id; users 表有 600w+的记录，把 users 分发到所有的 map 上也是个不小的开销，而且 map join 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。 改进方案： 123456select /*+mapjoin(x)*/* from log aleft outer join ( select /*+mapjoin(c)*/ d.* from ( select distinct user_id from log ) c join users d on c.user_id = d.user_id) xon a.user_id = x.user_id; 假如，log 里 user_id 有上百万个，这就又回到原来 map join 问题。所幸，每日的会员 uv 不会太多，有交易的会员不会太多，有点击的会员不会太多，有佣金的会员不会太多等 等。所以这个方法能解决很多场景下的数据倾斜问题","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （十八）Hive的Shell操作","slug":"2019-04-18-Hive学习之路 （十八）Hive的Shell操作","date":"2019-04-18T02:30:04.000Z","updated":"2019-09-18T15:20:23.574Z","comments":true,"path":"2019-04-18-Hive学习之路 （十八）Hive的Shell操作.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-18-Hive学习之路 （十八）Hive的Shell操作.html","excerpt":"** Hive学习之路 （十八）Hive的Shell操作：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十八）Hive的Shell操作","text":"** Hive学习之路 （十八）Hive的Shell操作：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十八）Hive的Shell操作 &lt;The rest of contents | 余下全文&gt; 一、Hive的命令行1、Hive支持的一些命令 Command Description quit Use quit or exit to leave the interactive shell. set key=value Use this to set value of particular configuration variable. One thing to note here is that if you misspell the variable name, cli will not show an error. set This will print a list of configuration variables that are overridden by user or hive. set -v This will print all hadoop and hive configuration variables. add FILE [file] [file]* Adds a file to the list of resources add jar jarname list FILE list all the files added to the distributed cache list FILE [file]* Check if given resources are already added to distributed cache ! [cmd] Executes a shell command from the hive shell dfs [dfs cmd] Executes a dfs command from the hive shell [query] Executes a hive query and prints results to standard out source FILE Used to execute a script file inside the CLI. 2、语法结构1hive [-hiveconf x=y]* [&lt;-i filename&gt;]* [&lt;-f filename&gt;|&lt;-e query-string&gt;] [-S] 说明： 1、-i 从文件初始化 HQL 2、-e 从命令行执行指定的 HQL 3、-f 执行 HQL 脚本 4、-v 输出执行的 HQL 语句到控制台 5、-p connect to Hive Server on port number 6、-hiveconf x=y（Use this to set hive/hadoop configuration variables） 7、-S：表示以不打印日志的形式执行命名操作 3、示例（1）运行一个查询1[hadoop@hadoop3 ~]$ hive -e &quot;select * from cookie.cookie1;&quot; （2）运行一个文件 编写hive.sql文件 运行编写的文件 （3）运行参数文件从配置文件启动 hive，并加载配置文件当中的配置参数 二、Hive的参数配置方式1、Hive的参数配置大全https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties 2、Hive的参数设置方式开发 Hive 应用时，不可避免地需要设定 Hive 的参数。设定 Hive 的参数可以调优 HQL 代码 的执行效率，或帮助定位问题。然而实践中经常遇到的一个问题是，为什么设定的参数没有 起作用？这通常是错误的设定方式导致的 对于一般参数，有以下三种设定方式： 1、配置文件 （全局有效） 2、命令行参数（对 hive 启动实例有效） 3、参数声明 （对 hive 的连接 session 有效） （1）配置文件Hive 的配置文件包括： A. 用户自定义配置文件：$HIVE_CONF_DIR/hive-site.xml B. 默认配置文件：$HIVE_CONF_DIR/hive-default.xml 用户自定义配置会覆盖默认配置。 另外，Hive 也会读入 Hadoop 的配置，因为 Hive 是作为 Hadoop 的客户端启动的，Hive 的配 置会覆盖 Hadoop 的配置。 配置文件的设定对本机启动的所有 Hive 进程都有效。 （2）命令行参数启动 Hive（客户端或 Server 方式）时，可以在命令行添加-hiveconf param=value 来设定参数，例如： 这一设定对本次启动的 session（对于 server 方式启动，则是所有请求的 session）有效。 （3）参数声明可以在 HQL 中使用 SET 关键字设定参数，例如： 这一设定的作用域也是 session 级的。 set hive.exec.reducers.bytes.per.reducer= 每个 reduce task 的平均负载数据量 Hive 会估算总数据量，然后用该值除以上述参数值，就能得出需要运行的 reduceTask 数 set hive.exec.reducers.max= 设置 reduce task 数量的上限 set mapreduce.job.reduces= 指定固定的 reduce task 数量 但是，这个参数在必要时&lt;业务逻辑决定只能用一个 reduce task&gt; hive 会忽略，比如在设置 了 set mapreduce.job.reduces = 3，但是 HQL 语句当中使用了 order by 的话，那么就会忽略该参数的设置。 上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置 文件设定。注意某些系统级的参数，例如 log4j 相关的设定，必须用前两种方式设定，因为 那些参数的读取在 session 建立以前已经完成了。","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （十七）Hive分析窗口函数(五) GROUPING SETS、GROUPING__ID、CUBE和ROLLUP","slug":"2019-04-17-Hive学习之路 （十七）Hive分析窗口函数(五) GROUPING SETS、GROUPING__ID、CUBE和ROLLUP","date":"2019-04-17T02:30:04.000Z","updated":"2019-09-18T15:20:18.926Z","comments":true,"path":"2019-04-17-Hive学习之路 （十七）Hive分析窗口函数(五) GROUPING SETS、GROUPING__ID、CUBE和ROLLUP.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-17-Hive学习之路 （十七）Hive分析窗口函数(五) GROUPING SETS、GROUPING__ID、CUBE和ROLLUP.html","excerpt":"** Hive学习之路 （十七）Hive分析窗口函数(五) GROUPING SETS、GROUPING__ID、CUBE和ROLLUP：** &lt;Excerpt in index | 首页摘要&gt; ​ GROUPING SETS,GROUPING__ID,CUBE,ROLLUP 这几个分析函数通常用于OLAP中，不能累加，而且需要根据不同维度上钻和下钻的指标统计，比如，分小时、天、月的UV数。","text":"** Hive学习之路 （十七）Hive分析窗口函数(五) GROUPING SETS、GROUPING__ID、CUBE和ROLLUP：** &lt;Excerpt in index | 首页摘要&gt; ​ GROUPING SETS,GROUPING__ID,CUBE,ROLLUP 这几个分析函数通常用于OLAP中，不能累加，而且需要根据不同维度上钻和下钻的指标统计，比如，分小时、天、月的UV数。 &lt;The rest of contents | 余下全文&gt; 数据准备数据格式12345678910111213142015-03,2015-03-10,cookie12015-03,2015-03-10,cookie52015-03,2015-03-12,cookie72015-04,2015-04-12,cookie32015-04,2015-04-13,cookie22015-04,2015-04-13,cookie42015-04,2015-04-16,cookie42015-03,2015-03-10,cookie22015-03,2015-03-10,cookie32015-04,2015-04-12,cookie52015-04,2015-04-13,cookie62015-04,2015-04-15,cookie32015-04,2015-04-15,cookie22015-04,2015-04-16,cookie1 创建表123456use cookie;drop table if exists cookie5;create table cookie5(month string, day string, cookieid string) row format delimited fields terminated by ',';load data local inpath \"/home/hadoop/cookie5.txt\" into table cookie5;select * from cookie5; 玩一玩GROUPING SETS和GROUPING__ID说明在一个GROUP BY查询中，根据不同的维度组合进行聚合，等价于将不同维度的GROUP BY结果集进行UNION ALL GROUPING__ID，表示结果属于哪一个分组集合。 查询语句123456789select month, day, count(distinct cookieid) as uv, GROUPING__IDfrom cookie.cookie5 group by month,day grouping sets (month,day) order by GROUPING__ID; 等价于123SELECT month,NULL,COUNT(DISTINCT cookieid) AS uv,1 AS GROUPING__ID FROM cookie5 GROUP BY month UNION ALL SELECT NULL,day,COUNT(DISTINCT cookieid) AS uv,2 AS GROUPING__ID FROM cookie5 GROUP BY day 查询结果 结果说明第一列是按照month进行分组 第二列是按照day进行分组 第三列是按照month或day分组是，统计这一组有几个不同的cookieid 第四列grouping_id表示这一组结果属于哪个分组集合，根据grouping sets中的分组条件month，day，1是代表month，2是代表day 再比如1234567SELECT month, day,COUNT(DISTINCT cookieid) AS uv,GROUPING__ID FROM cookie5 GROUP BY month,day GROUPING SETS (month,day,(month,day)) ORDER BY GROUPING__ID; 等价于12345SELECT month,NULL,COUNT(DISTINCT cookieid) AS uv,1 AS GROUPING__ID FROM cookie5 GROUP BY month UNION ALL SELECT NULL,day,COUNT(DISTINCT cookieid) AS uv,2 AS GROUPING__ID FROM cookie5 GROUP BY dayUNION ALL SELECT month,day,COUNT(DISTINCT cookieid) AS uv,3 AS GROUPING__ID FROM cookie5 GROUP BY month,day 玩一玩CUBE说明根据GROUP BY的维度的所有组合进行聚合 查询语句1234567SELECT month, day,COUNT(DISTINCT cookieid) AS uv,GROUPING__ID FROM cookie5 GROUP BY month,day WITH CUBE ORDER BY GROUPING__ID; 等价于1234567SELECT NULL,NULL,COUNT(DISTINCT cookieid) AS uv,0 AS GROUPING__ID FROM cookie5UNION ALL SELECT month,NULL,COUNT(DISTINCT cookieid) AS uv,1 AS GROUPING__ID FROM cookie5 GROUP BY month UNION ALL SELECT NULL,day,COUNT(DISTINCT cookieid) AS uv,2 AS GROUPING__ID FROM cookie5 GROUP BY dayUNION ALL SELECT month,day,COUNT(DISTINCT cookieid) AS uv,3 AS GROUPING__ID FROM cookie5 GROUP BY month,day 查询结果 玩一玩ROLLUP说明是CUBE的子集，以最左侧的维度为主，从该维度进行层级聚合 查询语句– 比如，以month维度进行层级聚合 123SELECT month, day, COUNT(DISTINCT cookieid) AS uv, GROUPING__ID FROM cookie5 GROUP BY month,day WITH ROLLUP ORDER BY GROUPING__ID; 可以实现这样的上钻过程：月天的UV-&gt;月的UV-&gt;总UV –把month和day调换顺序，则以day维度进行层级聚合： 可以实现这样的上钻过程：天月的UV-&gt;天的UV-&gt;总UV（这里，根据天和月进行聚合，和根据天聚合结果一样，因为有父子关系，如果是其他维度组合的话，就会不一样）","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （十六）Hive分析窗口函数(四) LAG、LEAD、FIRST_VALUE和LAST_VALUE","slug":"2019-04-16-Hive学习之路 （十六）Hive分析窗口函数(四) LAG、LEAD、FIRST_VALUE和LAST_VALUE","date":"2019-04-16T02:30:04.000Z","updated":"2019-09-18T15:20:15.070Z","comments":true,"path":"2019-04-16-Hive学习之路 （十六）Hive分析窗口函数(四) LAG、LEAD、FIRST_VALUE和LAST_VALUE.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-16-Hive学习之路 （十六）Hive分析窗口函数(四) LAG、LEAD、FIRST_VALUE和LAST_VALUE.html","excerpt":"** Hive学习之路 （十六）Hive分析窗口函数(四) LAG、LEAD、FIRST_VALUE和LAST_VALUE：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十六）Hive分析窗口函数(四) LAG、LEAD、FIRST_VALUE和LAST_VALUE","text":"** Hive学习之路 （十六）Hive分析窗口函数(四) LAG、LEAD、FIRST_VALUE和LAST_VALUE：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十六）Hive分析窗口函数(四) LAG、LEAD、FIRST_VALUE和LAST_VALUE &lt;The rest of contents | 余下全文&gt; 数据准备数据格式cookie4.txt 1234567891011121314cookie1,2015-04-10 10:00:02,url2cookie1,2015-04-10 10:00:00,url1cookie1,2015-04-10 10:03:04,1url3cookie1,2015-04-10 10:50:05,url6cookie1,2015-04-10 11:00:00,url7cookie1,2015-04-10 10:10:00,url4cookie1,2015-04-10 10:50:01,url5cookie2,2015-04-10 10:00:02,url22cookie2,2015-04-10 10:00:00,url11cookie2,2015-04-10 10:03:04,1url33cookie2,2015-04-10 10:50:05,url66cookie2,2015-04-10 11:00:00,url77cookie2,2015-04-10 10:10:00,url44cookie2,2015-04-10 10:50:01,url55 创建表123456use cookie;drop table if exists cookie4;create table cookie4(cookieid string, createtime string, url string) row format delimited fields terminated by &apos;,&apos;;load data local inpath &quot;/home/hadoop/cookie4.txt&quot; into table cookie4;select * from cookie4; 玩一玩LAG说明LAG(col,n,DEFAULT) 用于统计窗口内往上第n行值 第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL） 查询语句12345678select cookieid, createtime, url, row_number() over (partition by cookieid order by createtime) as rn, LAG(createtime,1,'1970-01-01 00:00:00') over (partition by cookieid order by createtime) as last_1_time, LAG(createtime,2) over (partition by cookieid order by createtime) as last_2_time from cookie.cookie4; 查询结果 结果说明123456789last_1_time: 指定了往上第1行的值，default为&apos;1970-01-01 00:00:00&apos; cookie1第一行，往上1行为NULL,因此取默认值 1970-01-01 00:00:00 cookie1第三行，往上1行值为第二行值，2015-04-10 10:00:02 cookie1第六行，往上1行值为第五行值，2015-04-10 10:50:01last_2_time: 指定了往上第2行的值，为指定默认值 cookie1第一行，往上2行为NULL cookie1第二行，往上2行为NULL cookie1第四行，往上2行为第二行值，2015-04-10 10:00:02 cookie1第七行，往上2行为第五行值，2015-04-10 10:50:01 玩一玩LEAD说明与LAG相反 LEAD(col,n,DEFAULT) 用于统计窗口内往下第n行值 第一个参数为列名，第二个参数为往下第n行（可选，默认为1），第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL） 查询语句12345678select cookieid, createtime, url, row_number() over (partition by cookieid order by createtime) as rn, LEAD(createtime,1,'1970-01-01 00:00:00') over (partition by cookieid order by createtime) as next_1_time, LEAD(createtime,2) over (partition by cookieid order by createtime) as next_2_time from cookie.cookie4; 查询结果结果说明–逻辑与LAG一样，只不过LAG是往上，LEAD是往下。 玩一玩FIRST_VALUE说明取分组内排序后，截止到当前行，第一个值 查询语句1234567select cookieid, createtime, url, row_number() over (partition by cookieid order by createtime) as rn, first_value(url) over (partition by cookieid order by createtime) as first1 from cookie.cookie4; 查询结果 玩一玩LAST_VALUE说明取分组内排序后，截止到当前行，最后一个值 查询语句1234567select cookieid, createtime, url, row_number() over (partition by cookieid order by createtime) as rn, last_value(url) over (partition by cookieid order by createtime) as last1 from cookie.cookie4; 查询结果如果不指定ORDER BY，则默认按照记录在文件中的偏移量进行排序，会出现错误的结果 如果想要取分组内排序后最后一个值，则需要变通一下查询语句123456789select cookieid, createtime, url, row_number() over (partition by cookieid order by createtime) as rn, LAST_VALUE(url) over (partition by cookieid order by createtime) as last1, FIRST_VALUE(url) over (partition by cookieid order by createtime desc) as last2 from cookie.cookie4 order by cookieid,createtime; 查询结果 提示：在使用分析函数的过程中，要特别注意ORDER BY子句，用的不恰当，统计出的结果就不是你所期望的。","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （十五）Hive分析窗口函数(三) CUME_DIST和PERCENT_RANK","slug":"2019-04-15-Hive学习之路 （十五）Hive分析窗口函数(三) CUME_DIST和PERCENT_RANK","date":"2019-04-15T02:30:04.000Z","updated":"2019-09-18T15:20:10.847Z","comments":true,"path":"2019-04-15-Hive学习之路 （十五）Hive分析窗口函数(三) CUME_DIST和PERCENT_RANK.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-15-Hive学习之路 （十五）Hive分析窗口函数(三) CUME_DIST和PERCENT_RANK.html","excerpt":"** Hive学习之路 （十五）Hive分析窗口函数(三) CUME_DIST和PERCENT_RANK：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十五）Hive分析窗口函数(三) CUME_DIST和PERCENT_RANK","text":"** Hive学习之路 （十五）Hive分析窗口函数(三) CUME_DIST和PERCENT_RANK：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十五）Hive分析窗口函数(三) CUME_DIST和PERCENT_RANK &lt;The rest of contents | 余下全文&gt; 数据准备数据格式cookie3.txt 12345d1,user1,1000d1,user2,2000d1,user3,3000d2,user4,4000d2,user5,5000 创建表123456use cookie;drop table if exists cookie3;create table cookie3(dept string, userid string, sal int) row format delimited fields terminated by ',';load data local inpath \"/home/hadoop/cookie3.txt\" into table cookie3;select * from cookie3; 玩一玩CUME_DIST说明–CUME_DIST ：小于等于当前值的行数/分组内总行数 查询语句比如，统计小于等于当前薪水的人数，所占总人数的比例 1234567select dept, userid, sal, cume_dist() over (order by sal) as rn1, cume_dist() over (partition by dept order by sal) as rn2from cookie.cookie3; 查询结果 结果说明12345rn1: 没有partition,所有数据均为1组，总行数为5， 第一行：小于等于1000的行数为1，因此，1/5=0.2 第三行：小于等于3000的行数为3，因此，3/5=0.6rn2: 按照部门分组，dpet=d1的行数为3, 第二行：小于等于2000的行数为2，因此，2/3=0.6666666666666666 玩一玩PERCENT_RANK说明 –PERCENT_RANK ：分组内当前行的RANK值-1/分组内总行数-1 查询语句1234567891011select dept, userid, sal, percent_rank() over (order by sal) as rn1, --分组内 rank() over (order by sal) as rn11, --分组内的rank值 sum(1) over (partition by null) as rn12, --分组内总行数 percent_rank() over (partition by dept order by sal) as rn2, rank() over (partition by dept order by sal) as rn21, sum(1) over (partition by dept) as rn22 from cookie.cookie3; 查询结果 结果说明–PERCENT_RANK ：分组内当前行的RANK值-1/分组内总行数-1 rn1 == (rn11-1) / (rn12-1) rn2 == (rn21-1) / (rn22-1) 12345678rn1: rn1 = (rn11-1) / (rn12-1) 第一行,(1-1)/(5-1)=0/4=0 第二行,(2-1)/(5-1)=1/4=0.25 第四行,(4-1)/(5-1)=3/4=0.75rn2: 按照dept分组， dept=d1的总行数为3 第一行，(1-1)/(3-1)=0 第三行，(3-1)/(3-1)=1","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （十四）Hive分析窗口函数(二) NTILE,ROW_NUMBER,RANK,DENSE_RANK","slug":"2019-04-14-Hive学习之路 （十四）Hive分析窗口函数(二) NTILE,ROW_NUMBER,RANK,DENSE_RANK","date":"2019-04-14T02:30:04.000Z","updated":"2019-09-18T15:20:06.142Z","comments":true,"path":"2019-04-14-Hive学习之路 （十四）Hive分析窗口函数(二) NTILE,ROW_NUMBER,RANK,DENSE_RANK.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-14-Hive学习之路 （十四）Hive分析窗口函数(二) NTILE,ROW_NUMBER,RANK,DENSE_RANK.html","excerpt":"** Hive学习之路 （十四）Hive分析窗口函数(二) NTILE,ROW_NUMBER,RANK,DENSE_RANK：** &lt;Excerpt in index | 首页摘要&gt; ​ 本文中介绍前几个序列函数，NTILE,ROW_NUMBER,RANK,DENSE_RANK，下面会一一解释各自的用途。 注意： 序列函数不支持WINDOW子句。（ROWS BETWEEN）","text":"** Hive学习之路 （十四）Hive分析窗口函数(二) NTILE,ROW_NUMBER,RANK,DENSE_RANK：** &lt;Excerpt in index | 首页摘要&gt; ​ 本文中介绍前几个序列函数，NTILE,ROW_NUMBER,RANK,DENSE_RANK，下面会一一解释各自的用途。 注意： 序列函数不支持WINDOW子句。（ROWS BETWEEN） &lt;The rest of contents | 余下全文&gt; 数据准备数据格式1234567891011121314cookie1,2015-04-10,1cookie1,2015-04-11,5cookie1,2015-04-12,7cookie1,2015-04-13,3cookie1,2015-04-14,2cookie1,2015-04-15,4cookie1,2015-04-16,4cookie2,2015-04-10,2cookie2,2015-04-11,3cookie2,2015-04-12,5cookie2,2015-04-13,6cookie2,2015-04-14,3cookie2,2015-04-15,9cookie2,2015-04-16,7 创建表12345use cookie;drop table if exists cookie2;create table cookie2(cookieid string, createtime string, pv int) row format delimited fields terminated by ',';load data local inpath \"/home/hadoop/cookie2.txt\" into table cookie2;select * from cookie2; 玩一玩NTILE说明NTILE(n)，用于将分组数据按照顺序切分成n片，返回当前切片值NTILE不支持ROWS BETWEEN，比如 NTILE(2) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)如果切片不均匀，默认增加第一个切片的分布 查询语句123456789select cookieid, createtime, pv, ntile(2) over (partition by cookieid order by createtime) as rn1, --分组内将数据分成2片 ntile(3) over (partition by cookieid order by createtime) as rn2, --分组内将数据分成2片 ntile(4) over (order by createtime) as rn3 --将所有数据分成4片from cookie.cookie2 order by cookieid,createtime; 查询结果 比如，统计一个cookie，pv数最多的前1/3的天查询语句123456select cookieid, createtime, pv, ntile(3) over (partition by cookieid order by pv desc ) as rn from cookie.cookie2; 查询结果 –rn = 1 的记录，就是我们想要的结果 玩一玩ROW_NUMBER说明ROW_NUMBER() –从1开始，按照顺序，生成分组内记录的序列–比如，按照pv降序排列，生成分组内每天的pv名次ROW_NUMBER() 的应用场景非常多，再比如，获取分组内排序第一的记录;获取一个session中的第一条refer等。 分组排序123456select cookieid, createtime, pv, row_number() over (partition by cookieid order by pv desc) as rnfrom cookie.cookie2; 查询结果 – 所以如果需要取每一组的前3名，只需要rn&lt;=3即可，适合TopN 玩一玩RANK 和 DENSE_RANK—RANK() 生成数据项在分组中的排名，排名相等会在名次中留下空位—DENSE_RANK() 生成数据项在分组中的排名，排名相等会在名次中不会留下空位 查询语句123456789select cookieid, createtime, pv, rank() over (partition by cookieid order by pv desc) as rn1, dense_rank() over (partition by cookieid order by pv desc) as rn2, row_number() over (partition by cookieid order by pv desc) as rn3from cookie.cookie2 where cookieid='cookie1'; 查询结果 ROW_NUMBER、RANK和DENSE_RANK的区别row_number： 按顺序编号，不留空位rank： 按顺序编号，相同的值编相同号，留空位dense_rank： 按顺序编号，相同的值编相同的号，不留空位","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （十三）Hive分析窗口函数(一) SUM,AVG,MIN,MAX","slug":"2019-04-13-Hive学习之路 （十三）Hive分析窗口函数(一) SUM,AVG,MIN,MAX","date":"2019-04-13T02:30:04.000Z","updated":"2019-09-18T15:20:01.531Z","comments":true,"path":"2019-04-13-Hive学习之路 （十三）Hive分析窗口函数(一) SUM,AVG,MIN,MAX.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-13-Hive学习之路 （十三）Hive分析窗口函数(一) SUM,AVG,MIN,MAX.html","excerpt":"** Hive学习之路 （十三）Hive分析窗口函数(一) SUM,AVG,MIN,MAX：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十三）Hive分析窗口函数(一) SUM,AVG,MIN,MAX","text":"** Hive学习之路 （十三）Hive分析窗口函数(一) SUM,AVG,MIN,MAX：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十三）Hive分析窗口函数(一) SUM,AVG,MIN,MAX &lt;The rest of contents | 余下全文&gt; 数据准备数据格式1234567cookie1,2015-04-10,1cookie1,2015-04-11,5cookie1,2015-04-12,7cookie1,2015-04-13,3cookie1,2015-04-14,2cookie1,2015-04-15,4cookie1,2015-04-16,4 创建数据库及表123456create database if not exists cookie;use cookie;drop table if exists cookie1;create table cookie1(cookieid string, createtime string, pv int) row format delimited fields terminated by &apos;,&apos;;load data local inpath &quot;/home/hadoop/cookie1.txt&quot; into table cookie1;select * from cookie1; 玩一玩SUM查询语句1234567891011select cookieid, createtime, pv, sum(pv) over (partition by cookieid order by createtime rows between unbounded preceding and current row) as pv1, sum(pv) over (partition by cookieid order by createtime) as pv2, sum(pv) over (partition by cookieid) as pv3, sum(pv) over (partition by cookieid order by createtime rows between 3 preceding and current row) as pv4, sum(pv) over (partition by cookieid order by createtime rows between 3 preceding and 1 following) as pv5, sum(pv) over (partition by cookieid order by createtime rows between current row and unbounded following) as pv6 from cookie1; 查询结果 说明123456pv1: 分组内从起点到当前行的pv累积，如，11号的pv1=10号的pv+11号的pv, 12号=10号+11号+12号pv2: 同pv1pv3: 分组内(cookie1)所有的pv累加pv4: 分组内当前行+往前3行，如，11号=10号+11号， 12号=10号+11号+12号， 13号=10号+11号+12号+13号， 14号=11号+12号+13号+14号pv5: 分组内当前行+往前3行+往后1行，如，14号=11号+12号+13号+14号+15号=5+7+3+2+4=21pv6: 分组内当前行+往后所有行，如，13号=13号+14号+15号+16号=3+2+4+4=13，14号=14号+15号+16号=2+4+4=10 如果不指定ROWS BETWEEN,默认为从起点到当前行;如果不指定ORDER BY，则将分组内所有值累加;关键是理解ROWS BETWEEN含义,也叫做WINDOW子句：PRECEDING：往前FOLLOWING：往后CURRENT ROW：当前行UNBOUNDED：起点， UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING：表示到后面的终点–其他AVG，MIN，MAX，和SUM用法一样。 玩一玩AVG查询语句1234567891011select cookieid, createtime, pv, avg(pv) over (partition by cookieid order by createtime rows between unbounded preceding and current row) as pv1, -- 默认为从起点到当前行 avg(pv) over (partition by cookieid order by createtime) as pv2, --从起点到当前行，结果同pv1 avg(pv) over (partition by cookieid) as pv3, --分组内所有行 avg(pv) over (partition by cookieid order by createtime rows between 3 preceding and current row) as pv4, --当前行+往前3行 avg(pv) over (partition by cookieid order by createtime rows between 3 preceding and 1 following) as pv5, --当前行+往前3行+往后1行 avg(pv) over (partition by cookieid order by createtime rows between current row and unbounded following) as pv6 --当前行+往后所有行from cookie1; 查询结果 玩一玩MIN查询语句1234567891011select cookieid, createtime, pv, min(pv) over (partition by cookieid order by createtime rows between unbounded preceding and current row) as pv1, -- 默认为从起点到当前行 min(pv) over (partition by cookieid order by createtime) as pv2, --从起点到当前行，结果同pv1 min(pv) over (partition by cookieid) as pv3, --分组内所有行 min(pv) over (partition by cookieid order by createtime rows between 3 preceding and current row) as pv4, --当前行+往前3行 min(pv) over (partition by cookieid order by createtime rows between 3 preceding and 1 following) as pv5, --当前行+往前3行+往后1行 min(pv) over (partition by cookieid order by createtime rows between current row and unbounded following) as pv6 --当前行+往后所有行from cookie1; 查询结果 玩一玩MAX查询语句1234567891011select cookieid, createtime, pv, max(pv) over (partition by cookieid order by createtime rows between unbounded preceding and current row) as pv1, -- 默认为从起点到当前行 max(pv) over (partition by cookieid order by createtime) as pv2, --从起点到当前行，结果同pv1 max(pv) over (partition by cookieid) as pv3, --分组内所有行 max(pv) over (partition by cookieid order by createtime rows between 3 preceding and current row) as pv4, --当前行+往前3行 max(pv) over (partition by cookieid order by createtime rows between 3 preceding and 1 following) as pv5, --当前行+往前3行+往后1行 max(pv) over (partition by cookieid order by createtime rows between current row and unbounded following) as pv6 --当前行+往后所有行from cookie1; 查询结果","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （十二）Hive SQL练习之影评案例","slug":"2019-04-12-Hive学习之路 （十二）Hive SQL练习之影评案例","date":"2019-04-12T02:30:04.000Z","updated":"2019-09-18T15:24:05.920Z","comments":true,"path":"2019-04-12-Hive学习之路 （十二）Hive SQL练习之影评案例.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-12-Hive学习之路 （十二）Hive SQL练习之影评案例.html","excerpt":"** Hive学习之路 （十二）Hive SQL练习之影评案例：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十二）Hive SQL练习之影评案例","text":"** Hive学习之路 （十二）Hive SQL练习之影评案例：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十二）Hive SQL练习之影评案例 &lt;The rest of contents | 余下全文&gt; 案例说明现有如此三份数据：1、users.dat 数据格式为： 2::M::56::16::70072， 共有6040条数据对应字段为：UserID BigInt, Gender String, Age Int, Occupation String, Zipcode String对应字段中文解释：用户id，性别，年龄，职业，邮政编码 2、movies.dat 数据格式为： 2::Jumanji (1995)::Adventure|Children’s|Fantasy， 共有3883条数据对应字段为：MovieID BigInt, Title String, Genres String对应字段中文解释：电影ID，电影名字，电影类型 3、ratings.dat 数据格式为： 1::1193::5::978300760， 共有1000209条数据对应字段为：UserID BigInt, MovieID BigInt, Rating Double, Timestamped String对应字段中文解释：用户ID，电影ID，评分，评分时间戳 题目要求 数据要求： （1）写shell脚本清洗数据。（hive不支持解析多字节的分隔符，也就是说hive只能解析’:’, 不支持解析’::’，所以用普通方式建表来使用是行不通的，要求对数据做一次简单清洗） （2）使用Hive能解析的方式进行 Hive要求： （1）正确建表，导入数据（三张表，三份数据），并验证是否正确 （2）求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数） （3）分别求男性，女性当中评分最高的10部电影（性别，电影名，影评分） （4）求movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，影评分） （5）求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（观影者，电影名，影评分） （6）求好片（评分&gt;=4.0）最多的那个年份的最好看的10部电影 （7）求1997年上映的电影中，评分最高的10部Comedy类电影 （8）该影评库中各种类型电影中评价最高的5部电影（类型，电影名，平均影评分） （9）各年评分最高的电影类型（年份，类型，影评分） （10）每个地区最高评分的电影名，把结果存入HDFS（地区，电影名，影评分） 数据下载https://files.cnblogs.com/files/qingyunzong/hive%E5%BD%B1%E8%AF%84%E6%A1%88%E4%BE%8B.zip 解析之前已经使用MapReduce程序将3张表格进行合并，所以只需要将合并之后的表格导入对应的表中进行查询即可。 1、正确建表，导入数据（三张表，三份数据），并验证是否正确（1）分析需求需要创建一个数据库movie，在movie数据库中创建3张表，t_user，t_movie，t_rating t_user:userid bigint,sex string,age int,occupation string,zipcode stringt_movie:movieid bigint,moviename string,movietype stringt_rating:userid bigint,movieid bigint,rate double,times string 原始数据是以::进行切分的，所以需要使用能解析多字节分隔符的Serde即可 使用RegexSerde 需要两个参数：input.regex = “(.)::(.)::(.*)”output.format.string = “%1$s %2$s %3$s” （2）创建数据库123drop database if exists movie;create database if not exists movie;use movie; （3）创建t_user表123456789create table t_user(userid bigint,sex string,age int,occupation string,zipcode string) row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' with serdeproperties('input.regex'='(.*)::(.*)::(.*)::(.*)::(.*)','output.format.string'='%1$s %2$s %3$s %4$s %5$s')stored as textfile; （4）创建t_movie表12345678use movie;create table t_movie(movieid bigint,moviename string,movietype string) row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' with serdeproperties('input.regex'='(.*)::(.*)::(.*)','output.format.string'='%1$s %2$s %3$s')stored as textfile; （5）创建t_rating表123456789use movie;create table t_rating(userid bigint,movieid bigint,rate double,times string) row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' with serdeproperties('input.regex'='(.*)::(.*)::(.*)::(.*)','output.format.string'='%1$s %2$s %3$s %4$s')stored as textfile; （6）导入数据12345670: jdbc:hive2://hadoop3:10000&gt; load data local inpath \"/home/hadoop/movie/users.dat\" into table t_user;No rows affected (0.928 seconds)0: jdbc:hive2://hadoop3:10000&gt; load data local inpath \"/home/hadoop/movie/movies.dat\" into table t_movie;No rows affected (0.538 seconds)0: jdbc:hive2://hadoop3:10000&gt; load data local inpath \"/home/hadoop/movie/ratings.dat\" into table t_rating;No rows affected (0.963 seconds)0: jdbc:hive2://hadoop3:10000&gt; （7）验证1select t.* from t_user t; 1select t.* from t_movie t; 1select t.* from t_rating t; 2、求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）（1）思路分析： 1、需求字段：电影名 t_movie.moviename 评分次数 t_rating.rate count() 2、核心SQL：按照电影名进行分组统计，求出每部电影的评分次数并按照评分次数降序排序 （2）完整SQL：123456create table answer2 as select a.moviename as moviename,count(a.moviename) as total from t_movie a join t_rating b on a.movieid=b.movieid group by a.moviename order by total desc limit 10; 1select * from answer2; 3、分别求男性，女性当中评分最高的10部电影（性别，电影名，影评分）（1）分析思路： 1、需求字段：性别 t_user.sex 电影名 t_movie.moviename 影评分 t_rating.rate 2、核心SQL：三表联合查询，按照性别过滤条件，电影名作为分组条件，影评分作为排序条件进行查询 （2）完整SQL：女性当中评分最高的10部电影（性别，电影名，影评分）评论次数大于等于50次 12345678910create table answer3_F as select \"F\" as sex, c.moviename as name, avg(a.rate) as avgrate, count(c.moviename) as total from t_rating a join t_user b on a.userid=b.userid join t_movie c on a.movieid=c.movieid where b.sex=\"F\" group by c.moviename having total &gt;= 50order by avgrate desc limit 10; 1select * from answer3_F； 男性当中评分最高的10部电影（性别，电影名，影评分）评论次数大于等于50次 12345678910create table answer3_M as select \"M\" as sex, c.moviename as name, avg(a.rate) as avgrate, count(c.moviename) as total from t_rating a join t_user b on a.userid=b.userid join t_movie c on a.movieid=c.movieid where b.sex=\"M\" group by c.moviename having total &gt;= 50order by avgrate desc limit 10; 1select * from answer3_M； 4、求movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，影评分）（1）分析思路： 1、需求字段：年龄段 t_user.age 影评分 t_rating.rate 2、核心SQL：t_user和t_rating表进行联合查询，用movieid=2116作为过滤条件，用年龄段作为分组条件 （2）完整SQL：123456create table answer4 as select a.age as age, avg(b.rate) as avgrate from t_user a join t_rating b on a.userid=b.userid where b.movieid=2116 group by a.age;select * from answer4; 5、求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（观影者，电影名，影评分）（1）分析思路： 1、需求字段：观影者 t_rating.userid 电影名 t_movie.moviename 影评分 t_rating.rate 2、核心SQL： A. 需要先求出最喜欢看电影的那位女性 需要查询的字段：性别：t_user.sex 观影次数：count(t_rating.userid) B. 根据A中求出的女性userid作为where过滤条件，以看过的电影的影评分rate作为排序条件进行排序，求出评分最高的10部电影 需要查询的字段：电影的ID：t_rating.movieid C. 求出B中10部电影的平均影评分 需要查询的字段：电影的ID：answer5_B.movieid 影评分：t_rating.rate （2）完整SQL： A. 需要先求出最喜欢看电影的那位女性 123456select a.userid, count(a.userid) as total from t_rating a join t_user b on a.userid = b.userid where b.sex=\"F\" group by a.userid order by total desc limit 1; B. 根据A中求出的女性userid作为where过滤条件，以看过的电影的影评分rate作为排序条件进行排序，求出评分最高的10部电影 123456create table answer5_B as select a.movieid as movieid, a.rate as rate from t_rating a where a.userid=1150 order by rate desc limit 10; 1select * from answer5_B; C. 求出B中10部电影的平均影评分 123456create table answer5_C as select b.movieid as movieid, c.moviename as moviename, avg(b.rate) as avgrate from answer5_B a join t_rating b on a.movieid=b.movieid join t_movie c on b.movieid=c.movieid group by b.movieid,c.moviename; 1select * from answer5_C; 6、求好片（评分&gt;=4.0）最多的那个年份的最好看的10部电影（1）分析思路： 1、需求字段：电影id t_rating.movieid 电影名 t_movie.moviename（包含年份） 影评分 t_rating.rate 上映年份 xxx.years 2、核心SQL： A. 需要将t_rating和t_movie表进行联合查询，将电影名当中的上映年份截取出来，保存到临时表answer6_A中 需要查询的字段：电影id t_rating.movieid 电影名 t_movie.moviename（包含年份） 影评分 t_rating.rate B. 从answer6_A按照年份进行分组条件，按照评分&gt;=4.0作为where过滤条件，按照count(years)作为排序条件进行查询 需要查询的字段：电影的ID：answer6_A.years C. 从answer6_A按照years=1998作为where过滤条件，按照评分作为排序条件进行查询 需要查询的字段：电影的ID：answer6_A.moviename 影评分：answer6_A.avgrate （2）完整SQL：A. 需要将t_rating和t_movie表进行联合查询，将电影名当中的上映年份截取出来 12345create table answer6_A asselect a.movieid as movieid, a.moviename as moviename, substr(a.moviename,-5,4) as years, avg(b.rate) as avgratefrom t_movie a join t_rating b on a.movieid=b.movieid group by a.movieid, a.moviename;select * from answer6_A; B. 从answer6_A按照年份进行分组条件，按照评分&gt;=4.0作为where过滤条件，按照count(years)作为排序条件进行查询 123456select years, count(years) as total from answer6_A a where avgrate &gt;= 4.0 group by years order by total desc limit 1; C. 从answer6_A按照years=1998作为where过滤条件，按照评分作为排序条件进行查询 123456create table answer6_C asselect a.moviename as name, a.avgrate as rate from answer6_A a where a.years=1998 order by rate desc limit 10; 1select * from answer6_C; 7、求1997年上映的电影中，评分最高的10部Comedy类电影（1）分析思路： 1、需求字段：电影id t_rating.movieid 电影名 t_movie.moviename（包含年份） 影评分 t_rating.rate 上映年份 xxx.years（最终查询结果可不显示） 电影类型 xxx.type（最终查询结果可不显示） 2、核心SQL： A. 需要电影类型，所有可以将第六步中求出answer6_A表和t_movie表进行联合查询 需要查询的字段：电影id answer6_A.movieid 电影名 answer6_A.moviename 影评分 answer6_A.rate 电影类型 t_movie.movietype 上映年份 answer6_A.years B. 从answer7_A按照电影类型中是否包含Comedy和按上映年份作为where过滤条件，按照评分作为排序条件进行查询，将结果保存到answer7_B中 需要查询的字段：电影的ID：answer7_A.id 电影的名称：answer7_A.name 电影的评分：answer7_A.rate （2）完整SQL：A. 需要电影类型，所有可以将第六步中求出answer6_A表和t_movie表进行联合查询 1234create table answer7_A as select b.movieid as id, b.moviename as name, b.years as years, b.avgrate as rate, a.movietype as type from t_movie a join answer6_A b on a.movieid=b.movieid;select t.* from answer7_A t; B. 从answer7_A按照电影类型中是否包含Comedy和按照评分&gt;=4.0作为where过滤条件，按照评分作为排序条件进行查询，将结果保存到answer7_B中 123456create table answer7_B as select t.id as id, t.name as name, t.rate as rate from answer7_A t where t.years=1997 and instr(lcase(t.type),'comedy') &gt;0 order by rate desclimit 10; 1select * from answer7_B; 8、该影评库中各种类型电影中评价最高的5部电影（类型，电影名，平均影评分）（1）分析思路： 1、需求字段：电影id movieid 电影名 moviename 影评分 rate（排序条件） 电影类型 type（分组条件） 2、核心SQL： A. 需要电影类型，所有需要将answer7_A中的type字段进行裂变，将结果保存到answer8_A中 需要查询的字段：电影id answer7_A.id 电影名 answer7_A.name（包含年份） 上映年份 answer7_A.years 影评分 answer7_A.rate 电影类型 answer7_A.movietype B. 求TopN，按照type分组，需要添加一列来记录每组的顺序，将结果保存到answer8_B中 row_number() ：用来生成 num字段的值 distribute by movietype ：按照type进行分组 sort by avgrate desc ：每组数据按照rate排降序 num：新列， 值就是每一条记录在每一组中按照排序规则计算出来的排序值 C. 从answer8_B中取出num列序号&lt;=5的 （2）完整SQL：A. 需要电影类型，所有需要将answer7_A中的type字段进行裂变，将结果保存到answer8_A中 12345create table answer8_A as select a.id as id, a.name as name, a.years as years, a.rate as rate, tv.type as type from answer7_A a lateral view explode(split(a.type,\"\\\\|\")) tv as type;select * from answer8_A; B. 求TopN，按照type分组，需要添加一列来记录每组的顺序，将结果保存到answer8_B中 1234create table answer8_B as select id,name,years,rate,type,row_number() over(distribute by type sort by rate desc ) as numfrom answer8_A;select * from answer8_B; C. 从answer8_B中取出num列序号&lt;=5的 1select a.* from answer8_B a where a.num &lt;= 5; 9、各年评分最高的电影类型（年份，类型，影评分）（1）分析思路： 1、需求字段：电影id movieid 电影名 moviename 影评分 rate（排序条件） 电影类型 type（分组条件） 上映年份 years（分组条件） 2、核心SQL： A. 需要按照电影类型和上映年份进行分组，按照影评分进行排序，将结果保存到answer9_A中 需要查询的字段： 上映年份 answer7_A.years 影评分 answer7_A.rate 电影类型 answer7_A.movietype B. 求TopN，按照years分组，需要添加一列来记录每组的顺序，将结果保存到answer9_B中 C. 按照num=1作为where过滤条件取出结果数据 （2）完整SQL：A. 需要按照电影类型和上映年份进行分组，按照影评分进行排序，将结果保存到answer9_A中 123456create table answer9_A as select a.years as years, a.type as type, avg(a.rate) as rate from answer8_A a group by a.years,a.type order by rate desc;select * from answer9_A; B. 求TopN，按照years分组，需要添加一列来记录每组的顺序，将结果保存到answer9_B中 1234create table answer9_B as select years,type,rate,row_number() over (distribute by years sort by rate) as numfrom answer9_A;select * from answer9_B; C. 按照num=1作为where过滤条件取出结果数据 1select * from answer9_B where num=1; 10、每个地区最高评分的电影名，把结果存入HDFS（地区，电影名，影评分）（1）分析思路： 1、需求字段：电影id t_movie.movieid 电影名 t_movie.moviename 影评分 t_rating.rate（排序条件） 地区 t_user.zipcode（分组条件） 2、核心SQL： A. 需要把三张表进行联合查询，取出电影id、电影名称、影评分、地区，将结果保存到answer10_A表中 需要查询的字段：电影id t_movie.movieid 电影名 t_movie.moviename 影评分 t_rating.rate（排序条件） 地区 t_user.zipcode（分组条件） B. 求TopN，按照地区分组，按照平均排序，添加一列num用来记录地区排名，将结果保存到answer10_B表中 C. 按照num=1作为where过滤条件取出结果数据 （2）完整SQL： A. 需要把三张表进行联合查询，取出电影id、电影名称、影评分、地区，将结果保存到answer10_A表中 123456create table answer10_A asselect c.movieid, c.moviename, avg(b.rate) as avgrate, a.zipcodefrom t_user a join t_rating b on a.userid=b.userid join t_movie c on b.movieid=c.movieid group by a.zipcode,c.movieid, c.moviename; 1select t.* from answer10_A t; B. 求TopN，按照地区分组，按照平均排序，添加一列num用来记录地区排名，将结果保存到answer10_B表中 1234create table answer10_B asselect movieid,moviename,avgrate,zipcode, row_number() over (distribute by zipcode sort by avgrate) as num from answer10_A; select t.* from answer10_B t; C. 按照num=1作为where过滤条件取出结果数据并保存到HDFS上 1insert overwrite directory \"/movie/answer10/\" select t.* from answer10_B t where t.num=1;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （十一）Hive的5个面试题","slug":"2019-04-11-Hive学习之路 （十一）Hive的5个面试题","date":"2019-04-11T02:30:04.000Z","updated":"2019-09-18T15:19:52.172Z","comments":true,"path":"2019-04-11-Hive学习之路 （十一）Hive的5个面试题.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-11-Hive学习之路 （十一）Hive的5个面试题.html","excerpt":"** Hive学习之路 （十一）Hive的5个面试题：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十一）Hive的5个面试题","text":"** Hive学习之路 （十一）Hive的5个面试题：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十一）Hive的5个面试题 &lt;The rest of contents | 余下全文&gt; 一、求单月访问次数和总访问次数1、数据说明数据字段说明1用户名，月份，访问次数 数据格式123456789101112131415A,2015-01,5A,2015-01,15B,2015-01,5A,2015-01,8B,2015-01,25A,2015-01,5A,2015-02,4A,2015-02,6B,2015-02,10B,2015-02,5A,2015-03,16A,2015-03,22B,2015-03,23B,2015-03,10B,2015-03,1 2、数据准备（1）创建表12345678use myhive;create external table if not exists t_access(uname string comment '用户名',umonth string comment '月份',ucount int comment '访问次数') comment '用户访问表' row format delimited fields terminated by \",\" location \"/hive/t_access\"; （2）导入数据1load data local inpath &quot;/home/hadoop/access.txt&quot; into table t_access; （3）验证数据1select * from t_access; 3、结果需求现要求出：每个用户截止到每月为止的最大单月访问次数和累计到该月的总访问次数，结果数据格式如下 4、需求分析此结果需要根据用户+月份进行分组 （1）先求出当月访问次数12345678910--求当月访问次数create table tmp_access(name string,mon string,num int); insert into table tmp_access select uname,umonth,sum(ucount) from t_access t group by t.uname,t.umonth;select * from tmp_access; （2）tmp_access进行自连接视图12345create view tmp_view as select a.name anme,a.mon amon,a.num anum,b.name bname,b.mon bmon,b.num bnum from tmp_access a join tmp_access b on a.name=b.name;select * from tmp_view; （3）进行比较统计1234select anme,amon,anum,max(bnum) as max_access,sum(bnum) as sum_access from tmp_view where amon&gt;=bmon group by anme,amon,anum; 二、学生课程成绩1、说明1234567use myhive;CREATE TABLE `course` ( `id` int, `sid` int , `course` string, `score` int ) ; 12345678// 插入数据// 字段解释：id, 学号， 课程， 成绩INSERT INTO `course` VALUES (1, 1, 'yuwen', 43);INSERT INTO `course` VALUES (2, 1, 'shuxue', 55);INSERT INTO `course` VALUES (3, 2, 'yuwen', 77);INSERT INTO `course` VALUES (4, 2, 'shuxue', 88);INSERT INTO `course` VALUES (5, 3, 'yuwen', 98);INSERT INTO `course` VALUES (6, 3, 'shuxue', 65); 2、需求求：所有数学课程成绩 大于 语文课程成绩的学生的学号 1、使用case…when…将不同的课程名称转换成不同的列12345create view tmp_course_view asselect sid, case course when \"shuxue\" then score else 0 end as shuxue, case course when \"yuwen\" then score else 0 end as yuwen from course; select * from tmp_course_view; 2、以sid分组合并取各成绩最大值1234create view tmp_course_view1 asselect aa.sid, max(aa.shuxue) as shuxue, max(aa.yuwen) as yuwen from tmp_course_view aa group by sid; select * from tmp_course_view1; 3、比较结果1select * from tmp_course_view1 where shuxue &gt; yuwen; 三、求每一年最大气温的那一天 + 温度1、说明数据格式 12010012325 具体数据 View Code 数据解释 12010012325表示在2010年01月23日的气温为25度 2、 需求比如：2010012325表示在2010年01月23日的气温为25度。现在要求使用hive，计算每一年出现过的最大气温的日期+温度。要计算出每一年的最大气温。我用select substr(data,1,4),max(substr(data,9,2)) from table2 group by substr(data,1,4);出来的是 年份 + 温度 这两列数据例如 2015 99 但是如果我是想select 的是：具体每一年最大气温的那一天 + 温度 。例如 20150109 99请问该怎么执行hive语句。。group by 只需要substr(data,1,4)，但是select substr(data,1,8)，又不在group by 的范围内。是我陷入了思维死角。一直想不出所以然。。求大神指点一下。在select 如果所需要的。不在group by的条件里。这种情况如何去分析？ 3、解析（1）创建一个临时表tmp_weather，将数据切分123create table tmp_weather as select substr(data,1,4) years,substr(data,5,2) months,substr(data,7,2) days,substr(data,9,2) temp from weather;select * from tmp_weather; （2）创建一个临时表tmp_year_weather123create table tmp_year_weather as select substr(data,1,4) years,max(substr(data,9,2)) max_temp from weather group by substr(data,1,4);select * from tmp_year_weather; （3）将2个临时表进行连接查询1select * from tmp_year_weather a join tmp_weather b on a.years=b.years and a.max_temp=b.temp; 四、求学生选课情况1、数据说明（1）数据格式12345678910111213id course 1,a 1,b 1,c 1,e 2,a 2,c 2,d 2,f 3,a 3,b 3,c 3,e （2）字段含义表示有id为1,2,3的学生选修了课程a,b,c,d,e,f中其中几门。 2、数据准备（1）建表t_course12create table t_course(id int,course string)row format delimited fields terminated by \",\"; （2）导入数据1load data local inpath \"/home/hadoop/course/course.txt\" into table t_course; 3、需求编写Hive的HQL语句来实现以下结果：表中的1表示选修，表中的0表示未选修 1234id a b c d e f1 1 1 1 0 1 02 1 0 1 1 0 13 1 1 1 0 1 0 4、解析第一步： 1select collect_set(course) as courses from id_course; 第二步： 1234567set hive.strict.checks.cartesian.product=false;create table id_courses as select t1.id as id,t1.course as id_courses,t2.course courses from ( select id as id,collect_set(course) as course from id_course group by id ) t1 join (select collect_set(course) as course from id_course) t2; 启用严格模式：hive.mapred.mode = strict // Deprecatedhive.strict.checks.large.query = true该设置会禁用：1. 不指定分页的orderby 2. 对分区表不指定分区进行查询 3. 和数据量无关，只是一个查询模式 hive.strict.checks.type.safety = true严格类型安全，该属性不允许以下操作：1. bigint和string之间的比较 2. bigint和double之间的比较 hive.strict.checks.cartesian.product = true该属性不允许笛卡尔积操作 第三步：得出最终结果：思路：拿出course字段中的每一个元素在id_courses中进行判断，看是否存在。 12345678select id,case when array_contains(id_courses, courses[0]) then 1 else 0 end as a,case when array_contains(id_courses, courses[1]) then 1 else 0 end as b,case when array_contains(id_courses, courses[2]) then 1 else 0 end as c,case when array_contains(id_courses, courses[3]) then 1 else 0 end as d,case when array_contains(id_courses, courses[4]) then 1 else 0 end as e,case when array_contains(id_courses, courses[5]) then 1 else 0 end as f from id_courses; 五、求月销售额和总销售额1、数据说明（1）数据格式123456789101112131415a,01,150a,01,200b,01,1000b,01,800c,01,250c,01,220b,01,6000a,02,2000a,02,3000b,02,1000b,02,1500c,02,350c,02,280a,03,350a,03,250 （2）字段含义店铺，月份，金额 2、数据准备（1）创建数据库表t_store1234567use class;create table t_store(name string,months int,money int) row format delimited fields terminated by \",\"; （2）导入数据1load data local inpath \"/home/hadoop/store.txt\" into table t_store; 3、需求编写Hive的HQL语句求出每个店铺的当月销售额和累计到当月的总销售额 4、解析（1）按照商店名称和月份进行分组统计 1234create table tmp_store1 as select name,months,sum(money) as money from t_store group by name,months;select * from tmp_store1; （2）对tmp_store1 表里面的数据进行自连接 12345create table tmp_store2 as select a.name aname,a.months amonths,a.money amoney,b.name bname,b.months bmonths,b.money bmoney from tmp_store1 a join tmp_store1 b on a.name=b.name order by aname,amonths;select * from tmp_store2; （3）比较统计 1select aname,amonths,amoney,sum(bmoney) as total from tmp_store2 where amonths &gt;= bmonths group by aname,amonths,amoney;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （十）Hive的高级操作","slug":"2019-04-10-Hive学习之路 （十）Hive的高级操作","date":"2019-04-10T02:30:04.000Z","updated":"2019-09-18T15:19:45.944Z","comments":true,"path":"2019-04-10-Hive学习之路 （十）Hive的高级操作.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-10-Hive学习之路 （十）Hive的高级操作.html","excerpt":"** Hive学习之路 （十）Hive的高级操作：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十）Hive的高级操作","text":"** Hive学习之路 （十）Hive的高级操作：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （十）Hive的高级操作 &lt;The rest of contents | 余下全文&gt; 一、负责数据类型1、array 现有数据如下： 1231 huangbo guangzhou,xianggang,shenzhen a1:30,a2:20,a3:100 beijing,112233,13522334455,5002 xuzheng xianggang b2:50,b3:40 tianjin,223344,13644556677,6003 wangbaoqiang beijing,zhejinag c1:200 chongqinjg,334455,15622334455,20 建表语句 123456789101112use class;create table cdt(id int, name string, work_location array&lt;string&gt;, piaofang map&lt;string,bigint&gt;, address struct&lt;location:string,zipcode:int,phone:string,value:int&gt;) row format delimited fields terminated by \"\\t\" collection items terminated by \",\" map keys terminated by \":\" lines terminated by \"\\n\"; 导入数据 10: jdbc:hive2://hadoop3:10000&gt; load data local inpath \"/home/hadoop/cdt.txt\" into table cdt; 查询语句 1select * from cdt; 1select name from cdt; 1select work_location from cdt; 1select work_location[0] from cdt; 1select work_location[1] from cdt; 2、map建表语句、导入数据同1 查询语句 1select piaofang from cdt; 1select piaofang[\"a1\"] from cdt; 3、struct建表语句、导入数据同1 查询语句 1select address from cdt; 1select address.location from cdt; 4、uniontype很少使用 参考资料：http://yugouai.iteye.com/blog/1849192 二、视图1、Hive 的视图和关系型数据库的视图区别和关系型数据库一样，Hive 也提供了视图的功能，不过请注意，Hive 的视图和关系型数据库的数据还是有很大的区别： （1）只有逻辑视图，没有物化视图； （2）视图只能查询，不能 Load/Insert/Update/Delete 数据； （3）视图在创建时候，只是保存了一份元数据，当查询视图的时候，才开始执行视图对应的 那些子查询 2、Hive视图的创建语句1create view view_cdt as select * from cdt; 3、Hive视图的查看语句12show views;desc view_cdt;-- 查看某个具体视图的信息 4、Hive视图的使用语句1select * from view_cdt; 5、Hive视图的删除语句1drop view view_cdt; 三、函数1、内置函数具体可看http://www.cnblogs.com/qingyunzong/p/8744593.html （1）查看内置函数1show functions; （2）显示函数的详细信息1desc function substr; （3）显示函数的扩展信息1desc function extended substr; 2、自定义函数UDF当 Hive 提供的内置函数无法满足业务处理需要时，此时就可以考虑使用用户自定义函数。 UDF（user-defined function）作用于单个数据行，产生一个数据行作为输出。（数学函数，字 符串函数） UDAF（用户定义聚集函数 User- Defined Aggregation Funcation）：接收多个输入数据行，并产 生一个输出数据行。（count，max） UDTF（表格生成函数 User-Defined Table Functions）：接收一行输入，输出多行（explode） (1) 简单UDF示例A. 导入hive需要的jar包，自定义一个java类继承UDF，重载 evaluate 方法ToLowerCase.java 1234567891011import org.apache.hadoop.hive.ql.exec.UDF;public class ToLowerCase extends UDF&#123; // 必须是 public，并且 evaluate 方法可以重载 public String evaluate(String field) &#123; String result = field.toLowerCase(); return result; &#125; &#125; B. 打成 jar 包上传到服务器C. 将 jar 包添加到 hive 的 classpath1add JAR /home/hadoop/udf.jar; D. 创建临时函数与开发好的 class 关联起来10: jdbc:hive2://hadoop3:10000&gt; create temporary function tolowercase as 'com.study.hive.udf.ToLowerCase'; E. 至此，便可以在 hql 在使用自定义的函数10: jdbc:hive2://hadoop3:10000&gt; select tolowercase('HELLO'); (2) JSON数据解析UDF开发现有原始 json 数据（rating.json）如下 {“movie”:”1193”,”rate”:”5”,”timeStamp”:”978300760”,”uid”:”1”} {“movie”:”661”,”rate”:”3”,”timeStamp”:”978302109”,”uid”:”1”} {“movie”:”914”,”rate”:”3”,”timeStamp”:”978301968”,”uid”:”1”} {“movie”:”3408”,”rate”:”4”,”timeStamp”:”978300275”,”uid”:”1”} {“movie”:”2355”,”rate”:”5”,”timeStamp”:”978824291”,”uid”:”1”} {“movie”:”1197”,”rate”:”3”,”timeStamp”:”978302268”,”uid”:”1”} {“movie”:”1287”,”rate”:”5”,”timeStamp”:”978302039”,”uid”:”1”} {“movie”:”2804”,”rate”:”5”,”timeStamp”:”978300719”,”uid”:”1”} {“movie”:”594”,”rate”:”4”,”timeStamp”:”978302268”,”uid”:”1”} 现在需要将数据导入到 hive 仓库中，并且最终要得到这么一个结果： 该怎么做、？？？（提示：可用内置 get_json_object 或者自定义函数完成） A. get_json_object(string json_string, string path)返回值: string 说明：解析json的字符串json_string,返回path指定的内容。如果输入的json字符串无效，那么返回NULL。 这个函数每次只能返回一个数据项。 10: jdbc:hive2://hadoop3:10000&gt; select get_json_object('&#123;\"movie\":\"594\",\"rate\":\"4\",\"timeStamp\":\"978302268\",\"uid\":\"1\"&#125;','$.movie'); 创建json表并将数据导入进去 123450: jdbc:hive2://hadoop3:10000&gt; create table json(data string);No rows affected (0.983 seconds)0: jdbc:hive2://hadoop3:10000&gt; load data local inpath '/home/hadoop/json.txt' into table json;No rows affected (1.046 seconds)0: jdbc:hive2://hadoop3:10000&gt; 1230: jdbc:hive2://hadoop3:10000&gt; select . . . . . . . . . . . . . . .&gt; get_json_object(data,'$.movie') as movie . . . . . . . . . . . . . . .&gt; from json； B. json_tuple(jsonStr, k1, k2, …)参数为一组键k1，k2……和JSON字符串，返回值的元组。该方法比 get_json_object 高效，因为可以在一次调用中输入多个键 12345670: jdbc:hive2://hadoop3:10000&gt; select . . . . . . . . . . . . . . .&gt; b.b_movie,. . . . . . . . . . . . . . .&gt; b.b_rate,. . . . . . . . . . . . . . .&gt; b.b_timeStamp,. . . . . . . . . . . . . . .&gt; b.b_uid . . . . . . . . . . . . . . .&gt; from json a . . . . . . . . . . . . . . .&gt; lateral view json_tuple(a.data,&apos;movie&apos;,&apos;rate&apos;,&apos;timeStamp&apos;,&apos;uid&apos;) b as b_movie,b_rate,b_timeStamp,b_uid; (3) Transform实现Hive 的 TRANSFORM 关键字提供了在 SQL 中调用自写脚本的功能。适合实现 Hive 中没有的 功能又不想写 UDF 的情况 具体以一个实例讲解。 Json 数据： {“movie”:”1193”,”rate”:”5”,”timeStamp”:”978300760”,”uid”:”1”} 需求：把 timestamp 的值转换成日期编号 1、先加载 rating.json 文件到 hive 的一个原始表 rate_json 12create table rate_json(line string) row format delimited;load data local inpath '/home/hadoop/rating.json' into table rate_json; 2、创建 rate 这张表用来存储解析 json 出来的字段： 12create table rate(movie int, rate int, unixtime int, userid int) row format delimited fieldsterminated by '\\t'; 解析 json，得到结果之后存入 rate 表： 123456insert into table rate selectget_json_object(line,'$.movie') as moive,get_json_object(line,'$.rate') as rate,get_json_object(line,'$.timeStamp') as unixtime,get_json_object(line,'$.uid') as useridfrom rate_json; 3、使用 transform+python 的方式去转换 unixtime 为 weekday 先编辑一个 python 脚本文件 12345678910########python######代码## vi weekday_mapper.py#!/bin/pythonimport sysimport datetimefor line in sys.stdin: line = line.strip() movie,rate,unixtime,userid = line.split('\\t') weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() print '\\t'.join([movie, rate, str(weekday),userid]) 保存文件 然后，将文件加入 hive 的 classpath： 123hive&gt;add file /home/hadoop/weekday_mapper.py;hive&gt; insert into table lastjsontable select transform(movie,rate,unixtime,userid)using 'python weekday_mapper.py' as(movie,rate,weekday,userid) from rate; 创建最后的用来存储调用 python 脚本解析出来的数据的表：lastjsontable 12create table lastjsontable(movie int, rate int, weekday int, userid int) row format delimitedfields terminated by '\\t'; 最后查询看数据是否正确 1select distinct(weekday) from lastjsontable; 四、特殊分隔符处理补充：hive 读取数据的机制： 1、 首先用 InputFormat&lt;默认是：org.apache.hadoop.mapred.TextInputFormat &gt;的一个具体实 现类读入文件数据，返回一条一条的记录（可以是行，或者是你逻辑中的“行”） 2、 然后利用 SerDe&lt;默认：org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&gt;的一个具体 实现类，对上面返回的一条一条的记录进行字段切割 Hive 对文件中字段的分隔符默认情况下只支持单字节分隔符，如果数据文件中的分隔符是多 字符的，如下所示： 01||huangbo 02||xuzheng 03||wangbaoqiang 1、使用RegexSerDe正则表达式解析创建表 1234create table t_bi_reg(id string,name string)row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe'with serdeproperties('input.regex'='(.*)\\\\|\\\\|(.*)','output.format.string'='%1$s %2$s')stored as textfile; 导入数据并查询 1230: jdbc:hive2://hadoop3:10000&gt; load data local inpath '/home/hadoop/data.txt' into table t_bi_reg;No rows affected (0.747 seconds)0: jdbc:hive2://hadoop3:10000&gt; select a.* from t_bi_reg a;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （九）Hive的内置函数","slug":"2019-04-09-Hive学习之路 （九）Hive的内置函数","date":"2019-04-09T02:30:04.000Z","updated":"2019-09-18T15:23:10.058Z","comments":true,"path":"2019-04-09-Hive学习之路 （九）Hive的内置函数.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-09-Hive学习之路 （九）Hive的内置函数.html","excerpt":"** Hive学习之路 （九）Hive的内置函数：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （九）Hive的内置函数","text":"** Hive学习之路 （九）Hive的内置函数：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （九）Hive的内置函数 &lt;The rest of contents | 余下全文&gt; 数学函数 Return Type Name (Signature) Description DOUBLE round(DOUBLE a) Returns the rounded BIGINT value of a.返回对a四舍五入的BIGINT值 DOUBLE round(DOUBLE a, INT d) Returns a rounded to d decimal places.返回DOUBLE型d的保留n位小数的DOUBLW型的近似值 DOUBLE bround(DOUBLE a) Returns the rounded BIGINT value of a using HALF_EVEN rounding mode (as of Hive 1.3.0, 2.0.0). Also known as Gaussian rounding or bankers’ rounding. Example: bround(2.5) = 2, bround(3.5) = 4. 银行家舍入法（14：舍，69：进，5-&gt;前位数是偶：舍，5-&gt;前位数是奇：进） DOUBLE bround(DOUBLE a, INT d) Returns a rounded to d decimal places using HALF_EVEN rounding mode (as of Hive 1.3.0, 2.0.0). Example: bround(8.25, 1) = 8.2, bround(8.35, 1) = 8.4. 银行家舍入法,保留d位小数 BIGINT floor(DOUBLE a) Returns the maximum BIGINT value that is equal to or less than a向下取整，最数轴上最接近要求的值的左边的值 如：6.10-&gt;6 -3.4-&gt;-4 BIGINT ceil(DOUBLE a), ceiling(DOUBLE a) Returns the minimum BIGINT value that is equal to or greater than a.求其不小于小给定实数的最小整数如：ceil(6) = ceil(6.1)= ceil(6.9) = 6 DOUBLE rand(), rand(INT seed) Returns a random number (that changes from row to row) that is distributed uniformly from 0 to 1. Specifying the seed will make sure the generated random number sequence is deterministic.每行返回一个DOUBLE型随机数seed是随机因子 DOUBLE exp(DOUBLE a), exp(DECIMAL a) Returns ea where e is the base of the natural logarithm. Decimal version added in Hive 0.13.0.返回e的a幂次方， a可为小数 DOUBLE ln(DOUBLE a), ln(DECIMAL a) Returns the natural logarithm of the argument a. Decimal version added in Hive 0.13.0.以自然数为底d的对数，a可为小数 DOUBLE log10(DOUBLE a), log10(DECIMAL a) Returns the base-10 logarithm of the argument a. Decimal version added in Hive 0.13.0.以10为底d的对数，a可为小数 DOUBLE log2(DOUBLE a), log2(DECIMAL a) Returns the base-2 logarithm of the argument a. Decimal version added in Hive 0.13.0.以2为底数d的对数，a可为小数 DOUBLE log(DOUBLE base, DOUBLE a)log(DECIMAL base, DECIMAL a) Returns the base-base logarithm of the argument a. Decimal versions added in Hive 0.13.0.以base为底的对数，base 与 a都是DOUBLE类型 DOUBLE pow(DOUBLE a, DOUBLE p), power(DOUBLE a, DOUBLE p) Returns ap.计算a的p次幂 DOUBLE sqrt(DOUBLE a), sqrt(DECIMAL a) Returns the square root of a. Decimal version added in Hive 0.13.0.计算a的平方根 STRING bin(BIGINT a) Returns the number in binary format (see http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_bin).**计算二进制a的STRING类型，a为BIGINT类型** STRING hex(BIGINT a) hex(STRING a) hex(BINARY a) If the argument is an INT or binary, hex returns the number as a STRING in hexadecimal format. Otherwise if the number is a STRING, it converts each character into its hexadecimal representation and returns the resulting STRING. (Seehttp://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_hex, BINARY version as of Hive 0.12.0.)计算十六进制a的STRING类型，如果a为STRING类型就转换成字符相对应的十六进制 BINARY unhex(STRING a) Inverse of hex. Interprets each pair of characters as a hexadecimal number and converts to the byte representation of the number. (BINARY version as of Hive 0.12.0, used to return a string.)hex的逆方法 STRING conv(BIGINT num, INT from_base, INT to_base), conv(STRING num, INT from_base, INT to_base) Converts a number from a given base to another (see http://dev.mysql.com/doc/refman/5.0/en/mathematical-functions.html#function_conv).**将GIGINT/STRING类型的num从from_base进制转换成to_base进制** DOUBLE abs(DOUBLE a) Returns the absolute value.计算a的绝对值 INT or DOUBLE pmod(INT a, INT b), pmod(DOUBLE a, DOUBLE b) Returns the positive value of a mod b.a对b取模 DOUBLE sin(DOUBLE a), sin(DECIMAL a) Returns the sine of a (a is in radians). Decimal version added in Hive 0.13.0.求a的正弦值 DOUBLE asin(DOUBLE a), asin(DECIMAL a) Returns the arc sin of a if -1&lt;=a&lt;=1 or NULL otherwise. Decimal version added in Hive 0.13.0.求d的反正弦值 DOUBLE cos(DOUBLE a), cos(DECIMAL a) Returns the cosine of a (a is in radians). Decimal version added in Hive 0.13.0.求余弦值 DOUBLE acos(DOUBLE a), acos(DECIMAL a) Returns the arccosine of a if -1&lt;=a&lt;=1 or NULL otherwise. Decimal version added in Hive 0.13.0.求反余弦值 DOUBLE tan(DOUBLE a), tan(DECIMAL a) Returns the tangent of a (a is in radians). Decimal version added in Hive 0.13.0.求正切值 DOUBLE atan(DOUBLE a), atan(DECIMAL a) Returns the arctangent of a. Decimal version added in Hive 0.13.0.求反正切值 DOUBLE degrees(DOUBLE a), degrees(DECIMAL a) Converts value of a from radians to degrees. Decimal version added in Hive 0.13.0.奖弧度值转换角度值 DOUBLE radians(DOUBLE a), radians(DOUBLE a) Converts value of a from degrees to radians. Decimal version added in Hive 0.13.0.将角度值转换成弧度值 INT or DOUBLE positive(INT a), positive(DOUBLE a) Returns a.返回a INT or DOUBLE negative(INT a), negative(DOUBLE a) Returns -a.返回a的相反数 DOUBLE or INT sign(DOUBLE a), sign(DECIMAL a) Returns the sign of a as ‘1.0’ (if a is positive) or ‘-1.0’ (if a is negative), ‘0.0’ otherwise. The decimal version returns INT instead of DOUBLE. Decimal version added in Hive 0.13.0.如果a是正数则返回1.0，是负数则返回-1.0，否则返回0.0 DOUBLE e() Returns the value of e.数学常数e DOUBLE pi() Returns the value of pi.数学常数pi BIGINT factorial(INT a) Returns the factorial of a (as of Hive 1.2.0). Valid a is [0..20]. 求a的阶乘 DOUBLE cbrt(DOUBLE a) Returns the cube root of a double value (as of Hive 1.2.0). 求a的立方根 INT BIGINT shiftleft(TINYINT|SMALLINT|INT a, INT b)shiftleft(BIGINT a, INT b) Bitwise left shift (as of Hive 1.2.0). Shifts a b positions to the left.Returns int for tinyint, smallint and int a. Returns bigint for bigint a.按位左移 INTBIGINT shiftright(TINYINT|SMALLINT|INT a, INTb)shiftright(BIGINT a, INT b) Bitwise right shift (as of Hive 1.2.0). Shifts a b positions to the right.Returns int for tinyint, smallint and int a. Returns bigint for bigint a.按拉右移 INTBIGINT shiftrightunsigned(TINYINT|SMALLINT|INTa, INT b),shiftrightunsigned(BIGINT a, INT b) Bitwise unsigned right shift (as of Hive 1.2.0). Shifts a b positions to the right.Returns int for tinyint, smallint and int a. Returns bigint for bigint a.无符号按位右移（&lt;&lt;&lt;） T greatest(T v1, T v2, …) Returns the greatest value of the list of values (as of Hive 1.1.0). Fixed to return NULL when one or more arguments are NULL, and strict type restriction relaxed, consistent with “&gt;” operator (as of Hive 2.0.0). 求最大值 T least(T v1, T v2, …) Returns the least value of the list of values (as of Hive 1.1.0). Fixed to return NULL when one or more arguments are NULL, and strict type restriction relaxed, consistent with “&lt;” operator (as of Hive 2.0.0). 求最小值 集合函数 Return Type Name(Signature) Description int size(Map&lt;K.V&gt;) Returns the number of elements in the map type.求map的长度 int size(Array) Returns the number of elements in the array type.求数组的长度 array map_keys(Map&lt;K.V&gt;) Returns an unordered array containing the keys of the input map.返回map中的所有key array map_values(Map&lt;K.V&gt;) Returns an unordered array containing the values of the input map.返回map中的所有value boolean array_contains(Array, value) Returns TRUE if the array contains value.如该数组Array包含value返回true。，否则返回false array sort_array(Array) Sorts the input array in ascending order according to the natural ordering of the array elements and returns it (as of version 0.9.0).按自然顺序对数组进行排序并返回 类型转换函数 Return Type Name(Signature) Description binary binary(string|binary) Casts the parameter into a binary.将输入的值转换成二进制 Expected “=” to follow “type” cast(expr as ) Converts the results of the expression expr to . For example, cast(‘1’ as BIGINT) will convert the string ‘1’ to its integral representation. A null is returned if the conversion does not succeed. If cast(expr as boolean) Hive returns true for a non-empty string.将expr转换成type类型 如：cast(“1” as BIGINT) 将字符串1转换成了BIGINT类型，如果转换失败将返回NULL 日期函数 Return Type Name(Signature) Description string from_unixtime(bigint unixtime[, string format]) Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the format of “1970-01-01 00:00:00”.将时间的秒值转换成format格式（format可为“yyyy-MM-dd hh:mm:ss”,“yyyy-MM-dd hh”,“yyyy-MM-dd hh:mm”等等）如from_unixtime(1250111000,”yyyy-MM-dd”) 得到2009-03-12 bigint unix_timestamp() Gets current Unix timestamp in seconds.获取本地时区下的时间戳 bigint unix_timestamp(string date) Converts time string in format yyyy-MM-dd HH:mm:ss to Unix timestamp (in seconds), using the default timezone and the default locale, return 0 if fail: unix_timestamp(‘2009-03-20 11:30:01’) = 1237573801将格式为yyyy-MM-dd HH:mm:ss的时间字符串转换成时间戳 如unix_timestamp(‘2009-03-20 11:30:01’) = 1237573801 bigint unix_timestamp(string date, string pattern) Convert time string with given pattern (see [http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html]) to Unix time stamp (in seconds), return 0 if fail: unix_timestamp(‘2009-03-20’, ‘yyyy-MM-dd’) = 1237532400.将指定时间字符串格式字符串转换成Unix时间戳，如果格式不对返回0 如：unix_timestamp(‘2009-03-20’, ‘yyyy-MM-dd’) = 1237532400 string to_date(string timestamp) Returns the date part of a timestamp string: to_date(“1970-01-01 00:00:00”) = “1970-01-01”.返回时间字符串的日期部分 int year(string date) Returns the year part of a date or a timestamp string: year(“1970-01-01 00:00:00”) = 1970, year(“1970-01-01”) = 1970.返回时间字符串的年份部分 int quarter(date/timestamp/string) Returns the quarter of the year for a date, timestamp, or string in the range 1 to 4 (as of Hive 1.3.0). Example: quarter(‘2015-04-08’) = 2.返回当前时间属性哪个季度 如quarter(‘2015-04-08’) = 2 int month(string date) Returns the month part of a date or a timestamp string: month(“1970-11-01 00:00:00”) = 11, month(“1970-11-01”) = 11.返回时间字符串的月份部分 int day(string date) dayofmonth(date) Returns the day part of a date or a timestamp string: day(“1970-11-01 00:00:00”) = 1, day(“1970-11-01”) = 1.返回时间字符串的天 int hour(string date) Returns the hour of the timestamp: hour(‘2009-07-30 12:58:59’) = 12, hour(‘12:58:59’) = 12.返回时间字符串的小时 int minute(string date) Returns the minute of the timestamp.返回时间字符串的分钟 int second(string date) Returns the second of the timestamp.返回时间字符串的秒 int weekofyear(string date) Returns the week number of a timestamp string: weekofyear(“1970-11-01 00:00:00”) = 44, weekofyear(“1970-11-01”) = 44.返回时间字符串位于一年中的第几个周内 如weekofyear(“1970-11-01 00:00:00”) = 44, weekofyear(“1970-11-01”) = 44 int datediff(string enddate, string startdate) Returns the number of days from startdate to enddate: datediff(‘2009-03-01’, ‘2009-02-27’) = 2.计算开始时间startdate到结束时间enddate相差的天数 string date_add(string startdate, int days) Adds a number of days to startdate: date_add(‘2008-12-31’, 1) = ‘2009-01-01’.从开始时间startdate加上days string date_sub(string startdate, int days) Subtracts a number of days to startdate: date_sub(‘2008-12-31’, 1) = ‘2008-12-30’.从开始时间startdate减去days timestamp from_utc_timestamp(timestamp, string timezone) Assumes given timestamp is UTC and converts to given timezone (as of Hive 0.8.0). For example, from_utc_timestamp(‘1970-01-01 08:00:00’,’PST’) returns 1970-01-01 00:00:00.如果给定的时间戳并非UTC，则将其转化成指定的时区下时间戳 timestamp to_utc_timestamp(timestamp, string timezone) Assumes given timestamp is in given timezone and converts to UTC (as of Hive 0.8.0). For example, to_utc_timestamp(‘1970-01-01 00:00:00’,’PST’) returns 1970-01-01 08:00:00.如果给定的时间戳指定的时区下时间戳，则将其转化成UTC下的时间戳 date current_date Returns the current date at the start of query evaluation (as of Hive 1.2.0). All calls of current_date within the same query return the same value.返回当前时间日期 timestamp current_timestamp Returns the current timestamp at the start of query evaluation (as of Hive 1.2.0). All calls of current_timestamp within the same query return the same value.返回当前时间戳 string add_months(string start_date, int num_months) Returns the date that is num_months after start_date (as of Hive 1.1.0). start_date is a string, date or timestamp. num_months is an integer. The time part of start_date is ignored. If start_date is the last day of the month or if the resulting month has fewer days than the day component of start_date, then the result is the last day of the resulting month. Otherwise, the result has the same day component as start_date.返回当前时间下再增加num_months个月的日期 string last_day(string date) Returns the last day of the month which the date belongs to (as of Hive 1.1.0). date is a string in the format ‘yyyy-MM-dd HH:mm:ss’ or ‘yyyy-MM-dd’. The time part of date is ignored.返回这个月的最后一天的日期，忽略时分秒部分（HH:mm:ss） string next_day(string start_date, string day_of_week) Returns the first date which is later than start_date and named as day_of_week (as of Hive1.2.0). start_date is a string/date/timestamp. day_of_week is 2 letters, 3 letters or full name of the day of the week (e.g. Mo, tue, FRIDAY). The time part of start_date is ignored. Example: next_day(‘2015-01-14’, ‘TU’) = 2015-01-20.返回当前时间的下一个星期X所对应的日期 如：next_day(‘2015-01-14’, ‘TU’) = 2015-01-20 以2015-01-14为开始时间，其下一个星期二所对应的日期为2015-01-20 string trunc(string date, string format) Returns date truncated to the unit specified by the format (as of Hive 1.2.0). Supported formats: MONTH/MON/MM, YEAR/YYYY/YY. Example: trunc(‘2015-03-17’, ‘MM’) = 2015-03-01.返回时间的最开始年份或月份 如trunc(“2016-06-26”,“MM”)=2016-06-01 trunc(“2016-06-26”,“YY”)=2016-01-01 注意所支持的格式为MONTH/MON/MM, YEAR/YYYY/YY double months_between(date1, date2) Returns number of months between dates date1 and date2 (as of Hive 1.2.0). If date1 is later than date2, then the result is positive. If date1 is earlier than date2, then the result is negative. If date1 and date2 are either the same days of the month or both last days of months, then the result is always an integer. Otherwise the UDF calculates the fractional portion of the result based on a 31-day month and considers the difference in time components date1 and date2. date1 and date2 type can be date, timestamp or string in the format ‘yyyy-MM-dd’ or ‘yyyy-MM-dd HH:mm:ss’. The result is rounded to 8 decimal places. Example: months_between(‘1997-02-28 10:30:00’, ‘1996-10-30’) = 3.94959677返回date1与date2之间相差的月份，如date1&gt;date2，则返回正，如果date1&lt;date2,则返回负，否则返回0.0 如：months_between(‘1997-02-28 10:30:00’, ‘1996-10-30’) = 3.94959677 1997-02-28 10:30:00与1996-10-30相差3.94959677个月 string date_format(date/timestamp/string ts, string fmt) Converts a date/timestamp/string to a value of string in the format specified by the date format fmt (as of Hive 1.2.0). Supported formats are Java SimpleDateFormat formats –https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html. The second argument fmt should be constant. Example: date_format(‘2015-04-08’, ‘y’) = ‘2015’.date_format can be used to implement other UDFs, e.g.:dayname(date) is date_format(date, ‘EEEE’)dayofyear(date) is date_format(date, ‘D’)按指定格式返回时间date 如：date_format(“2016-06-22”,”MM-dd”)=06-22 条件函数 Return Type Name(Signature) Description T if(boolean testCondition, T valueTrue, T valueFalseOrNull) Returns valueTrue when testCondition is true, returns valueFalseOrNull otherwise.如果testCondition 为true就返回valueTrue,否则返回valueFalseOrNull ，（valueTrue，valueFalseOrNull为泛型） T nvl(T value, T default_value) Returns default value if value is null else returns value (as of HIve 0.11).如果value值为NULL就返回default_value,否则返回value T COALESCE(T v1, T v2, …) Returns the first v that is not NULL, or NULL if all v’s are NULL.返回第一非null的值，如果全部都为NULL就返回NULL 如：COALESCE (NULL,44,55)=44/strong&gt; T CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END When a = b, returns c; when a = d, returns e; else returns f.如果a=b就返回c,a=d就返回e，否则返回f 如CASE 4 WHEN 5 THEN 5 WHEN 4 THEN 4 ELSE 3 END 将返回4 T CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END When a = true, returns b; when c = true, returns d; else returns e.如果a=ture就返回b,c= ture就返回d，否则返回e 如：CASE WHEN 5&gt;0 THEN 5 WHEN 4&gt;0 THEN 4 ELSE 0 END 将返回5；CASE WHEN 5&lt;0 THEN 5 WHEN 4&lt;0 THEN 4 ELSE 0 END 将返回0 boolean isnull( a ) Returns true if a is NULL and false otherwise.如果a为null就返回true，否则返回false boolean isnotnull ( a ) Returns true if a is not NULL and false otherwise.如果a为非null就返回true，否则返回false 字符函数 Return Type Name(Signature) Description int ascii(string str) Returns the numeric value of the first character of str.返回str中首个ASCII字符串的整数值 string base64(binary bin) Converts the argument from binary to a base 64 string (as of Hive 0.12.0)..将二进制bin转换成64位的字符串 string concat(string|binary A, string|binary B…) Returns the string or bytes resulting from concatenating the strings or bytes passed in as parameters in order. For example, concat(‘foo’, ‘bar’) results in ‘foobar’. Note that this function can take any number of input strings..对二进制字节码或字符串按次序进行拼接 array&lt;struct&lt;string,double&gt;&gt; context_ngrams(array&lt;array&gt;, array, int K, int pf) Returns the top-k contextual N-grams from a set of tokenized sentences, given a string of “context”. See StatisticsAndDataMining for more information..与ngram类似，但context_ngram()允许你预算指定上下文(数组)来去查找子序列，具体看StatisticsAndDataMining(这里的解释更易懂) string concat_ws(string SEP, string A, string B…) Like concat() above, but with custom separator SEP..与concat()类似，但使用指定的分隔符喜进行分隔 string concat_ws(string SEP, array) Like concat_ws() above, but taking an array of strings. (as of Hive 0.9.0).拼接Array中的元素并用指定分隔符进行分隔 string decode(binary bin, string charset) Decodes the first argument into a String using the provided character set (one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’). If either argument is null, the result will also be null. (As of Hive 0.12.0.).使用指定的字符集charset将二进制值bin解码成字符串，支持的字符集有：’US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’，如果任意输入参数为NULL都将返回NULL binary encode(string src, string charset) Encodes the first argument into a BINARY using the provided character set (one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’). If either argument is null, the result will also be null. (As of Hive 0.12.0.).使用指定的字符集charset将字符串编码成二进制值，支持的字符集有：’US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’，如果任一输入参数为NULL都将返回NULL int find_in_set(string str, string strList) Returns the first occurance of str in strList where strList is a comma-delimited string. Returns null if either argument is null. Returns 0 if the first argument contains any commas. For example, find_in_set(‘ab’, ‘abc,b,ab,c,def’) returns 3..返回以逗号分隔的字符串中str出现的位置，如果参数str为逗号或查找失败将返回0，如果任一参数为NULL将返回NULL回 string format_number(number x, int d) Formats the number X to a format like ‘#,###,###.##’, rounded to D decimal places, and returns the result as a string. If D is 0, the result has no decimal point or fractional part. (As of Hive 0.10.0; bug with float types fixed in Hive 0.14.0, decimal type support added in Hive 0.14.0).将数值X转换成”#,###,###.##”格式字符串，并保留d位小数，如果d为0，将进行四舍五入且不保留小数 string get_json_object(string json_string, string path) Extracts json object from a json string based on json path specified, and returns json string of the extracted json object. It will return null if the input json string is invalid. NOTE: The json path can only have the characters [0-9a-z_], i.e., no upper-case or special characters. Also, the keys *cannot start with numbers.* This is due to restrictions on Hive column names..从指定路径上的JSON字符串抽取出JSON对象，并返回这个对象的JSON格式，如果输入的JSON是非法的将返回NULL,注意此路径上JSON字符串只能由数字 字母 下划线组成且不能有大写字母和特殊字符，且key不能由数字开头，这是由于Hive对列名的限制 boolean in_file(string str, string filename) Returns true if the string str appears as an entire line in filename..如果文件名为filename的文件中有一行数据与字符串str匹配成功就返回true int instr(string str, string substr) Returns the position of the first occurrence of substr in str. Returns null if either of the arguments are null and returns 0 if substr could not be found in str. Be aware that this is not zero based. The first character in str has index 1..查找字符串str中子字符串substr出现的位置，如果查找失败将返回0，如果任一参数为Null将返回null，注意位置为从1开始的 int length(string A) Returns the length of the string..返回字符串的长度 int locate(string substr, string str[, int pos]) Returns the position of the first occurrence of substr in str after position pos..查找字符串str中的pos位置后字符串substr第一次出现的位置 string lower(string A) lcase(string A) Returns the string resulting from converting all characters of B to lower case. For example, lower(‘fOoBaR’) results in ‘foobar’..将字符串A的所有字母转换成小写字母 string lpad(string str, int len, string pad) Returns str, left-padded with pad to a length of len..从左边开始对字符串str使用字符串pad填充，最终len长度为止，如果字符串str本身长度比len大的话，将去掉多余的部分 string ltrim(string A) Returns the string resulting from trimming spaces from the beginning(left hand side) of A. For example, ltrim(‘ foobar ‘) results in ‘foobar ‘..去掉字符串A前面的空格 array&lt;struct&lt;string,double&gt;&gt; ngrams(array&lt;array&gt;, int N, int K, int pf) Returns the top-k N-grams from a set of tokenized sentences, such as those returned by the sentences() UDAF. See StatisticsAndDataMining for more information..返回出现次数TOP K的的子序列,n表示子序列的长度，具体看StatisticsAndDataMining (这里的解释更易懂) string parse_url(string urlString, string partToExtract [, string keyToExtract]) Returns the specified part from the URL. Valid values for partToExtract include HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO. For example, parse_url(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;, ‘HOST’) returns ‘facebook.com’. Also a value of a particular key in QUERY can be extracted by providing the key as the third argument, for example, parse_url(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;, ‘QUERY’, ‘k1’) returns ‘v1’..返回从URL中抽取指定部分的内容，参数url是URL字符串，而参数partToExtract是要抽取的部分，这个参数包含(HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO,例如：parse_url(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;, ‘HOST’) =’facebook.com’，如果参数partToExtract值为QUERY则必须指定第三个参数key 如：parse_url(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;, ‘QUERY’, ‘k1’) =‘v1’ string printf(String format, Obj… args) Returns the input formatted according do printf-style format strings (as of Hive0.9.0)..按照printf风格格式输出字符串 string regexp_extract(string subject, string pattern, int index) Returns the string extracted using the pattern. For example, regexp_extract(‘foothebar’, ‘foo(.?)(bar)’, 2) returns ‘bar.’ Note that some care is necessary in using predefined character classes: using ‘\\s’ as the second argument will match the letter s; ‘\\s’ is necessary to match whitespace, etc. The ‘index’ parameter is the Java regex Matcher group() method index. See docs/api/java/util/regex/Matcher.html for more information on the ‘index’ or Java regex group() method..*抽取字符串subject中符合正则表达式pattern的第index个部分的子字符串，注意些预定义字符的使用，如第二个参数如果使用’\\s’将被匹配到s,’\\s’才是匹配空格** string regexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT) Returns the string resulting from replacing all substrings in INITIAL_STRING that match the java regular expression syntax defined in PATTERN with instances of REPLACEMENT. For example, regexp_replace(“foobar”, “oo|ar”, “”) returns ‘fb.’ Note that some care is necessary in using predefined character classes: using ‘\\s’ as the second argument will match the letter s; ‘\\s’ is necessary to match whitespace, etc..按照Java正则表达式PATTERN将字符串INTIAL_STRING中符合条件的部分成REPLACEMENT所指定的字符串，如里REPLACEMENT这空的话，抽符合正则的部分将被去掉 如：regexp_replace(“foobar”, “oo|ar”, “”) = ‘fb.’ 注意些预定义字符的使用，如第二个参数如果使用’\\s’将被匹配到s,’\\s’才是匹配空格 string repeat(string str, int n) Repeats str n times..重复输出n次字符串str string reverse(string A) Returns the reversed string..反转字符串 string rpad(string str, int len, string pad) Returns str, right-padded with pad to a length of len..从右边开始对字符串str使用字符串pad填充，最终len长度为止，如果字符串str本身长度比len大的话，将去掉多余的部分 string rtrim(string A) Returns the string resulting from trimming spaces from the end(right hand side) of A. For example, rtrim(‘ foobar ‘) results in ‘ foobar’..去掉字符串后面出现的空格 array&lt;array&gt; sentences(string str, string lang, string locale) Tokenizes a string of natural language text into words and sentences, where each sentence is broken at the appropriate sentence boundary and returned as an array of words. The ‘lang’ and ‘locale’ are optional arguments. For example, sentences(‘Hello there! How are you?’) returns ( (“Hello”, “there”), (“How”, “are”, “you”) )..字符串str将被转换成单词数组，如：sentences(‘Hello there! How are you?’) =( (“Hello”, “there”), (“How”, “are”, “you”) ) string space(int n) Returns a string of n spaces..返回n个空格 array split(string str, string pat) Splits str around pat (pat is a regular expression)..按照正则表达式pat来分割字符串str,并将分割后的数组字符串的形式返回 map&lt;string,string&gt; str_to_map(text[, delimiter1, delimiter2]) Splits text into key-value pairs using two delimiters. Delimiter1 separates text into K-V pairs, and Delimiter2 splits each K-V pair. Default delimiters are ‘,’ for delimiter1 and ‘=’ for delimiter2..将字符串str按照指定分隔符转换成Map，第一个参数是需要转换字符串，第二个参数是键值对之间的分隔符，默认为逗号;第三个参数是键值之间的分隔符，默认为”=” string substr(string|binary A, int start) substring(string|binary A, int start) Returns the substring or slice of the byte array of A starting from start position till the end of string A. For example, substr(‘foobar’, 4) results in ‘bar’ (see [http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_substr])..**对于字符串A,从start位置开始截取字符串并返回** string substr(string|binary A, int start, int len) substring(string|binary A, int start, int len) Returns the substring or slice of the byte array of A starting from start position with length len. For example, substr(‘foobar’, 4, 1) results in ‘b’ (see [http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_substr])..**对于二进制/字符串A,从start位置开始截取长度为length的字符串并返回** string substring_index(string A, string delim, int count) Returns the substring from string A before count occurrences of the delimiter delim (as of Hive 1.3.0). If count is positive, everything to the left of the final delimiter (counting from the left) is returned. If count is negative, everything to the right of the final delimiter (counting from the right) is returned. Substring_index performs a case-sensitive match when searching for delim. Example: substring_index(‘www.apache.org&#39;, ‘.’, 2) = ‘www.apache&#39;..**截取第count分隔符之前的字符串，如count为正则从左边开始截取，如果为负则从右边开始截取** string translate(string|char|varchar input, string|char|varchar from, string|char|varchar to) Translates the input string by replacing the characters present in the from string with the corresponding characters in the to string. This is similar to the translatefunction in PostgreSQL. If any of the parameters to this UDF are NULL, the result is NULL as well. (Available as of Hive 0.10.0, for string types)Char/varchar support added as of Hive 0.14.0..将input出现在from中的字符串替换成to中的字符串 如：translate(“MOBIN”,”BIN”,”M”)=”MOM” string trim(string A) Returns the string resulting from trimming spaces from both ends of A. For example, trim(‘ foobar ‘) results in ‘foobar’.将字符串A前后出现的空格去掉 binary unbase64(string str) Converts the argument from a base 64 string to BINARY. (As of Hive 0.12.0.).将64位的字符串转换二进制值 string upper(string A) ucase(string A) Returns the string resulting from converting all characters of A to upper case. For example, upper(‘fOoBaR’) results in ‘FOOBAR’..将字符串A中的字母转换成大写字母 string initcap(string A) Returns string, with the first letter of each word in uppercase, all other letters in lowercase. Words are delimited by whitespace. (As of Hive 1.1.0.).将字符串A转换第一个字母大写其余字母的字符串 int levenshtein(string A, string B) Returns the Levenshtein distance between two strings (as of Hive 1.2.0). For example, levenshtein(‘kitten’, ‘sitting’) results in 3..计算两个字符串之间的差异大小 如：levenshtein(‘kitten’, ‘sitting’) = 3 string soundex(string A) Returns soundex code of the string (as of Hive 1.2.0). For example, soundex(‘Miller’) results in M460..将普通字符串转换成soundex字符串 聚合函数 Return Type Name(Signature) Description BIGINT count(*), count(expr), count(DISTINCT expr[, expr…]) count() - Returns the total number of retrieved rows, including rows containing NULL values.*统计总行数，包括含有NULL值的行count(expr) - Returns the number of rows for which the supplied expression is non-NULL.统计提供非NULL的expr表达式值的行数count(DISTINCT expr[, expr]) - Returns the number of rows for which the supplied expression(s) are unique and non-NULL. Execution of this can be optimized with hive.optimize.distinct.rewrite.统计提供非NULL且去重后的expr表达式值的行数** DOUBLE sum(col), sum(DISTINCT col) Returns the sum of the elements in the group or the sum of the distinct values of the column in the group.sum(col),表示求指定列的和，sum(DISTINCT col)表示求去重后的列的和 DOUBLE avg(col), avg(DISTINCT col) Returns the average of the elements in the group or the average of the distinct values of the column in the group.avg(col),表示求指定列的平均值，avg(DISTINCT col)表示求去重后的列的平均值 DOUBLE min(col) Returns the minimum of the column in the group.求指定列的最小值 DOUBLE max(col) Returns the maximum value of the column in the group.求指定列的最大值 DOUBLE variance(col), var_pop(col) Returns the variance of a numeric column in the group.求指定列数值的方差 DOUBLE var_samp(col) Returns the unbiased sample variance of a numeric column in the group.求指定列数值的样本方差 DOUBLE stddev_pop(col) Returns the standard deviation of a numeric column in the group.求指定列数值的标准偏差 DOUBLE stddev_samp(col) Returns the unbiased sample standard deviation of a numeric column in the group.求指定列数值的样本标准偏差 DOUBLE covar_pop(col1, col2) Returns the population covariance of a pair of numeric columns in the group.求指定列数值的协方差 DOUBLE covar_samp(col1, col2) Returns the sample covariance of a pair of a numeric columns in the group.求指定列数值的样本协方差 DOUBLE corr(col1, col2) Returns the Pearson coefficient of correlation of a pair of a numeric columns in the group.返回两列数值的相关系数 DOUBLE percentile(BIGINT col, p) Returns the exact pth percentile of a column in the group (does not work with floating point types). p must be between 0 and 1. NOTE: A true percentile can only be computed for integer values. Use PERCENTILE_APPROX if your input is non-integral.返回col的p%分位数 表生成函数 Return Type Name(Signature) Description Array Type explode(array&lt;TYPE&gt; a) For each element in a, generates a row containing that element.对于a中的每个元素，将生成一行且包含该元素 N rows explode(ARRAY) Returns one row for each element from the array..每行对应数组中的一个元素 N rows explode(MAP) Returns one row for each key-value pair from the input map with two columns in each row: one for the key and another for the value. (As of Hive 0.8.0.).每行对应每个map键-值，其中一个字段是map的键，另一个字段是map的值 N rows posexplode(ARRAY) Behaves like explode for arrays, but includes the position of items in the original array by returning a tuple of (pos, value). (As of Hive 0.13.0.).与explode类似，不同的是还返回各元素在数组中的位置 N rows stack(INT n, v_1, v_2, …, v_k) Breaks up v_1, …, v_k into n rows. Each row will have k/n columns. n must be constant..把M列转换成N行，每行有M/N个字段，其中n必须是个常数 tuple json_tuple(jsonStr, k1, k2, …) Takes a set of names (keys) and a JSON string, and returns a tuple of values. This is a more efficient version of the get_json_object UDF because it can get multiple keys with just one call..从一个JSON字符串中获取多个键并作为一个元组返回，与get_json_object不同的是此函数能一次获取多个键值 tuple parse_url_tuple(url, p1, p2, …) This is similar to the parse_url() UDF but can extract multiple parts at once out of a URL. Valid part names are: HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, USERINFO, QUERY:..返回从URL中抽取指定N部分的内容，参数url是URL字符串，而参数p1,p2,….是要抽取的部分，这个参数包含HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, USERINFO, QUERY: inline(ARRAY&lt;STRUCT[,STRUCT]&gt;) Explodes an array of structs into a table. (As of Hive 0.10.).将结构体数组提取出来并插入到表中","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （八）Hive中文乱码","slug":"2019-04-08-Hive学习之路 （八）Hive中文乱码","date":"2019-04-08T02:30:04.000Z","updated":"2019-09-18T15:18:37.925Z","comments":true,"path":"2019-04-08-Hive学习之路 （八）Hive中文乱码.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-08-Hive学习之路 （八）Hive中文乱码.html","excerpt":"** Hive学习之路 （八）Hive中文乱码：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （八）Hive中文乱码","text":"** Hive学习之路 （八）Hive中文乱码：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （八）Hive中文乱码 &lt;The rest of contents | 余下全文&gt; Hive注释中文乱码创建表的时候，comment说明字段包含中文，表成功创建成功之后，中文说明显示乱码 1234567891011121314create external table movie(userID int comment '用户ID',movieID int comment '电影ID',rating int comment '电影评分',timestamped bigint comment '评分时间戳',movieName string comment '电影名字', movieType string comment '电影类型', sex string comment '性别', age int comment '年龄', occupation string comment '职业', zipcode string comment '邮政编码') comment '影评三表合一' row format delimited fields terminated by \",\"location '/hive/movie'; 这是因为在MySQL中的元数据出现乱码 针对元数据库metastore中的表,分区,视图的编码设置因为我们知道 metastore 支持数据库级别，表级别的字符集是 latin1 那么我们只需要把相应注释的地方的字符集由 latin1 改成 utf-8，就可以了。用到注释的就三个地方，表、分区、视图。如下修改分为两个步骤： 1、进入数据库 Metastore 中执行以下 5 条 SQL 语句（1）修改表字段注解和表注解12alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8；alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8； （2）修改分区字段注解12alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 ;alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8; （3）修改索引注解1alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8; 2、修改 metastore 的连接 URL 修改hive-site.xml配置文件 12345&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://IP:3306/db_name?createDatabaseIfNotExist=true&amp;amp;useUnicode=true&amp;characterEncoding=UTF-8&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt; 验证做完可以解决乱码问题","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （七）Hive的DDL操作","slug":"2019-04-07-Hive学习之路 （七）Hive的DDL操作","date":"2019-04-07T02:30:04.000Z","updated":"2019-09-18T15:18:33.956Z","comments":true,"path":"2019-04-07-Hive学习之路 （七）Hive的DDL操作.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-07-Hive学习之路 （七）Hive的DDL操作.html","excerpt":"** Hive学习之路 （七）Hive的DDL操作：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （七）Hive的DDL操作","text":"** Hive学习之路 （七）Hive的DDL操作：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （七）Hive的DDL操作 &lt;The rest of contents | 余下全文&gt; 库操作1、创建库语法结构 CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] //关于数据块的描述 [LOCATION hdfs_path] //指定数据库在HDFS上的存储位置 [WITH DBPROPERTIES (property_name=property_value, …)]; //指定数据块属性 默认地址：/user/hive/warehouse/db_name.db/table_name/partition_name/… 创建库的方式（1）创建普通的数据库 1234567891011120: jdbc:hive2://hadoop3:10000&gt; create database t1;No rows affected (0.308 seconds)0: jdbc:hive2://hadoop3:10000&gt; show databases;+----------------+| database_name |+----------------+| default || myhive || t1 |+----------------+3 rows selected (0.393 seconds)0: jdbc:hive2://hadoop3:10000&gt; （2）创建库的时候检查存与否1230: jdbc:hive2://hadoop3:10000&gt; create database if not exists t1;No rows affected (0.176 seconds)0: jdbc:hive2://hadoop3:10000&gt; （3）创建库的时候带注释1230: jdbc:hive2://hadoop3:10000&gt; create database if not exists t2 comment 'learning hive';No rows affected (0.217 seconds)0: jdbc:hive2://hadoop3:10000&gt; （4）创建带属性的库1230: jdbc:hive2://hadoop3:10000&gt; create database if not exists t3 with dbproperties('creator'='hadoop','date'='2018-04-05');No rows affected (0.255 seconds)0: jdbc:hive2://hadoop3:10000&gt; 2、查看库查看库的方式（1）查看有哪些数据库1234567891011120: jdbc:hive2://hadoop3:10000&gt; show databases;+----------------+| database_name |+----------------+| default || myhive || t1 || t2 || t3 |+----------------+5 rows selected (0.164 seconds)0: jdbc:hive2://hadoop3:10000&gt; （2）显示数据库的详细属性信息语法 desc database [extended] dbname; 示例 123456780: jdbc:hive2://hadoop3:10000&gt; desc database extended t3;+----------+----------+------------------------------------------+-------------+-------------+------------------------------------+| db_name | comment | location | owner_name | owner_type | parameters |+----------+----------+------------------------------------------+-------------+-------------+------------------------------------+| t3 | | hdfs://myha01/user/hive/warehouse/t3.db | hadoop | USER | &#123;date=2018-04-05, creator=hadoop&#125; |+----------+----------+------------------------------------------+-------------+-------------+------------------------------------+1 row selected (0.11 seconds)0: jdbc:hive2://hadoop3:10000&gt; （3）查看正在使用哪个库123456780: jdbc:hive2://hadoop3:10000&gt; select current_database();+----------+| _c0 |+----------+| default |+----------+1 row selected (1.36 seconds)0: jdbc:hive2://hadoop3:10000&gt; （4）查看创建库的详细语句123456789101112130: jdbc:hive2://hadoop3:10000&gt; show create database t3;+----------------------------------------------+| createdb_stmt |+----------------------------------------------+| CREATE DATABASE `t3` || LOCATION || 'hdfs://myha01/user/hive/warehouse/t3.db' || WITH DBPROPERTIES ( || 'creator'='hadoop', || 'date'='2018-04-05') |+----------------------------------------------+6 rows selected (0.155 seconds)0: jdbc:hive2://hadoop3:10000&gt; 3、删除库说明删除库操作 12drop database dbname;drop database if exists dbname; 默认情况下，hive 不允许删除包含表的数据库，有两种解决办法： 1、 手动删除库下所有表，然后删除库 2、 使用 cascade 关键字 drop database if exists dbname cascade; 默认情况下就是 restrict drop database if exists myhive ==== drop database if exists myhive restrict 示例（1）删除不含表的数据库123456789101112131415161718190: jdbc:hive2://hadoop3:10000&gt; show tables in t1;+-----------+| tab_name |+-----------++-----------+No rows selected (0.147 seconds)0: jdbc:hive2://hadoop3:10000&gt; drop database t1;No rows affected (0.178 seconds)0: jdbc:hive2://hadoop3:10000&gt; show databases;+----------------+| database_name |+----------------+| default || myhive || t2 || t3 |+----------------+4 rows selected (0.124 seconds)0: jdbc:hive2://hadoop3:10000&gt; （2）删除含有表的数据库1230: jdbc:hive2://hadoop3:10000&gt; drop database if exists t3 cascade;No rows affected (1.56 seconds)0: jdbc:hive2://hadoop3:10000&gt; 4、切换库语法 use database_name 示例1230: jdbc:hive2://hadoop3:10000&gt; use t2;No rows affected (0.109 seconds)0: jdbc:hive2://hadoop3:10000&gt; 表操作1、创建表语法 CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], …)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], …)] [CLUSTERED BY (col_name, col_name, …) [SORTED BY (col_name [ASC|DESC], …)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path] 详情请参见： https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualD DL-CreateTable 123456•CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXIST 选项来忽略这个异常•EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION）•LIKE 允许用户复制现有的表结构，但是不复制数据•COMMENT可以为表与字段增加描述•PARTITIONED BY 指定分区•ROW FORMAT DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] 用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive 通过 SerDe 确定表的具体的列的数据。 •STORED AS SEQUENCEFILE //序列化文件 | TEXTFILE //普通的文本文件格式 | RCFILE //行列存储相结合的文件 | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname //自定义文件格式 如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCE 。•LOCATION指定表在HDFS的存储路径 最佳实践： 如果一份数据已经存储在HDFS上，并且要被多个用户或者客户端使用，最好创建外部表 反之，最好创建内部表。 如果不指定，就按照默认的规则存储在默认的仓库路径中。 示例使用t2数据库进行操作 （1）创建默认的内部表12345678910111213140: jdbc:hive2://hadoop3:10000&gt; create table student(id int, name string, sex string, age int,department string) row format delimited fields terminated by \",\";No rows affected (0.222 seconds)0: jdbc:hive2://hadoop3:10000&gt; desc student;+-------------+------------+----------+| col_name | data_type | comment |+-------------+------------+----------+| id | int | || name | string | || sex | string | || age | int | || department | string | |+-------------+------------+----------+5 rows selected (0.168 seconds)0: jdbc:hive2://hadoop3:10000&gt; （2）外部表1230: jdbc:hive2://hadoop3:10000&gt; create external table student_ext(id int, name string, sex string, age int,department string) row format delimited fields terminated by &quot;,&quot; location &quot;/hive/student&quot;;No rows affected (0.248 seconds)0: jdbc:hive2://hadoop3:10000&gt; （3）分区表1234560: jdbc:hive2://hadoop3:10000&gt; create external table student_ptn(id int, name string, sex string, age int,department string). . . . . . . . . . . . . . .&gt; partitioned by (city string). . . . . . . . . . . . . . .&gt; row format delimited fields terminated by \",\". . . . . . . . . . . . . . .&gt; location \"/hive/student_ptn\";No rows affected (0.24 seconds)0: jdbc:hive2://hadoop3:10000&gt; 添加分区 123450: jdbc:hive2://hadoop3:10000&gt; alter table student_ptn add partition(city=&quot;beijing&quot;);No rows affected (0.269 seconds)0: jdbc:hive2://hadoop3:10000&gt; alter table student_ptn add partition(city=&quot;shenzhen&quot;);No rows affected (0.236 seconds)0: jdbc:hive2://hadoop3:10000&gt; 如果某张表是分区表。那么每个分区的定义，其实就表现为了这张表的数据存储目录下的一个子目录如果是分区表。那么数据文件一定要存储在某个分区中，而不能直接存储在表中。 （4）分桶表1234560: jdbc:hive2://hadoop3:10000&gt; create external table student_bck(id int, name string, sex string, age int,department string). . . . . . . . . . . . . . .&gt; clustered by (id) sorted by (id asc, name desc) into 4 buckets. . . . . . . . . . . . . . .&gt; row format delimited fields terminated by &quot;,&quot;. . . . . . . . . . . . . . .&gt; location &quot;/hive/student_bck&quot;;No rows affected (0.216 seconds)0: jdbc:hive2://hadoop3:10000&gt; （5）使用CTAS创建表作用： 就是从一个查询SQL的结果来创建一个表进行存储 现象student表中导入数据 1234567891011121314151617181920212223242526272829300: jdbc:hive2://hadoop3:10000&gt; load data local inpath &quot;/home/hadoop/student.txt&quot; into table student;No rows affected (0.715 seconds)0: jdbc:hive2://hadoop3:10000&gt; select * from student;+-------------+---------------+--------------+--------------+---------------------+| student.id | student.name | student.sex | student.age | student.department |+-------------+---------------+--------------+--------------+---------------------+| 95002 | 刘晨 | 女 | 19 | IS || 95017 | 王风娟 | 女 | 18 | IS || 95018 | 王一 | 女 | 19 | IS || 95013 | 冯伟 | 男 | 21 | CS || 95014 | 王小丽 | 女 | 19 | CS || 95019 | 邢小丽 | 女 | 19 | IS || 95020 | 赵钱 | 男 | 21 | IS || 95003 | 王敏 | 女 | 22 | MA || 95004 | 张立 | 男 | 19 | IS || 95012 | 孙花 | 女 | 20 | CS || 95010 | 孔小涛 | 男 | 19 | CS || 95005 | 刘刚 | 男 | 18 | MA || 95006 | 孙庆 | 男 | 23 | CS || 95007 | 易思玲 | 女 | 19 | MA || 95008 | 李娜 | 女 | 18 | CS || 95021 | 周二 | 男 | 17 | MA || 95022 | 郑明 | 男 | 20 | MA || 95001 | 李勇 | 男 | 20 | CS || 95011 | 包小柏 | 男 | 18 | MA || 95009 | 梦圆圆 | 女 | 18 | MA || 95015 | 王君 | 男 | 18 | MA |+-------------+---------------+--------------+--------------+---------------------+21 rows selected (0.342 seconds)0: jdbc:hive2://hadoop3:10000&gt; 使用CTAS创建表 123456789101112131415161718192021220: jdbc:hive2://hadoop3:10000&gt; create table student_ctas as select * from student where id &lt; 95012;WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.No rows affected (34.514 seconds)0: jdbc:hive2://hadoop3:10000&gt; select * from student_ctas. . . . . . . . . . . . . . .&gt; ;+------------------+--------------------+-------------------+-------------------+--------------------------+| student_ctas.id | student_ctas.name | student_ctas.sex | student_ctas.age | student_ctas.department |+------------------+--------------------+-------------------+-------------------+--------------------------+| 95002 | 刘晨 | 女 | 19 | IS || 95003 | 王敏 | 女 | 22 | MA || 95004 | 张立 | 男 | 19 | IS || 95010 | 孔小涛 | 男 | 19 | CS || 95005 | 刘刚 | 男 | 18 | MA || 95006 | 孙庆 | 男 | 23 | CS || 95007 | 易思玲 | 女 | 19 | MA || 95008 | 李娜 | 女 | 18 | CS || 95001 | 李勇 | 男 | 20 | CS || 95011 | 包小柏 | 男 | 18 | MA || 95009 | 梦圆圆 | 女 | 18 | MA |+------------------+--------------------+-------------------+-------------------+--------------------------+11 rows selected (0.445 seconds)0: jdbc:hive2://hadoop3:10000&gt; （6）复制表结构1230: jdbc:hive2://hadoop3:10000&gt; create table student_copy like student;No rows affected (0.217 seconds)0: jdbc:hive2://hadoop3:10000&gt; 注意： 如果在table的前面没有加external关键字，那么复制出来的新表。无论如何都是内部表如果在table的前面有加external关键字，那么复制出来的新表。无论如何都是外部表 2、查看表（1）查看表列表查看当前使用的数据库中有哪些表123456789101112130: jdbc:hive2://hadoop3:10000&gt; show tables;+---------------+| tab_name |+---------------+| student || student_bck || student_copy || student_ctas || student_ext || student_ptn |+---------------+6 rows selected (0.163 seconds)0: jdbc:hive2://hadoop3:10000&gt; 查看非当前使用的数据库中有哪些表123456780: jdbc:hive2://hadoop3:10000&gt; show tables in myhive;+-----------+| tab_name |+-----------+| student |+-----------+1 row selected (0.144 seconds)0: jdbc:hive2://hadoop3:10000&gt; 查看数据库中以xxx开头的表1234567890: jdbc:hive2://hadoop3:10000&gt; show tables like 'student_c*';+---------------+| tab_name |+---------------+| student_copy || student_ctas |+---------------+2 rows selected (0.13 seconds)0: jdbc:hive2://hadoop3:10000&gt; （2）查看表的详细信息查看表的信息1234567891011120: jdbc:hive2://hadoop3:10000&gt; desc student;+-------------+------------+----------+| col_name | data_type | comment |+-------------+------------+----------+| id | int | || name | string | || sex | string | || age | int | || department | string | |+-------------+------------+----------+5 rows selected (0.149 seconds)0: jdbc:hive2://hadoop3:10000&gt; 查看表的详细信息（格式不友好）10: jdbc:hive2://hadoop3:10000&gt; desc extended student; 查看表的详细信息（格式友好）10: jdbc:hive2://hadoop3:10000&gt; desc formatted student; 查看分区信息10: jdbc:hive2://hadoop3:10000&gt; show partitions student_ptn; （3）查看表的详细建表语句10: jdbc:hive2://hadoop3:10000&gt; show create table student_ptn; 3、修改表（1）修改表名10: jdbc:hive2://hadoop3:10000&gt; alter table student rename to new_student; （2）修改字段定义A. 增加一个字段10: jdbc:hive2://hadoop3:10000&gt; alter table new_student add columns (score int); B. 修改一个字段的定义10: jdbc:hive2://hadoop3:10000&gt; alter table new_student change name new_name string; C. 删除一个字段不支持 D. 替换所有字段10: jdbc:hive2://hadoop3:10000&gt; alter table new_student replace columns (id int, name string, address string); （3）修改分区信息A. 添加分区静态分区 添加一个 10: jdbc:hive2://hadoop3:10000&gt; alter table student_ptn add partition(city=&quot;chongqing&quot;); 添加多个 10: jdbc:hive2://hadoop3:10000&gt; alter table student_ptn add partition(city=&quot;chongqing2&quot;) partition(city=&quot;chongqing3&quot;) partition(city=&quot;chongqing4&quot;); 动态分区 先向student_ptn表中插入数据，数据格式如下图 10: jdbc:hive2://hadoop3:10000&gt; load data local inpath &quot;/home/hadoop/student.txt&quot; into table student_ptn partition(city=&quot;beijing&quot;); 现在我把这张表的内容直接插入到另一张表student_ptn_age中，并实现sex为动态分区（不指定到底是哪中性别，让系统自己分配决定） 首先创建student_ptn_age并指定分区为age 10: jdbc:hive2://hadoop3:10000&gt; create table student_ptn_age(id int,name string,sex string,department string) partitioned by (age int); 从student_ptn表中查询数据并插入student_ptn_age表中 123450: jdbc:hive2://hadoop3:10000&gt; insert overwrite table student_ptn_age partition(age). . . . . . . . . . . . . . .&gt; select id,name,sex,department，age from student_ptn;WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.No rows affected (27.905 seconds)0: jdbc:hive2://hadoop3:10000&gt; B. 修改分区修改分区，一般来说，都是指修改分区的数据存储目录 在添加分区的时候，直接指定当前分区的数据存储目录 12340: jdbc:hive2://hadoop3:10000&gt; alter table student_ptn add if not exists partition(city=&apos;beijing&apos;) . . . . . . . . . . . . . . .&gt; location &apos;/student_ptn_beijing&apos; partition(city=&apos;cc&apos;) location &apos;/student_cc&apos;;No rows affected (0.306 seconds)0: jdbc:hive2://hadoop3:10000&gt; 修改已经指定好的分区的数据存储目录 10: jdbc:hive2://hadoop3:10000&gt; alter table student_ptn partition (city=&apos;beijing&apos;) set location &apos;/student_ptn_beijing&apos;; 此时原先的分区文件夹仍存在，但是在往分区添加数据时，只会添加到新的分区目录 C. 删除分区10: jdbc:hive2://hadoop3:10000&gt; alter table student_ptn drop partition (city=&apos;beijing&apos;); 4、删除表10: jdbc:hive2://hadoop3:10000&gt; drop table new_student; 5、清空表10: jdbc:hive2://hadoop3:10000&gt; truncate table student_ptn; 其他辅助命令","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （六）Hive SQL之数据类型和存储格式","slug":"2019-04-06-Hive学习之路 （六）Hive SQL之数据类型和存储格式","date":"2019-04-06T02:30:04.000Z","updated":"2019-09-18T15:22:11.211Z","comments":true,"path":"2019-04-06-Hive学习之路 （六）Hive SQL之数据类型和存储格式.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-06-Hive学习之路 （六）Hive SQL之数据类型和存储格式.html","excerpt":"** Hive学习之路 （六）Hive SQL之数据类型和存储格式：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （六）Hive SQL之数据类型和存储格式","text":"** Hive学习之路 （六）Hive SQL之数据类型和存储格式：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （六）Hive SQL之数据类型和存储格式 &lt;The rest of contents | 余下全文&gt; 一、数据类型1、基本数据类型Hive 支持关系型数据中大多数基本数据类型 类型 描述 示例 boolean true/false TRUE tinyint 1字节的有符号整数 -128~127 1Y smallint 2个字节的有符号整数，-32768~32767 1S int 4个字节的带符号整数 1 bigint 8字节带符号整数 1L float 4字节单精度浮点数 1.0 double 8字节双精度浮点数 1.0 deicimal 任意精度的带符号小数 1.0 String 字符串，变长 “a”,’b’ varchar 变长字符串 “a”,’b’ char 固定长度字符串 “a”,’b’ binary 字节数组 无法表示 timestamp 时间戳，纳秒精度 122327493795 date 日期 ‘2018-04-07’ 和其他的SQL语言一样，这些都是保留字。需要注意的是所有的这些数据类型都是对Java中接口的实现，因此这些类型的具体行为细节和Java中对应的类型是完全一致的。例如，string类型实现的是Java中的String，float实现的是Java中的float，等等。 2、复杂类型 类型 描述 示例 array 有序的的同类型的集合 array(1,2) map key-value,key必须为原始类型，value可以任意类型 map(‘a’,1,’b’,2) struct 字段集合,类型可以不同 struct(‘1’,1,1.0), named_stract(‘col1’,’1’,’col2’,1,’clo3’,1.0) 二、存储格式Hive会为每个创建的数据库在HDFS上创建一个目录，该数据库的表会以子目录形式存储，表中的数据会以表目录下的文件形式存储。对于default数据库，默认的缺省数据库没有自己的目录，default数据库下的表默认存放在/user/hive/warehouse目录下。 （1）textfiletextfile为默认格式，存储方式为行存储。数据不做压缩，磁盘开销大，数据解析开销大。 （2）SequenceFileSequenceFile是Hadoop API提供的一种二进制文件支持，其具有使用方便、可分割、可压缩的特点。 SequenceFile支持三种压缩选择：NONE, RECORD, BLOCK。 Record压缩率低，一般建议使用BLOCK压缩。 （3）RCFile一种行列存储相结合的存储方式。 （4）ORCFile数据按照行分块，每个块按照列存储，其中每个块都存储有一个索引。hive给出的新格式，属于RCFILE的升级版,性能有大幅度提升,而且数据可以压缩存储,压缩快 快速列存取。 （5）ParquetParquet也是一种行式存储，同时具有很好的压缩性能；同时可以减少大量的表扫描和反序列化的时间。 三、数据格式当数据存储在文本文件中，必须按照一定格式区别行和列，并且在Hive中指明这些区分符。Hive默认使用了几个平时很少出现的字符，这些字符一般不会作为内容出现在记录中。 Hive默认的行和列分隔符如下表所示。 分隔符 描述 \\n 对于文本文件来说，每行是一条记录，所以\\n 来分割记录 ^A (Ctrl+A) 分割字段，也可以用\\001 来表示 ^B (Ctrl+B) 用于分割 Arrary 或者 Struct 中的元素，或者用于 map 中键值之间的分割，也可以用\\002 分割。 ^C 用于 map 中键和值自己分割，也可以用\\003 表示。","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （五）DbVisualizer配置连接hive","slug":"2019-04-05-Hive学习之路 （五）DbVisualizer配置连接hive","date":"2019-04-05T02:30:04.000Z","updated":"2019-09-18T15:18:22.038Z","comments":true,"path":"2019-04-05-Hive学习之路 （五）DbVisualizer配置连接hive.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-05-Hive学习之路 （五）DbVisualizer配置连接hive.html","excerpt":"** Hive学习之路 （五）DbVisualizer配置连接hive：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （五）DbVisualizer配置连接hive","text":"** Hive学习之路 （五）DbVisualizer配置连接hive：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （五）DbVisualizer配置连接hive &lt;The rest of contents | 余下全文&gt; 一、安装DbVisualizer下载地址http://www.dbvis.com/ 也可以从网上下载破解版程序，此处使用的版本是DbVisualizer 9.1.1 具体的安装步骤可以百度，或是修改安装目录之后默认安装就可以 二、配置DbVisualizer里的hive jdbc1、在DbVisualizer的安装目录jdbc文件夹下新建hive文件夹D:\\Program Files\\DbVisualizer\\jdbc 2、拷贝Hadoop的相关jar包放入新建的hive文件夹里面jar包位置: (1) hadoop-2.7.5/share/hadoop/common/hadoop-common-2.7.5.jar把图中红框中的jar包拷贝到新建的hive文件夹里面 (2) hadoop-2.7.5/share/hadoop/common/lib/把图中涉及到的jar包拷贝到新建的hive文件夹里面 3、拷贝Hive的相关jar包放入新建的hive文件夹里面jar包位置: (1) apache-hive-2.3.3-bin/jdbc/lib把图中涉及到的jar包拷贝到新建的hive文件夹里面 4、结果 5、在tools/Driver manager中进行配置 打开DbVisualizer，此时会进行加载刚添加的jar包 6、在Tool–Driver manager中进行配置 点击左上角的添加 完成之后关闭窗口 点击添加连接数据库 选择驱动 点击完成","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （四）Hive的连接3种连接方式","slug":"2019-04-04-Hive学习之路 （四）Hive的连接3种连接方式","date":"2019-04-04T02:30:04.000Z","updated":"2019-09-18T15:18:17.717Z","comments":true,"path":"2019-04-04-Hive学习之路 （四）Hive的连接3种连接方式.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-04-Hive学习之路 （四）Hive的连接3种连接方式.html","excerpt":"** Hive学习之路 （四）Hive的连接3种连接方式：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （四）Hive的连接3种连接方式","text":"** Hive学习之路 （四）Hive的连接3种连接方式：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （四）Hive的连接3种连接方式 &lt;The rest of contents | 余下全文&gt; 一、CLI连接进入到 bin 目录下，直接输入命令： [hadoop@hadoop3 ~]$ hiveSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] Logging initialized using configuration in jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/hive-common-2.3.3.jar!/hive-log4j2.properties Async: trueHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.hive&gt; show databases;OKdefaultmyhiveTime taken: 6.569 seconds, Fetched: 2 row(s)hive&gt; 启动成功的话如上图所示，接下来便可以做 hive 相关操作 补充： 1、上面的 hive 命令相当于在启动的时候执行：hive –service cli 2、使用 hive –help，可以查看 hive 命令可以启动那些服务 3、通过 hive –service serviceName –help 可以查看某个具体命令的使用方式 二、HiveServer2/beeline在现在使用的最新的 hive-2.3.3 版本中：都需要对 hadoop 集群做如下改变，否则无法使用 1、修改 hadoop 集群的 hdfs-site.xml 配置文件加入一条配置信息，表示启用 webhdfs 1234&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 2、修改 hadoop 集群的 core-site.xml 配置文件加入两条配置信息：表示设置 hadoop 的代理用户 12345678&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 配置解析： hadoop.proxyuser.hadoop.hosts 配置成*的意义，表示任意节点使用 hadoop 集群的代理用户 hadoop 都能访问 hdfs 集群，hadoop.proxyuser.hadoop.groups 表示代理用户的组所属 以上操作做好了之后（最好重启一下HDFS集群），请继续做如下两步： 第一步：先启动 hiveserver2 服务 启动方式，（假如是在 hadoop3 上）： 启动为前台：hiveserver2 1234567[hadoop@hadoop3 ~]$ hiveserver22018-04-04 10:21:49: Starting HiveServer2SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] 启动会多一个进程 启动为后台： 123nohup hiveserver2 1&gt;/home/hadoop/hiveserver.log 2&gt;/home/hadoop/hiveserver.err &amp;或者：nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp;或者：nohup hiveserver2 &gt;/dev/null 2&gt;&amp;1 &amp; 以上 3 个命令是等价的，第一个表示记录日志，第二个和第三个表示不记录日志 命令中的 1 和 2 的意义分别是： 1：表示标准日志输出 2：表示错误日志输出 如果我没有配置日志的输出路径，日志会生成在当前工作目录，默认的日志名称叫做： nohup.xxx 123[hadoop@hadoop3 ~]$ nohup hiveserver2 1&gt;/home/hadoop/log/hivelog/hiveserver.log 2&gt;/home/hadoop/log/hivelog/hiveserver.err &amp;[1] 4352[hadoop@hadoop3 ~]$ PS：nohup 命令：如果你正在运行一个进程，而且你觉得在退出帐户时该进程还不会结束， 那么可以使用 nohup 命令。该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。 nohup 就是不挂起的意思(no hang up)。 该命令的一般形式为：nohup command &amp; 第二步：然后启动 beeline 客户端去连接： 执行命令： 1234567891011[hadoop@hadoop3 ~]$ beeline -u jdbc:hive2//hadoop3:10000 -n hadoopSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]scan complete in 1msscan complete in 2374msNo known driver to handle &quot;jdbc:hive2//hadoop3:10000&quot;Beeline version 2.3.3 by Apache Hivebeeline&gt; -u : 指定元数据库的链接信息 -n : 指定用户名和密码 另外还有一种方式也可以去连接： 先执行 beeline 然后按图所示输入：!connect jdbc:hive2://hadoop02:10000 按回车，然后输入用户名，这个 用户名就是安装 hadoop 集群的用户名 123456789101112131415[hadoop@hadoop3 ~]$ beelineSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Beeline version 2.3.3 by Apache Hivebeeline&gt; !connect jdbc:hive2://hadoop3:10000Connecting to jdbc:hive2://hadoop3:10000Enter username for jdbc:hive2://hadoop3:10000: hadoopEnter password for jdbc:hive2://hadoop3:10000: ******Connected to: Apache Hive (version 2.3.3)Driver: Hive JDBC (version 2.3.3)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://hadoop3:10000&gt; 接下来便可以做 hive 操作 三、Web UI 1、 下载对应版本的 src 包：apache-hive-2.3.2-src.tar.gz 2、 上传，解压 tar -zxvf apache-hive-2.3.2-src.tar.gz 3、 然后进入目录${HIVE_SRC_HOME}/hwi/web，执行打包命令: jar -cvf hive-hwi-2.3.2.war * 在当前目录会生成一个 hive-hwi-2.3.2.war 4、 得到 hive-hwi-2.3.2.war 文件，复制到 hive 下的 lib 目录中 cp hive-hwi-2.3.2.war ${HIVE_HOME}/lib/ 5、 修改配置文件 hive-site.xml 123456789101112131415&lt;property&gt; &lt;name&gt;hive.hwi.listen.host&lt;/name&gt; &lt;value&gt;0.0.0.0&lt;/value&gt; &lt;description&gt;监听的地址&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.hwi.listen.port&lt;/name&gt; &lt;value&gt;9999&lt;/value&gt; &lt;description&gt;监听的端口号&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.hwi.war.file&lt;/name&gt; &lt;value&gt;lib/hive-hwi-2.3.2.war&lt;/value&gt; &lt;description&gt;war 包所在的地址&lt;/description&gt;&lt;/property&gt; 6、 复制所需 jar 包 1、cp ${JAVA_HOME}/lib/tools.jar ${HIVE_HOME}/lib 2、再寻找三个 jar 包，都放入${HIVE_HOME}/lib 目录： commons-el-1.0.jar jasper-compiler-5.5.23.jar jasper-runtime-5.5.23.jar 不然启动 hwi 服务的时候会报错。 7、 安装 ant 1、 上传 ant 包：apache-ant-1.9.4-bin.tar.gz 2、 解压 tar -zxvf apache-ant-1.9.4-bin.tar.gz -C ~/apps/ 3、 配置环境变量 vi /etc/profile 在最后增加两行： export ANT_HOME=/home/hadoop/apps/apache-ant-1.9.4 export PATH=$PATH:$ANT_HOME/bin 配置完环境变量别忘记执行：source /etc/profile 4、 验证是否安装成功 8、上面的步骤都配置完，基本就大功告成了。进入${HIVE_HOME}/bin 目录： ${HIVE_HOME}/bin/hive –service hwi 或者让在后台运行： nohup bin/hive –service hwi &gt; /dev/null 2&gt; /dev/null &amp; 9、 前面配置了端口号为 9999，所以这里直接在浏览器中输入: hadoop02:9999/hwi 10、至此大功告成","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （三）Hive元数据信息对应MySQL数据库表","slug":"2019-04-03-Hive学习之路 （三）Hive元数据信息对应MySQL数据库表","date":"2019-04-03T02:30:04.000Z","updated":"2019-09-18T15:18:13.072Z","comments":true,"path":"2019-04-03-Hive学习之路 （三）Hive元数据信息对应MySQL数据库表.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-03-Hive学习之路 （三）Hive元数据信息对应MySQL数据库表.html","excerpt":"** Hive学习之路 （三）Hive元数据信息对应MySQL数据库表：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （三）Hive元数据信息对应MySQL数据库表","text":"** Hive学习之路 （三）Hive元数据信息对应MySQL数据库表：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （三）Hive元数据信息对应MySQL数据库表 &lt;The rest of contents | 余下全文&gt; 概述Hive 的元数据信息通常存储在关系型数据库中，常用MySQL数据库作为元数据库管理。上一篇hive的安装也是将元数据信息存放在MySQL数据库中。 Hive的元数据信息在MySQL数据中有57张表 一、存储Hive版本的元数据表（VERSION） VERSION – 查询版本信息 该表比较简单，但很重要。 VER_ID SCHEMA_VERSION VERSION_COMMENT ID主键 Hive版本 版本说明 1 0.13.0 Set by MetaStore 如果该表出现问题，根本进入不了Hive-Cli。 比如该表不存在，当启动Hive-Cli时候，就会报错”Table ‘hive.version’ doesn’t exist”。 二、Hive数据库相关的元数据表（DBS、DATABASE_PARAMS）1、DBSDBS – 存储Hive中所有数据库的基本信息 元数据表字段 说明 示例数据 DB_ID 数据库ID 2 DESC 数据库描述 测试库 DB_LOCATION_URI 数据库HDFS路径 hdfs://namenode/user/hive/warehouse/lxw1234.db NAME 数据库名 lxw1234 OWNER_NAME 数据库所有者用户名 lxw1234 OWNER_TYPE 所有者角色 USER 2、DATABASE_PARAMSDATABASE_PARAMS –该表存储数据库的相关参数，在CREATE DATABASE时候用 WITH DBPROPERTIES (property_name=property_value, …)指定的参数。 元数据表字段 说明 示例数据 DB_ID 数据库ID 2 PARAM_KEY 参数名 createdby PARAM_VALUE 参数值 lxw1234 注意： DBS和DATABASE_PARAMS这两张表通过DB_ID字段关联。 三、Hive表和视图相关的元数据表主要有TBLS、TABLE_PARAMS、TBL_PRIVS，这三张表通过TBL_ID关联。 1、TBLS 该表中存储Hive表、视图、索引表的基本信息。 元数据表字段 说明 示例数据 TBL_ID 表ID 1 CREATE_TIME 创建时间 1436317071 DB_ID 数据库ID 2，对应DBS中的DB_ID LAST_ACCESS_TIME 上次访问时间 1436317071 OWNER 所有者 liuxiaowen RETENTION 保留字段 0 SD_ID 序列化配置信息 86，对应SDS表中的SD_ID TBL_NAME 表名 lxw1234 TBL_TYPE 表类型 MANAGED_TABLE、EXTERNAL_TABLE、INDEX_TABLE、VIRTUAL_VIEW VIEW_EXPANDED_TEXT 视图的详细HQL语句 select lxw1234.pt, lxw1234.pcid from liuxiaowen.lxw1234 VIEW_ORIGINAL_TEXT 视图的原始HQL语句 select * from lxw1234 2、TABLE_PARAMS该表存储表/视图的属性信息。 元数据表字段 说明 示例数据 TBL_ID 表ID 1 PARAM_KEY 属性名 totalSize、numRows、EXTERNAL PARAM_VALUE 属性值 970107336、21231028、TRUE 3、TBL_PRIVS 该表存储表/视图的授权信息 元数据表字段 说明 示例数据 TBL_GRANT_ID 授权ID 1 CREATE_TIME 授权时间 1436320455 GRANT_OPTION 0 GRANTOR 授权执行用户 liuxiaowen GRANTOR_TYPE 授权者类型 USER PRINCIPAL_NAME 被授权用户 username PRINCIPAL_TYPE 被授权用户类型 USER TBL_PRIV 权限 Select、Alter TBL_ID 表ID 22，对应TBLS表中的TBL_ID 四、Hive文件存储信息相关的元数据表 主要涉及SDS、SD_PARAMS、SERDES、SERDE_PARAMS 由于HDFS支持的文件格式很多，而建Hive表时候也可以指定各种文件格式，Hive在将HQL解析成MapReduce时候，需要知道去哪里，使用哪种格式去读写HDFS文件，而这些信息就保存在这几张表中。 1、SDS 该表保存文件存储的基本信息，如INPUT_FORMAT、OUTPUT_FORMAT、是否压缩等。 TBLS表中的SD_ID与该表关联，可以获取Hive表的存储信息。 元数据表字段 说明 示例数据 SD_ID 存储信息ID 1 CD_ID 字段信息ID 21，对应CDS表 INPUT_FORMAT 文件输入格式 org.apache.hadoop.mapred.TextInputFormat IS_COMPRESSED 是否压缩 0 IS_STOREDASSUBDIRECTORIES 是否以子目录存储 0 LOCATION HDFS路径 hdfs://namenode/hivedata/warehouse/ut.db/t_lxw NUM_BUCKETS 分桶数量 5 OUTPUT_FORMAT 文件输出格式 org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat SERDE_ID 序列化类ID 3，对应SERDES表 2、SD_PARAMS 该表存储Hive存储的属性信息，在创建表时候使用 STORED BY ‘storage.handler.class.name’ [WITH SERDEPROPERTIES (…)指定。 元数据表字段 说明 示例数据 SD_ID 存储配置ID 1 PARAM_KEY 存储属性名 PARAM_VALUE 存储属性值 3、SERDES 该表存储序列化使用的类信息 元数据表字段 说明 示例数据 SERDE_ID 序列化类配置ID 1 NAME 序列化类别名 SLIB 序列化类 org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe 4、SERDE_PARAMS 该表存储序列化的一些属性、格式信息,比如：行、列分隔符 元数据表字段 说明 示例数据 SERDE_ID 序列化类配置ID 1 PARAM_KEY 属性名 field.delim PARAM_VALUE 属性值 , 五、Hive表字段相关的元数据表主要涉及COLUMNS_V2 1、COLUMNS_V2该表存储表对应的字段信息。 元数据表字段 说明 示例数据 CD_ID 字段信息ID 1 COMMENT 字段注释 COLUMN_NAME 字段名 pt TYPE_NAME 字段类型 string INTEGER_IDX 字段顺序 2 六、Hive表分区相关的元数据表主要涉及PARTITIONS、PARTITION_KEYS、PARTITION_KEY_VALS、PARTITION_PARAMS 1、PARTITIONS 该表存储表分区的基本信息。 元数据表字段 说明 示例数据 PART_ID 分区ID 1 CREATE_TIME 分区创建时间 LAST_ACCESS_TIME 最后一次访问时间 PART_NAME 分区名 pt=2015-06-12 SD_ID 分区存储ID 21 TBL_ID 表ID 2 2、PARTITION_KEYS该表存储分区的字段信息。 元数据表字段 说明 示例数据 TBL_ID 表ID 2 PKEY_COMMENT 分区字段说明 PKEY_NAME 分区字段名 pt PKEY_TYPE 分区字段类型 string INTEGER_IDX 分区字段顺序 1 3、PARTITION_KEY_VALS该表存储分区字段值。 元数据表字段 说明 示例数据 PART_ID 分区ID 2 PART_KEY_VAL 分区字段值 2015-06-12 INTEGER_IDX 分区字段值顺序 0 4、PARTITION_PARAMS该表存储分区的属性信息。 元数据表字段 说明 示例数据 PART_ID 分区ID 2 PARAM_KEY 分区属性名 numFiles、numRows PARAM_VALUE 分区属性值 15、502195 七、其他不常用的元数据表 DB_PRIVS 数据库权限信息表。通过GRANT语句对数据库授权后，将会在这里存储。 IDXS 索引表，存储Hive索引相关的元数据 INDEX_PARAMS 索引相关的属性信息。 TAB_COL_STATS 表字段的统计信息。使用ANALYZE语句对表字段分析后记录在这里。 TBL_COL_PRIVS 表字段的授权信息 PART_PRIVS 分区的授权信息 PART_COL_STATS 分区字段的统计信息。 PART_COL_PRIVS 分区字段的权限信息。 FUNCS 用户注册的函数信息 FUNC_RU 用户注册函数的资源信息","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （二）Hive安装","slug":"2019-04-02-Hive学习之路 （二）Hive安装","date":"2019-04-02T02:30:04.000Z","updated":"2019-09-18T15:18:09.261Z","comments":true,"path":"2019-04-02-Hive学习之路 （二）Hive安装.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-02-Hive学习之路 （二）Hive安装.html","excerpt":"** Hive学习之路 （二）Hive安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （二）Hive安装","text":"** Hive学习之路 （二）Hive安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （二）Hive安装 &lt;The rest of contents | 余下全文&gt; Hive的下载下载地址http://mirrors.hust.edu.cn/apache/ 选择合适的Hive版本进行下载，进到stable-2文件夹可以看到稳定的2.x的版本是2.3.3 Hive的安装1、本人使用MySQL做为Hive的元数据库，所以先安装MySQL。MySql安装过程http://www.cnblogs.com/qingyunzong/p/8294876.html 2、上传Hive安装包 3、解压安装包1[hadoop@hadoop3 ~]$ tar -zxvf apache-hive-2.3.3-bin.tar.gz -C apps/ 4、修改配置文件配置文件所在目录apache-hive-2.3.3-bin/conf 12345678910111213[hadoop@hadoop3 apps]$ cd apache-hive-2.3.3-bin/[hadoop@hadoop3 apache-hive-2.3.3-bin]$ lsbin binary-package-licenses conf examples hcatalog jdbc lib LICENSE NOTICE RELEASE_NOTES.txt scripts[hadoop@hadoop3 apache-hive-2.3.3-bin]$ cd conf/[hadoop@hadoop3 conf]$ lsbeeline-log4j2.properties.template ivysettings.xmlhive-default.xml.template llap-cli-log4j2.properties.templatehive-env.sh.template llap-daemon-log4j2.properties.templatehive-exec-log4j2.properties.template parquet-logging.propertieshive-log4j2.properties.template[hadoop@hadoop3 conf]$ pwd/home/hadoop/apps/apache-hive-2.3.3-bin/conf[hadoop@hadoop3 conf]$ 新建hive-site.xml并添加以下内容 12[hadoop@hadoop3 conf]$ touch hive-site.xml[hadoop@hadoop3 conf]$ vi hive-site.xml 1234567891011121314151617181920212223&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop1:3306/hivedb?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;!-- 如果 mysql 和 hive 在同一个服务器节点，那么请更改 hadoop02 为 localhost --&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 以下可选配置，该配置信息用来指定 Hive 数据仓库的数据存储在 HDFS 上的目录 12345&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/hive/warehouse&lt;/value&gt; &lt;description&gt;hive default warehouse, if nessecory, change it&lt;/description&gt;&lt;/property&gt; 5、 一定要记得加入 MySQL 驱动包（mysql-connector-java-5.1.40-bin.jar）该 jar 包放置在 hive 的根路径下的 lib 目录 6、 安装完成，配置环境变量1234[hadoop@hadoop3 lib]$ vi ~/.bashrc #Hiveexport HIVE_HOME=/home/hadoop/apps/apache-hive-2.3.3-binexport PATH=$PATH:$HIVE_HOME/bin 使修改的配置文件立即生效 1[hadoop@hadoop3 lib]$ source ~/.bashrc 7、 验证 Hive 安装1234567891011121314[hadoop@hadoop3 ~]$ hive --helpUsage ./hive &lt;parameters&gt; --service serviceName &lt;service parameters&gt;Service List: beeline cleardanglingscratchdir cli hbaseimport hbaseschematool help hiveburninclient hiveserver2 hplsql jar lineage llapdump llap llapstatus metastore metatool orcfiledump rcfilecat schemaTool version Parameters parsed: --auxpath : Auxiliary jars --config : Hive configuration directory --service : Starts specific service/component. cli is defaultParameters used: HADOOP_HOME or HADOOP_PREFIX : Hadoop install directory HIVE_OPT : Hive optionsFor help on a particular service: ./hive --service serviceName --helpDebug help: ./hive --debug --help[hadoop@hadoop3 ~]$ 8、 初始化元数据库 注意：当使用的 hive 是 2.x 之前的版本，不做初始化也是 OK 的，当 hive 第一次启动的 时候会自动进行初始化，只不过会不会生成足够多的元数据库中的表。在使用过程中会 慢慢生成。但最后进行初始化。如果使用的 2.x 版本的 Hive，那么就必须手动初始化元 数据库。使用命令： 1234567891011121314[hadoop@hadoop3 ~]$ schematool -dbType mysql -initSchemaSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Metastore connection URL: jdbc:mysql://hadoop1:3306/hivedb?createDatabaseIfNotExist=trueMetastore Connection Driver : com.mysql.jdbc.DriverMetastore connection User: rootStarting metastore schema initialization to 2.3.0Initialization script hive-schema-2.3.0.mysql.sqlInitialization script completedschemaTool completed[hadoop@hadoop3 ~]$ 9、 启动 Hive 客户端hive –service cli和hive效果一样 12345678910[hadoop@hadoop3 ~]$ hive --service cliSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Logging initialized using configuration in jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/hive-common-2.3.3.jar!/hive-log4j2.properties Async: trueHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.hive&gt; 基本使用现有一个文件student.txt，将其存入hive中，student.txt数据格式如下： 12345678910111213141516171819202195002,刘晨,女,19,IS95017,王风娟,女,18,IS95018,王一,女,19,IS95013,冯伟,男,21,CS95014,王小丽,女,19,CS95019,邢小丽,女,19,IS95020,赵钱,男,21,IS95003,王敏,女,22,MA95004,张立,男,19,IS95012,孙花,女,20,CS95010,孔小涛,男,19,CS95005,刘刚,男,18,MA95006,孙庆,男,23,CS95007,易思玲,女,19,MA95008,李娜,女,18,CS95021,周二,男,17,MA95022,郑明,男,20,MA95001,李勇,男,20,CS95011,包小柏,男,18,MA95009,梦圆圆,女,18,MA95015,王君,男,18,MA 1、创建一个数据库myhive1234hive&gt; create database myhive;OKTime taken: 7.847 secondshive&gt; 2、使用新的数据库myhive1234hive&gt; use myhive;OKTime taken: 0.047 secondshive&gt; 3、查看当前正在使用的数据库12345hive&gt; select current_database();OKmyhiveTime taken: 0.728 seconds, Fetched: 1 row(s)hive&gt; 4、在数据库myhive创建一张student表1234hive&gt; create table student(id int, name string, sex string, age int, department string) row format delimited fields terminated by \",\";OKTime taken: 0.718 secondshive&gt; 5、往表中加载数据12345hive&gt; load data local inpath \"/home/hadoop/student.txt\" into table student;Loading data to table myhive.studentOKTime taken: 1.854 secondshive&gt; 6、查询数据12345678910111213141516171819202122232425hive&gt; select * from student;OK95002 刘晨 女 19 IS95017 王风娟 女 18 IS95018 王一 女 19 IS95013 冯伟 男 21 CS95014 王小丽 女 19 CS95019 邢小丽 女 19 IS95020 赵钱 男 21 IS95003 王敏 女 22 MA95004 张立 男 19 IS95012 孙花 女 20 CS95010 孔小涛 男 19 CS95005 刘刚 男 18 MA95006 孙庆 男 23 CS95007 易思玲 女 19 MA95008 李娜 女 18 CS95021 周二 男 17 MA95022 郑明 男 20 MA95001 李勇 男 20 CS95011 包小柏 男 18 MA95009 梦圆圆 女 18 MA95015 王君 男 18 MATime taken: 2.455 seconds, Fetched: 21 row(s)hive&gt; 7、查看表结构123456789hive&gt; desc student;OKid int name string sex string age int department string Time taken: 0.102 seconds, Fetched: 5 row(s)hive&gt; 1234567891011hive&gt; desc extended student;OKid int name string sex string age int department string Detailed Table Information Table(tableName:student, dbName:myhive, owner:hadoop, createTime:1522750487, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:null), FieldSchema(name:name, type:string, comment:null), FieldSchema(name:sex, type:string, comment:null), FieldSchema(name:age, type:int, comment:null), FieldSchema(name:department, type:string, comment:null)], location:hdfs://myha01/user/hive/warehouse/myhive.db/student, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:&#123;serialization.format=,, field.delim=,&#125;), bucketCols:[], sortCols:[], parameters:&#123;&#125;, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:&#123;&#125;), storedAsSubDirectories:false), partitionKeys:[], parameters:&#123;transient_lastDdlTime=1522750695, totalSize=523, numRows=0, rawDataSize=0, numFiles=1&#125;, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, rewriteEnabled:false) Time taken: 0.127 seconds, Fetched: 7 row(s)hive&gt; 1234567891011121314151617181920212223242526272829303132333435363738hive&gt; desc formatted student;OK# col_name data_type comment id int name string sex string age int department string # Detailed Table Information Database: myhive Owner: hadoop CreateTime: Tue Apr 03 18:14:47 CST 2018 LastAccessTime: UNKNOWN Retention: 0 Location: hdfs://myha01/user/hive/warehouse/myhive.db/student Table Type: MANAGED_TABLE Table Parameters: numFiles 1 numRows 0 rawDataSize 0 totalSize 523 transient_lastDdlTime 1522750695 # Storage Information SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: field.delim , serialization.format , Time taken: 0.13 seconds, Fetched: 34 row(s)hive&gt;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hive学习之路 （一）Hive初识","slug":"2019-04-01-Hive学习之路 （一）Hive初识","date":"2019-04-01T02:30:04.000Z","updated":"2019-09-18T15:18:05.203Z","comments":true,"path":"2019-04-01-Hive学习之路 （一）Hive初识.html","link":"","permalink":"http://zhangfuxin.cn/2019-04-01-Hive学习之路 （一）Hive初识.html","excerpt":"** Hive学习之路 （一）Hive初识：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （一）Hive初识","text":"** Hive学习之路 （一）Hive初识：** &lt;Excerpt in index | 首页摘要&gt; ​ Hive学习之路 （一）Hive初识 &lt;The rest of contents | 余下全文&gt; Hive 简介什么是Hive1、Hive 由 Facebook 实现并开源 2、是基于 Hadoop 的一个数据仓库工具 3、可以将结构化的数据映射为一张数据库表 4、并提供 HQL(Hive SQL)查询功能 5、底层数据是存储在 HDFS 上 6、Hive的本质是将 SQL 语句转换为 MapReduce 任务运行 7、使不熟悉 MapReduce 的用户很方便地利用 HQL 处理和计算 HDFS 上的结构化的数据，适用于离线的批量数据计算。 数据仓库之父比尔·恩门（Bill Inmon）在 1991 年出版的“Building the Data Warehouse”（《建 立数据仓库》）一书中所提出的定义被广泛接受——数据仓库（Data Warehouse）是一个面 向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反映历史 变化（Time Variant）的数据集合，用于支持管理决策(Decision Making Support)。 Hive 依赖于 HDFS 存储数据，Hive 将 HQL 转换成 MapReduce 执行，所以说 Hive 是基于 Hadoop 的一个数据仓库工具，实质就是一款基于 HDFS 的 MapReduce 计算框架，对存储在 HDFS 中的数据进行分析和管理 为什么使用 Hive直接使用 MapReduce 所面临的问题： 1、人员学习成本太高 2、项目周期要求太短 3、MapReduce实现复杂查询逻辑开发难度太大 为什么要使用 Hive： 1、更友好的接口：操作接口采用类 SQL 的语法，提供快速开发的能力 2、更低的学习成本：避免了写 MapReduce，减少开发人员的学习成本 3、更好的扩展性：可自由扩展集群规模而无需重启服务，还支持用户自定义函数 Hive 特点优点： 1、可扩展性,横向扩展，Hive 可以自由的扩展集群的规模，一般情况下不需要重启服务 横向扩展：通过分担压力的方式扩展集群的规模 纵向扩展：一台服务器cpu i7-6700k 4核心8线程，8核心16线程，内存64G =&gt; 128G 2、延展性，Hive 支持自定义函数，用户可以根据自己的需求来实现自己的函数 3、良好的容错性，可以保障即使有节点出现问题，SQL 语句仍可完成执行 缺点： 1、Hive 不支持记录级别的增删改操作，但是用户可以通过查询生成新表或者将查询结 果导入到文件中（当前选择的 hive-2.3.2 的版本支持记录级别的插入操作） 2、Hive 的查询延时很严重，因为 MapReduce Job 的启动过程消耗很长时间，所以不能 用在交互查询系统中。 3、Hive 不支持事务（因为不没有增删改，所以主要用来做 OLAP（联机分析处理），而 不是 OLTP（联机事务处理），这就是数据处理的两大级别）。 Hive 和 RDBMS 的对比 总结： Hive 具有 SQL 数据库的外表，但应用场景完全不同，Hive 只适合用来做海量离线数 据统计分析，也就是数据仓库。 Hive的架构 从上图看出hive的内部架构由四部分组成： 1、用户接口: shell/CLI, jdbc/odbc, webui Command Line Interface CLI，Shell 终端命令行（Command Line Interface），采用交互形式使用 Hive 命令行与 Hive 进行交互，最常用（学习，调试，生产） JDBC/ODBC，是 Hive 的基于 JDBC 操作提供的客户端，用户（开发员，运维人员）通过 这连接至 Hive server 服务 Web UI，通过浏览器访问 Hive 2、跨语言服务 ： thrift server 提供了一种能力，让用户可以使用多种不同的语言来操纵hive Thrift 是 Facebook 开发的一个软件框架，可以用来进行可扩展且跨语言的服务的开发， Hive 集成了该服务，能让不同的编程语言调用 Hive 的接口 3、底层的Driver： 驱动器Driver，编译器Compiler，优化器Optimizer，执行器Executor Driver 组件完成 HQL 查询语句从词法分析，语法分析，编译，优化，以及生成逻辑执行 计划的生成。生成的逻辑执行计划存储在 HDFS 中，并随后由 MapReduce 调用执行 Hive 的核心是驱动引擎， 驱动引擎由四部分组成： (1) 解释器：解释器的作用是将 HiveSQL 语句转换为抽象语法树（AST） (2) 编译器：编译器是将语法树编译为逻辑执行计划 (3) 优化器：优化器是对逻辑执行计划进行优化 (4) 执行器：执行器是调用底层的运行框架执行逻辑执行计划 4、元数据存储系统 ： RDBMS MySQL 元数据，通俗的讲，就是存储在 Hive 中的数据的描述信息。 Hive 中的元数据通常包括：表的名字，表的列和分区及其属性，表的属性（内部表和 外部表），表的数据所在目录 Metastore 默认存在自带的 Derby 数据库中。缺点就是不适合多用户操作，并且数据存 储目录不固定。数据库跟着 Hive 走，极度不方便管理 解决方案：通常存我们自己创建的 MySQL 库（本地 或 远程） Hive 和 MySQL 之间通过 MetaStore 服务交互 执行流程 HiveQL 通过命令行或者客户端提交，经过 Compiler 编译器，运用 MetaStore 中的元数 据进行类型检测和语法分析，生成一个逻辑方案(Logical Plan)，然后通过的优化处理，产生 一个 MapReduce 任务。 Hive的数据组织1、Hive 的存储结构包括数据库、表、视图、分区和表数据等。数据库，表，分区等等都对 应 HDFS 上的一个目录。表数据对应 HDFS 对应目录下的文件。 2、Hive 中所有的数据都存储在 HDFS 中，没有专门的数据存储格式，因为 Hive 是读模式 （Schema On Read），可支持 TextFile，SequenceFile，RCFile 或者自定义格式等 3、 只需要在创建表的时候告诉 Hive 数据中的列分隔符和行分隔符，Hive 就可以解析数据 Hive 的默认列分隔符：控制符 Ctrl + A，\\x01 Hive 的 Hive 的默认行分隔符：换行符 \\n 4、Hive 中包含以下数据模型： database：在 HDFS 中表现为${hive.metastore.warehouse.dir}目录下一个文件夹 table：在 HDFS 中表现所属 database 目录下一个文件夹 external table：与 table 类似，不过其数据存放位置可以指定任意 HDFS 目录路径 partition：在 HDFS 中表现为 table 目录下的子目录 bucket：在 HDFS 中表现为同一个表目录或者分区目录下根据某个字段的值进行 hash 散 列之后的多个文件 view：与传统数据库类似，只读，基于基本表创建 5、Hive 的元数据存储在 RDBMS 中，除元数据外的其它所有数据都基于 HDFS 存储。默认情 况下，Hive 元数据保存在内嵌的 Derby 数据库中，只能允许一个会话连接，只适合简单的 测试。实际生产环境中不适用，为了支持多用户会话，则需要一个独立的元数据库，使用 MySQL 作为元数据库，Hive 内部对 MySQL 提供了很好的支持。 6、Hive 中的表分为内部表、外部表、分区表和 Bucket 表 内部表和外部表的区别： 删除内部表，删除表元数据和数据 删除外部表，删除元数据，不删除数据 内部表和外部表的使用选择： 大多数情况，他们的区别不明显，如果数据的所有处理都在 Hive 中进行，那么倾向于 选择内部表，但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。 使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中 使用外部表的场景是针对一个数据集有多个不同的 Schema 通过外部表和内部表的区别和使用选择的对比可以看出来，hive 其实仅仅只是对存储在 HDFS 上的数据提供了一种新的抽象。而不是管理存储在 HDFS 上的数据。所以不管创建内部 表还是外部表，都可以对 hive 表的数据存储目录中的数据进行增删操作。 分区表和分桶表的区别： Hive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。同 时表和分区也可以进一步被划分为 Buckets，分桶表的原理和 MapReduce 编程中的 HashPartitioner 的原理类似。 分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于 Hive 是读模式，所 以对添加进分区的数据不做模式校验，分桶表中的数据是按照某些分桶字段进行 hash 散列 形成的多个文件，所以数据的准确性也高很多","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://zhangfuxin.cn/tags/Hive/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"一文读懂 Spark","slug":"2018-10-30-spark-一文读懂spark","date":"2018-10-30T04:30:04.000Z","updated":"2019-09-15T23:44:33.871Z","comments":true,"path":"2018-10-30-spark-一文读懂spark.html","link":"","permalink":"http://zhangfuxin.cn/2018-10-30-spark-一文读懂spark.html","excerpt":"** 一文读懂 Spark：** &lt;Excerpt in index | 首页摘要&gt; ​ Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。","text":"** 一文读懂 Spark：** &lt;Excerpt in index | 首页摘要&gt; ​ Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。 &lt;The rest of contents | 余下全文&gt; 1.1 前言​ Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。 ​ Apache Spark 诞生于大名鼎鼎的 AMPLab（这里还诞生过 Mesos 和 Alluxio），从创立之初就带有浓厚的学术气质，其设计目标是为各种大数据处理需求提供一个统一的技术栈。如今 Spark 背后的商业公司 Databricks 创始人也是来自 AMPLab 的博士毕业生。 ​ Spark 本身使用 Scala 语言编写，Scala 是一门融合了面向对象与函数式的“双范式”语言，运行在 JVM 之上。Spark 大量使用了它的函数式、即时代码生成等特性。Spark 目前提供了 Java、Scala、Python、R 四种语言的 API，前两者因为同样运行在 JVM 上可以达到更原生的支持。 MapReduce 的问题所在 ​ Hadoop 是大数据处理领域的开创者。严格来说，Hadoop 不只是一个软件，而是一整套生态系统，例如 MapReduce 负责进行分布式计算，而 HDFS 负责存储大量文件。 ​ MapReduce 模型的诞生是大数据处理从无到有的飞跃。但随着技术的进步，对大数据处理的需求也变得越来越复杂，MapReduce 的问题也日渐凸显。通常，我们将 MapReduce 的输入和输出数据保留在 HDFS 上，很多时候，复杂的 ETL、数据清洗等工作无法用一次 MapReduce 完成，所以需要将多个 MapReduce 过程连接起来： ▲ 上图中只有两个 MapReduce 串联，实际上可能有几十个甚至更多，依赖关系也更复杂。 这种方式下，每次中间结果都要写入 HDFS 落盘保存，代价很大（别忘了，HDFS 的每份数据都需要冗余若干份拷贝）。另外，由于本质上是多次 MapReduce 任务，调度也比较麻烦，实时性无从谈起。 Spark 与 RDD 模型针对上面的问题，如果能把中间结果保存在内存里，岂不是快的多？之所以不能这么做，最大的障碍是：分布式系统必须能容忍一定的故障，所谓 fault-tolerance。如果只是放在内存中，一旦某个计算节点宕机，其他节点无法恢复出丢失的数据，只能重启整个计算任务，这对于动辄成百上千节点的集群来说是不可接受的。 一般来说，想做到 fault-tolerance 只有两个方案：要么存储到外部（例如 HDFS），要么拷贝到多个副本。Spark 大胆地提出了第三种——重算一遍。但是之所以能做到这一点，是依赖于一个额外的假设：所有计算过程都是确定性的（deterministic）。Spark 借鉴了函数式编程思想，提出了 RDD（Resilient Distributed Datasets），译作“弹性分布式数据集”。 RDD 是一个只读的、分区的（partitioned）数据集合。RDD 要么来源于不可变的外部文件（例如 HDFS 上的文件），要么由确定的算子由其他 RDD 计算得到。RDD 通过算子连接构成有向无环图（DAG），上图演示了一个简单的例子，其中节点对应 RDD，边对应算子。 回到刚刚的问题，RDD 如何做到 fault-tolerance？很简单，RDD 中的每个分区都能被确定性的计算出来，所以一旦某个分区丢失了，另一个计算节点可以从它的前继节点出发、用同样的计算过程重算一次，即可得到完全一样的 RDD 分区。这个过程可以递归的进行下去。 ▲ 上图演示了 RDD 分区的恢复。为了简洁并没有画出分区，实际上恢复是以分区为单位的。 Spark 的编程接口和 Java 8 的 Stream 很相似：RDD 作为数据，在多种算子间变换，构成对执行计划 DAG 的描述。最后，一旦遇到类似 collect()这样的输出命令，执行计划会被发往 Spark 集群、开始计算。不难发现，算子分成两类： map()、filter()、join() 等算子称为 Transformation，它们输入一个或多个 RDD，输出一个 RDD。 collect()、count()、save() 等算子称为 Action，它们通常是将数据收集起来返回； ▲ 上图的例子用来收集包含“HDFS”关键字的错误日志时间戳。当执行到 collect() 时，右边的执行计划开始运行。 像之前提到的，RDD 的数据由多个分区（partition）构成，这些分区可以分布在集群的各个机器上，这也就是 RDD 中 “distributed” 的含义。熟悉 DBMS 的同学可以把 RDD 理解为逻辑执行计划，partition 理解为物理执行计划。 此外，RDD 还包含它的每个分区的依赖分区（dependency），以及一个函数指出如何计算出本分区的数据。Spark 的设计者发现，依赖关系依据执行方式的不同可以很自然地分成两种：窄依赖（Narrow Dependency）和宽依赖（Wide Dependency），举例来说： map()、filter() 等算子构成窄依赖：生产的每个分区只依赖父 RDD 中的一个分区。 groupByKey() 等算子构成宽依赖：生成的每个分区依赖父 RDD 中的多个分区（往往是全部分区）。 ▲ 左图展示了宽依赖和窄依赖，其中 Join 算子因为 Join key 分区情况不同二者皆有；右图展示了执行过程，由于宽依赖的存在，执行计划被分成 3 个阶段。 在执行时，窄依赖可以很容易的按流水线（pipeline）的方式计算：对于每个分区从前到后依次代入各个算子即可。然而，宽依赖需要等待前继 RDD 中所有分区计算完成；换句话说，宽依赖就像一个栅栏（barrier）会阻塞到之前的所有计算完成。整个计算过程被宽依赖分割成多个阶段（stage），如上右图所示。 了解 MapReduce 的同学可能已经发现，宽依赖本质上就是一个 MapReduce 过程。但是相比 MapReduce 自己写 Map 和 Reduce 函数的编程接口，Spark 的接口要容易的多；并且在 Spark 中，多个阶段的 MapReduce 只需要构造一个 DAG 即可。 声明式接口：Spark SQL命令式编程中，你需要编写一个程序。下面给出了一种伪代码实现： employees = db.getAllEmployees() countByDept = dict() // 统计各部门女生人数 (dept_id -&gt; count) for employee in employees: if (employee.gender == ‘female’) countByDept[employee.dept_id] += 1 results = list() // 加上 dept.name 列 depts = db.getAllDepartments() for dept in depts: if (countByDept containsKey dept.id) results.add(row(dept.id, dept.name, countByDept[dept.id])) return results; 声明式编程中，你只要用关系代数的运算表达出结果： employees.join(dept, employees.deptId == dept.id) .where(employees.gender == ‘female’) .groupBy(dept.id, dept.name) .agg() 等价地，如果你更熟悉 SQL，也可以写成这样： SELECTdept.id,dept.name,COUNT(*)FROMemployees JOINdept ONemployees.dept_id ==dept.idWHEREemployees.gender =’female’GROUPBYdept.id,dept.name 显然，声明式的要简洁的多！但声明式编程依赖于执行者产生真正的程序代码，所以除了上面这段程序，还需要把数据模型（即 schema）一并告知执行者。声明式编程最广为人知的形式就是 SQL。 Spark SQL 就是这样一个基于 SQL 的声明式编程接口。你可以将它看作在 Spark 之上的一层封装，在 RDD 计算模型的基础上，提供了 DataFrame API 以及一个内置的 SQL 执行计划优化器 Catalyst。 ▲ 上图黄色部分是 Spark SQL 中新增的部分。 DataFrame 就像数据库中的表，除了数据之外它还保存了数据的 schema 信息。计算中，schema 信息也会经过算子进行相应的变换。DataFrame 的数据是行（row）对象组成的 RDD，对 DataFrame 的操作最终会变成对底层 RDD 的操作。 Catalyst 是一个内置的 SQL 优化器，负责把用户输入的 SQL 转化成执行计划。Catelyst 强大之处是它利用了 Scala 提供的代码生成（codegen）机制，物理执行计划经过编译，产出的执行代码效率很高，和直接操作 RDD 的命令式代码几乎没有分别。 ▲ 上图是 Catalyst 的工作流程，与大多数 SQL 优化器一样是一个 Cost-Based Optimizer (CBO)，但最后使用代码生成（codegen）转化成直接对 RDD 的操作。 流计算框架：Spark Streaming以往，批处理和流计算被看作大数据系统的两个方面。我们常常能看到这样的架构——以 Kafka、Storm 为代表的流计算框架用于实时计算，而 Spark 或 MapReduce 则负责每天、每小时的数据批处理。在 ETL 等场合，这样的设计常常导致同样的计算逻辑被实现两次，耗费人力不说，保证一致性也是个问题。 Spark Streaming 正是诞生于此类需求。传统的流计算框架大多注重于低延迟，采用了持续的（continuous）算子模型；而 Spark Streaming 基于 Spark，另辟蹊径提出了 D-Stream（Discretized Streams）方案：将流数据切成很小的批（micro-batch），用一系列的短暂、无状态、确定性的批处理实现流处理。 Spark Streaming 的做法在流计算框架中很有创新性，它虽然牺牲了低延迟（一般流计算能做到 100ms 级别，Spark Streaming 延迟一般为 1s 左右），但是带来了三个诱人的优势： 更高的吞吐量（大约是 Storm 的 2-5 倍） 更快速的失败恢复（通常只要 1-2s），因此对于 straggler（性能拖后腿的节点）直接杀掉即可 开发者只需要维护一套 ETL 逻辑即可同时用于批处理和流计算 ▲ 上左图中，为了在持续算子模型的流计算系统中保证一致性，不得不在主备机之间使用同步机制，导致性能损失，Spark Streaming 完全没有这个问题；右图是 D-Stream 的原理示意图。 你可能会困惑，流计算中的状态一直是个难题。但我们刚刚提到 D-Stream 方案是无状态的，那诸如 word count 之类的问题，怎么做到保持 count 算子的状态呢？ 答案是通过 RDD：将前一个时间步的 RDD 作为当前时间步的 RDD 的前继节点，就能造成状态不断更替的效果。实际上，新的状态 RDD 总是不断生成，而旧的 RDD 并不会被“替代”，而是作为新 RDD 的前继依赖。对于底层的 Spark 框架来说，并没有时间步的概念，有的只是不断扩张的 DAG 图和新的 RDD 节点。 ▲ 上图是流式计算 word count 的例子，count 结果在不同时间步中不断累积。 那么另一个问题也随之而来：随着时间的推进，上图中的状态 RDD counts会越来越多，他的祖先（lineage）变得越来越长，极端情况下，恢复过程可能溯源到很久之前。这是不可接受的！因此，Spark Streming 会定期地对状态 RDD 做 checkpoint，将其持久化到 HDFS 等存储中，这被称为 lineage cut，在它之前更早的 RDD 就可以没有顾虑地清理掉了。 关于流行的几个开源流计算框架的对比，可以参考文章 Comparison of Apache Stream Processing Frameworks。 流计算与 SQL：Spark Structured Streaming 出人意料的是，Spark Structured Streaming 的流式计算引擎并没有复用 Spark Streaming，而是在 Spark SQL 上设计了新的一套引擎。因此，从 Spark SQL 迁移到 Spark Structured Streaming 十分容易，但从 Spark Streaming 迁移过来就要困难得多。 很自然的，基于这样的模型，Spark SQL 中的大部分接口、实现都得以在 Spark Structured Streaming 中直接复用。将用户的 SQL 执行计划转化成流计算执行计划的过程被称为增量化（incrementalize），这一步是由 Spark 框架自动完成的。对于用户来说只要知道：每次计算的输入是某一小段时间的流数据，而输出是对应数据产生的计算结果。 ▲ 左图是 Spark Structured Streaming 模型示意图；右图展示了同一个任务的批处理、流计算版本，可以看到，除了输入输出不同，内部计算过程完全相同。 与 Spark SQL 相比，流式 SQL 计算还有两个额外的特性，分别是窗口（window）和水位（watermark）。 窗口（window）是对过去某段时间的定义。批处理中，查询通常是全量的（例如：总用户量是多少）；而流计算中，我们通常关心近期一段时间的数据（例如：最近24小时新增的用户量是多少）。用户通过选用合适的窗口来获得自己所需的计算结果，常见的窗口有滑动窗口（Sliding Window）、滚动窗口（Tumbling Window）等。 水位（watermark）用来丢弃过早的数据。在流计算中，上游的输入事件可能存在不确定的延迟，而流计算系统的内存是有限的、只能保存有限的状态，一定时间之后必须丢弃历史数据。以双流 A JOIN B 为例，假设窗口为 1 小时，那么 A 中比当前时间减 1 小时更早的数据（行）会被丢弃；如果 B 中出现 1 小时前的事件，因为无法处理只能忽略。 ▲ 上图为水位的示意图，“迟到”太久的数据（行）由于已经低于当前水位无法处理，将被忽略。 水位和窗口的概念都是因时间而来。在其他流计算系统中，也存在相同或类似的概念。 关于 SQL 的流计算模型，常常被拿来对比的还有另一个流计算框架 Apache Flink。与 Spark 相比，它们的实现思路有很大不同，但在模型上是很相似的。 系统架构 驱动程序（Driver）即用户编写的程序，对应一个 SparkContext，负责任务的构造、调度、故障恢复等。驱动程序可以直接运行在客户端，例如用户的应用程序中；也可以托管在 Master 上，这被称为集群模式（cluster mode），通常用于流计算等长期任务。 Cluster Manager顾名思义负责集群的资源分配，Spark 自带的 Spark Master 支持任务的资源分配，并包含一个 Web UI 用来监控任务运行状况。多个 Master 可以构成一主多备，通过 ZooKeeper 进行协调和故障恢复。通常 Spark 集群使用 Spark Master 即可，但如果用户的集群中不仅有 Spark 框架、还要承担其他任务，官方推荐使用 Mesos 作为集群调度器。 Worker节点负责执行计算任务，上面保存了 RDD 等数据。 总结Spark 是一个同时支持批处理和流计算的分布式计算系统。Spark 的所有计算均构建于 RDD 之上，RDD 通过算子连接形成 DAG 的执行计划，RDD 的确定性及不可变性是 Spark 实现故障恢复的基础。Spark Streaming 的 D-Stream 本质上也是将输入数据分成一个个 micro-batch 的 RDD。 Spark SQL 是在 RDD 之上的一层封装，相比原始 RDD，DataFrame API 支持数据表的 schema 信息，从而可以执行 SQL 关系型查询，大幅降低了开发成本。Spark Structured Streaming 是 Spark SQL 的流计算版本，它将输入的数据流看作不断追加的数据行。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"HBase学习之路 （十一）HBase的协过滤器","slug":"2018-06-11-HBase学习之路 （十一）HBase的协过滤器","date":"2018-06-11T02:30:04.000Z","updated":"2019-09-19T06:52:38.903Z","comments":true,"path":"2018-06-11-HBase学习之路 （十一）HBase的协过滤器.html","link":"","permalink":"http://zhangfuxin.cn/2018-06-11-HBase学习之路 （十一）HBase的协过滤器.html","excerpt":"** HBase学习之路 （十一）HBase的协过滤器：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （十一）HBase的协过滤器","text":"** HBase学习之路 （十一）HBase的协过滤器：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （十一）HBase的协过滤器 &lt;The rest of contents | 余下全文&gt; 协处理器—Coprocessor1、 起源 Hbase 作为列族数据库最经常被人诟病的特性包括：无法轻易建立“二级索引”，难以执 行求和、计数、排序等操作。比如，在旧版本的(&lt;0.92)Hbase 中，统计数据表的总行数，需 要使用 Counter 方法，执行一次 MapReduce Job 才能得到。虽然 HBase 在数据存储层中集成 了 MapReduce，能够有效用于数据表的分布式计算。然而在很多情况下，做一些简单的相 加或者聚合计算的时候，如果直接将计算过程放置在 server 端，能够减少通讯开销，从而获 得很好的性能提升。于是，HBase 在 0.92 之后引入了协处理器(coprocessors)，实现一些激动 人心的新特性：能够轻易建立二次索引、复杂过滤器(谓词下推)以及访问控制等。 2、介绍 协处理器有两种：observer 和 endpoint Observer 类似于传统数据库中的触发器，当发生某些事件的时候这类协处理器会被 Server 端调用。Observer Coprocessor 就是一些散布在 HBase Server 端代码中的 hook 钩子， 在固定的事件发生时被调用。比如：put 操作之前有钩子函数 prePut，该函数在 put 操作执 行前会被 Region Server 调用；在 put 操作之后则有 postPut 钩子函数 以 HBase0.92 版本为例，它提供了三种观察者接口： RegionObserver：提供客户端的数据操纵事件钩子：Get、Put、Delete、Scan 等。 WALObserver：提供 WAL 相关操作钩子。 MasterObserver：提供 DDL-类型的操作钩子。如创建、删除、修改数据表等。 到 0.96 版本又新增一个 RegionServerObserver 下图是以 RegionObserver 为例子讲解 Observer 这种协处理器的原理： 1、客户端发出 put 请求 2、该请求被分派给合适的 RegionServer 和 region 3、coprocessorHost 拦截该请求，然后在该表上登记的每个 RegionObserver 上调用 prePut() 4、如果没有被 prePut()拦截，该请求继续送到 region，然后进行处理 5、region 产生的结果再次被 CoprocessorHost 拦截，调用 postPut() 6、假如没有 postPut()拦截该响应，最终结果被返回给客户端 Endpoint 协处理器类似传统数据库中的存储过程，客户端可以调用这些 Endpoint 协处 理器执行一段 Server 端代码，并将 Server 端代码的结果返回给客户端进一步处理，最常见 的用法就是进行聚集操作。如果没有协处理器，当用户需要找出一张表中的最大数据，即 max 聚合操作，就必须进行全表扫描，在客户端代码内遍历扫描结果，并执行求最大值的 操作。这样的方法无法利用底层集群的并发能力，而将所有计算都集中到 Client 端统一执行， 势必效率低下。利用 Coprocessor，用户可以将求最大值的代码部署到 HBase Server 端，HBase 将利用底层 cluster 的多个节点并发执行求最大值的操作。即在每个 Region 范围内执行求最 大值的代码，将每个 Region 的最大值在 Region Server 端计算出，仅仅将该 max 值返回给客 户端。在客户端进一步将多个 Region 的最大值进一步处理而找到其中的最大值。这样整体 的执行效率就会提高很多 下图是 EndPoint 的工作原理： 3、总结 Observer 允许集群在正常的客户端操作过程中可以有不同的行为表现 Endpoint 允许扩展集群的能力，对客户端应用开放新的运算命令 Observer 类似于 RDBMS 中的触发器，主要在服务端工作 Endpoint 类似于 RDBMS 中的存储过程，主要在服务端工作 Observer 可以实现权限管理、优先级设置、监控、ddl 控制、二级索引等功能 Endpoint 可以实现 min、max、avg、sum、distinct、group by 等功能 协处理加载方式 协处理器的加载方式有两种，我们称之为静态加载方式（Static Load）和动态加载方式 （Dynamic Load）。静态加载的协处理器称之为 System Coprocessor，动态加载的协处理器称 之为 Table Coprocessor。 1、 静态加载通过修改 hbase-site.xml 这个文件来实现，启动全局 aggregation，能过操纵所有的表上 的数据。只需要添加如下代码： 1234&lt;property&gt;&lt;name&gt;hbase.coprocessor.user.region.classes&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&lt;/value&gt;&lt;/property&gt; 为所有 table 加载了一个 cp class，可以用”,”分割加载多个 class 2、 动态加载启用表 aggregation，只对特定的表生效。通过 HBase Shell 来实现。 （1）停用表 disable ‘guanzhu’ （2）添加协处理器 alter ‘guanzhu’, METHOD =&gt; ‘table_att’, ‘coprocessor’ =&gt; ‘hdfs://myha01/hbase/guanzhu.jar|com.study.hbase.cp.HbaseCoprocessorTest|1001|’ （3）启用表 enable ‘guanzhu’ 3、 协处理器卸载同样是3步 123disable &apos;mytable&apos;alter &apos;mytable&apos;,METHOD=&gt;&apos;table_att_unset&apos;,NAME=&gt;&apos;coprocessor$1&apos;enable &apos;mytable&apos; 案例（二级索引）说明：二狗子是王宝强的粉丝 关注表：二狗子关注了王宝强 rowKey=’ergouzi’ cell=”star:wangbaoqiang” 1put &apos;guanzhu&apos;, &apos;ergouzi&apos;, &apos;cf:star&apos;, &apos;wangbaoqiang&apos; 粉丝表：二狗子是王宝强的粉丝 rowKey=”wangbaoqiang” cell=”fensi:ergouzi” 1put &apos;fans&apos;, &apos;wangbaoqiang&apos;, &apos;cf:fensi&apos;, &apos;ergouzi&apos; java实现代码 1234567891011121314151617181920212223242526272829303132333435363738394041public class HbaseCoprocessorTest extends BaseRegionObserver&#123; static Configuration conf = HBaseConfiguration.create(); static Connection conn = null; static Table table = null; static &#123; conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;); try &#123; conn = ConnectionFactory.createConnection(conf); table = conn.getTable(TableName.valueOf(&quot;fans&quot;)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 此方法是在真正的put方法调用之前进行调用 * 参数put为table.put(put)里面的参数put对象，是要进行插入的那条数据 * * 例如：要向关注表里面插入一条数据 姓名：二狗子 关注的明星：王宝强 * shell语句：put &apos;guanzhu&apos;,&apos;ergouzi&apos;, &apos;cf:star&apos;, &apos;wangbaoqiang&apos; * * */ @Override public void prePut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException &#123; //获取put对象里面的rowkey&apos;ergouzi&apos; byte[] row = put.getRow(); //获取put对象里面的cell List&lt;Cell&gt; list = put.get(&quot;cf&quot;.getBytes(), &quot;star&quot;.getBytes()); Cell cell = list.get(0); //创建一个新的put对象 Put new_put = new Put(cell.getValueArray()); new_put.addColumn(&quot;cf&quot;.getBytes(), &quot;fensi&quot;.getBytes(), row); table.put(new_put); conn.close(); &#125;&#125; 打成jar包，命名为guanzhu.jar，将其上传到HDFS目录/hbase下面 1[hadoop@hadoop1 ~]$ hadoop fs -put guanzhu.jar /hbase 打开hbase shell命令，按顺序呢执行（提前已经创建好guanzhu和fans表） 123456789101112131415161718192021222324252627282930313233343536373839hbase(main):001:0&gt; disable &apos;guanzhu&apos;0 row(s) in 2.8850 secondshbase(main):002:0&gt; alter &apos;guanzhu&apos;, METHOD =&gt; &apos;table_att&apos;, &apos;coprocessor&apos; =&gt; &apos;hdfs://myha01/hbase/guanzhu.jar|com.study.hbase.cp.HbaseCoprocessorTest|1001|&apos;Updating all regions with the new schema...1/1 regions updated.Done.0 row(s) in 2.7570 secondshbase(main):003:0&gt; enable &apos;guanzhu&apos;0 row(s) in 2.3400 secondshbase(main):004:0&gt; desc &apos;guanzhu&apos;Table guanzhu is ENABLED guanzhu, &#123;TABLE_ATTRIBUTES =&gt; &#123;coprocessor$1 =&gt; &apos;hdfs://myha01/hbase/guanzhu.jar|com.study.hbase.cp.HbaseCoprocessorTest|1001|&apos;&#125; COLUMN FAMILIES DESCRIPTION &#123;NAME =&gt; &apos;cf&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125; 1 row(s) in 0.0500 secondshbase(main):005:0&gt; put &apos;guanzhu&apos;, &apos;ergouzi&apos;, &apos;cf:star&apos;, &apos;wangbaoqiang&apos;0 row(s) in 0.3050 secondshbase(main):006:0&gt; scan &apos;guanzhu&apos;ROW COLUMN+CELL ergouzi column=cf:star, timestamp=1522759023001, value=wangbaoqiang 1 row(s) in 0.0790 secondshbase(main):007:0&gt; scan &apos;fans&apos;ROW COLUMN+CELL \\x00\\x00\\x00\\x19\\x00\\x00\\x00 column=cf:fensi, timestamp=1522759022996, value=ergouzi \\x0C\\x00\\x07ergouzi\\x02cfsta r\\x7F\\xFF\\xFF\\xFF\\xFF\\xFF\\xF F\\xFF\\x04wangbaoqiang 1 row(s) in 0.0330 secondshbase(main):008:0&gt;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://zhangfuxin.cn/tags/HBase/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"HBase学习之路 （十）HBase表的设计原则","slug":"2018-06-10-HBase学习之路 （十）HBase表的设计原则","date":"2018-06-10T02:30:04.000Z","updated":"2019-09-19T06:52:32.948Z","comments":true,"path":"2018-06-10-HBase学习之路 （十）HBase表的设计原则.html","link":"","permalink":"http://zhangfuxin.cn/2018-06-10-HBase学习之路 （十）HBase表的设计原则.html","excerpt":"** HBase学习之路 （十）HBase表的设计原则：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （十）HBase表的设计原则","text":"** HBase学习之路 （十）HBase表的设计原则：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （十）HBase表的设计原则 &lt;The rest of contents | 余下全文&gt; 建表高级属性 下面几个 shell 命令在 hbase 操作中可以起到很大的作用，且主要体现在建表的过程中，看 下面几个 create 属性 1、 BLOOMFILTER 默认是 NONE 是否使用布隆过虑及使用何种方式，布隆过滤可以每列族单独启用 使用 HColumnDescriptor.setBloomFilterType(NONE | ROW | ROWCOL) 对列族单独启用布隆 Default = ROW 对行进行布隆过滤 对 ROW，行键的哈希在每次插入行时将被添加到布隆 对 ROWCOL，行键 + 列族 + 列族修饰的哈希将在每次插入行时添加到布隆 使用方法: create ‘table’,{BLOOMFILTER =&gt;’ROW’} 作用：用布隆过滤可以节省读磁盘过程，可以有助于降低读取延迟 2、 VERSIONS 默认是 1 这个参数的意思是数据保留 1 个 版本，如果我们认为我们的数据没有这么大 的必要保留这么多，随时都在更新，而老版本的数据对我们毫无价值，那将此参数设为 1 能 节约 2/3 的空间 使用方法: create ‘table’,{VERSIONS=&gt;’2’} 附：MIN_VERSIONS =&gt; ‘0’是说在 compact 操作执行之后，至少要保留的版本 3、 COMPRESSION 默认值是 NONE 即不使用压缩，这个参数意思是该列族是否采用压缩，采用什么压缩算 法，方法: create ‘table’,{NAME=&gt;’info’,COMPRESSION=&gt;’SNAPPY’} ，建议采用 SNAPPY 压缩算 法 ，HBase 中，在 Snappy 发布之前（Google 2011 年对外发布 Snappy），采用的 LZO 算法，目标是达到尽可能快的压缩和解压速度，同时减少对 CPU 的消耗； 在 Snappy 发布之后，建议采用 Snappy 算法（参考《HBase: The Definitive Guide》），具体 可以根据实际情况对 LZO 和 Snappy 做过更详细的对比测试后再做选择。 如果建表之初没有压缩，后来想要加入压缩算法，可以通过 alter 修改 schema 4、 TTL 默认是 2147483647 即：Integer.MAX_VALUE 值大概是 68 年，这个参数是说明该列族数据的存活时间，单位是 s 这个参数可以根据具体的需求对数据设定存活时间，超过存过时间的数据将在表中不在 显示，待下次 major compact 的时候再彻底删除数据 注意的是 TTL 设定之后 MIN_VERSIONS=&gt;’0’ 这样设置之后，TTL 时间戳过期后，将全部 彻底删除该 family 下所有的数据，如果 MIN_VERSIONS 不等于 0 那将保留最新的 MIN_VERSIONS 个版本的数据，其它的全部删除，比如 MIN_VERSIONS=&gt;’1’ 届时将保留一个 最新版本的数据，其它版本的数据将不再保存。 5、 alter使用方法： 如 修改压缩算法 12345disable &apos;table&apos;alter &apos;table&apos;,&#123;NAME=&gt;&apos;info&apos;,COMPRESSION=&gt;&apos;snappy&apos;&#125;enable &apos;table&apos; 但是需要执行 major_compact ‘table’ 命令之后 才会做实际的操作。 6、 describe/desc 这个命令查看了 create table 的各项参数或者是默认值。 使用方式：describe ‘user_info’ 7、 disable_all/enable_all disable_all ‘toplist.*’ disable_all 支持正则表达式，并列出当前匹配的表的如下： 12345678toplist_a_total_1001toplist_a_total_1002toplist_a_total_1008toplist_a_total_1009toplist_a_total_1019toplist_a_total_1035...Disable the above 25 tables (y/n)? 并给出确认提示 8、 drop_all 这个命令和 disable_all 的使用方式是一样的 9、 hbase 预分区 默认情况下，在创建 HBase 表的时候会自动创建一个 region 分区，当导入数据的时候， 所有的 HBase 客户端都向这一个 region 写数据，直到这个 region 足够大了才进行切分。一 种可以加快批量写入速度的方法是通过预先创建一些空的 regions，这样当数据写入 HBase 时，会按照 region 分区情况，在集群内做数据的负载均衡。 命令方式： 123456# create table with specific split pointshbase&gt;create &apos;table1&apos;,&apos;f1&apos;,SPLITS =&gt; [&apos;\\x10\\x00&apos;, &apos;\\x20\\x00&apos;, &apos;\\x30\\x00&apos;, &apos;\\x40\\x00&apos;]# create table with four regions based on random bytes keyshbase&gt;create &apos;table2&apos;,&apos;f1&apos;, &#123; NUMREGIONS =&gt; 8 , SPLITALGO =&gt; &apos;UniformSplit&apos; &#125;# create table with five regions based on hex keyshbase&gt;create &apos;table3&apos;,&apos;f1&apos;, &#123; NUMREGIONS =&gt; 10, SPLITALGO =&gt; &apos;HexStringSplit&apos; &#125; 也可以使用 api 的方式: 123hbase org.apache.hadoop.hbase.util.RegionSplitter test_table HexStringSplit -c 10 -f infohbase org.apache.hadoop.hbase.util.RegionSplitter splitTable HexStringSplit -c 10 -f info 参数： test_table 是表名 HexStringSplit 是split 方式 -c 是分 10 个 region -f 是 family 可在 UI 上查看结果，如图： 这样就可以将表预先分为 15 个区，减少数据达到 storefile 大小的时候自动分区的时间 消耗，并且还有以一个优势，就是合理设计 rowkey 能让各个 region 的并发请求平均分配(趋 于均匀) 使 IO 效率达到最高，但是预分区需要将 filesize 设置一个较大的值，设置哪个参数 呢 hbase.hregion.max.filesize 这个值默认是 10G 也就是说单个 region 默认大小是 10G 这个参数的默认值在 0.90 到 0.92 到 0.94.3 各版本的变化：256M–1G–10G 但是如果 MapReduce Input 类型为 TableInputFormat 使用 hbase 作为输入的时候，就要注意 了，每个 region 一个 map，如果数据小于 10G 那只会启用一个 map 造成很大的资源浪费， 这时候可以考虑适当调小该参数的值，或者采用预分配 region 的方式，并将检测如果达到 这个值，再手动分配 region。 表设计1、列簇设计 追求的原则是：在合理范围内能尽量少的减少列簇就尽量减少列簇。 最优设计是：将所有相关性很强的 key-value 都放在同一个列簇下，这样既能做到查询效率 最高，也能保持尽可能少的访问不同的磁盘文件。 以用户信息为例，可以将必须的基本信息存放在一个列族，而一些附加的额外信息可以放在 另一列族。 2、RowKey 设计 HBase 中，表会被划分为 1…n 个 Region，被托管在 RegionServer 中。Region 二个重要的 属性：StartKey 与 EndKey 表示这个 Region 维护的 rowKey 范围，当我们要读/写数据时，如 果 rowKey 落在某个 start-end key 范围内，那么就会定位到目标 region 并且读/写到相关的数 据 那怎么快速精准的定位到我们想要操作的数据，就在于我们的 rowkey 的设计了 Rowkey 设计三原则1、 rowkey 长度原则 Rowkey 是一个二进制码流，Rowkey 的长度被很多开发者建议说设计在 10~100 个字节，不 过建议是越短越好，不要超过 16 个字节。 原因如下： 1、数据的持久化文件 HFile 中是按照 KeyValue 存储的，如果 Rowkey 过长比如 100 个字 节，1000 万列数据光 Rowkey 就要占用 100*1000 万=10 亿个字节，将近 1G 数据，这会极大 影响 HFile 的存储效率； 2、MemStore 将缓存部分数据到内存，如果 Rowkey 字段过长内存的有效利用率会降低， 系统将无法缓存更多的数据，这会降低检索效率。因此 Rowkey 的字节长度越短越好。 3、目前操作系统是都是 64 位系统，内存 8 字节对齐。控制在 16 个字节，8 字节的整数 倍利用操作系统的最佳特性。 2、rowkey 散列原则 如果 Rowkey 是按时间戳的方式递增，不要将时间放在二进制码的前面，建议将 Rowkey 的高位作为散列字段，由程序循环生成，低位放时间字段，这样将提高数据均衡分布在每个 Regionserver 实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息将产生所有 新数据都在一个 RegionServer 上堆积的热点现象，这样在做数据检索的时候负载将会集中 在个别 RegionServer，降低查询效率。 3、 rowkey 唯一原则 必须在设计上保证其唯一性。rowkey 是按照字典顺序排序存储的，因此，设计 rowkey 的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问 的数据放到一块。 数据热点 HBase 中的行是按照 rowkey 的字典顺序排序的，这种设计优化了 scan 操作，可以将相 关的行以及会被一起读取的行存取在临近位置，便于 scan。然而糟糕的 rowkey 设计是热点 的源头。 热点发生在大量的 client 直接访问集群的一个或极少数个节点（访问可能是读， 写或者其他操作）。大量访问会使热点 region 所在的单个机器超出自身承受能力，引起性能 下降甚至 region 不可用，这也会影响同一个 RegionServer 上的其他 region，由于主机无法服 务其他 region 的请求。 设计良好的数据访问模式以使集群被充分，均衡的利用。 为了避免写热点，设计 rowkey 使得不同行在同一个 region，但是在更多数据情况下，数据 应该被写入集群的多个 region，而不是一个。 防止数据热点的有效措施 加盐 这里所说的加盐不是密码学中的加盐，而是在 rowkey 的前面增加随机数，具体就是给 rowkey 分配一个随机前缀以使得它和之前的 rowkey 的开头不同。分配的前缀种类数量应该 和你想使用数据分散到不同的 region 的数量一致。加盐之后的 rowkey 就会根据随机生成的 前缀分散到各个 region 上，以避免热点。 哈希 哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是 可以预测的。使用确定的哈希可以让客户端重构完整的 rowkey，可以使用 get 操作准确获取 某一个行数据 反转 第三种防止热点的方法是反转固定长度或者数字格式的 rowkey。这样可以使得 rowkey 中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机 rowkey，但是牺 牲了 rowkey 的有序性。 反转 rowkey 的例子以手机号为 rowkey，可以将手机号反转后的字符串作为 rowkey，这 样的就避免了以手机号那样比较固定开头导致热点问题 时间戳反转 一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为 rowkey 的一部分对这个问题十分有用，可以用 Long.Max_Value - timestamp 追加到 key 的末尾，例 如 [key][reverse_timestamp] , [key] 的最新值可以通过 scan [key]获得[key]的第一条记录，因 为 HBase 中 rowkey 是有序的，第一条记录是最后录入的数据。比如需要保存一个用户的操 作记录，按照操作时间倒序排序，在设计 rowkey 的时候，可以这样设计 [userId 反转][Long.Max_Value - timestamp]，在查询用户的所有操作记录数据的时候，直接指 定 反 转 后 的 userId ， startRow 是 [userId 反 转 ][000000000000],stopRow 是 [userId 反 转][Long.Max_Value - timestamp] 如果需要查询某段时间的操作记录，startRow 是[user 反转][Long.Max_Value - 起始时间]， stopRow 是[userId 反转][Long.Max_Value - 结束时间]","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://zhangfuxin.cn/tags/HBase/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"HBase学习之路 （九）HBase phoenix的使用","slug":"2018-06-09-HBase学习之路 （九）HBase phoenix的使用","date":"2018-06-09T02:30:04.000Z","updated":"2019-09-19T06:52:26.665Z","comments":true,"path":"2018-06-09-HBase学习之路 （九）HBase phoenix的使用.html","link":"","permalink":"http://zhangfuxin.cn/2018-06-09-HBase学习之路 （九）HBase phoenix的使用.html","excerpt":"** HBase学习之路 （九）HBase phoenix的使用：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （九）HBase phoenix的使用","text":"** HBase学习之路 （九）HBase phoenix的使用：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （九）HBase phoenix的使用 &lt;The rest of contents | 余下全文&gt; HBase phoenix的下载下载地址http://mirror.bit.edu.cn/apache/phoenix/ 选择对应的hbase版本进行下载，测试使用的是hbase-1.2.6版本","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://zhangfuxin.cn/tags/HBase/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"HBase学习之路 （七）HBase 原理","slug":"2018-06-08-HBase学习之路 （七）HBase 原理","date":"2018-06-08T02:30:04.000Z","updated":"2019-09-19T06:52:20.252Z","comments":true,"path":"2018-06-08-HBase学习之路 （七）HBase 原理.html","link":"","permalink":"http://zhangfuxin.cn/2018-06-08-HBase学习之路 （七）HBase 原理.html","excerpt":"** HBase学习之路 （七）HBase 原理：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （七）HBase 原理","text":"** HBase学习之路 （七）HBase 原理：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （七）HBase 原理 &lt;The rest of contents | 余下全文&gt; 系统架构错误图解 这张图是有一个错误点：应该是每一个 RegionServer 就只有一个 HLog，而不是一个 Region 有一个 HLog。 正确图解 从HBase的架构图上可以看出，HBase中的组件包括Client、Zookeeper、HMaster、HRegionServer、HRegion、Store、MemStore、StoreFile、HFile、HLog等，接下来介绍他们的作用。 Client 1、HBase 有两张特殊表： .META.：记录了用户所有表拆分出来的的 Region 映射信息，.META.可以有多个 Regoin -ROOT-：记录了.META.表的 Region 信息，-ROOT-只有一个 Region，无论如何不会分裂 2、Client 访问用户数据前需要首先访问 ZooKeeper，找到-ROOT-表的 Region 所在的位置，然 后访问-ROOT-表，接着访问.META.表，最后才能找到用户数据的位置去访问，中间需要多次 网络操作，不过 client 端会做 cache 缓存。 ZooKeeper 1、ZooKeeper 为 HBase 提供 Failover 机制，选举 Master，避免单点 Master 单点故障问题 2、存储所有 Region 的寻址入口：-ROOT-表在哪台服务器上。-ROOT-这张表的位置信息 3、实时监控 RegionServer 的状态，将 RegionServer 的上线和下线信息实时通知给 Master 4、存储 HBase 的 Schema，包括有哪些 Table，每个 Table 有哪些 Column Family Master 1、为 RegionServer 分配 Region 2、负责 RegionServer 的负载均衡 3、发现失效的 RegionServer 并重新分配其上的 Region 4、HDFS 上的垃圾文件（HBase）回收 5、处理 Schema 更新请求（表的创建，删除，修改，列簇的增加等等） RegionServer 1、RegionServer 维护 Master 分配给它的 Region，处理对这些 Region 的 IO 请求 2、RegionServer 负责 Split 在运行过程中变得过大的 Region，负责 Compact 操作 可以看到，client 访问 HBase 上数据的过程并不需要 master 参与（寻址访问 zookeeper 和 RegioneServer，数据读写访问 RegioneServer），Master 仅仅维护者 Table 和 Region 的元数据信息，负载很低。 .META. 存的是所有的 Region 的位置信息，那么 RegioneServer 当中 Region 在进行分裂之后 的新产生的 Region，是由 Master 来决定发到哪个 RegioneServer，这就意味着，只有 Master 知道 new Region 的位置信息，所以，由 Master 来管理.META.这个表当中的数据的 CRUD 所以结合以上两点表明，在没有 Region 分裂的情况，Master 宕机一段时间是可以忍受的。 HRegion table在行的方向上分隔为多个Region。Region是HBase中分布式存储和负载均衡的最小单元，即不同的region可以分别在不同的Region Server上，但同一个Region是不会拆分到多个server上。Region按大小分隔，每个表一般是只有一个region。随着数据不断插入表，region不断增大，当region的某个列族达到一个阈值时就会分成两个新的region。每个region由以下信息标识：&lt; 表名,startRowkey,创建时间&gt;由目录表(-ROOT-和.META.)记录该region的endRowkey Store 每一个region由一个或多个store组成，至少是一个store，hbase会把一起访问的数据放在一个store里面，即为每个 ColumnFamily建一个store，如果有几个ColumnFamily，也就有几个Store。一个Store由一个memStore和0或者 多个StoreFile组成。 HBase以store的大小来判断是否需要切分region MemStore memStore 是放在内存里的。保存修改的数据即keyValues。当memStore的大小达到一个阀值（默认128MB）时，memStore会被flush到文 件，即生成一个快照。目前hbase 会有一个线程来负责memStore的flush操作。 StoreFile memStore内存中的数据写到文件后就是StoreFile，StoreFile底层是以HFile的格式保存。 HFile HBase中KeyValue数据的存储格式，HFile是Hadoop的 二进制格式文件，实际上StoreFile就是对Hfile做了轻量级包装，即StoreFile底层就是HFile HLog HLog(WAL log)：WAL意为write ahead log，用来做灾难恢复使用，HLog记录数据的所有变更，一旦region server 宕机，就可以从log中进行恢复。HLog文件就是一个普通的Hadoop Sequence File， Sequence File的value是key时HLogKey对象，其中记录了写入数据的归属信息，除了table和region名字外，还同时包括sequence number和timestamp，timestamp是写入时间，sequence number的起始值为0，或者是最近一次存入文件系统中的sequence number。 Sequence File的value是HBase的KeyValue对象，即对应HFile中的KeyValue。 物理存储整体的物理结构 1、Table 中的所有行都按照 RowKsey 的字典序排列。 2、Table 在行的方向上分割为多个 HRegion。 3、HRegion 按大小分割的(默认 10G)，每个表一开始只有一个 HRegion，随着数据不断插入 表，HRegion 不断增大，当增大到一个阀值的时候，HRegion 就会等分会两个新的 HRegion。 当表中的行不断增多，就会有越来越多的 HRegion。 4、HRegion 是 Hbase 中分布式存储和负载均衡的最小单元。最小单元就表示不同的 HRegion 可以分布在不同的 HRegionserver 上。但一个 HRegion 是不会拆分到多个 server 上的。 5、HRegion 虽然是负载均衡的最小单元，但并不是物理存储的最小单元。事实上，HRegion 由一个或者多个 Store 组成，每个 Store 保存一个 Column Family。每个 Strore 又由一个 memStore 和 0 至多个 StoreFile 组成 StoreFile 和 HFile 结构 StoreFile 以 HFile 格式保存在 HDFS 上，请看下图 HFile 的数据组织格式： 首先 HFile 文件是不定长的，长度固定的只有其中的两块：Trailer 和 FileInfo。 正如图中所示： Trailer 中有指针指向其他数据块的起始点。 FileInfo 中记录了文件的一些 Meta 信息，例如：AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY 等。 HFile 分为六个部分： Data Block 段–保存表中的数据，这部分可以被压缩 Meta Block 段 (可选的)–保存用户自定义的 kv 对，可以被压缩。 File Info 段–Hfile 的元信息，不被压缩，用户也可以在这一部分添加自己的元信息。 Data Block Index 段–Data Block 的索引。每条索引的 key 是被索引的 block 的第一条记录的 key。 Meta Block Index 段 (可选的)–Meta Block 的索引。 Trailer 段–这一段是定长的。保存了每一段的偏移量，读取一个 HFile 时，会首先读取 Trailer， Trailer保存了每个段的起始位置(段的Magic Number用来做安全check)，然后，DataBlock Index 会被读取到内存中，这样，当检索某个 key 时，不需要扫描整个 HFile，而只需从内存中找 到key所在的block，通过一次磁盘io将整个block读取到内存中，再找到需要的key。DataBlock Index 采用 LRU 机制淘汰。 HFile 的 Data Block，Meta Block 通常采用压缩方式存储，压缩之后可以大大减少网络 IO 和磁 盘 IO，随之而来的开销当然是需要花费 cpu 进行压缩和解压缩。 目标 Hfile 的压缩支持两种方式：Gzip，LZO。 Data Index 和 Meta Index 块记录了每个 Data 块和 Meta 块的起始点。 Data Block 是 HBase I/O 的基本单元，为了提高效率，HRegionServer 中有基于 LRU 的 Block Cache 机制。每个 Data 块的大小可以在创建一个 Table 的时候通过参数指定，大号的 Block 有利于顺序 Scan，小号 Block 利于随机查询。 每个 Data 块除了开头的 Magic 以外就是一个 个 KeyValue 对拼接而成, Magic 内容就是一些随机数字，目的是防止数据损坏。 HFile 里面的每个 KeyValue 对就是一个简单的 byte 数组。但是这个 byte 数组里面包含了很 多项，并且有固定的结构。我们来看看里面的具体结构： 开始是两个固定长度的数值，分别表示 Key 的长度和 Value 的长度。紧接着是 Key，开始是 固定长度的数值，表示 RowKey 的长度，紧接着是 RowKey，然后是固定长度的数值，表示 Family 的长度，然后是 Family，接着是 Qualifier，然后是两个固定长度的数值，表示 Time Stamp 和 Key Type（Put/Delete）。Value 部分没有这么复杂的结构，就是纯粹的二进制数据了。 MemStore 和 StoreFile 一个 Hregion 由多个 Store 组成，每个 Store 包含一个列族的所有数据。 Store 包括位于内存的一个 memstore 和位于硬盘的多个 storefile 组成。 写操作先写入 memstore，当 memstore 中的数据量达到某个阈值，HRegionServer 启动 flushcache 进程写入 storefile，每次写入形成单独一个 Hfile。 当总 storefile 大小超过一定阈值后，会把当前的 region 分割成两个，并由 HMaster 分配给相 应的 region 服务器，实现负载均衡。 客户端检索数据时，先在 memstore 找，找不到再找 storefile。 Hbase WAL HLog预写 WAL 意为 Write ahead log(http://en.wikipedia.org/wiki/Write-ahead_logging)，类似 mysql 中的 binlog，用来做灾难恢复之用，Hlog 记录数据的所有变更，一旦数据修改，就可以从 log 中 进行恢复。 每个 Region Server 维护一个 Hlog,而不是每个 Region 一个。这样不同 region(来自不同 table) 的日志会混在一起，这样做的目的是不断追加单个文件相对于同时写多个文件而言，可以减 少磁盘寻址次数，因此可以提高对 table 的写性能。带来的麻烦是，如果一台 region server 下线，为了恢复其上的 region，需要将 region server 上的 log 进行拆分，然后分发到其它 region server 上进行恢复。 HLog 文件就是一个普通的 Hadoop Sequence File（序列化文件）： 1、HLog Sequence File 的 Key 是 HLogKey 对象，HLogKey 中记录了写入数据的归属信息，除 了 table 和 region 名字外，同时还包括 sequence number 和 timestamp，timestamp 是”写入 时间”，sequence number 的起始值为 0，或者是最近一次存入文件系统中 sequence number。 2、HLog Sequece File 的 Value 是 HBase 的 KeyValue 对象，即对应 HFile 中的 KeyValue。 Region 寻址机制 既然读写都在 RegionServer 上发生，我们前面有讲到，每个 RegionSever 为一定数量的 Region 服务，那么 Client 要对某一行数据做读写的时候如何能知道具体要去访问哪个 RegionServer 呢？那就是接下来我们要讨论的问题 老的 Region 寻址方式 在 HBase-0.96 版本以前，HBase 有两个特殊的表，分别是-ROOT-表和.META.表，其中-ROOT的位置存储在 ZooKeeper 中，-ROOT-本身存储了.META. Table 的 RegionInfo 信息，并且-ROOT不会分裂，只有一个 Region。而.META.表可以被切分成多个 Region。读取的流程如下图所示： 详细步骤： 第 1 步：Client 请求 ZooKeeper 获得-ROOT-所在的 RegionServer 地址 第 2 步：Client 请求-ROOT-所在的 RS 地址，获取.META.表的地址，Client 会将-ROOT-的相关 信息 cache 下来，以便下一次快速访问 第 3 步：Client 请求.META.表的 RegionServer 地址，获取访问数据所在 RegionServer 的地址， Client 会将.META.的相关信息 cache 下来，以便下一次快速访问 第 4 步：Client 请求访问数据所在 RegionServer 的地址，获取对应的数据 从上面的路径我们可以看出，用户需要 3 次请求才能直到用户 Table 真正的位置，这在一定 程序带来了性能的下降。在 0.96 之前使用 3 层设计的主要原因是考虑到元数据可能需要很 大。但是真正集群运行，元数据的大小其实很容易计算出来。在 BigTable 的论文中，每行 METADATA 数据存储大小为 1KB 左右，如果按照一个 Region 为 128M 的计算，3 层设计可以支持的 Region 个数为 2^34 个，采用 2 层设计可以支持 2^17（131072）。那么 2 层设计的情 况下一个集群可以存储 4P 的数据。这仅仅是一个 Region 只有 128M 的情况下。如果是 10G 呢? 因此，通过计算，其实 2 层设计就可以满足集群的需求。因此在 0.96 版本以后就去掉 了-ROOT-表了。 新的 Region 寻址方式 如上面的计算，2 层结构其实完全能满足业务的需求，因此 0.96 版本以后将-ROOT-表去掉了。 如下图所示： 访问路径变成了 3 步： 第 1 步：Client 请求 ZooKeeper 获取.META.所在的 RegionServer 的地址。 第 2 步：Client 请求.META.所在的 RegionServer 获取访问数据所在的 RegionServer 地址，Client 会将.META.的相关信息 cache 下来，以便下一次快速访问。 第 3 步：Client 请求数据所在的 RegionServer，获取所需要的数据。 总结去掉-ROOT-的原因有如下 2 点： 其一：提高性能 其二：2 层结构已经足以满足集群的需求 这里还有一个问题需要说明，那就是 Client 会缓存.META.的数据，用来加快访问，既然有缓 存，那它什么时候更新？如果.META.更新了，比如 Region1 不在 RerverServer2 上了，被转移 到了 RerverServer3 上。Client 的缓存没有更新会有什么情况？ 其实，Client 的元数据缓存不更新，当.META.的数据发生更新。如上面的例子，由于 Region1 的位置发生了变化，Client 再次根据缓存去访问的时候，会出现错误，当出现异常达到重试 次数后就会去.META.所在的 RegionServer 获取最新的数据，如果.META.所在的 RegionServer 也变了，Client 就会去 ZooKeeper 上获取.META.所在的 RegionServer 的最新地址。 读写过程读请求过程 1、客户端通过 ZooKeeper 以及-ROOT-表和.META.表找到目标数据所在的 RegionServer(就是 数据所在的 Region 的主机地址) 2、联系 RegionServer 查询目标数据 3、RegionServer 定位到目标数据所在的 Region，发出查询请求 4、Region 先在 Memstore 中查找，命中则返回 5、如果在 Memstore 中找不到，则在 Storefile 中扫描 为了能快速的判断要查询的数据在不在这个 StoreFile 中，应用了 BloomFilter （BloomFilter，布隆过滤器：迅速判断一个元素是不是在一个庞大的集合内，但是他有一个 弱点：它有一定的误判率） （误判率：原本不存在与该集合的元素，布隆过滤器有可能会判断说它存在，但是，如果 布隆过滤器，判断说某一个元素不存在该集合，那么该元素就一定不在该集合内） 写请求过程 1、Client 先根据 RowKey 找到对应的 Region 所在的 RegionServer 2、Client 向 RegionServer 提交写请求 3、RegionServer 找到目标 Region 4、Region 检查数据是否与 Schema 一致 5、如果客户端没有指定版本，则获取当前系统时间作为数据版本 6、将更新写入 WAL Log 7、将更新写入 Memstore 8、判断 Memstore 的是否需要 flush 为 StoreFile 文件。 Hbase 在做数据插入操作时，首先要找到 RowKey 所对应的的 Region，怎么找到的？其实这 个简单，因为.META.表存储了每张表每个 Region 的起始 RowKey 了。 建议：在做海量数据的插入操作，避免出现递增 rowkey 的 put 操作 如果 put 操作的所有 RowKey 都是递增的，那么试想，当插入一部分数据的时候刚好进行分 裂，那么之后的所有数据都开始往分裂后的第二个 Region 插入，就造成了数据热点现象。 细节描述： HBase 使用 MemStore 和 StoreFile 存储对表的更新。 数据在更新时首先写入 HLog(WAL Log)，再写入内存(MemStore)中，MemStore 中的数据是排 序的，当 MemStore 累计到一定阈值时，就会创建一个新的 MemStore，并且将老的 MemStore 添加到 flush 队列，由单独的线程 flush 到磁盘上，成为一个 StoreFile。于此同时，系统会在 ZooKeeper 中记录一个 redo point，表示这个时刻之前的变更已经持久化了。当系统出现意外时，可能导致内存(MemStore)中的数据丢失，此时使用 HLog(WAL Log)来恢复 checkpoint 之后的数据。 StoreFile 是只读的，一旦创建后就不可以再修改。因此 HBase 的更新/修改其实是不断追加 的操作。当一个 Store 中的 StoreFile 达到一定的阈值后，就会进行一次合并(minor_compact, major_compact)，将对同一个 key 的修改合并到一起，形成一个大的 StoreFile，当 StoreFile 的大小达到一定阈值后，又会对 StoreFile 进行 split，等分为两个 StoreFile。由于对表的更 新是不断追加的，compact 时，需要访问 Store 中全部的 StoreFile 和 MemStore，将他们按 rowkey 进行合并，由于 StoreFile 和 MemStore 都是经过排序的，并且 StoreFile 带有内存中 索引，合并的过程还是比较快。 major_compact 和 minor_compact 的区别： minor_compact 仅仅合并小文件（HFile） major_compact 合并一个 region 内的所有文件 Client 写入 -&gt; 存入 MemStore，一直到 MemStore 满 -&gt; Flush 成一个 StoreFile，直至增长到 一定阈值 -&gt; 触发 Compact 合并操作 -&gt; 多个 StoreFile 合并成一个 StoreFile，同时进行版本 合并和数据删除 -&gt; 当 StoreFiles Compact 后，逐步形成越来越大的 StoreFile -&gt; 单个 StoreFile 大小超过一定阈值后，触发 Split 操作，把当前 Region Split 成 2 个 Region，Region 会下线， 新 Split 出的 2 个孩子 Region 会被 HMaster 分配到相应的 HRegionServer 上，使得原先 1 个 Region 的压力得以分流到 2 个 Region 上由此过程可知，HBase 只是增加数据，有所得更新 和删除操作，都是在 Compact 阶段做的，所以，用户写操作只需要进入到内存即可立即返 回，从而保证 I/O 高性能。 写入数据的过程补充： 工作机制：每个 HRegionServer 中都会有一个 HLog 对象，HLog 是一个实现 Write Ahead Log 的类，每次用户操作写入 Memstore 的同时，也会写一份数据到 HLog 文件，HLog 文件定期 会滚动出新，并删除旧的文件(已持久化到 StoreFile 中的数据)。当 HRegionServer 意外终止 后，HMaster 会通过 ZooKeeper 感知，HMaster 首先处理遗留的 HLog 文件，将不同 Region 的 log数据拆分，分别放到相应 Region 目录下，然后再将失效的 Region（带有刚刚拆分的 log） 重新分配，领取到这些 Region 的 HRegionServer 在 load Region 的过程中，会发现有历史 HLog 需要处理，因此会 Replay HLog 中的数据到 MemStore 中，然后 flush 到 StoreFiles，完成数据 恢复。 RegionServer 工作机制Region 分配 任何时刻，一个 Region 只能分配给一个 RegionServer。master 记录了当前有哪些可用的 RegionServer。以及当前哪些 Region 分配给了哪些 RegionServer，哪些 Region 还没有分配。 当需要分配的新的 Region，并且有一个 RegionServer 上有可用空间时，Master 就给这个 RegionServer 发送一个装载请求，把 Region 分配给这个 RegionServer。RegionServer 得到请 求后，就开始对此 Region 提供服务。 RegionServer 上线 Master 使用 zookeeper 来跟踪 RegionServer 状态。当某个 RegionServer 启动时，会首先在 ZooKeeper 上的 server 目录下建立代表自己的 znode。由于 Master 订阅了 server 目录上的变 更消息，当 server 目录下的文件出现新增或删除操作时，Master 可以得到来自 ZooKeeper 的实时通知。因此一旦 RegionServer 上线，Master 能马上得到消息。 RegionServer 下线 当 RegionServer 下线时，它和 zookeeper 的会话断开，ZooKeeper 而自动释放代表这台 server 的文件上的独占锁。Master 就可以确定： 1、RegionServer 和 ZooKeeper 之间的网络断开了。 2、RegionServer 挂了。 无论哪种情况，RegionServer都无法继续为它的Region提供服务了，此时Master会删除server 目录下代表这台 RegionServer 的 znode 数据，并将这台 RegionServer 的 Region 分配给其它还 活着的同志。 Master 工作机制Master 上线 Master 启动进行以下步骤: 1、从 ZooKeeper 上获取唯一一个代表 Active Master 的锁，用来阻止其它 Master 成为 Master。 2、扫描 ZooKeeper 上的 server 父节点，获得当前可用的 RegionServer 列表。 3、和每个 RegionServer 通信，获得当前已分配的 Region 和 RegionServer 的对应关系。 4、扫描.META. Region 的集合，计算得到当前还未分配的 Region，将他们放入待分配 Region 列表。 Master 下线 由于 Master 只维护表和 Region 的元数据，而不参与表数据 IO 的过程，Master 下线仅 导致所有元数据的修改被冻结(无法创建删除表，无法修改表的 schema，无法进行 Region 的负载均衡，无法处理 Region 上下线，无法进行 Region 的合并，唯一例外的是 Region 的 split 可以正常进行，因为只有 RegionServer 参与)，表的数据读写还可以正常进行。因此 Master 下线短时间内对整个 hbase 集群没有影响。 从上线过程可以看到，Master 保存的信息全是可以冗余信息（都可以从系统其它地方 收集到或者计算出来） 因此，一般 HBase 集群中总是有一个 Master 在提供服务，还有一个以上的 Master 在等 待时机抢占它的位置。","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://zhangfuxin.cn/tags/HBase/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"HBase学习之路 （六）过滤器","slug":"2018-06-07-HBase学习之路 （六）过滤器","date":"2018-06-07T02:30:04.000Z","updated":"2019-09-19T06:36:20.656Z","comments":true,"path":"2018-06-07-HBase学习之路 （六）过滤器.html","link":"","permalink":"http://zhangfuxin.cn/2018-06-07-HBase学习之路 （六）过滤器.html","excerpt":"** HBase学习之路 （六）过滤器：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （六）过滤器","text":"** HBase学习之路 （六）过滤器：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （六）过滤器 &lt;The rest of contents | 余下全文&gt; 过滤器（Filter） 基础API中的查询操作在面对大量数据的时候是非常苍白的，这里Hbase提供了高级的查询方法：Filter。Filter可以根据簇、列、版本等更多的条件来对数据进行过滤，基于Hbase本身提供的三维有序（主键有序、列有序、版本有序），这些Filter可以高效的完成查询过滤的任务。带有Filter条件的RPC查询请求会把Filter分发到各个RegionServer，是一个服务器端（Server-side）的过滤器，这样也可以降低网络传输的压力。 要完成一个过滤的操作，至少需要两个参数。一个是抽象的操作符，Hbase提供了枚举类型的变量来表示这些抽象的操作符：LESS/LESS_OR_EQUAL/EQUAL/NOT_EUQAL等；另外一个就是具体的比较器（Comparator），代表具体的比较逻辑，如果可以提高字节级的比较、字符串级的比较等。有了这两个参数，我们就可以清晰的定义筛选的条件，过滤数据。 抽象操作符（比较运算符） LESS &lt; LESS_OR_EQUAL &lt;= EQUAL = NOT_EQUAL &lt;&gt; GREATER_OR_EQUAL &gt;= GREATER &gt; NO_OP 排除所有 比较器（指定比较机制） BinaryComparator 按字节索引顺序比较指定字节数组，采用 Bytes.compareTo(byte[]) BinaryPrefixComparator 跟前面相同，只是比较左端的数据是否相同 NullComparator 判断给定的是否为空 BitComparator 按位比较 RegexStringComparator 提供一个正则的比较器，仅支持 EQUAL 和非 EQUAL SubstringComparator 判断提供的子串是否出现在 value 中 HBase过滤器的分类比较过滤器1、行键过滤器 RowFilter12Filter rowFilter = new RowFilter(CompareOp.GREATER, new BinaryComparator(&quot;95007&quot;.getBytes()));scan.setFilter(rowFilter); 123456789101112131415161718192021222324252627282930 1 public class HbaseFilterTest &#123; 2 3 private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;; 4 private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;; 5 6 private static Connection conn = null; 7 private static Admin admin = null; 8 9 public static void main(String[] args) throws Exception &#123;10 11 Configuration conf = HBaseConfiguration.create();12 conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);13 conn = ConnectionFactory.createConnection(conf);14 admin = conn.getAdmin();15 Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));16 17 Scan scan = new Scan();18 19 Filter rowFilter = new RowFilter(CompareOp.GREATER, new BinaryComparator(&quot;95007&quot;.getBytes()));20 scan.setFilter(rowFilter);21 ResultScanner resultScanner = table.getScanner(scan);22 for(Result result : resultScanner) &#123;23 List&lt;Cell&gt; cells = result.listCells();24 for(Cell cell : cells) &#123;25 System.out.println(cell);26 &#125;27 &#125;28 29 30 &#125; 运行结果部分截图 2、列簇过滤器 FamilyFilter12Filter familyFilter = new FamilyFilter(CompareOp.EQUAL, new BinaryComparator(&quot;info&quot;.getBytes()));scan.setFilter(familyFilter); 123456789101112131415161718192021222324252627282930313233 1 public class HbaseFilterTest &#123; 2 3 private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;; 4 private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;; 5 6 private static Connection conn = null; 7 private static Admin admin = null; 8 9 public static void main(String[] args) throws Exception &#123;10 11 Configuration conf = HBaseConfiguration.create();12 conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);13 conn = ConnectionFactory.createConnection(conf);14 admin = conn.getAdmin();15 Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));16 17 Scan scan = new Scan();18 19 Filter familyFilter = new FamilyFilter(CompareOp.EQUAL, new BinaryComparator(&quot;info&quot;.getBytes()));20 scan.setFilter(familyFilter);21 ResultScanner resultScanner = table.getScanner(scan);22 for(Result result : resultScanner) &#123;23 List&lt;Cell&gt; cells = result.listCells();24 for(Cell cell : cells) &#123;25 System.out.println(cell);26 &#125;27 &#125;28 29 30 &#125;31 32 33 &#125; 3、列过滤器 QualifierFilter12Filter qualifierFilter = new QualifierFilter(CompareOp.EQUAL, new BinaryComparator(&quot;name&quot;.getBytes()));scan.setFilter(qualifierFilter); 123456789101112131415161718192021222324252627282930313233 1 public class HbaseFilterTest &#123; 2 3 private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;; 4 private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;; 5 6 private static Connection conn = null; 7 private static Admin admin = null; 8 9 public static void main(String[] args) throws Exception &#123;10 11 Configuration conf = HBaseConfiguration.create();12 conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);13 conn = ConnectionFactory.createConnection(conf);14 admin = conn.getAdmin();15 Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));16 17 Scan scan = new Scan();18 19 Filter qualifierFilter = new QualifierFilter(CompareOp.EQUAL, new BinaryComparator(&quot;name&quot;.getBytes()));20 scan.setFilter(qualifierFilter);21 ResultScanner resultScanner = table.getScanner(scan);22 for(Result result : resultScanner) &#123;23 List&lt;Cell&gt; cells = result.listCells();24 for(Cell cell : cells) &#123;25 System.out.println(cell);26 &#125;27 &#125;28 29 30 &#125;31 32 33 &#125; 4、值过滤器 ValueFilter12Filter valueFilter = new ValueFilter(CompareOp.EQUAL, new SubstringComparator(&quot;男&quot;));scan.setFilter(valueFilter); 123456789101112131415161718192021222324252627282930313233 1 public class HbaseFilterTest &#123; 2 3 private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;; 4 private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;; 5 6 private static Connection conn = null; 7 private static Admin admin = null; 8 9 public static void main(String[] args) throws Exception &#123;10 11 Configuration conf = HBaseConfiguration.create();12 conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);13 conn = ConnectionFactory.createConnection(conf);14 admin = conn.getAdmin();15 Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));16 17 Scan scan = new Scan();18 19 Filter valueFilter = new ValueFilter(CompareOp.EQUAL, new SubstringComparator(&quot;男&quot;));20 scan.setFilter(valueFilter);21 ResultScanner resultScanner = table.getScanner(scan);22 for(Result result : resultScanner) &#123;23 List&lt;Cell&gt; cells = result.listCells();24 for(Cell cell : cells) &#123;25 System.out.println(cell);26 &#125;27 &#125;28 29 30 &#125;31 32 33 &#125; 5、时间戳过滤器 TimestampsFilter1234List&lt;Long&gt; list = new ArrayList&lt;&gt;();list.add(1522469029503l);TimestampsFilter timestampsFilter = new TimestampsFilter(list);scan.setFilter(timestampsFilter); 123456789101112131415161718192021222324252627282930313233343536 1 public class HbaseFilterTest &#123; 2 3 private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;; 4 private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;; 5 6 private static Connection conn = null; 7 private static Admin admin = null; 8 9 public static void main(String[] args) throws Exception &#123;10 11 Configuration conf = HBaseConfiguration.create();12 conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);13 conn = ConnectionFactory.createConnection(conf);14 admin = conn.getAdmin();15 Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));16 17 Scan scan = new Scan();18 19 List&lt;Long&gt; list = new ArrayList&lt;&gt;();20 list.add(1522469029503l);21 TimestampsFilter timestampsFilter = new TimestampsFilter(list);22 scan.setFilter(timestampsFilter);23 ResultScanner resultScanner = table.getScanner(scan);24 for(Result result : resultScanner) &#123;25 List&lt;Cell&gt; cells = result.listCells();26 for(Cell cell : cells) &#123;27 System.out.println(Bytes.toString(cell.getRow()) + &quot;\\t&quot; + Bytes.toString(cell.getFamily()) + &quot;\\t&quot; + Bytes.toString(cell.getQualifier())28 + &quot;\\t&quot; + Bytes.toString(cell.getValue()) + &quot;\\t&quot; + cell.getTimestamp());29 &#125;30 &#125;31 32 33 &#125;34 35 36 &#125; 专用过滤器1、单列值过滤器 SingleColumnValueFilter —-会返回满足条件的整行12345678SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter( &quot;info&quot;.getBytes(), //列簇 &quot;name&quot;.getBytes(), //列 CompareOp.EQUAL, new SubstringComparator(&quot;刘晨&quot;));//如果不设置为 true，则那些不包含指定 column 的行也会返回singleColumnValueFilter.setFilterIfMissing(true);scan.setFilter(singleColumnValueFilter); 12345678910111213141516171819202122232425262728293031323334353637383940 1 public class HbaseFilterTest2 &#123; 2 3 private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;; 4 private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;; 5 6 private static Connection conn = null; 7 private static Admin admin = null; 8 9 public static void main(String[] args) throws Exception &#123;10 11 Configuration conf = HBaseConfiguration.create();12 conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);13 conn = ConnectionFactory.createConnection(conf);14 admin = conn.getAdmin();15 Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));16 17 Scan scan = new Scan();18 19 SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter(20 &quot;info&quot;.getBytes(), 21 &quot;name&quot;.getBytes(), 22 CompareOp.EQUAL, 23 new SubstringComparator(&quot;刘晨&quot;));24 singleColumnValueFilter.setFilterIfMissing(true);25 26 scan.setFilter(singleColumnValueFilter);27 ResultScanner resultScanner = table.getScanner(scan);28 for(Result result : resultScanner) &#123;29 List&lt;Cell&gt; cells = result.listCells();30 for(Cell cell : cells) &#123;31 System.out.println(Bytes.toString(cell.getRow()) + &quot;\\t&quot; + Bytes.toString(cell.getFamily()) + &quot;\\t&quot; + Bytes.toString(cell.getQualifier())32 + &quot;\\t&quot; + Bytes.toString(cell.getValue()) + &quot;\\t&quot; + cell.getTimestamp());33 &#125;34 &#125;35 36 37 &#125;38 39 40 &#125; 2、单列值排除器 SingleColumnValueExcludeFilter12345678SingleColumnValueExcludeFilter singleColumnValueExcludeFilter = new SingleColumnValueExcludeFilter( &quot;info&quot;.getBytes(), &quot;name&quot;.getBytes(), CompareOp.EQUAL, new SubstringComparator(&quot;刘晨&quot;));singleColumnValueExcludeFilter.setFilterIfMissing(true); scan.setFilter(singleColumnValueExcludeFilter); 12345678910111213141516171819202122232425262728293031323334353637383940 1 public class HbaseFilterTest2 &#123; 2 3 private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;; 4 private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;; 5 6 private static Connection conn = null; 7 private static Admin admin = null; 8 9 public static void main(String[] args) throws Exception &#123;10 11 Configuration conf = HBaseConfiguration.create();12 conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);13 conn = ConnectionFactory.createConnection(conf);14 admin = conn.getAdmin();15 Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));16 17 Scan scan = new Scan();18 19 SingleColumnValueExcludeFilter singleColumnValueExcludeFilter = new SingleColumnValueExcludeFilter(20 &quot;info&quot;.getBytes(), 21 &quot;name&quot;.getBytes(), 22 CompareOp.EQUAL, 23 new SubstringComparator(&quot;刘晨&quot;));24 singleColumnValueExcludeFilter.setFilterIfMissing(true);25 26 scan.setFilter(singleColumnValueExcludeFilter);27 ResultScanner resultScanner = table.getScanner(scan);28 for(Result result : resultScanner) &#123;29 List&lt;Cell&gt; cells = result.listCells();30 for(Cell cell : cells) &#123;31 System.out.println(Bytes.toString(cell.getRow()) + &quot;\\t&quot; + Bytes.toString(cell.getFamily()) + &quot;\\t&quot; + Bytes.toString(cell.getQualifier())32 + &quot;\\t&quot; + Bytes.toString(cell.getValue()) + &quot;\\t&quot; + cell.getTimestamp());33 &#125;34 &#125;35 36 37 &#125;38 39 40 &#125; 3、前缀过滤器 PrefixFilter—-针对行键123PrefixFilter prefixFilter = new PrefixFilter(&quot;9501&quot;.getBytes()); scan.setFilter(prefixFilter); 1234567891011121314151617181920212223242526272829303132333435 1 public class HbaseFilterTest2 &#123; 2 3 private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;; 4 private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;; 5 6 private static Connection conn = null; 7 private static Admin admin = null; 8 9 public static void main(String[] args) throws Exception &#123;10 11 Configuration conf = HBaseConfiguration.create();12 conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);13 conn = ConnectionFactory.createConnection(conf);14 admin = conn.getAdmin();15 Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));16 17 Scan scan = new Scan();18 19 PrefixFilter prefixFilter = new PrefixFilter(&quot;9501&quot;.getBytes());20 21 scan.setFilter(prefixFilter);22 ResultScanner resultScanner = table.getScanner(scan);23 for(Result result : resultScanner) &#123;24 List&lt;Cell&gt; cells = result.listCells();25 for(Cell cell : cells) &#123;26 System.out.println(Bytes.toString(cell.getRow()) + &quot;\\t&quot; + Bytes.toString(cell.getFamily()) + &quot;\\t&quot; + Bytes.toString(cell.getQualifier())27 + &quot;\\t&quot; + Bytes.toString(cell.getValue()) + &quot;\\t&quot; + cell.getTimestamp());28 &#125;29 &#125;30 31 32 &#125;33 34 35 &#125; 4、列前缀过滤器 ColumnPrefixFilter123ColumnPrefixFilter columnPrefixFilter = new ColumnPrefixFilter(&quot;name&quot;.getBytes()); scan.setFilter(columnPrefixFilter); 1234567891011121314151617181920212223242526272829303132333435 1 public class HbaseFilterTest2 &#123; 2 3 private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;; 4 private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;; 5 6 private static Connection conn = null; 7 private static Admin admin = null; 8 9 public static void main(String[] args) throws Exception &#123;10 11 Configuration conf = HBaseConfiguration.create();12 conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);13 conn = ConnectionFactory.createConnection(conf);14 admin = conn.getAdmin();15 Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));16 17 Scan scan = new Scan();18 19 ColumnPrefixFilter columnPrefixFilter = new ColumnPrefixFilter(&quot;name&quot;.getBytes());20 21 scan.setFilter(columnPrefixFilter);22 ResultScanner resultScanner = table.getScanner(scan);23 for(Result result : resultScanner) &#123;24 List&lt;Cell&gt; cells = result.listCells();25 for(Cell cell : cells) &#123;26 System.out.println(Bytes.toString(cell.getRow()) + &quot;\\t&quot; + Bytes.toString(cell.getFamily()) + &quot;\\t&quot; + Bytes.toString(cell.getQualifier())27 + &quot;\\t&quot; + Bytes.toString(cell.getValue()) + &quot;\\t&quot; + cell.getTimestamp());28 &#125;29 &#125;30 31 32 &#125;33 34 35 &#125; 5、分页过滤器 PageFilter","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://zhangfuxin.cn/tags/HBase/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"HBase学习之路 （五）MapReduce操作Hbase","slug":"2018-06-06-HBase学习之路 （五）MapReduce操作Hbase","date":"2018-06-06T02:30:04.000Z","updated":"2019-09-19T06:31:42.935Z","comments":true,"path":"2018-06-06-HBase学习之路 （五）MapReduce操作Hbase.html","link":"","permalink":"http://zhangfuxin.cn/2018-06-06-HBase学习之路 （五）MapReduce操作Hbase.html","excerpt":"** HBase学习之路 （五）MapReduce操作Hbase：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （五）MapReduce操作Hbase","text":"** HBase学习之路 （五）MapReduce操作Hbase：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （五）MapReduce操作Hbase &lt;The rest of contents | 余下全文&gt; MapReduce从HDFS读取数据存储到HBase中现有HDFS中有一个student.txt文件，格式如下 12345678910111213141516171819202195002,刘晨,女,19,IS95017,王风娟,女,18,IS95018,王一,女,19,IS95013,冯伟,男,21,CS95014,王小丽,女,19,CS95019,邢小丽,女,19,IS95020,赵钱,男,21,IS95003,王敏,女,22,MA95004,张立,男,19,IS95012,孙花,女,20,CS95010,孔小涛,男,19,CS95005,刘刚,男,18,MA95006,孙庆,男,23,CS95007,易思玲,女,19,MA95008,李娜,女,18,CS95021,周二,男,17,MA95022,郑明,男,20,MA95001,李勇,男,20,CS95011,包小柏,男,18,MA95009,梦圆圆,女,18,MA95015,王君,男,18,MA 将HDFS上的这个文件里面的数据写入到HBase数据块中 MapReduce实现代码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.hbase.mapreduce.TableReducer;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class ReadHDFSDataToHbaseMR extends Configured implements Tool&#123; public static void main(String[] args) throws Exception &#123; int run = ToolRunner.run(new ReadHDFSDataToHbaseMR(), args); System.exit(run); &#125; @Override public int run(String[] arg0) throws Exception &#123; Configuration conf = HBaseConfiguration.create(); conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://myha01/&quot;); conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;); System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;); FileSystem fs = FileSystem.get(conf);// conf.addResource(&quot;config/core-site.xml&quot;);// conf.addResource(&quot;config/hdfs-site.xml&quot;); Job job = Job.getInstance(conf); job.setJarByClass(ReadHDFSDataToHbaseMR.class); job.setMapperClass(HDFSToHbaseMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); TableMapReduceUtil.initTableReducerJob(&quot;student&quot;, HDFSToHbaseReducer.class, job,null,null,null,null,false); job.setOutputKeyClass(NullWritable.class); job.setOutputValueClass(Put.class); Path inputPath = new Path(&quot;/student/input/&quot;); Path outputPath = new Path(&quot;/student/output/&quot;); if(fs.exists(outputPath)) &#123; fs.delete(outputPath,true); &#125; FileInputFormat.addInputPath(job, inputPath); FileOutputFormat.setOutputPath(job, outputPath); boolean isDone = job.waitForCompletion(true); return isDone ? 0 : 1; &#125; public static class HDFSToHbaseMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; context.write(value, NullWritable.get()); &#125; &#125; /** * 95015,王君,男,18,MA * */ public static class HDFSToHbaseReducer extends TableReducer&lt;Text, NullWritable, NullWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values,Context context) throws IOException, InterruptedException &#123; String[] split = key.toString().split(&quot;,&quot;); Put put = new Put(split[0].getBytes()); put.addColumn(&quot;info&quot;.getBytes(), &quot;name&quot;.getBytes(), split[1].getBytes()); put.addColumn(&quot;info&quot;.getBytes(), &quot;sex&quot;.getBytes(), split[2].getBytes()); put.addColumn(&quot;info&quot;.getBytes(), &quot;age&quot;.getBytes(), split[3].getBytes()); put.addColumn(&quot;info&quot;.getBytes(), &quot;department&quot;.getBytes(), split[4].getBytes()); context.write(NullWritable.get(), put); &#125; &#125; &#125; MapReduce从HBase读取数据计算平均年龄并存储到HDFS中123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138import java.io.IOException;import java.util.List;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.CellUtil;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.client.Scan;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.hbase.mapreduce.TableMapper;import org.apache.hadoop.hbase.util.Bytes;import org.apache.hadoop.io.DoubleWritable;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class ReadHbaseDataToHDFS extends Configured implements Tool&#123; public static void main(String[] args) throws Exception &#123; int run = ToolRunner.run(new ReadHbaseDataToHDFS(), args); System.exit(run); &#125; @Override public int run(String[] arg0) throws Exception &#123; Configuration conf = HBaseConfiguration.create(); conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://myha01/&quot;); conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;); System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;); FileSystem fs = FileSystem.get(conf);// conf.addResource(&quot;config/core-site.xml&quot;);// conf.addResource(&quot;config/hdfs-site.xml&quot;); Job job = Job.getInstance(conf); job.setJarByClass(ReadHbaseDataToHDFS.class); // 取对业务有用的数据 info,age Scan scan = new Scan(); scan.addColumn(&quot;info&quot;.getBytes(), &quot;age&quot;.getBytes()); TableMapReduceUtil.initTableMapperJob( &quot;student&quot;.getBytes(), // 指定表名 scan, // 指定扫描数据的条件 HbaseToHDFSMapper.class, // 指定mapper class Text.class, // outputKeyClass mapper阶段的输出的key的类型 IntWritable.class, // outputValueClass mapper阶段的输出的value的类型 job, // job对象 false ); job.setReducerClass(HbaseToHDFSReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(DoubleWritable.class); Path outputPath = new Path(&quot;/student/avg/&quot;); if(fs.exists(outputPath)) &#123; fs.delete(outputPath,true); &#125; FileOutputFormat.setOutputPath(job, outputPath); boolean isDone = job.waitForCompletion(true); return isDone ? 0 : 1; &#125; public static class HbaseToHDFSMapper extends TableMapper&lt;Text, IntWritable&gt;&#123; Text outKey = new Text(&quot;age&quot;); IntWritable outValue = new IntWritable(); // key是hbase中的行键 // value是hbase中的所行键的所有数据 @Override protected void map(ImmutableBytesWritable key, Result value,Context context) throws IOException, InterruptedException &#123; boolean isContainsColumn = value.containsColumn(&quot;info&quot;.getBytes(), &quot;age&quot;.getBytes()); if(isContainsColumn) &#123; List&lt;Cell&gt; listCells = value.getColumnCells(&quot;info&quot;.getBytes(), &quot;age&quot;.getBytes()); System.out.println(&quot;listCells:\\t&quot;+listCells); Cell cell = listCells.get(0); System.out.println(&quot;cells:\\t&quot;+cell); byte[] cloneValue = CellUtil.cloneValue(cell); String ageValue = Bytes.toString(cloneValue); outValue.set(Integer.parseInt(ageValue)); context.write(outKey,outValue); &#125; &#125; &#125; public static class HbaseToHDFSReducer extends Reducer&lt;Text, IntWritable, Text, DoubleWritable&gt;&#123; DoubleWritable outValue = new DoubleWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; int count = 0; int sum = 0; for(IntWritable value : values) &#123; count++; sum += value.get(); &#125; double avgAge = sum * 1.0 / count; outValue.set(avgAge); context.write(key, outValue); &#125; &#125; &#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://zhangfuxin.cn/tags/HBase/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"HBase学习之路 （三）HBase集群Shell操作","slug":"2018-06-03-HBase学习之路 （三）HBase集群Shell操作","date":"2018-06-03T02:30:04.000Z","updated":"2019-09-19T06:28:14.630Z","comments":true,"path":"2018-06-03-HBase学习之路 （三）HBase集群Shell操作.html","link":"","permalink":"http://zhangfuxin.cn/2018-06-03-HBase学习之路 （三）HBase集群Shell操作.html","excerpt":"** HBase学习之路 （三）HBase集群Shell操作：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （三）HBase集群Shell操作","text":"** HBase学习之路 （三）HBase集群Shell操作：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （三）HBase集群Shell操作 &lt;The rest of contents | 余下全文&gt; 进入HBase命令行在你安装的随意台服务器节点上，执行命令：hbase shell，会进入到你的 hbase shell 客 户端 1234567891011[hadoop@hadoop1 ~]$ hbase shellSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/hadoop/apps/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]HBase Shell; enter &apos;help&lt;RETURN&gt;&apos; for list of supported commands.Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase ShellVersion 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017hbase(main):001:0&gt; 说明，先看一下提示。其实是不是有一句很重要的话： 12HBase Shell; enter &apos;help&lt;RETURN&gt;&apos; for list of supported commands.Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell 讲述了怎么获得帮助，怎么退出客户端 help 获取帮助 help：获取所有命令提示 help “dml” ：获取一组命令的提示 help “put” ：获取一个单独命令的提示帮助 exit 退出 hbase shell 客户端 HBase表的操作 关于表的操作包括（创建create，查看表列表list。查看表的详细信息desc，删除表drop，清空表truncate，修改表的定义alter） 创建create可以输入以下命令进行查看帮助命令 1hbase(main):001:0&gt; help &apos;create&apos; View Code 可以看到其中一条提示 1hbase&gt; create &apos;t1&apos;, &#123;NAME =&gt; &apos;f1&apos;&#125;, &#123;NAME =&gt; &apos;f2&apos;&#125;, &#123;NAME =&gt; &apos;f3&apos;&#125; 其中t1是表名，f1,f2,f3是列簇的名，如： 12345hbase(main):002:0&gt; create &apos;myHbase&apos;,&#123;NAME =&gt; &apos;myCard&apos;,VERSIONS =&gt; 5&#125;0 row(s) in 3.1270 seconds=&gt; Hbase::Table - myHbasehbase(main):003:0&gt; 创建了一个名为myHbase的表，表里面有1个列簇，名为myCard，保留5个版本信息 查看表列表list可以输入以下命令进行查看帮助命令 123456789hbase(main):003:0&gt; help &apos;list&apos;List all tables in hbase. Optional regular expression parameter couldbe used to filter the output. Examples: hbase&gt; list hbase&gt; list &apos;abc.*&apos; hbase&gt; list &apos;ns:abc.*&apos; hbase&gt; list &apos;ns:.*&apos;hbase(main):004:0&gt; 直接输入list进行查看 1234567hbase(main):004:0&gt; listTABLE myHbase 1 row(s) in 0.0650 seconds=&gt; [&quot;myHbase&quot;]hbase(main):005:0&gt; 只有一条结果，就是刚刚创建的表myHbase 查看表的详细信息desc一个大括号，就相当于一个列簇。 12345678910hbase(main):006:0&gt; desc &apos;myHbase&apos;Table myHbase is ENABLED myHbase COLUMN FAMILIES DESCRIPTION &#123;NAME =&gt; &apos;myCard&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;5&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125; 1 row(s) in 0.2160 secondshbase(main):007:0&gt; 修改表的定义alter添加一个列簇hbase(main):007:0&gt; alter ‘myHbase’, NAME =&gt; ‘myInfo’Updating all regions with the new schema…1/1 regions updated.Done.0 row(s) in 2.0690 seconds hbase(main):008:0&gt; desc ‘myHbase’Table myHbase is ENABLEDmyHbaseCOLUMN FAMILIES DESCRIPTION{NAME =&gt; ‘myCard’, BLOOMFILTER =&gt; ‘ROW’, VERSIONS =&gt; ‘5’, IN_MEMORY =&gt; ‘false’, KEEP_DELETED_CELLS =&gt; ‘FALSE’, DATA_BLOCK_ENCODING =&gt; ‘NONE’, TTL =&gt; ‘FOREVER’, COMPRESSION =&gt; ‘NONE’, MIN_VERSIONS =&gt; ‘0’, BLOCKCACHE =&gt; ‘true’, BLOCKSIZE =&gt; ‘65536’, REPLICATION_SCOPE =&gt; ‘0’}{NAME =&gt; ‘myInfo’, BLOOMFILTER =&gt; ‘ROW’, VERSIONS =&gt; ‘1’, IN_MEMORY =&gt; ‘false’, KEEP_DELETED_CELLS =&gt; ‘FALSE’, DATA_BLOCK_ENCODING =&gt; ‘NONE’, TTL =&gt; ‘FOREVER’, COMPRESSION =&gt; ‘NONE’, MIN_VERSIONS =&gt; ‘0’, BLOCKCACHE =&gt; ‘true’, BLOCKSIZE =&gt; ‘65536’, REPLICATION_SCOPE =&gt; ‘0’}2 row(s) in 0.0420 seconds hbase(main):009:0&gt; 删除一个列簇12345678910111213141516hbase(main):009:0&gt; alter &apos;myHbase&apos;, NAME =&gt; &apos;myCard&apos;, METHOD =&gt; &apos;delete&apos;Updating all regions with the new schema...1/1 regions updated.Done.0 row(s) in 2.1920 secondshbase(main):010:0&gt; desc &apos;myHbase&apos;Table myHbase is ENABLED myHbase COLUMN FAMILIES DESCRIPTION &#123;NAME =&gt; &apos;myInfo&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125; 1 row(s) in 0.0290 secondshbase(main):011:0&gt; 删除一个列簇也可以执行以下命令 1alter &apos;myHbase&apos;, &apos;delete&apos; =&gt; &apos;myCard&apos; 添加列簇hehe同时删除列簇myInfo12345678910111213141516171819hbase(main):011:0&gt; alter &apos;myHbase&apos;, &#123;NAME =&gt; &apos;hehe&apos;&#125;, &#123;NAME =&gt; &apos;myInfo&apos;, METHOD =&gt; &apos;delete&apos;&#125;Updating all regions with the new schema...1/1 regions updated.Done.Updating all regions with the new schema...1/1 regions updated.Done.0 row(s) in 3.8260 secondshbase(main):012:0&gt; desc &apos;myHbase&apos;Table myHbase is ENABLED myHbase COLUMN FAMILIES DESCRIPTION &#123;NAME =&gt; &apos;hehe&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125; 1 row(s) in 0.0410 secondshbase(main):013:0&gt; 清空表truncate1234567hbase(main):013:0&gt; truncate &apos;myHbase&apos;Truncating &apos;myHbase&apos; table (it may take a while): - Disabling table... - Truncating table...0 row(s) in 3.6760 secondshbase(main):014:0&gt; 删除表drop1234567891011hbase(main):014:0&gt; drop &apos;myHbase&apos;ERROR: Table myHbase is enabled. Disable it first.Here is some help for this command:Drop the named table. Table must first be disabled: hbase&gt; drop &apos;t1&apos; hbase&gt; drop &apos;ns1:t1&apos;hbase(main):015:0&gt; 直接删除表会报错，根据提示需要先停用表 123456789101112hbase(main):015:0&gt; disable &apos;myHbase&apos;0 row(s) in 2.2620 secondshbase(main):016:0&gt; drop &apos;myHbase&apos;0 row(s) in 1.2970 secondshbase(main):017:0&gt; listTABLE 0 row(s) in 0.0110 seconds=&gt; []hbase(main):018:0&gt; HBase表中数据的操作关于数据的操作（增put，删delete，查get + scan, 改==变相的增加） 创建 user 表，包含 info、data 两个列簇 12345hbase(main):018:0&gt; create &apos;user_info&apos;,&#123;NAME=&gt;&apos;base_info&apos;,VERSIONS=&gt;3 &#125;,&#123;NAME=&gt;&apos;extra_info&apos;,VERSIONS=&gt;1 &#125; 0 row(s) in 4.2670 seconds=&gt; Hbase::Table - user_infohbase(main):019:0&gt; 增put查看帮助，需要传入表名，rowkey，列簇名、值等 1234567891011121314151617hbase(main):019:0&gt; help &apos;put&apos;Put a cell &apos;value&apos; at specified table/row/column and optionallytimestamp coordinates. To put a cell value into table &apos;ns1:t1&apos; or &apos;t1&apos;at row &apos;r1&apos; under column &apos;c1&apos; marked with the time &apos;ts1&apos;, do: hbase&gt; put &apos;ns1:t1&apos;, &apos;r1&apos;, &apos;c1&apos;, &apos;value&apos; hbase&gt; put &apos;t1&apos;, &apos;r1&apos;, &apos;c1&apos;, &apos;value&apos; hbase&gt; put &apos;t1&apos;, &apos;r1&apos;, &apos;c1&apos;, &apos;value&apos;, ts1 hbase&gt; put &apos;t1&apos;, &apos;r1&apos;, &apos;c1&apos;, &apos;value&apos;, &#123;ATTRIBUTES=&gt;&#123;&apos;mykey&apos;=&gt;&apos;myvalue&apos;&#125;&#125; hbase&gt; put &apos;t1&apos;, &apos;r1&apos;, &apos;c1&apos;, &apos;value&apos;, ts1, &#123;ATTRIBUTES=&gt;&#123;&apos;mykey&apos;=&gt;&apos;myvalue&apos;&#125;&#125; hbase&gt; put &apos;t1&apos;, &apos;r1&apos;, &apos;c1&apos;, &apos;value&apos;, ts1, &#123;VISIBILITY=&gt;&apos;PRIVATE|SECRET&apos;&#125;The same commands also can be run on a table reference. Suppose you had a referencet to table &apos;t1&apos;, the corresponding command would be: hbase&gt; t.put &apos;r1&apos;, &apos;c1&apos;, &apos;value&apos;, ts1, &#123;ATTRIBUTES=&gt;&#123;&apos;mykey&apos;=&gt;&apos;myvalue&apos;&#125;&#125;hbase(main):020:0&gt; 向 user 表中插入信息，row key 为 user0001，列簇 base_info 中添加 name 列标示符，值为 zhangsan1 1234hbase(main):020:0&gt; put &apos;user_info&apos;, &apos;user0001&apos;, &apos;base_info:name&apos;, &apos;zhangsan1&apos;0 row(s) in 0.2900 secondshbase(main):021:0&gt; 此处可以多添加几条数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152put &apos;user_info&apos;, &apos;zhangsan_20150701_0001&apos;, &apos;base_info:name&apos;, &apos;zhangsan1&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0002&apos;, &apos;base_info:name&apos;, &apos;zhangsan2&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0003&apos;, &apos;base_info:name&apos;, &apos;zhangsan3&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0004&apos;, &apos;base_info:name&apos;, &apos;zhangsan4&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0005&apos;, &apos;base_info:name&apos;, &apos;zhangsan5&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0006&apos;, &apos;base_info:name&apos;, &apos;zhangsan6&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0007&apos;, &apos;base_info:name&apos;, &apos;zhangsan7&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0008&apos;, &apos;base_info:name&apos;, &apos;zhangsan8&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0001&apos;, &apos;base_info:age&apos;, &apos;21&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0002&apos;, &apos;base_info:age&apos;, &apos;22&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0003&apos;, &apos;base_info:age&apos;, &apos;23&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0004&apos;, &apos;base_info:age&apos;, &apos;24&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0005&apos;, &apos;base_info:age&apos;, &apos;25&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0006&apos;, &apos;base_info:age&apos;, &apos;26&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0007&apos;, &apos;base_info:age&apos;, &apos;27&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0008&apos;, &apos;base_info:age&apos;, &apos;28&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0001&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0002&apos;, &apos;extra_info:Hobbies&apos;, &apos;sport&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0003&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0004&apos;, &apos;extra_info:Hobbies&apos;, &apos;sport&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0005&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0006&apos;, &apos;extra_info:Hobbies&apos;, &apos;sport&apos;put &apos;user_info&apos;, &apos;zhangsan_20150701_0007&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0001&apos;, &apos;base_info:name&apos;, &apos;baiyc1&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0002&apos;, &apos;base_info:name&apos;, &apos;baiyc2&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0003&apos;, &apos;base_info:name&apos;, &apos;baiyc3&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0004&apos;, &apos;base_info:name&apos;, &apos;baiyc4&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0005&apos;, &apos;base_info:name&apos;, &apos;baiyc5&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0006&apos;, &apos;base_info:name&apos;, &apos;baiyc6&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0007&apos;, &apos;base_info:name&apos;, &apos;baiyc7&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0008&apos;, &apos;base_info:name&apos;, &apos;baiyc8&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0001&apos;, &apos;base_info:age&apos;, &apos;21&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0002&apos;, &apos;base_info:age&apos;, &apos;22&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0003&apos;, &apos;base_info:age&apos;, &apos;23&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0004&apos;, &apos;base_info:age&apos;, &apos;24&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0005&apos;, &apos;base_info:age&apos;, &apos;25&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0006&apos;, &apos;base_info:age&apos;, &apos;26&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0007&apos;, &apos;base_info:age&apos;, &apos;27&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0008&apos;, &apos;base_info:age&apos;, &apos;28&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0001&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0002&apos;, &apos;extra_info:Hobbies&apos;, &apos;sport&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0003&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0004&apos;, &apos;extra_info:Hobbies&apos;, &apos;sport&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0005&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0006&apos;, &apos;extra_info:Hobbies&apos;, &apos;sport&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0007&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;put &apos;user_info&apos;, &apos;baiyc_20150716_0008&apos;, &apos;extra_info:Hobbies&apos;, &apos;sport&apos; 查get + scan获取 user 表中 row key 为 user0001 的所有信息 123456hbase(main):022:0&gt; get &apos;user_info&apos;, &apos;user0001&apos;COLUMN CELL base_info:name timestamp=1522320801670, value=zhangsan1 1 row(s) in 0.1310 secondshbase(main):023:0&gt; 获取user表中row key为rk0001，info列簇的所有信息 123456hbase(main):025:0&gt; get &apos;user_info&apos;, &apos;rk0001&apos;, &apos;base_info&apos;COLUMN CELL base_info:name timestamp=1522321247732, value=zhangsan 1 row(s) in 0.0320 secondshbase(main):026:0&gt; 查询user_info表中的所有信息 1234567hbase(main):026:0&gt; scan &apos;user_info&apos;ROW COLUMN+CELL rk0001 column=base_info:name, timestamp=1522321247732, value=zhangsan user0001 column=base_info:name, timestamp=1522320801670, value=zhangsan1 2 row(s) in 0.0970 secondshbase(main):027:0&gt; 查询user_info表中列簇为base_info的信息 1234567hbase(main):027:0&gt; scan &apos;user_info&apos;, &#123;COLUMNS =&gt; &apos;base_info&apos;&#125;ROW COLUMN+CELL rk0001 column=base_info:name, timestamp=1522321247732, value=zhangsan user0001 column=base_info:name, timestamp=1522320801670, value=zhangsan1 2 row(s) in 0.0620 secondshbase(main):028:0&gt; 删delete删除user_info表row key为rk0001，列标示符为base_info:name的数据 123456789hbase(main):028:0&gt; delete &apos;user_info&apos;, &apos;rk0001&apos;, &apos;base_info:name&apos;0 row(s) in 0.0780 secondshbase(main):029:0&gt; scan &apos;user_info&apos;, &#123;COLUMNS =&gt; &apos;base_info&apos;&#125;ROW COLUMN+CELL user0001 column=base_info:name, timestamp=1522320801670, value=zhangsan1 1 row(s) in 0.0530 secondshbase(main):030:0&gt;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://zhangfuxin.cn/tags/HBase/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"HBase学习之路 （二）HBase集群安装","slug":"2018-06-02-HBase学习之路 （二）HBase集群安装","date":"2018-06-02T02:30:04.000Z","updated":"2019-09-19T06:24:02.462Z","comments":true,"path":"2018-06-02-HBase学习之路 （二）HBase集群安装.html","link":"","permalink":"http://zhangfuxin.cn/2018-06-02-HBase学习之路 （二）HBase集群安装.html","excerpt":"** HBase学习之路 （二）HBase集群安装：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （二）HBase集群安装","text":"** HBase学习之路 （二）HBase集群安装：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （二）HBase集群安装 &lt;The rest of contents | 余下全文&gt; 前提1、HBase 依赖于 HDFS 做底层的数据存储 2、HBase 依赖于 MapReduce 做数据计算 3、HBase 依赖于 ZooKeeper 做服务协调 4、HBase源码是java编写的，安装需要依赖JDK 版本选择打开官方的版本说明http://hbase.apache.org/1.2/book.html JDK的选择 Hadoop的选择 此处我们的hadoop版本用的的是2.7.5，HBase选择的版本是1.2.6 安装1、zookeeper的安装参考http://www.cnblogs.com/qingyunzong/p/8619184.html 2、Hadoopd的安装参考http://www.cnblogs.com/qingyunzong/p/8634335.html 3、下载安装包找到官网下载 hbase 安装包 hbase-1.2.6-bin.tar.gz，这里给大家提供一个下载地址： http://mirrors.hust.edu.cn/apache/hbase/ 4、上传服务器并解压缩到指定目录123[hadoop@hadoop1 ~]$ lsapps data hbase-1.2.6-bin.tar.gz hello.txt log zookeeper.out[hadoop@hadoop1 ~]$ tar -zxvf hbase-1.2.6-bin.tar.gz -C apps/ 5、修改配置文件配置文件目录在安装包的conf文件夹中 （1）修改hbase-env.sh12[hadoop@hadoop1 conf]$ vi hbase-env.shexport JAVA_HOME=/usr/local/jdk1.8.0_73export HBASE_MANAGES_ZK=false （2）修改hbase-site.xml1[hadoop@hadoop1 conf]$ vi hbase-site.xml 12345678910111213141516171819&lt;configuration&gt; &lt;property&gt; &lt;!-- 指定 hbase 在 HDFS 上存储的路径 --&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://myha01/hbase126&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定 hbase 是分布式的 --&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定 zk 的地址，多个用“,”分割 --&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop1:2181,hadoop2:2181,hadoop3:2181,hadoop4:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; （3）修改regionservers12345[hadoop@hadoop1 conf]$ vi regionservers hadoop1hadoop2hadoop3hadoop4 （4）修改backup-masters该文件是不存在的，先自行创建 12[hadoop@hadoop1 conf]$ vi backup-mastershadoop4 （5）修改hdfs-site.xml 和 core-site.xml最重要一步，要把 hadoop 的 hdfs-site.xml 和 core-site.xml 放到 hbase-1.2.6/conf 下 12[hadoop@hadoop1 conf]$ cd ~/apps/hadoop-2.7.5/etc/hadoop/[hadoop@hadoop1 hadoop]$ cp core-site.xml hdfs-site.xml ~/apps/hbase-1.2.6/conf/ 6、将HBase安装包分发到其他节点分发之前先删除HBase目录下的docs文件夹， 1[hadoop@hadoop1 hbase-1.2.6]$ rm -rf docs/ 在进行分发 123[hadoop@hadoop1 apps]$ scp -r hbase-1.2.6/ hadoop2:$PWD[hadoop@hadoop1 apps]$ scp -r hbase-1.2.6/ hadoop3:$PWD[hadoop@hadoop1 apps]$ scp -r hbase-1.2.6/ hadoop4:$PWD 7、 同步时间HBase 集群对于时间的同步要求的比 HDFS 严格，所以，集群启动之前千万记住要进行 时间同步，要求相差不要超过 30s 8、配置环境变量所有服务器都有进行配置 1234[hadoop@hadoop1 apps]$ vi ~/.bashrc #HBaseexport HBASE_HOME=/home/hadoop/apps/hbase-1.2.6export PATH=$PATH:$HBASE_HOME/bin 使环境变量立即生效 1[hadoop@hadoop1 apps]$ source ~/.bashrc 启动HBase集群严格按照启动顺序进行 1、启动zookeeper集群每个zookeeper节点都要执行以下命令 12345[hadoop@hadoop1 apps]$ zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /home/hadoop/apps/zookeeper-3.4.10/bin/../conf/zoo.cfgStarting zookeeper ... STARTED[hadoop@hadoop1 apps]$ 2、启动HDFS集群及YARN集群如果需要运行MapReduce程序则启动yarn集群，否则不需要启动 12345678910111213141516[hadoop@hadoop1 apps]$ start-dfs.shStarting namenodes on [hadoop1 hadoop2]hadoop2: starting namenode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-namenode-hadoop2.outhadoop1: starting namenode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-namenode-hadoop1.outhadoop3: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop3.outhadoop4: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop4.outhadoop2: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop2.outhadoop1: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop1.outStarting journal nodes [hadoop1 hadoop2 hadoop3]hadoop3: starting journalnode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-journalnode-hadoop3.outhadoop2: starting journalnode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-journalnode-hadoop2.outhadoop1: starting journalnode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-journalnode-hadoop1.outStarting ZK Failover Controllers on NN hosts [hadoop1 hadoop2]hadoop2: starting zkfc, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-zkfc-hadoop2.outhadoop1: starting zkfc, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-zkfc-hadoop1.out[hadoop@hadoop1 apps]$ 启动完成之后检查以下namenode的状态 12345[hadoop@hadoop1 apps]$ hdfs haadmin -getServiceState nn1standby[hadoop@hadoop1 apps]$ hdfs haadmin -getServiceState nn2active[hadoop@hadoop1 apps]$ 3、启动HBase保证 ZooKeeper 集群和 HDFS 集群启动正常的情况下启动 HBase 集群 启动命令：start-hbase.sh，在哪台节点上执行此命令，哪个节点就是主节点 12345678910111213141516[hadoop@hadoop1 conf]$ start-hbase.shstarting master, logging to /home/hadoop/apps/hbase-1.2.6/logs/hbase-hadoop-master-hadoop1.outJava HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0hadoop3: starting regionserver, logging to /home/hadoop/apps/hbase-1.2.6/logs/hbase-hadoop-regionserver-hadoop3.outhadoop4: starting regionserver, logging to /home/hadoop/apps/hbase-1.2.6/logs/hbase-hadoop-regionserver-hadoop4.outhadoop2: starting regionserver, logging to /home/hadoop/apps/hbase-1.2.6/logs/hbase-hadoop-regionserver-hadoop2.outhadoop3: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0hadoop3: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0hadoop4: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0hadoop4: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0hadoop2: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0hadoop2: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0hadoop1: starting regionserver, logging to /home/hadoop/apps/hbase-1.2.6/logs/hbase-hadoop-regionserver-hadoop1.outhadoop4: starting master, logging to /home/hadoop/apps/hbase-1.2.6/logs/hbase-hadoop-master-hadoop4.out[hadoop@hadoop1 conf]$ 观看启动日志可以看到： （1）首先在命令执行节点启动 master （2）然后分别在 hadoop02,hadoop03,hadoop04,hadoop05 启动 regionserver （3）然后在 backup-masters 文件中配置的备节点上再启动一个 master 主进程 验证启动是否正常1、检查各进程是否启动正常 主节点和备用节点都启动 hmaster 进程 各从节点都启动 hregionserver 进程 按照对应的配置信息各个节点应该要启动的进程如上图所示 2、通过访问浏览器页面hadoop1 hadop4 从图中可以看出hadoop4是备用节点 3、验证高可用干掉hadoop1上的hbase进程，观察备用节点是否启用 12345678910[hadoop@hadoop1 conf]$ jps4960 HMaster2960 QuorumPeerMain3169 NameNode3699 DFSZKFailoverController3285 DataNode5098 HRegionServer5471 Jps3487 JournalNode[hadoop@hadoop1 conf]$ kill -9 4960 hadoop1界面访问不了 hadoop4变成主节点 4、如果有节点相应的进程没有启动，那么可以手动启动启动HMaster进程 123456789101112131415161718[hadoop@hadoop3 conf]$ jps3360 Jps2833 JournalNode2633 QuorumPeerMain3179 HRegionServer2732 DataNode[hadoop@hadoop3 conf]$ hbase-daemon.sh start masterstarting master, logging to /home/hadoop/apps/hbase-1.2.6/logs/hbase-hadoop-master-hadoop3.outJava HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0[hadoop@hadoop3 conf]$ jps2833 JournalNode3510 Jps3432 HMaster2633 QuorumPeerMain3179 HRegionServer2732 DataNode[hadoop@hadoop3 conf]$ 启动HRegionServer进程 1[hadoop@hadoop3 conf]$ hbase-daemon.sh start regionserver","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://zhangfuxin.cn/tags/HBase/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"HBase学习之路 （一）HBase基础介绍","slug":"2018-06-01-HBase学习之路 （一）HBase基础介绍","date":"2018-06-01T02:30:04.000Z","updated":"2019-09-19T06:21:23.208Z","comments":true,"path":"2018-06-01-HBase学习之路 （一）HBase基础介绍.html","link":"","permalink":"http://zhangfuxin.cn/2018-06-01-HBase学习之路 （一）HBase基础介绍.html","excerpt":"** HBase学习之路 （一）HBase基础介绍：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （一）HBase基础介绍","text":"** HBase学习之路 （一）HBase基础介绍：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （一）HBase基础介绍 &lt;The rest of contents | 余下全文&gt; 产生背景自 1970 年以来，关系数据库用于数据存储和维护有关问题的解决方案。大数据的出现后， 好多公司实现处理大数据并从中受益，并开始选择像 Hadoop 的解决方案。Hadoop 使用分 布式文件系统，用于存储大数据，并使用 MapReduce 来处理。Hadoop 擅长于存储各种格式 的庞大的数据，任意的格式甚至非结构化的处理。 Hadoop 的限制 Hadoop 只能执行批量处理，并且只以顺序方式访问数据。这意味着必须搜索整个数据集， 即使是最简单的搜索工作。 当处理结果在另一个庞大的数据集，也是按顺序处理一个巨大的数据集。在这一点上，一个 新的解决方案，需要访问数据中的任何点（随机访问）单元。 Hadoop 随机存取数据库 应用程序，如 HBase，Cassandra，CouchDB，Dynamo 和 MongoDB 都是一些存储大量数据和 以随机方式访问数据的数据库。 总结： （1）海量数据量存储成为瓶颈，单台机器无法负载大量数据 （2）单台机器 IO 读写请求成为海量数据存储时候高并发大规模请求的瓶颈 （3）随着数据规模越来越大，大量业务场景开始考虑数据存储横向水平扩展，使得存储服 务可以增加/删除，而目前的关系型数据库更专注于一台机器 HBase简介HBase 是 BigTable 的开源（源码使用 Java 编写）版本。是 Apache Hadoop 的数据库，是建 立在 HDFS 之上，被设计用来提供高可靠性、高性能、列存储、可伸缩、多版本的 NoSQL 的分布式数据存储系统，实现对大型数据的实时、随机的读写访问。 HBase 依赖于 HDFS 做底层的数据存储，BigTable 依赖 Google GFS 做数据存储 HBase 依赖于 MapReduce 做数据计算，BigTable 依赖 Google MapReduce 做数据计算 HBase 依赖于 ZooKeeper 做服务协调，BigTable 依赖 Google Chubby 做服务协调 NoSQL = NO SQL NoSQL = Not Only SQL：会有一些把 NoSQL 数据的原生查询语句封装成 SQL，比如 HBase 就有 Phoenix 工具 关系型数据库 和 非关系型数据库的典型代表NoSQL：hbase, redis, mongodb RDBMS：mysql,oracle,sql server,db2 HBase 这个 NoSQL 数据库的要点① 它介于 NoSQL 和 RDBMS 之间，仅能通过主键(rowkey)和主键的 range 来检索数据 ② HBase 查询数据功能很简单，不支持 join 等复杂操作 ③ 不支持复杂的事务，只支持行级事务(可通过 hive 支持来实现多表 join 等复杂操作)。 ④ HBase 中支持的数据类型：byte[]（底层所有数据的存储都是字节数组） ⑤ 主要用来存储结构化和半结构化的松散数据。 结构化、半结构化和非结构化结构化：数据结构字段含义确定，清晰，典型的如数据库中的表结构 半结构化：具有一定结构，但语义不够确定，典型的如 HTML 网页，有些字段是确定的(title)， 有些不确定(table) 非结构化：杂乱无章的数据，很难按照一个概念去进行抽取，无规律性 与 Hadoop 一样，HBase 目标主要依靠横向扩展，通过不断增加廉价的商用服务器，来增加 计算和存储能力。 HBase 中的表特点1、大：一个表可以有上十亿行，上百万列 2、面向列：面向列(族)的存储和权限控制，列(簇)独立检索。 3、稀疏：对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。 4、无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一 张表中不同的行可以有截然不同的列 HBase表结构逻辑视图初次接触HBase，可能看到以下描述会懵：“基于列存储”，“稀疏MAP”，“RowKey”,“ColumnFamily”。 其实没那么高深，我们需要分两步来理解HBase, 就能够理解为什么HBase能够“快速地”“分布式地”处理“大量数据”了。 1.内存结构 2.文件存储结构 名词概念加入我们有如下一张表 Rowkey的概念Rowkey的概念和mysql中的主键是完全一样的，Hbase使用Rowkey来唯一的区分某一行的数据。 由于Hbase只支持3中查询方式： 1、基于Rowkey的单行查询 2、基于Rowkey的范围扫描 3、全表扫描 因此，Rowkey对Hbase的性能影响非常大，Rowkey的设计就显得尤为的重要。设计的时候要兼顾基于Rowkey的单行查询也要键入Rowkey的范围扫描。具体Rowkey要如何设计后续会整理相关的文章做进一步的描述。这里大家只要有一个概念就是Rowkey的设计极为重要。 rowkey 行键可以是任意字符串(最大长度是 64KB，实际应用中长度一般为 10-100bytes)，最好是 16。在 HBase 内部，rowkey 保存为字节数组。HBase 会对表中的数据按照 rowkey 排序 (字典顺序) Column的概念列，可理解成MySQL列。 ColumnFamily的概念列族, HBase引入的概念。 Hbase通过列族划分数据的存储，列族下面可以包含任意多的列，实现灵活的数据存取。就像是家族的概念，我们知道一个家族是由于很多个的家庭组成的。列族也类似，列族是由一个一个的列组成（任意多）。 Hbase表的创建的时候就必须指定列族。就像关系型数据库创建的时候必须指定具体的列是一样的。 Hbase的列族不是越多越好，官方推荐的是列族最好小于或者等于3。我们使用的场景一般是1个列族。 TimeStamp的概念TimeStamp对Hbase来说至关重要，因为它是实现Hbase多版本的关键。在Hbase中使用不同的timestame来标识相同rowkey行对应的不通版本的数据。 HBase 中通过 rowkey 和 columns 确定的为一个存储单元称为 cell。每个 cell 都保存着同一份 数据的多个版本。版本通过时间戳来索引。时间戳的类型是 64 位整型。时间戳可以由 hbase(在数据写入时 自动)赋值，此时时间戳是精确到毫秒的当前系统时间。时间戳也可以由 客户显式赋值。如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。 每个 cell 中，不同版本的数据按照时间 倒序排序，即最新的数据排在最前面。 为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，hbase 提供了两种数据版 本回收方式： 保存数据的最后 n 个版本 保存最近一段时间内的版本（设置数据的生命周期 TTL）。 用户可以针对每个列簇进行设置。 单元格（Cell）由{rowkey, column( = + ), version} 唯一确定的单元。 Cell 中的数据是没有类型的，全部是字节码形式存贮。","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://zhangfuxin.cn/tags/HBase/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Kafka学习之路 （五）Kafka在zookeeper中的存储","slug":"2018-05-08-Kafka学习之路 （五）Kafka在zookeeper中的存储","date":"2018-05-08T02:30:04.000Z","updated":"2019-09-19T05:55:00.451Z","comments":true,"path":"2018-05-08-Kafka学习之路 （五）Kafka在zookeeper中的存储.html","link":"","permalink":"http://zhangfuxin.cn/2018-05-08-Kafka学习之路 （五）Kafka在zookeeper中的存储.html","excerpt":"** Kafka学习之路 （五）Kafka在zookeeper中的存储：** &lt;Excerpt in index | 首页摘要&gt; ​ Kafka学习之路 （五）Kafka在zookeeper中的存储","text":"** Kafka学习之路 （五）Kafka在zookeeper中的存储：** &lt;Excerpt in index | 首页摘要&gt; ​ Kafka学习之路 （五）Kafka在zookeeper中的存储 &lt;The rest of contents | 余下全文&gt; 一、Kafka在zookeeper中存储结构图 二、分析2.1 topic注册信息/brokers/topics/[topic] : 存储某个topic的partitions所有分配信息 1[zk: localhost:2181(CONNECTED) 1] get /brokers/topics/topic2 12345678910111213141516171819202122Schema:&#123; &quot;version&quot;: &quot;版本编号目前固定为数字1&quot;, &quot;partitions&quot;: &#123; &quot;partitionId编号&quot;: [ 同步副本组brokerId列表 ], &quot;partitionId编号&quot;: [ 同步副本组brokerId列表 ], ....... &#125;&#125;Example:&#123;&quot;version&quot;: 1,&quot;partitions&quot;: &#123;&quot;2&quot;: [1, 2, 3],&quot;1&quot;: [0, 1, 2],&quot;0&quot;: [3, 0, 1],&#125;&#125; 2.2 partition状态信息/brokers/topics/[topic]/partitions/[0…N] 其中[0..N]表示partition索引号 /brokers/topics/[topic]/partitions/[partitionId]/state 1234567891011121314151617Schema:&#123;&quot;controller_epoch&quot;: 表示kafka集群中的中央控制器选举次数,&quot;leader&quot;: 表示该partition选举leader的brokerId,&quot;version&quot;: 版本编号默认为1,&quot;leader_epoch&quot;: 该partition leader选举次数,&quot;isr&quot;: [同步副本组brokerId列表]&#125; Example:&#123;&quot;controller_epoch&quot;: 1,&quot;leader&quot;: 3,&quot;version&quot;: 1,&quot;leader_epoch&quot;: 0,&quot;isr&quot;: [3, 0, 1]&#125; 2.3 Broker注册信息/brokers/ids/[0…N] 每个broker的配置文件中都需要指定一个数字类型的id(全局不可重复),此节点为临时znode(EPHEMERAL) 1234567891011121314151617Schema:&#123;&quot;jmx_port&quot;: jmx端口号,&quot;timestamp&quot;: kafka broker初始启动时的时间戳,&quot;host&quot;: 主机名或ip地址,&quot;version&quot;: 版本编号默认为1,&quot;port&quot;: kafka broker的服务端端口号,由server.properties中参数port确定&#125; Example:&#123;&quot;jmx_port&quot;: -1,&quot;timestamp&quot;:&quot;1525741823119&quot;&quot;version&quot;: 1,&quot;host&quot;: &quot;hadoop1&quot;,&quot;port&quot;: 9092&#125; 2.4 Controller epoch/controller_epoch –&gt; int (epoch) 此值为一个数字,kafka集群中第一个broker第一次启动时为1，以后只要集群中center controller中央控制器所在broker变更或挂掉，就会重新选举新的center controller，每次center controller变更controller_epoch值就会 + 1; 2.5 Controller注册信息/controller -&gt; int (broker id of the controller) 存储center controller中央控制器所在kafka broker的信息 12345678910111213Schema:&#123;&quot;version&quot;: 版本编号默认为1,&quot;brokerid&quot;: kafka集群中broker唯一编号,&quot;timestamp&quot;: kafka broker中央控制器变更时的时间戳&#125; Example:&#123;&quot;version&quot;: 1,&quot;brokerid&quot;: 0,&quot;timestamp&quot;: &quot;1525741822769&quot;&#125; 2.6 补充Consumer and Consumer group a.每个consumer客户端被创建时,会向zookeeper注册自己的信息;b.此作用主要是为了”负载均衡”.c.同一个Consumer Group中的Consumers，Kafka将相应Topic中的每个消息只发送给其中一个Consumer。d.Consumer Group中的每个Consumer读取Topic的一个或多个Partitions，并且是唯一的Consumer；e.一个Consumer group的多个consumer的所有线程依次有序地消费一个topic的所有partitions,如果Consumer group中所有consumer总线程大于partitions数量，则会出现空闲情况; 举例说明： kafka集群中创建一个topic为report-log 4 partitions 索引编号为0,1,2,3 假如有目前有三个消费者node：注意–&gt;一个consumer中一个消费线程可以消费一个或多个partition. 如果每个consumer创建一个consumer thread线程,各个node消费情况如下，node1消费索引编号为0,1分区，node2费索引编号为2,node3费索引编号为3 如果每个consumer创建2个consumer thread线程，各个node消费情况如下(是从consumer node先后启动状态来确定的)，node1消费索引编号为0,1分区；node2费索引编号为2,3；node3为空闲状态 总结：从以上可知，Consumer Group中各个consumer是根据先后启动的顺序有序消费一个topic的所有partitions的。 如果Consumer Group中所有consumer的总线程数大于partitions数量，则可能consumer thread或consumer会出现空闲状态。 2.7 Consumer均衡算法当一个group中,有consumer加入或者离开时,会触发partitions均衡.均衡的最终目的,是提升topic的并发消费能力.1) 假如topic1,具有如下partitions: P0,P1,P2,P32) 加入group中,有如下consumer: C0,C13) 首先根据partition索引号对partitions排序: P0,P1,P2,P34) 根据(consumer.id + ‘-‘+ thread序号)排序: C0,C15) 计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整)6) 然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1) * M -1)] 2.8 Consumer注册信息每个consumer都有一个唯一的ID(consumerId可以通过配置文件指定,也可以由系统生成),此id用来标记消费者信息. /consumers/[groupId]/ids/[consumerIdString] 是一个临时的znode,此节点的值为请看consumerIdString产生规则,即表示此consumer目前所消费的topic + partitions列表. consumerId产生规则： StringconsumerUuid = null; if(config.consumerId!=null &amp;&amp; config.consumerId) consumerUuid = consumerId; else { String uuid = UUID.randomUUID() consumerUuid = “%s-%d-%s”.format( InetAddress.getLocalHost.getHostName, System.currentTimeMillis, uuid.getMostSignificantBits().toHexString.substring(0,8)); ​ }​ String consumerIdString = config.groupId + “_” + consumerUuid; 1[zk: localhost:2181(CONNECTED) 11] get /consumers/console-consumer-2304/ids/console-consumer-2304_hadoop2-1525747915241-6b48ff32 12345678910111213141516171819Schema:&#123;&quot;version&quot;: 版本编号默认为1,&quot;subscription&quot;: &#123; //订阅topic列表&quot;topic名称&quot;: consumer中topic消费者线程数&#125;,&quot;pattern&quot;: &quot;static&quot;,&quot;timestamp&quot;: &quot;consumer启动时的时间戳&quot;&#125; Example:&#123;&quot;version&quot;: 1,&quot;subscription&quot;: &#123;&quot;topic2&quot;: 1&#125;,&quot;pattern&quot;: &quot;white_list&quot;,&quot;timestamp&quot;: &quot;1525747915336&quot;&#125; 2.9 Consumer owner/consumers/[groupId]/owners/[topic]/[partitionId] -&gt; consumerIdString + threadId索引编号 a) 首先进行”Consumer Id注册”; b) 然后在”Consumer id 注册”节点下注册一个watch用来监听当前group中其他consumer的”退出”和”加入”;只要此znode path下节点列表变更,都会触发此group下consumer的负载均衡.(比如一个consumer失效,那么其他consumer接管partitions). c) 在”Broker id 注册”节点下,注册一个watch用来监听broker的存活情况;如果broker列表变更,将会触发所有的groups下的consumer重新balance. 2.10 Consumer offset/consumers/[groupId]/offsets/[topic]/[partitionId] -&gt; long (offset) 用来跟踪每个consumer目前所消费的partition中最大的offset 此znode为持久节点,可以看出offset跟group_id有关,以表明当消费者组(consumer group)中一个消费者失效, 重新触发balance,其他consumer可以继续消费. 2.11 Re-assign partitions/admin/reassign_partitions 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&#123; &quot;fields&quot;:[ &#123; &quot;name&quot;:&quot;version&quot;, &quot;type&quot;:&quot;int&quot;, &quot;doc&quot;:&quot;version id&quot; &#125;, &#123; &quot;name&quot;:&quot;partitions&quot;, &quot;type&quot;:&#123; &quot;type&quot;:&quot;array&quot;, &quot;items&quot;:&#123; &quot;fields&quot;:[ &#123; &quot;name&quot;:&quot;topic&quot;, &quot;type&quot;:&quot;string&quot;, &quot;doc&quot;:&quot;topic of the partition to be reassigned&quot; &#125;, &#123; &quot;name&quot;:&quot;partition&quot;, &quot;type&quot;:&quot;int&quot;, &quot;doc&quot;:&quot;the partition to be reassigned&quot; &#125;, &#123; &quot;name&quot;:&quot;replicas&quot;, &quot;type&quot;:&quot;array&quot;, &quot;items&quot;:&quot;int&quot;, &quot;doc&quot;:&quot;a list of replica ids&quot; &#125; ], &#125; &quot;doc&quot;:&quot;an array of partitions to be reassigned to new replicas&quot; &#125; &#125; ]&#125; Example:&#123; &quot;version&quot;: 1, &quot;partitions&quot;: [ &#123; &quot;topic&quot;: &quot;Foo&quot;, &quot;partition&quot;: 1, &quot;replicas&quot;: [0, 1, 3] &#125; ] &#125; 2.12 Preferred replication election/admin/preferred_replica_election 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&#123; &quot;fields&quot;:[ &#123; &quot;name&quot;:&quot;version&quot;, &quot;type&quot;:&quot;int&quot;, &quot;doc&quot;:&quot;version id&quot; &#125;, &#123; &quot;name&quot;:&quot;partitions&quot;, &quot;type&quot;:&#123; &quot;type&quot;:&quot;array&quot;, &quot;items&quot;:&#123; &quot;fields&quot;:[ &#123; &quot;name&quot;:&quot;topic&quot;, &quot;type&quot;:&quot;string&quot;, &quot;doc&quot;:&quot;topic of the partition for which preferred replica election should be triggered&quot; &#125;, &#123; &quot;name&quot;:&quot;partition&quot;, &quot;type&quot;:&quot;int&quot;, &quot;doc&quot;:&quot;the partition for which preferred replica election should be triggered&quot; &#125; ], &#125; &quot;doc&quot;:&quot;an array of partitions for which preferred replica election should be triggered&quot; &#125; &#125; ]&#125; 例子: &#123; &quot;version&quot;: 1, &quot;partitions&quot;: [ &#123; &quot;topic&quot;: &quot;Foo&quot;, &quot;partition&quot;: 1 &#125;, &#123; &quot;topic&quot;: &quot;Bar&quot;, &quot;partition&quot;: 0 &#125; ] &#125; 2.13 删除topics/admin/delete_topics 12345678910111213Schema:&#123; &quot;fields&quot;: [ &#123;&quot;name&quot;: &quot;version&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;version id&quot;&#125;, &#123;&quot;name&quot;: &quot;topics&quot;, &quot;type&quot;: &#123; &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;string&quot;, &quot;doc&quot;: &quot;an array of topics to be deleted&quot;&#125; &#125; ]&#125; 例子:&#123; &quot;version&quot;: 1, &quot;topics&quot;: [&quot;foo&quot;, &quot;bar&quot;]&#125; 2.14 Topic配置/config/topics/[topic_name]","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"http://zhangfuxin.cn/tags/Flume/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Kafka学习之路 （四）Kafka的安装","slug":"2018-05-07-Kafka学习之路 （四）Kafka的安装","date":"2018-05-07T02:30:04.000Z","updated":"2019-09-19T05:51:15.109Z","comments":true,"path":"2018-05-07-Kafka学习之路 （四）Kafka的安装.html","link":"","permalink":"http://zhangfuxin.cn/2018-05-07-Kafka学习之路 （四）Kafka的安装.html","excerpt":"** Kafka学习之路 （四）Kafka的安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Kafka学习之路 （四）Kafka的安装","text":"** Kafka学习之路 （四）Kafka的安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Kafka学习之路 （四）Kafka的安装 &lt;The rest of contents | 余下全文&gt; 一、下载下载地址： http://kafka.apache.org/downloads.html http://mirrors.hust.edu.cn/apache/ 二、安装前提（zookeeper安装）参考http://www.cnblogs.com/qingyunzong/p/8634335.html#_label4_0 三、安装此处使用版本为kafka_2.11-0.8.2.0.tgz 2.1 上传解压缩123[hadoop@hadoop1 ~]$ tar -zxvf kafka_2.11-0.8.2.0.tgz -C apps[hadoop@hadoop1 ~]$ cd apps/[hadoop@hadoop1 apps]$ ln -s kafka_2.11-0.8.2.0/ kafka 2.2 修改配置文件进入kafka的安装配置目录 1[hadoop@hadoop1 ~]$ cd apps/kafka/config/ 主要关注：server.properties 这个文件即可，我们可以发现在目录下： 有很多文件，这里可以发现有Zookeeper文件，我们可以根据Kafka内带的zk集群来启动，但是建议使用独立的zk集群 server.properties（broker.id和host.name每个节点都不相同） 1234567891011121314151617181920212223242526272829303132333435//当前机器在集群中的唯一标识，和zookeeper的myid性质一样broker.id=0//当前kafka对外提供服务的端口默认是9092port=9092//这个参数默认是关闭的，在0.8.1有个bug，DNS解析问题，失败率的问题。host.name=hadoop1//这个是borker进行网络处理的线程数num.network.threads=3//这个是borker进行I/O处理的线程数num.io.threads=8//发送缓冲区buffer大小，数据不是一下子就发送的，先回存储到缓冲区了到达一定的大小后在发送，能提高性能socket.send.buffer.bytes=102400//kafka接收缓冲区大小，当数据到达一定大小后在序列化到磁盘socket.receive.buffer.bytes=102400//这个参数是向kafka请求消息或者向kafka发送消息的请请求的最大数，这个值不能超过java的堆栈大小socket.request.max.bytes=104857600//消息存放的目录，这个目录可以配置为“，”逗号分割的表达式，上面的num.io.threads要大于这个目录的个数这个目录，//如果配置多个目录，新创建的topic他把消息持久化的地方是，当前以逗号分割的目录中，那个分区数最少就放那一个log.dirs=/home/hadoop/log/kafka-logs//默认的分区数，一个topic默认1个分区数num.partitions=1//每个数据目录用来日志恢复的线程数目num.recovery.threads.per.data.dir=1//默认消息的最大持久化时间，168小时，7天log.retention.hours=168//这个参数是：因为kafka的消息是以追加的形式落地到文件，当超过这个值的时候，kafka会新起一个文件log.segment.bytes=1073741824//每隔300000毫秒去检查上面配置的log失效时间log.retention.check.interval.ms=300000//是否启用log压缩，一般不用启用，启用的话可以提高性能log.cleaner.enable=false//设置zookeeper的连接端口zookeeper.connect=192.168.123.102:2181,192.168.123.103:2181,192.168.123.104:2181//设置zookeeper的连接超时时间zookeeper.connection.timeout.ms=6000 producer.properties 1metadata.broker.list=192.168.123.102:9092,192.168.123.103:9092,192.168.123.104:9092 consumer.properties 1zookeeper.connect=192.168.123.102:2181,192.168.123.103:2181,192.168.123.104:2181 2.3 将kafka的安装包分发到其他节点123[hadoop@hadoop1 apps]$ scp -r kafka_2.11-0.8.2.0/ hadoop2:$PWD[hadoop@hadoop1 apps]$ scp -r kafka_2.11-0.8.2.0/ hadoop3:$PWD[hadoop@hadoop1 apps]$ scp -r kafka_2.11-0.8.2.0/ hadoop4:$PWD 2.4 创建软连接1[hadoop@hadoop1 apps]$ ln -s kafka_2.11-0.8.2.0/ kafka 2.5 修改环境变量1234[hadoop@hadoop1 ~]$ vi .bashrc #Kafkaexport KAFKA_HOME=/home/hadoop/apps/kafkaexport PATH=$PATH:$KAFKA_HOME/bin 保存使其立即生效 1[hadoop@hadoop1 ~]$ source ~/.bashrc 三、启动3.1 首先启动zookeeper集群所有zookeeper节点都需要执行 1[hadoop@hadoop1 ~]$ zkServer.sh start 3.2 启动Kafka集群服务1[hadoop@hadoop1 kafka]$ bin/kafka-server-start.sh config/server.properties hadoop1 Hadoop2 hadoop3 hadoop4 3.3 创建的topic1[hadoop@hadoop1 kafka]$ bin/kafka-topics.sh --create --zookeeper hadoop1:2181 --replication-factor 3 --partitions 3 --topic topic2 3.4 查看topic副本信息1[hadoop@hadoop1 kafka]$ bin/kafka-topics.sh --describe --zookeeper hadoop1:2181 --topic topic2 3.5 查看已经创建的topic信息1[hadoop@hadoop1 kafka]$ bin/kafka-topics.sh --list --zookeeper hadoop1:2181 3.6 生产者发送消息1[hadoop@hadoop1 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop1:9092 --topic topic2 hadoop1显示接收到消息 3.7 消费者消费消息在hadoop2上消费消息 1[hadoop@hadoop2 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop1:2181 --from-beginning --topic topic2","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"http://zhangfuxin.cn/tags/Flume/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Kafka学习之路 （三）Kafka的高可用","slug":"2018-05-06-Kafka学习之路 （三）Kafka的高可用","date":"2018-05-06T02:30:04.000Z","updated":"2019-09-19T05:49:24.858Z","comments":true,"path":"2018-05-06-Kafka学习之路 （三）Kafka的高可用.html","link":"","permalink":"http://zhangfuxin.cn/2018-05-06-Kafka学习之路 （三）Kafka的高可用.html","excerpt":"** Kafka学习之路 （三）Kafka的高可用：** &lt;Excerpt in index | 首页摘要&gt; ​ Kafka学习之路 （三）Kafka的高可用","text":"** Kafka学习之路 （三）Kafka的高可用：** &lt;Excerpt in index | 首页摘要&gt; ​ Kafka学习之路 （三）Kafka的高可用 &lt;The rest of contents | 余下全文&gt; 一、高可用的由来1.1 为何需要Replication 在Kafka在0.8以前的版本中，是没有Replication的，一旦某一个Broker宕机，则其上所有的Partition数据都不可被消费，这与Kafka数据持久性及Delivery Guarantee的设计目标相悖。同时Producer都不能再将数据存于这些Partition中。 如果Producer使用同步模式则Producer会在尝试重新发送message.send.max.retries（默认值为3）次后抛出Exception，用户可以选择停止发送后续数据也可选择继续选择发送。而前者会造成数据的阻塞，后者会造成本应发往该Broker的数据的丢失。 如果Producer使用异步模式，则Producer会尝试重新发送message.send.max.retries（默认值为3）次后记录该异常并继续发送后续数据，这会造成数据丢失并且用户只能通过日志发现该问题。同时，Kafka的Producer并未对异步模式提供callback接口。 由此可见，在没有Replication的情况下，一旦某机器宕机或者某个Broker停止工作则会造成整个系统的可用性降低。随着集群规模的增加，整个集群中出现该类异常的几率大大增加，因此对于生产系统而言Replication机制的引入非常重要。 1.2 Leader Election 引入Replication之后，同一个Partition可能会有多个Replica，而这时需要在这些Replication之间选出一个Leader，Producer和Consumer只与这个Leader交互，其它Replica作为Follower从Leader中复制数据。 因为需要保证同一个Partition的多个Replica之间的数据一致性（其中一个宕机后其它Replica必须要能继续服务并且即不能造成数据重复也不能造成数据丢失）。如果没有一个Leader，所有Replica都可同时读/写数据，那就需要保证多个Replica之间互相（N×N条通路）同步数据，数据的一致性和有序性非常难保证，大大增加了Replication实现的复杂性，同时也增加了出现异常的几率。而引入Leader后，只有Leader负责数据读写，Follower只向Leader顺序Fetch数据（N条通路），系统更加简单且高效。 二、Kafka HA设计解析2.1 如何将所有Replica均匀分布到整个集群为了更好的做负载均衡，Kafka尽量将所有的Partition均匀分配到整个集群上。一个典型的部署方式是一个Topic的Partition数量大于Broker的数量。同时为了提高Kafka的容错能力，也需要将同一个Partition的Replica尽量分散到不同的机器。实际上，如果所有的Replica都在同一个Broker上，那一旦该Broker宕机，该Partition的所有Replica都无法工作，也就达不到HA的效果。同时，如果某个Broker宕机了，需要保证它上面的负载可以被均匀的分配到其它幸存的所有Broker上。 Kafka分配Replica的算法如下： 1.将所有Broker（假设共n个Broker）和待分配的Partition排序 2.将第i个Partition分配到第（i mod n）个Broker上 3.将第i个Partition的第j个Replica分配到第（(i + j) mode n）个Broker上 2.2 Data Replication（副本策略）Kafka的高可靠性的保障来源于其健壮的副本（replication）策略。 2.2.1 消息传递同步策略Producer在发布消息到某个Partition时，先通过ZooKeeper找到该Partition的Leader，然后无论该Topic的Replication Factor为多少，Producer只将该消息发送到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。一旦Leader收到了ISR中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW并且向Producer发送ACK。 为了提高性能，每个Follower在接收到数据后就立马向Leader发送ACK，而非等到数据写入Log中。因此，对于已经commit的消息，Kafka只能保证它被存于多个Replica的内存中，而不能保证它们被持久化到磁盘中，也就不能完全保证异常发生后该条消息一定能被Consumer消费。 Consumer读消息也是从Leader读取，只有被commit过的消息才会暴露给Consumer。 Kafka Replication的数据流如下图所示： 2.2.2 ACK前需要保证有多少个备份对于Kafka而言，定义一个Broker是否“活着”包含两个条件： 一是它必须维护与ZooKeeper的session（这个通过ZooKeeper的Heartbeat机制来实现）。 二是Follower必须能够及时将Leader的消息复制过来，不能“落后太多”。 Leader会跟踪与其保持同步的Replica列表，该列表称为ISR（即in-sync Replica）。如果一个Follower宕机，或者落后太多，Leader将把它从ISR中移除。这里所描述的“落后太多”指Follower复制的消息落后于Leader后的条数超过预定值（该值可在$KAFKA_HOME/config/server.properties中通过replica.lag.max.messages配置，其默认值是4000）或者Follower超过一定时间（该值可在$KAFKA_HOME/config/server.properties中通过replica.lag.time.max.ms来配置，其默认值是10000）未向Leader发送fetch请求。 Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，完全同步复制要求所有能工作的Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率（高吞吐率是Kafka非常重要的一个特性）。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下如果Follower都复制完都落后于Leader，而如果Leader突然宕机，则会丢失数据。而Kafka的这种使用ISR的方式则很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，这样极大的提高复制性能（批量写磁盘），极大减少了Follower与Leader的差距。 需要说明的是，Kafka只解决fail/recover，不处理“Byzantine”（“拜占庭”）问题。一条消息只有被ISR里的所有Follower都从Leader复制过去才会被认为已提交。这样就避免了部分数据被写进了Leader，还没来得及被任何Follower复制就宕机了，而造成数据丢失（Consumer无法消费这些数据）。而对于Producer而言，它可以选择是否等待消息commit，这可以通过request.required.acks来设置。这种机制确保了只要ISR有一个或以上的Follower，一条被commit的消息就不会丢失。 2.2.3 Leader Election算法Leader选举本质上是一个分布式锁，有两种方式实现基于ZooKeeper的分布式锁： 节点名称唯一性：多个客户端创建一个节点，只有成功创建节点的客户端才能获得锁 临时顺序节点：所有客户端在某个目录下创建自己的临时顺序节点，只有序号最小的才获得锁 一种非常常用的选举leader的方式是“Majority Vote”（“少数服从多数”），但Kafka并未采用这种方式。这种模式下，如果我们有2f+1个Replica（包含Leader和Follower），那在commit之前必须保证有f+1个Replica复制完消息，为了保证正确选出新的Leader，fail的Replica不能超过f个。因为在剩下的任意f+1个Replica里，至少有一个Replica包含有最新的所有消息。这种方式有个很大的优势，系统的latency只取决于最快的几个Broker，而非最慢那个。Majority Vote也有一些劣势，为了保证Leader Election的正常进行，它所能容忍的fail的follower个数比较少。如果要容忍1个follower挂掉，必须要有3个以上的Replica，如果要容忍2个Follower挂掉，必须要有5个以上的Replica。也就是说，在生产环境下为了保证较高的容错程度，必须要有大量的Replica，而大量的Replica又会在大数据量下导致性能的急剧下降。这就是这种算法更多用在ZooKeeper这种共享集群配置的系统中而很少在需要存储大量数据的系统中使用的原因。例如HDFS的HA Feature是基于majority-vote-based journal，但是它的数据存储并没有使用这种方式。 Kafka在ZooKeeper中动态维护了一个ISR（in-sync replicas），这个ISR里的所有Replica都跟上了leader，只有ISR里的成员才有被选为Leader的可能。在这种模式下，对于f+1个Replica，一个Partition能在保证不丢失已经commit的消息的前提下容忍f个Replica的失败。在大多数使用场景中，这种模式是非常有利的。事实上，为了容忍f个Replica的失败，Majority Vote和ISR在commit前需要等待的Replica数量是一样的，但是ISR需要的总的Replica的个数几乎是Majority Vote的一半。 虽然Majority Vote与ISR相比有不需等待最慢的Broker这一优势，但是Kafka作者认为Kafka可以通过Producer选择是否被commit阻塞来改善这一问题，并且节省下来的Replica和磁盘使得ISR模式仍然值得。 2.2.4 如何处理所有Replica都不工作在ISR中至少有一个follower时，Kafka可以确保已经commit的数据不丢失，但如果某个Partition的所有Replica都宕机了，就无法保证数据不丢失了。这种情况下有两种可行的方案： 1.等待ISR中的任一个Replica“活”过来，并且选它作为Leader 2.选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader 这就需要在可用性和一致性当中作出一个简单的折衷。如果一定要等待ISR中的Replica“活”过来，那不可用的时间就可能会相对较长。而且如果ISR中的所有Replica都无法“活”过来了，或者数据都丢失了，这个Partition将永远不可用。选择第一个“活”过来的Replica作为Leader，而这个Replica不是ISR中的Replica，那即使它并不保证已经包含了所有已commit的消息，它也会成为Leader而作为consumer的数据源（前文有说明，所有读写都由Leader完成）。Kafka0.8.*使用了第二种方式。根据Kafka的文档，在以后的版本中，Kafka支持用户通过配置选择这两种方式中的一种，从而根据不同的使用场景选择高可用性还是强一致性。 2.2.5 选举Leader最简单最直观的方案是，所有Follower都在ZooKeeper上设置一个Watch，一旦Leader宕机，其对应的ephemeral znode会自动删除，此时所有Follower都尝试创建该节点，而创建成功者（ZooKeeper保证只有一个能创建成功）即是新的Leader，其它Replica即为Follower。 但是该方法会有3个问题： 1.split-brain 这是由ZooKeeper的特性引起的，虽然ZooKeeper能保证所有Watch按顺序触发，但并不能保证同一时刻所有Replica“看”到的状态是一样的，这就可能造成不同Replica的响应不一致 2.herd effect 如果宕机的那个Broker上的Partition比较多，会造成多个Watch被触发，造成集群内大量的调整 3.ZooKeeper负载过重 每个Replica都要为此在ZooKeeper上注册一个Watch，当集群规模增加到几千个Partition时ZooKeeper负载会过重。 Kafka 0.8.*的Leader Election方案解决了上述问题，它在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式（比ZooKeeper Queue的方式更高效）通知需为为此作为响应的Broker。同时controller也负责增删Topic以及Replica的重新分配。 三、HA相关ZooKeeper结构 3.1 admin该目录下znode只有在有相关操作时才会存在，操作结束时会将其删除 /admin/reassign_partitions用于将一些Partition分配到不同的broker集合上。对于每个待重新分配的Partition，Kafka会在该znode上存储其所有的Replica和相应的Broker id。该znode由管理进程创建并且一旦重新分配成功它将会被自动移除。 3.2 broker即/brokers/ids/[brokerId]）存储“活着”的broker信息。 topic注册信息（/brokers/topics/[topic]），存储该topic的所有partition的所有replica所在的broker id，第一个replica即为preferred replica，对一个给定的partition，它在同一个broker上最多只有一个replica,因此broker id可作为replica id。 3.3 controller/controller -&gt; int (broker id of the controller)存储当前controller的信息 /controller_epoch -&gt; int (epoch)直接以整数形式存储controller epoch，而非像其它znode一样以JSON字符串形式存储。 四、producer发布消息4.1 写入方式producer 采用 push 模式将消息发布到 broker，每条消息都被 append 到 patition 中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障 kafka 吞吐率）。 4.2 消息路由producer 发送消息到 broker 时，会根据分区算法选择将其存储到哪一个 partition。其路由机制为： 1231、 指定了 patition，则直接使用；2、 未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition3、 patition 和 key 都未指定，使用轮询选出一个 patition。 4.3 写入流程producer 写入消息序列图如下所示： 流程说明： 123456&gt; 1、 producer 先从 zookeeper 的 &quot;/brokers/.../state&quot; 节点找到该 partition 的 leader &gt; 2、 producer 将消息发送给该 leader &gt; 3、 leader 将消息写入本地 log &gt; 4、 followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK &gt; 5、 leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK&gt; 五、broker保存消息5.1 存储方式物理上把 topic 分成一个或多个 patition（对应 server.properties 中的 num.partitions=3 配置），每个 patition 物理上对应一个文件夹（该文件夹存储该 patition 的所有消息和索引文件），如下： 5.2 存储策略无论消息是否被消费，kafka 都会保留所有消息。有两种策略可以删除旧数据： 121、 基于时间：log.retention.hours=168 2、 基于大小：log.retention.bytes=1073741824 六、Topic的创建和删除6.1 创建topic创建 topic 的序列图如下所示： 流程说明： 123456&gt; 1、 controller 在 ZooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被创建，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。&gt; 2、 controller从 /brokers/ids 读取当前所有可用的 broker 列表，对于 set_p 中的每一个 partition：&gt; 2.1、 从分配给该 partition 的所有 replica（称为AR）中任选一个可用的 broker 作为新的 leader，并将AR设置为新的 ISR &gt; 2.2、 将新的 leader 和 ISR 写入 /brokers/topics/[topic]/partitions/[partition]/state &gt; 3、 controller 通过 RPC 向相关的 broker 发送 LeaderAndISRRequest。&gt; 6.2 删除topic删除 topic 的序列图如下所示： 流程说明： 123&gt; 1、 controller 在 zooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被删除，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。 &gt; 2、 若 delete.topic.enable=false，结束；否则 controller 注册在 /admin/delete_topics 上的 watch 被 fire，controller 通过回调向对应的 broker 发送 StopReplicaRequest。&gt; 七、broker failoverkafka broker failover 序列图如下所示： 流程说明： 123456789&gt; 1、 controller 在 zookeeper 的 /brokers/ids/[brokerId] 节点注册 Watcher，当 broker 宕机时 zookeeper 会 fire watch&gt; 2、 controller 从 /brokers/ids 节点读取可用broker &gt; 3、 controller决定set_p，该集合包含宕机 broker 上的所有 partition &gt; 4、 对 set_p 中的每一个 partition &gt; 4.1、 从/brokers/topics/[topic]/partitions/[partition]/state 节点读取 ISR &gt; 4.2、 决定新 leader &gt; 4.3、 将新 leader、ISR、controller_epoch 和 leader_epoch 等信息写入 state 节点&gt; 5、 通过 RPC 向相关 broker 发送 leaderAndISRRequest 命令&gt; 八、controller failover当 controller 宕机时会触发 controller failover。每个 broker 都会在 zookeeper 的 “/controller” 节点注册 watcher，当 controller 宕机时 zookeeper 中的临时节点消失，所有存活的 broker 收到 fire 的通知，每个 broker 都尝试创建新的 controller path，只有一个竞选成功并当选为 controller。 当新的 controller 当选时，会触发 KafkaController.onControllerFailover 方法，在该方法中完成如下操作： 12345678910111213&gt; 1、 读取并增加 Controller Epoch。 &gt; 2、 在 reassignedPartitions Patch(/admin/reassign_partitions) 上注册 watcher。 &gt; 3、 在 preferredReplicaElection Path(/admin/preferred_replica_election) 上注册 watcher。 &gt; 4、 通过 partitionStateMachine 在 broker Topics Patch(/brokers/topics) 上注册 watcher。 &gt; 5、 若 delete.topic.enable=true（默认值是 false），则 partitionStateMachine 在 Delete Topic Patch(/admin/delete_topics) 上注册 watcher。 &gt; 6、 通过 replicaStateMachine在 Broker Ids Patch(/brokers/ids)上注册Watch。 &gt; 7、 初始化 ControllerContext 对象，设置当前所有 topic，“活”着的 broker 列表，所有 partition 的 leader 及 ISR等。 &gt; 8、 启动 replicaStateMachine 和 partitionStateMachine。 &gt; 9、 将 brokerState 状态设置为 RunningAsController。 &gt; 10、 将每个 partition 的 Leadership 信息发送给所有“活”着的 broker。 &gt; 11、 若 auto.leader.rebalance.enable=true（默认值是true），则启动 partition-rebalance 线程。 &gt; 12、 若 delete.topic.enable=true 且Delete Topic Patch(/admin/delete_topics)中有值，则删除相应的Topic。&gt;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"http://zhangfuxin.cn/tags/Flume/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Kafka学习之路 （二）Kafka的架构","slug":"2018-05-05-Kafka学习之路 （二）Kafka的架构","date":"2018-05-05T02:30:04.000Z","updated":"2019-09-19T05:47:13.712Z","comments":true,"path":"2018-05-05-Kafka学习之路 （二）Kafka的架构.html","link":"","permalink":"http://zhangfuxin.cn/2018-05-05-Kafka学习之路 （二）Kafka的架构.html","excerpt":"** Kafka学习之路 （二）Kafka的架构：** &lt;Excerpt in index | 首页摘要&gt; ​ Kafka学习之路 （二）Kafka的架构","text":"** Kafka学习之路 （二）Kafka的架构：** &lt;Excerpt in index | 首页摘要&gt; ​ Kafka学习之路 （二）Kafka的架构 &lt;The rest of contents | 余下全文&gt; 一、Kafka的架构 如上图所示，一个典型的Kafka集群中包含若干Producer（可以是web前端产生的Page View，或者是服务器日志，系统CPU、Memory等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。 二、Topics和PartitionTopic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic，可以简单理解为必须指明把这条消息放进哪个queue里。为了使得Kafka的吞吐率可以线性提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件。创建一个topic时，同时可以指定分区数目，分区数越多，其吞吐量也越大，但是需要的资源也越多，同时也会导致更高的不可用性，kafka在接收到生产者发送的消息之后，会根据均衡策略将消息存储到不同的分区中。因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。 对于传统的message queue而言，一般会删除已经被消费的消息，而Kafka集群会保留所有的消息，无论其被消费与否。当然，因为磁盘限制，不可能永久保留所有数据（实际上也没必要），因此Kafka提供两种策略删除旧数据。一是基于时间，二是基于Partition文件大小。例如可以通过配置$KAFKA_HOME/config/server.properties，让Kafka删除一周前的数据，也可在Partition文件超过1GB时删除旧数据，配置如下所示： 12345678# The minimum age of a log file to be eligible for deletionlog.retention.hours=168# The maximum size of a log segment file. When this size is reached a new log segment will be created.log.segment.bytes=1073741824# The interval at which log segments are checked to see if they can be deleted according to the retention policieslog.retention.check.interval.ms=300000# If log.cleaner.enable=true is set the cleaner will be enabled and individual logs can then be marked for log compaction.log.cleaner.enable=false 因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高Kafka性能无关。选择怎样的删除策略只与磁盘以及具体的需求有关。另外，Kafka会为每一个Consumer Group保留一些metadata信息——当前消费的消息的position，也即offset。这个offset由Consumer控制。正常情况下Consumer会在消费完一条消息后递增该offset。当然，Consumer也可将offset设成一个较小的值，重新消费一些消息。因为offet由Consumer控制，所以Kafka broker是无状态的，它不需要标记哪些消息被哪些消费过，也不需要通过broker去保证同一个Consumer Group只有一个Consumer能消费某一条消息，因此也就不需要锁机制，这也为Kafka的高吞吐率提供了有力保障。 三、Producer消息路由Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。如果Partition机制设置合理，所有消息可以均匀分布到不同的Partition里，这样就实现了负载均衡。如果一个Topic对应一个文件，那这个文件所在的机器I/O将会成为这个Topic的性能瓶颈，而有了Partition后，不同的消息可以并行写入不同broker的不同Partition里，极大的提高了吞吐率。可以在$KAFKA_HOME/config/server.properties中通过配置项num.partitions来指定新建Topic的默认Partition数量，也可在创建Topic时通过参数指定，同时也可以在Topic创建之后通过Kafka提供的工具修改。 在发送一条消息时，可以指定这条消息的key，Producer根据这个key和Partition机制来判断应该将这条消息发送到哪个Parition。Paritition机制可以通过指定Producer的paritition. class这一参数来指定，该class必须实现kafka.producer.Partitioner接口。 四、Consumer Group使用Consumer high level API时，同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。 这是Kafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。如果需要实现广播，只要每个Consumer有一个独立的Group就可以了。要实现单播只要所有的Consumer在同一个Group里。用Consumer Group还可以将Consumer进行自由的分组而不需要多次发送消息到不同的Topic。 实际上，Kafka的设计理念之一就是同时提供离线处理和实时处理。根据这一特性，可以使用Storm这种实时流处理系统对消息进行实时在线处理，同时使用Hadoop这种批处理系统进行离线处理，还可以同时将数据实时备份到另一个数据中心，只需要保证这三个操作所使用的Consumer属于不同的Consumer Group即可。 五、Push vs. Pull作为一个消息系统，Kafka遵循了传统的方式，选择由Producer向broker push消息并由Consumer从broker pull消息。一些logging-centric system，比如Facebook的Scribe和Cloudera的Flume，采用push模式。事实上，push模式和pull模式各有优劣。 push模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。push模式的目标是尽可能以最快速度传递消息，但是这样很容易造成Consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据Consumer的消费能力以适当的速率消费消息。 对于Kafka而言，pull模式更合适。pull模式可简化broker的设计，Consumer可自主控制消费消息的速率，同时Consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。 六、Kafka delivery guarantee有这么几种可能的delivery guarantee： At most once 消息可能会丢，但绝不会重复传输 At least one 消息绝不会丢，但可能会重复传输 Exactly once 每条消息肯定会被传输一次且仅传输一次，很多时候这是用户所想要的。 当Producer向broker发送消息时，一旦这条消息被commit，因数replication的存在，它就不会丢。但是如果Producer发送数据给broker后，遇到网络问题而造成通信中断，那Producer就无法判断该条消息是否已经commit。虽然Kafka无法确定网络故障期间发生了什么，但是Producer可以生成一种类似于主键的东西，发生故障时幂等性的重试多次，这样就做到了Exactly once。 接下来讨论的是消息从broker到Consumer的delivery guarantee语义。（仅针对Kafka consumer high level API）。Consumer在从broker读取消息后，可以选择commit，该操作会在Zookeeper中保存该Consumer在该Partition中读取的消息的offset。该Consumer下一次再读该Partition时会从下一条开始读取。如未commit，下一次读取的开始位置会跟上一次commit之后的开始位置相同。当然可以将Consumer设置为autocommit，即Consumer一旦读到数据立即自动commit。如果只讨论这一读取消息的过程，那Kafka是确保了Exactly once。但实际使用中应用程序并非在Consumer读取完数据就结束了，而是要进行进一步处理，而数据处理与commit的顺序在很大程度上决定了消息从broker和consumer的delivery guarantee semantic。 Kafka默认保证At least once，并且允许通过设置Producer异步提交来实现At most once。而Exactly once要求与外部存储系统协作，幸运的是Kafka提供的offset可以非常直接非常容易得使用这种方式。","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"http://zhangfuxin.cn/tags/Flume/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Kafka学习之路 （一）Kafka的简介","slug":"2018-05-04-Kafka学习之路 （一）Kafka的简介","date":"2018-05-04T02:30:04.000Z","updated":"2019-09-19T05:40:21.566Z","comments":true,"path":"2018-05-04-Kafka学习之路 （一）Kafka的简介.html","link":"","permalink":"http://zhangfuxin.cn/2018-05-04-Kafka学习之路 （一）Kafka的简介.html","excerpt":"** Kafka学习之路 （一）Kafka的简介：** &lt;Excerpt in index | 首页摘要&gt; ​ Kafka学习之路 （一）Kafka的简介","text":"** Kafka学习之路 （一）Kafka的简介：** &lt;Excerpt in index | 首页摘要&gt; ​ Kafka学习之路 （一）Kafka的简介 &lt;The rest of contents | 余下全文&gt; 正文 一、简介1.1 概述Kafka是最初由Linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当做MQ系统），常见可以用于web/nginx日志、访问日志，消息服务等等，Linkedin于2010年贡献给了Apache基金会并成为顶级开源项目。 主要应用场景是：日志收集系统和消息系统。 Kafka主要设计目标如下： 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间的访问性能。 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条消息的传输。 支持Kafka Server间的消息分区，及分布式消费，同时保证每个partition内的消息顺序传输。 同时支持离线数据处理和实时数据处理。 Scale out:支持在线水平扩展 1.2 消息系统介绍一个消息系统负责将数据从一个应用传递到另外一个应用，应用只需关注于数据，无需关注数据在两个或多个应用间是如何传递的。分布式消息传递基于可靠的消息队列，在客户端应用和消息系统之间异步传递消息。有两种主要的消息传递模式：点对点传递模式、发布-订阅模式。大部分的消息系统选用发布-订阅模式。Kafka就是一种发布-订阅模式。 1.3 点对点消息传递模式在点对点消息系统中，消息持久化到一个队列中。此时，将有一个或多个消费者消费队列中的数据。但是一条消息只能被消费一次。当一个消费者消费了队列中的某条数据之后，该条数据则从消息队列中删除。该模式即使有多个消费者同时消费数据，也能保证数据处理的顺序。这种架构描述示意图如下： 生产者发送一条消息到queue，只有一个消费者能收到。 1.4 发布-订阅消息传递模式在发布-订阅消息系统中，消息被持久化到一个topic中。与点对点消息系统不同的是，消费者可以订阅一个或多个topic，消费者可以消费该topic中所有的数据，同一条数据可以被多个消费者消费，数据被消费后不会立马删除。在发布-订阅消息系统中，消息的生产者称为发布者，消费者称为订阅者。该模式的示例图如下： 发布者发送到topic的消息，只有订阅了topic的订阅者才会收到消息。 二、Kafka的优点2.1 解耦在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。消息系统在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口。这允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 2.2 冗余（副本）有些情况下，处理数据的过程会失败。除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 2.3 扩展性因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。不需要改变代码、不需要调节参数。扩展就像调大电力按钮一样简单。 2.4 灵活性&amp;峰值处理能力在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见；如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 2.5 可恢复性系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 2.6 顺序保证在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。Kafka保证一个Partition内的消息的有序性。 2.7 缓冲在任何重要的系统中，都会有需要不同的处理时间的元素。例如，加载一张图片比应用过滤器花费更少的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行———写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。 2.8 异步通信很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 三、常用Message Queue对比3.1 RabbitMQRabbitMQ是使用Erlang编写的一个开源的消息队列，本身支持很多的协议：AMQP，XMPP, SMTP, STOMP，也正因如此，它非常重量级，更适合于企业级的开发。同时实现了Broker构架，这意味着消息在发送给客户端时先在中心队列排队。对路由，负载均衡或者数据持久化都有很好的支持。 3.2 RedisRedis是一个基于Key-Value对的NoSQL数据库，开发维护很活跃。虽然它是一个Key-Value数据库存储系统，但它本身支持MQ功能，所以完全可以当做一个轻量级的队列服务来使用。对于RabbitMQ和Redis的入队和出队操作，各执行100万次，每10万次记录一次执行时间。测试数据分为128Bytes、512Bytes、1K和10K四个不同大小的数据。实验表明：入队时，当数据比较小时Redis的性能要高于RabbitMQ，而如果数据大小超过了10K，Redis则慢的无法忍受；出队时，无论数据大小，Redis都表现出非常好的性能，而RabbitMQ的出队性能则远低于Redis。 3.3 ZeroMQZeroMQ号称最快的消息队列系统，尤其针对大吞吐量的需求场景。ZeroMQ能够实现RabbitMQ不擅长的高级/复杂的队列，但是开发人员需要自己组合多种技术框架，技术上的复杂度是对这MQ能够应用成功的挑战。ZeroMQ具有一个独特的非中间件的模式，你不需要安装和运行一个消息服务器或中间件，因为你的应用程序将扮演这个服务器角色。你只需要简单的引用ZeroMQ程序库，可以使用NuGet安装，然后你就可以愉快的在应用程序之间发送消息了。但是ZeroMQ仅提供非持久性的队列，也就是说如果宕机，数据将会丢失。其中，Twitter的Storm 0.9.0以前的版本中默认使用ZeroMQ作为数据流的传输（Storm从0.9版本开始同时支持ZeroMQ和Netty作为传输模块）。 3.4 ActiveMQActiveMQ是Apache下的一个子项目。 类似于ZeroMQ，它能够以代理人和点对点的技术实现队列。同时类似于RabbitMQ，它少量代码就可以高效地实现高级应用场景。 3.5 Kafka/JafkaKafka是Apache下的一个子项目，是一个高性能跨语言分布式发布/订阅消息队列系统，而Jafka是在Kafka之上孵化而来的，即Kafka的一个升级版。具有以下特性：快速持久化，可以在O(1)的系统开销下进行消息持久化；高吞吐，在一台普通的服务器上既可以达到10W/s的吞吐速率；完全的分布式系统，Broker、Producer、Consumer都原生自动支持分布式，自动实现负载均衡；支持Hadoop数据并行加载，对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka通过Hadoop的并行加载机制统一了在线和离线的消息处理。Apache Kafka相对于ActiveMQ是一个非常轻量级的消息系统，除了性能非常好之外，还是一个工作良好的分布式系统。 四、Kafka中的术语解释4.1 概述在深入理解Kafka之前，先介绍一下Kafka中的术语。下图展示了Kafka的相关术语以及之间的关系： 上图中一个topic配置了3个partition。Partition1有两个offset：0和1。Partition2有4个offset。Partition3有1个offset。副本的id和副本所在的机器的id恰好相同。 如果一个topic的副本数为3，那么Kafka将在集群中为每个partition创建3个相同的副本。集群中的每个broker存储一个或多个partition。多个producer和consumer可同时生产和消费数据。 4.2 brokerKafka 集群包含一个或多个服务器，服务器节点称为broker。 broker存储topic的数据。如果某topic有N个partition，集群有N个broker，那么每个broker存储该topic的一个partition。 如果某topic有N个partition，集群有(N+M)个broker，那么其中有N个broker存储该topic的一个partition，剩下的M个broker不存储该topic的partition数据。 如果某topic有N个partition，集群中broker数目少于N个，那么一个broker存储该topic的一个或多个partition。在实际生产环境中，尽量避免这种情况的发生，这种情况容易导致Kafka集群数据不均衡。 4.3 Topic每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处） 类似于数据库的表名 4.3 Partitiontopic中的数据分割为一个或多个partition。每个topic至少有一个partition。每个partition中的数据使用多个segment文件存储。partition中的数据是有序的，不同partition间的数据丢失了数据的顺序。如果topic有多个partition，消费数据时就不能保证数据的顺序。在需要严格保证消息的消费顺序的场景下，需要将partition数目设为1。 4.4 Producer生产者即数据的发布者，该角色将消息发布到Kafka的topic中。broker接收到生产者发送的消息后，broker将该消息追加到当前用于追加数据的segment文件中。生产者发送的消息，存储到一个partition中，生产者也可以指定数据存储的partition。 4.5 Consumer消费者可以从broker中读取数据。消费者可以消费多个topic中的数据。 4.6 Consumer Group每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。 4.7 Leader每个partition有多个副本，其中有且仅有一个作为Leader，Leader是当前负责数据的读写的partition。 4.8 FollowerFollower跟随Leader，所有写请求都通过Leader路由，数据变更会广播给所有Follower，Follower与Leader保持数据同步。如果Leader失效，则从Follower中选举出一个新的Leader。当Follower与Leader挂掉、卡住或者同步太慢，leader会把这个follower从“in sync replicas”（ISR）列表中删除，重新创建一个Follower。","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"http://zhangfuxin.cn/tags/Flume/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Flume学习之路 （三）Flume的配置方式","slug":"2018-05-03-Flume学习之路 （三）Flume的配置方式","date":"2018-05-03T02:30:04.000Z","updated":"2019-09-19T05:36:43.343Z","comments":true,"path":"2018-05-03-Flume学习之路 （三）Flume的配置方式.html","link":"","permalink":"http://zhangfuxin.cn/2018-05-03-Flume学习之路 （三）Flume的配置方式.html","excerpt":"** Flume学习之路 （三）Flume的配置方式：** &lt;Excerpt in index | 首页摘要&gt; ​ Flume学习之路 （三）Flume的配置方式","text":"** Flume学习之路 （三）Flume的配置方式：** &lt;Excerpt in index | 首页摘要&gt; ​ Flume学习之路 （三）Flume的配置方式 &lt;The rest of contents | 余下全文&gt; 一、单一代理流配置1.1 官网介绍http://flume.apache.org/FlumeUserGuide.html#avro-source 通过一个通道将来源和接收器链接。需要列出源，接收器和通道，为给定的代理，然后指向源和接收器及通道。一个源的实例可以指定多个通道，但只能指定一个接收器实例。格式如下： 实例解析：一个代理名为agent_foo，外部通过avro客户端，并且发送数据通过内存通道给hdfs。在配置文件foo.config的可能看起来像这样： 案例说明：这将使事件流从avro-appserver-src-1到hdfs-sink-1通过内存通道mem-channel-1。当代理开始foo.config作为其配置文件，它会实例化流。 配置单个组件 定义流之后，需要设置每个源，接收器和通道的属性。可以分别设定组件的属性值。 “type”属性必须为每个组件设置，以了解它需要什么样的对象。每个源，接收器和通道类型有其自己的一套，它所需的性能，以实现预期的功能。所有这些，必须根据需要设置。在前面的例子中，从hdfs-sink-1中的流到HDFS，通过内存通道mem-channel-1的avro-appserver-src-1源。下面是 一个例子，显示了这些组件的配置。 1.2 测试示例（一）通过flume来监控一个目录，当目录中有新文件时，将文件内容输出到控制台。 1234567891011121314151617181920#配置一个agent，agent的名称可以自定义（如a1）#指定agent的sources（如s1）、sinks（如k1）、channels（如c1）#分别指定agent的sources，sinks,channels的名称 名称可以自定义a1.sources = s1 a1.sinks = k1 a1.channels = c1 #描述source#配置目录scourcea1.sources.s1.type =spooldir a1.sources.s1.spoolDir =/home/hadoop/logs a1.sources.s1.fileHeader= true a1.sources.s1.channels =c1 #配置sink a1.sinks.k1.type = logger a1.sinks.k1.channel = c1 #配置channel(内存做缓存)a1.channels.c1.type = memory 启动命令 1[hadoop@hadoop1 ~]$ flume-ng agent --conf conf --conf-file /home/hadoop/apps/flume/examples/case_spool.properties --name a1 -Dflume.root.logger=INFO,console 将123.log移动到logs目录 运行结果 1.3 测试案例（二）案例2：实时模拟从web服务器中读取数据到hdfs中 此处使用exec source详细参考http://www.cnblogs.com/qingyunzong/p/8995554.html 里面的2.3Exec Source介绍 回到顶部 二、单代理多流配置单个Flume代理可以包含几个独立的流。你可以在一个配置文件中列出多个源，接收器和通道。这些组件可以连接形成多个流。 可以连接源和接收器到其相应的通道，设置两个不同的流。例如，如果需要设置一个agent_foo代理两个流，一个从外部Avro客户端到HDFS，另外一个是tail的输出到Avro接收器，然后在这里是做一个配置 2.1 官方案例 三、配置多代理流程设置一个多层的流，需要有一个指向下一跳avro源的第一跳的avro 接收器。这将导致第一Flume代理转发事件到下一个Flume代理。例如，如果定期发送的文件，每个事件（1文件）AVRO客户端使用本地Flume 代理，那么这个当地的代理可以转发到另一个有存储的代理。 配置如下 3.1 官方案例 这里连接从weblog-agent的avro-forward-sink 到hdfs-agent的avro-collection-source收集源。最终结果从外部源的appserver最终存储在HDFS的事件。 3.2 测试案例case_avro.properties 123456789101112131415a1.sources = s1a1.sinks = k1a1.channels = c1a1.sources.s1.type = avroa1.sources.s1.channels = c1a1.sources.s1.bind = 192.168.123.102a1.sources.s1.port = 22222a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.sinks.k1.type = loggera1.sinks.k1.channel = c1 case_avro_sink.properties 1234567891011121314151617a2.sources = s1a2.sinks = k1a2.channels = c1 a2.sources.s1.type = syslogtcpa2.sources.s1.channels = c1a2.sources.s1.host = 192.168.123.102a2.sources.s1.port = 33333 a2.channels.c1.type = memorya2.channels.c1.capacity = 1000a2.channels.c1.transactionCapacity = 100 a2.sinks.k1.type = avroa2.sinks.k1.hostname = 192.168.123.102a2.sinks.k1.port = 22222a2.sinks.k1.channel = c1 说明：case_avro_sink.properties是前面的Agent，case_avro.properties是后面的Agent #**先启动Avro的Source,监听端口** 1[hadoop@hadoop1 ~]$ flume-ng agent --conf conf --conf-file ~/apps/flume/examples/case_avro.properties --name a1 -Dflume.root.logger=DEBUG,console -Dorg.apache.flume.log.printconfig=true -Dorg.apache.flume.log.rawdata=true #**再启动Avro**的Sink 1flume-ng agent --conf conf --conf-file ~/apps/flume/examples/case_avro_sink.properties --name a2 -Dflume.root.logger=DEBUG,console -Dorg.apache.flume.log.printconfig=true -Dorg.apache.flume.log.rawdata=true 可以看到已经建立连接 #**在Avro Sink**上生成测试log 1[hadoop@hadoop1 ~]$ echo &quot;hello flume avro sink&quot; | nc 192.168.123.102 33333 查看其它结果 四、多路复用流Flume支持扇出流从一个源到多个通道。有两种模式的扇出，复制和复用。在复制流的事件被发送到所有的配置通道。在复用的情况下，事件被发送到合格的渠 道只有一个子集。扇出流，需要指定源和扇出通道的规则。这是通过添加一个通道“选择”，可以复制或复用。再进一步指定选择的规则，如果它是一个多路。如果你 不指定一个选择，则默认情况下它复制。 复用的选择集的属性进一步分叉。这需要指定一个事件属性映射到一组通道。选择配置属性中的每个事件头检查。如果指定的值相匹配，那么该事件被发送到所有的通道映射到该值。如果没有匹配，那么该事件被发送到设置为默认配置的通道。 映射允许每个值通道可以重叠。默认值可以包含任意数量的通道。下面的示例中有一个单一的流复用两条路径。代理有一个单一的avro源和连接道两个接收器的两个通道。 4.1 官方案例 “State”作为Header的选择检查。如果值是“CA”，然后将其发送到mem-channel-1，如果它的“AZ”的，那么jdbc- channel-2，如果它的“NY”那么发到这两个。如果“State”头未设置或不匹配的任何三个，然后去默认的mem-channel-1通道。 4.2 测试案例（一）复制case_replicate_sink.properties 123456789101112131415161718192021222324252627a1.sources = s1a1.sinks = k1 k2a1.channels = c1 c2a1.sources.s1.type = syslogtcpa1.sources.s1.channels = c1 c2a1.sources.s1.host = 192.168.123.102a1.sources.s1.port = 6666a1.sources.s1.selector.type = replicatinga1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.channels.c2.type = memorya1.channels.c2.capacity = 1000a1.channels.c2.transactionCapacity = 100a1.sinks.k1.type = avroa1.sinks.k1.hostname = 192.168.123.102a1.sinks.k1.port = 7777a1.sinks.k1.channel = c1a1.sinks.k1.type = avroa1.sinks.k1.hostname = 192.168.123.102a1.sinks.k1.port = 7777a1.sinks.k1.channel = c2 case_replicate_s1.properties 123456789101112131415a2.sources = s1a2.sinks = k1a2.channels = c1 a2.sources.s1.type = avroa2.sources.s1.channels = c1a2.sources.s1.host = 192.168.123.102a2.sources.s1.port = 7777 a2.channels.c1.type = memorya2.channels.c1.capacity = 1000a2.channels.c1.transactionCapacity = 100 a2.sinks.k1.type = loggera2.sinks.k1.channel = c1 case_replicate_s2.properties 123456789101112131415a3.sources = s1a3.sinks = k1a3.channels = c1a3.sources.s1.type = avroa3.sources.s1.channels = c1a3.sources.s1.host = 192.168.123.102a3.sources.s1.port = 7777a3.channels.c1.type = memorya3.channels.c1.capacity = 1000a3.channels.c1.transactionCapacity = 100a3.sinks.k1.type = loggera3.sinks.k1.channel = c1 #**先启动Avro的Source,监听端口** 1flume-ng agent --conf conf --conf-file ~/apps/flume/examples/case_replicate_s1.properties --name a2 -Dflume.root.logger=DEBUG,console -Dorg.apache.flume.log.printconfig=true -Dorg.apache.flume.log.rawdata=true 1flume-ng agent --conf conf --conf-file ~/apps/flume/examples/case_replicate_s2.properties --name a3 -Dflume.root.logger=DEBUG,console -Dorg.apache.flume.log.printconfig=true -Dorg.apache.flume.log.rawdata=true #**再启动Avro**的Sink 1flume-ng agent --conf conf --conf-file ~/apps/flume/examples/case_replicate_sink.properties --name a1 -Dflume.root.logger=DEBUG,console -Dorg.apache.flume.log.printconfig=true -Dorg.apache.flume.log.rawdata=true #**生成测试log** echo “hello via channel selector” | nc 192.168.123.102 6666 4.3 测试案例（二）复用case_multi_sink.properties 1234567891011121314151617181920212223242526272829303132333435#2个channel和2个sink的配置文件a1.sources = r1a1.sinks = k1 k2a1.channels = c1 c2 # Describe/configure the sourcea1.sources.r1.type = org.apache.flume.source.http.HTTPSourcea1.sources.r1.port = 5140a1.sources.r1.host = 0.0.0.0a1.sources.r1.selector.type = multiplexinga1.sources.r1.channels = c1 c2 a1.sources.r1.selector.header = statea1.sources.r1.selector.mapping.CZ = c1a1.sources.r1.selector.mapping.US = c2a1.sources.r1.selector.default = c1 # Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.channel = c1a1.sinks.k1.hostname = 172.25.4.23a1.sinks.k1.port = 4545 a1.sinks.k2.type = avroa1.sinks.k2.channel = c2a1.sinks.k2.hostname = 172.25.4.33a1.sinks.k2.port = 4545# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 a1.channels.c2.type = memorya1.channels.c2.capacity = 1000a1.channels.c2.transactionCapacity = 100 case_ multi _s1.properties 12345678910111213141516171819# Name the components on this agenta2.sources = r1a2.sinks = k1a2.channels = c1 # Describe/configure the sourcea2.sources.r1.type = avroa2.sources.r1.channels = c1a2.sources.r1.bind = 172.25.4.23a2.sources.r1.port = 4545 # Describe the sinka2.sinks.k1.type = logger a2.sinks.k1.channel = c1# Use a channel which buffers events in memorya2.channels.c1.type = memorya2.channels.c1.capacity = 1000a2.channels.c1.transactionCapacity = 100 case_ multi _s2.properties 12345678910111213141516171819# Name the components on this agenta3.sources = r1a3.sinks = k1a3.channels = c1 # Describe/configure the sourcea3.sources.r1.type = avroa3.sources.r1.channels = c1a3.sources.r1.bind = 172.25.4.33a3.sources.r1.port = 4545 # Describe the sinka3.sinks.k1.type = logger a3.sinks.k1.channel = c1# Use a channel which buffers events in memorya3.channels.c1.type = memorya3.channels.c1.capacity = 1000a3.channels.c1.transactionCapacity = 100 #**先启动Avro的Source,监听端口** flume-ng agent -c . -f case_ multi _s1.conf -n a2 -Dflume.root.logger=INFO,console flume-ng agent -c . -f case_ multi _s2.conf -n a3 -Dflume.root.logger=INFO,console #**再启动Avro**的Sink flume-ng agent -c . -f case_multi_sink.conf -n a1-Dflume.root.logger=INFO,console #**根据配置文件生成测试的header** 为state**的POST**请求 curl -X POST -d ‘[{ “headers” :{“state” : “CZ”},”body” : “TEST1”}]’ http://localhost:5140 curl -X POST -d ‘[{ “headers” :{“state” : “US”},”body” : “TEST2”}]’ http://localhost:5140 curl -X POST -d ‘[{ “headers” :{“state” : “SH”},”body” : “TEST3”}]’ http://localhost:5140","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"http://zhangfuxin.cn/tags/Flume/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Flume学习之路 （二）Flume的Source类型","slug":"2018-05-02-Flume学习之路 （二）Flume的Source类型","date":"2018-05-02T02:30:04.000Z","updated":"2019-09-19T05:33:07.317Z","comments":true,"path":"2018-05-02-Flume学习之路 （二）Flume的Source类型.html","link":"","permalink":"http://zhangfuxin.cn/2018-05-02-Flume学习之路 （二）Flume的Source类型.html","excerpt":"** Flume学习之路 （二）Flume的Source类型：** &lt;Excerpt in index | 首页摘要&gt; ​ Flume学习之路 （二）Flume的Source类型","text":"** Flume学习之路 （二）Flume的Source类型：** &lt;Excerpt in index | 首页摘要&gt; ​ Flume学习之路 （二）Flume的Source类型 &lt;The rest of contents | 余下全文&gt; 一、概述官方文档介绍：http://flume.apache.org/FlumeUserGuide.html#flume-sources 二、Flume Sources 描述2.1 Avro Source2.1.1 介绍监听Avro端口，从Avro client streams接收events。当与另一个（前一跳）Flume agent内置的Avro Sink配对时，它可以创建分层收集拓扑。字体加粗的属性必须进行设置。 2.1.2 示例示例一：示例请参考官方文档 示例二： 1234567891011121314151617181920212223#配置一个agent，agent的名称可以自定义（如a1）#指定agent的sources（如s1）、sinks（如k1）、channels（如c1）#分别指定agent的sources，sinks,channels的名称 名称可以自定义a1.sources = s1a1.sinks = k1a1.channels = c1#配置sourcea1.sources.s1.channels = c1a1.sources.s1.type = avroa1.sources.s1.bind = 192.168.123.102a1.sources.s1.port = 6666#配置channelsa1.channels.c1.type = memory#配置sinksa1.sinks.k1.channel = c1a1.sinks.k1.type = logger#为sources和sinks绑定channelsa1.sources.s1.channels = c1a1.sinks.k1.channel = c1 启动flume 1[hadoop@hadoop1 ~]$ flume-ng agent --conf conf --conf-file ~/apps/flume/examples/single_avro.properties --name a1 -Dflume.root.logger=DEBUG,console -Dorg.apache.flume.log.printconfig=true -Dorg.apache.flume.log.rawdata=true 通过flume提供的avro客户端向指定机器指定端口发送日志信息： 1[hadoop@hadoop1 ~]$ flume-ng avro-client -c ~/apps/flume/conf -H 192.168.123.102 -p 6666 -F 666.txt 接收到的信息 2.2 Thrift Source2.2.1 介绍ThriftSource 与Avro Source 基本一致。只要把source的类型改成thrift即可，例如a1.sources.r1.type = thrift，比较简单，不做赘述。 2.3 Exec Source2.3.1 介绍ExecSource的配置就是设定一个Unix(linux)命令，然后通过这个命令不断输出数据。如果进程退出，Exec Source也一起退出，不会产生进一步的数据。 下面是官网给出的source的配置，加粗的参数是必选，描述就不解释了。 2.3.2 示例1234567891011121314151617#配置文件#Name the components on this agent a1.sources= s1 a1.sinks= k1 a1.channels= c1 #配置sourcesa1.sources.s1.type = exec a1.sources.s1.command = tail -F /home/hadoop/logs/test.log a1.sources.s1.channels = c1 #配置sinks a1.sinks.k1.type= logger a1.sinks.k1.channel= c1 #配置channela1.channels.c1.type= memory 启动命令 1[hadoop@hadoop1 ~]$ flume-ng agent --conf conf --conf-file ~/apps/flume/examples/case_exec.properties --name a1 -Dflume.root.logger=DEBUG,console -Dorg.apache.flume.log.printconfig=true -Dorg.apache.flume.log.rawdata=true 继续往日志里添加数据 接收到的信息 2.4 JMS Source2.4.1 介绍从JMS系统（消息、主题）中读取数据，ActiveMQ已经测试过 2.4.2 官网示例 2.5 Spooling Directory Source2.5.1 介绍Spooling Directory Source监测配置的目录下新增的文件，并将文件中的数据读取出来。其中，Spool Source有2个注意地方，第一个是拷贝到spool目录下的文件不可以再打开编辑，第二个是spool目录下不可包含相应的子目录。这个主要用途作为对日志的准实时监控。 下面是官网给出的source的配置，加粗的参数是必选。可选项太多，这边就介绍一个fileSuffix，即文件读取后添加的后缀名，这个是可以更改。 2.5.2 示例12345678910111213141516a1.sources = s1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.s1.type =spooldir a1.sources.s1.spoolDir =/home/hadoop/logs a1.sources.s1.fileHeader= true a1.sources.s1.channels =c1 # Describe the sink a1.sinks.k1.type = logger a1.sinks.k1.channel = c1 # Use a channel which buffers events inmemory a1.channels.c1.type = memory 启动命令 1[hadoop@hadoop1 ~]$ flume-ng agent --conf conf --conf-file /home/hadoop/apps/flume/examples/case_spool.properties --name a1 -Dflume.root.logger=INFO,console 讲123.log移动到logs目录 运行结果 2.6 其他参考https://blog.csdn.net/looklook5/article/details/40400885","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"http://zhangfuxin.cn/tags/Flume/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"HBase学习之路 （四）HBase的API操作","slug":"2018-06-04-HBase学习之路 （四）HBase的API操作","date":"2018-05-01T02:30:04.000Z","updated":"2019-09-19T06:29:47.342Z","comments":true,"path":"2018-06-04-HBase学习之路 （四）HBase的API操作.html","link":"","permalink":"http://zhangfuxin.cn/2018-06-04-HBase学习之路 （四）HBase的API操作.html","excerpt":"** HBase学习之路 （四）HBase的API操作：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （四）HBase的API操作","text":"** HBase学习之路 （四）HBase的API操作：** &lt;Excerpt in index | 首页摘要&gt; ​ HBase学习之路 （四）HBase的API操作 &lt;The rest of contents | 余下全文&gt; Eclipse环境搭建具体的jar的引入方式可以参考http://www.cnblogs.com/qingyunzong/p/8623309.html HBase API操作表和数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550 1 import java.io.IOException; 2 import java.util.Date; 3 4 import org.apache.hadoop.conf.Configuration; 5 import org.apache.hadoop.hbase.HBaseConfiguration; 6 import org.apache.hadoop.hbase.HColumnDescriptor; 7 import org.apache.hadoop.hbase.HTableDescriptor; 8 import org.apache.hadoop.hbase.TableName; 9 import org.apache.hadoop.hbase.client.Admin; 10 import org.apache.hadoop.hbase.client.Connection; 11 import org.apache.hadoop.hbase.client.ConnectionFactory; 12 import org.apache.hadoop.hbase.client.Delete; 13 import org.apache.hadoop.hbase.client.Get; 14 import org.apache.hadoop.hbase.client.Put; 15 import org.apache.hadoop.hbase.client.Result; 16 import org.apache.hadoop.hbase.client.ResultScanner; 17 import org.apache.hadoop.hbase.client.Scan; 18 import org.apache.hadoop.hbase.client.Table; 19 20 import com.study.hbase.service.HBaseUtils; 21 22 public class HBaseUtilsImpl implements HBaseUtils &#123; 23 24 private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;; 25 private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;; 26 27 private static Connection conn = null; 28 private static Admin admin = null; 29 30 public static void main(String[] args) throws Exception &#123; 31 32 getConnection(); 33 getAdmin(); 34 35 HBaseUtilsImpl hbu = new HBaseUtilsImpl(); 36 37 38 //hbu.getAllTables(); 39 40 //hbu.descTable(&quot;people&quot;); 41 42 //String[] infos = &#123;&quot;info&quot;,&quot;family&quot;&#125;; 43 //hbu.createTable(&quot;people&quot;, infos); 44 45 //String[] add = &#123;&quot;cs1&quot;,&quot;cs2&quot;&#125;; 46 //String[] remove = &#123;&quot;cf1&quot;,&quot;cf2&quot;&#125;; 47 48 //HColumnDescriptor hc = new HColumnDescriptor(&quot;sixsixsix&quot;); 49 50 //hbu.modifyTable(&quot;stu&quot;,hc); 51 //hbu.getAllTables(); 52 53 54 hbu.putData(&quot;huoying&quot;, &quot;rk001&quot;, &quot;cs2&quot;, &quot;name&quot;, &quot;aobama&quot;,new Date().getTime()); 55 hbu.getAllTables(); 56 57 conn.close(); 58 &#125; 59 60 // 获取连接 61 public static Connection getConnection() &#123; 62 // 创建一个可以用来管理hbase配置信息的conf对象 63 Configuration conf = HBaseConfiguration.create(); 64 // 设置当前的程序去寻找的hbase在哪里 65 conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE); 66 try &#123; 67 conn = ConnectionFactory.createConnection(conf); 68 &#125; catch (IOException e) &#123; 69 e.printStackTrace(); 70 &#125; 71 return conn; 72 &#125; 73 74 // 获取管理员对象 75 public static Admin getAdmin() &#123; 76 try &#123; 77 admin = conn.getAdmin(); 78 &#125; catch (IOException e) &#123; 79 e.printStackTrace(); 80 &#125; 81 return admin; 82 &#125; 83 84 // 查询所有表 85 @Override 86 public void getAllTables() throws Exception &#123; 87 //获取列簇的描述信息 88 HTableDescriptor[] listTables = admin.listTables(); 89 for (HTableDescriptor listTable : listTables) &#123; 90 //转化为表名 91 String tbName = listTable.getNameAsString(); 92 //获取列的描述信息 93 HColumnDescriptor[] columnFamilies = listTable.getColumnFamilies(); 94 System.out.println(&quot;tableName:&quot;+tbName); 95 for(HColumnDescriptor columnFamilie : columnFamilies) &#123; 96 //获取列簇的名字 97 String columnFamilyName = columnFamilie.getNameAsString(); 98 System.out.print(&quot;\\t&quot;+&quot;columnFamilyName:&quot;+columnFamilyName); 99 &#125;100 System.out.println();101 &#125;102 103 &#125;104 105 // 创建表，传参，表名和列簇的名字106 @Override107 public void createTable(String tableName, String[] family) throws Exception &#123;108 109 TableName name = TableName.valueOf(tableName);110 //判断表是否存在111 if(admin.tableExists(name)) &#123;112 System.out.println(&quot;table已经存在！&quot;);113 &#125;else &#123;114 //表的列簇示例115 HTableDescriptor htd = new HTableDescriptor(name);116 //向列簇中添加列的信息117 for(String str : family) &#123;118 HColumnDescriptor hcd = new HColumnDescriptor(str);119 htd.addFamily(hcd);120 &#125;121 //创建表122 admin.createTable(htd);123 //判断表是否创建成功124 if(admin.tableExists(name)) &#123;125 System.out.println(&quot;table创建成功&quot;);126 &#125;else &#123;127 System.out.println(&quot;table创建失败&quot;);128 &#125;129 &#125; 130 131 &#125;132 133 // 创建表，传参:封装好的多个列簇134 @Override135 public void createTable(HTableDescriptor htds) throws Exception &#123;136 //获得表的名字137 String tbName = htds.getNameAsString();138 139 admin.createTable(htds);140 &#125;141 142 // 创建表，传参，表名和封装好的多个列簇143 @Override144 public void createTable(String tableName, HTableDescriptor htds) throws Exception &#123;145 146 TableName name = TableName.valueOf(tableName);147 148 if(admin.tableExists(name)) &#123;149 System.out.println(&quot;table已经存在！&quot;);150 &#125;else &#123;151 admin.createTable(htds);152 boolean flag = admin.tableExists(name);153 System.out.println(flag ? &quot;创建成功&quot; : &quot;创建失败&quot;);154 &#125;155 156 &#125;157 158 159 // 查看表的列簇属性160 @Override161 public void descTable(String tableName) throws Exception &#123;162 //转化为表名163 TableName name = TableName.valueOf(tableName);164 //判断表是否存在165 if(admin.tableExists(name)) &#123;166 //获取表中列簇的描述信息167 HTableDescriptor tableDescriptor = admin.getTableDescriptor(name);168 //获取列簇中列的信息169 HColumnDescriptor[] columnFamilies = tableDescriptor.getColumnFamilies();170 for(HColumnDescriptor columnFamily : columnFamilies) &#123;171 System.out.println(columnFamily);172 &#125;173 174 &#125;else &#123;175 System.out.println(&quot;table不存在&quot;);176 &#125;177 178 &#125;179 180 // 判断表存在不存在181 @Override182 public boolean existTable(String tableName) throws Exception &#123;183 TableName name = TableName.valueOf(tableName);184 return admin.tableExists(name);185 &#125;186 187 // disable表188 @Override189 public void disableTable(String tableName) throws Exception &#123;190 191 TableName name = TableName.valueOf(tableName);192 193 if(admin.tableExists(name)) &#123;194 if(admin.isTableEnabled(name)) &#123;195 admin.disableTable(name);196 &#125;else &#123;197 System.out.println(&quot;table不是活动状态&quot;);198 &#125;199 &#125;else &#123;200 System.out.println(&quot;table不存在&quot;);201 &#125;202 203 &#125;204 205 // drop表206 @Override207 public void dropTable(String tableName) throws Exception &#123;208 //转化为表名209 TableName name = TableName.valueOf(tableName);210 //判断表是否存在211 if(admin.tableExists(name)) &#123;212 //判断表是否处于可用状态213 boolean tableEnabled = admin.isTableEnabled(name);214 215 if(tableEnabled) &#123;216 //使表变成不可用状态217 admin.disableTable(name);218 &#125;219 //删除表220 admin.deleteTable(name);221 //判断表是否存在222 if(admin.tableExists(name)) &#123;223 System.out.println(&quot;删除失败&quot;);224 &#125;else &#123;225 System.out.println(&quot;删除成功&quot;);226 &#125;227 228 &#125;else &#123;229 System.out.println(&quot;table不存在&quot;);230 &#125; 231 232 233 &#125;234 235 // 修改表(增加和删除)236 @Override237 public void modifyTable(String tableName) throws Exception &#123;238 //转化为表名239 TableName name = TableName.valueOf(tableName);240 //判断表是否存在241 if(admin.tableExists(name)) &#123;242 //判断表是否可用状态243 boolean tableEnabled = admin.isTableEnabled(name);244 245 if(tableEnabled) &#123;246 //使表变成不可用247 admin.disableTable(name);248 &#125;249 //根据表名得到表250 HTableDescriptor tableDescriptor = admin.getTableDescriptor(name);251 //创建列簇结构对象252 HColumnDescriptor columnFamily1 = new HColumnDescriptor(&quot;cf1&quot;.getBytes());253 HColumnDescriptor columnFamily2 = new HColumnDescriptor(&quot;cf2&quot;.getBytes());254 255 tableDescriptor.addFamily(columnFamily1);256 tableDescriptor.addFamily(columnFamily2);257 //替换该表所有的列簇258 admin.modifyTable(name, tableDescriptor);259 260 &#125;else &#123;261 System.out.println(&quot;table不存在&quot;);262 &#125; 263 &#125;264 265 // 修改表(增加和删除)266 @Override267 public void modifyTable(String tableName, String[] addColumn, String[] removeColumn) throws Exception &#123;268 //转化为表名269 TableName name = TableName.valueOf(tableName);270 //判断表是否存在271 if(admin.tableExists(name)) &#123;272 //判断表是否可用状态273 boolean tableEnabled = admin.isTableEnabled(name);274 275 if(tableEnabled) &#123;276 //使表变成不可用277 admin.disableTable(name);278 &#125;279 //根据表名得到表280 HTableDescriptor tableDescriptor = admin.getTableDescriptor(name);281 //创建列簇结构对象，添加列282 for(String add : addColumn) &#123;283 HColumnDescriptor addColumnDescriptor = new HColumnDescriptor(add);284 tableDescriptor.addFamily(addColumnDescriptor);285 &#125;286 //创建列簇结构对象，删除列287 for(String remove : removeColumn) &#123;288 HColumnDescriptor removeColumnDescriptor = new HColumnDescriptor(remove);289 tableDescriptor.removeFamily(removeColumnDescriptor.getName());290 &#125;291 292 admin.modifyTable(name, tableDescriptor);293 294 295 &#125;else &#123;296 System.out.println(&quot;table不存在&quot;);297 &#125; 298 299 &#125;300 301 @Override302 public void modifyTable(String tableName, HColumnDescriptor hcds) throws Exception &#123;303 //转化为表名304 TableName name = TableName.valueOf(tableName);305 //根据表名得到表306 HTableDescriptor tableDescriptor = admin.getTableDescriptor(name);307 //获取表中所有的列簇信息308 HColumnDescriptor[] columnFamilies = tableDescriptor.getColumnFamilies();309 310 boolean flag = false;311 //判断参数中传入的列簇是否已经在表中存在312 for(HColumnDescriptor columnFamily : columnFamilies) &#123;313 if(columnFamily.equals(hcds)) &#123;314 flag = true;315 &#125;316 &#125; 317 //存在提示，不存在直接添加该列簇信息318 if(flag) &#123;319 System.out.println(&quot;该列簇已经存在&quot;);320 &#125;else &#123;321 tableDescriptor.addFamily(hcds);322 admin.modifyTable(name, tableDescriptor);323 &#125;324 325 &#125;326 327 328 /**添加数据329 *tableName: 表明330 *rowKey: 行键331 *familyName:列簇332 *columnName:列名333 *value: 值334 */335 @Override336 public void putData(String tableName, String rowKey, String familyName, String columnName, String value)337 throws Exception &#123;338 //转化为表名339 TableName name = TableName.valueOf(tableName);340 //添加数据之前先判断表是否存在，不存在的话先创建表341 if(admin.tableExists(name)) &#123;342 343 &#125;else &#123;344 //根据表明创建表结构345 HTableDescriptor tableDescriptor = new HTableDescriptor(name);346 //定义列簇的名字347 HColumnDescriptor columnFamilyName = new HColumnDescriptor(familyName);348 tableDescriptor.addFamily(columnFamilyName);349 admin.createTable(tableDescriptor);350 351 &#125;352 353 Table table = conn.getTable(name);354 Put put = new Put(rowKey.getBytes());355 356 put.addColumn(familyName.getBytes(), columnName.getBytes(), value.getBytes());357 table.put(put);358 359 &#125;360 361 @Override362 public void putData(String tableName, String rowKey, String familyName, String columnName, String value,363 long timestamp) throws Exception &#123;364 365 // 转化为表名366 TableName name = TableName.valueOf(tableName);367 // 添加数据之前先判断表是否存在，不存在的话先创建表368 if (admin.tableExists(name)) &#123;369 370 &#125; else &#123;371 // 根据表明创建表结构372 HTableDescriptor tableDescriptor = new HTableDescriptor(name);373 // 定义列簇的名字374 HColumnDescriptor columnFamilyName = new HColumnDescriptor(familyName);375 tableDescriptor.addFamily(columnFamilyName);376 admin.createTable(tableDescriptor);377 378 &#125;379 380 Table table = conn.getTable(name);381 Put put = new Put(rowKey.getBytes());382 383 //put.addColumn(familyName.getBytes(), columnName.getBytes(), value.getBytes());384 put.addImmutable(familyName.getBytes(), columnName.getBytes(), timestamp, value.getBytes());385 table.put(put);386 387 &#125;388 389 390 // 根据rowkey查询数据391 @Override392 public Result getResult(String tableName, String rowKey) throws Exception &#123;393 394 Result result;395 TableName name = TableName.valueOf(tableName);396 if(admin.tableExists(name)) &#123;397 Table table = conn.getTable(name);398 Get get = new Get(rowKey.getBytes());399 result = table.get(get);400 401 &#125;else &#123;402 result = null;403 &#125;404 405 return result;406 &#125;407 408 // 根据rowkey查询数据409 @Override410 public Result getResult(String tableName, String rowKey, String familyName) throws Exception &#123;411 Result result;412 TableName name = TableName.valueOf(tableName);413 if(admin.tableExists(name)) &#123;414 Table table = conn.getTable(name);415 Get get = new Get(rowKey.getBytes());416 get.addFamily(familyName.getBytes());417 result = table.get(get);418 419 &#125;else &#123;420 result = null;421 &#125;422 423 return result;424 &#125;425 426 // 根据rowkey查询数据427 @Override428 public Result getResult(String tableName, String rowKey, String familyName, String columnName) throws Exception &#123;429 430 Result result;431 TableName name = TableName.valueOf(tableName);432 if(admin.tableExists(name)) &#123;433 Table table = conn.getTable(name);434 Get get = new Get(rowKey.getBytes());435 get.addColumn(familyName.getBytes(), columnName.getBytes());436 result = table.get(get);437 438 &#125;else &#123;439 result = null;440 &#125;441 442 return result;443 &#125;444 445 // 查询指定version446 @Override447 public Result getResultByVersion(String tableName, String rowKey, String familyName, String columnName,448 int versions) throws Exception &#123;449 450 Result result;451 TableName name = TableName.valueOf(tableName);452 if(admin.tableExists(name)) &#123;453 Table table = conn.getTable(name);454 Get get = new Get(rowKey.getBytes());455 get.addColumn(familyName.getBytes(), columnName.getBytes());456 get.setMaxVersions(versions);457 result = table.get(get);458 459 &#125;else &#123;460 result = null;461 &#125;462 463 return result;464 &#125;465 466 // scan全表数据467 @Override468 public ResultScanner getResultScann(String tableName) throws Exception &#123;469 470 ResultScanner result;471 TableName name = TableName.valueOf(tableName);472 if(admin.tableExists(name)) &#123;473 Table table = conn.getTable(name);474 Scan scan = new Scan();475 result = table.getScanner(scan);476 477 &#125;else &#123;478 result = null;479 &#125;480 481 return result;482 &#125;483 484 // scan全表数据485 @Override486 public ResultScanner getResultScann(String tableName, Scan scan) throws Exception &#123;487 488 ResultScanner result;489 TableName name = TableName.valueOf(tableName);490 if(admin.tableExists(name)) &#123;491 Table table = conn.getTable(name);492 result = table.getScanner(scan);493 494 &#125;else &#123;495 result = null;496 &#125;497 498 return result;499 &#125;500 501 // 删除数据（指定的列）502 @Override503 public void deleteColumn(String tableName, String rowKey) throws Exception &#123;504 505 TableName name = TableName.valueOf(tableName);506 if(admin.tableExists(name)) &#123;507 Table table = conn.getTable(name);508 Delete delete = new Delete(rowKey.getBytes());509 table.delete(delete);510 511 &#125;else &#123;512 System.out.println(&quot;table不存在&quot;);513 &#125;514 515 516 &#125;517 518 // 删除数据（指定的列）519 @Override520 public void deleteColumn(String tableName, String rowKey, String falilyName) throws Exception &#123;521 522 TableName name = TableName.valueOf(tableName);523 if(admin.tableExists(name)) &#123;524 Table table = conn.getTable(name);525 Delete delete = new Delete(rowKey.getBytes());526 delete.addFamily(falilyName.getBytes());527 table.delete(delete);528 529 &#125;else &#123;530 System.out.println(&quot;table不存在&quot;);531 &#125;532 533 &#125;534 535 // 删除数据（指定的列）536 @Override537 public void deleteColumn(String tableName, String rowKey, String falilyName, String columnName) throws Exception &#123;538 TableName name = TableName.valueOf(tableName);539 if(admin.tableExists(name)) &#123;540 Table table = conn.getTable(name);541 Delete delete = new Delete(rowKey.getBytes());542 delete.addColumn(falilyName.getBytes(), columnName.getBytes());543 table.delete(delete);544 545 &#125;else &#123;546 System.out.println(&quot;table不存在&quot;);547 &#125;548 &#125;549 550 &#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hbase","slug":"Hbase","permalink":"http://zhangfuxin.cn/tags/Hbase/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Flume学习之路 （一）Flume的基础介绍","slug":"2018-05-01-Flume学习之路 （一）Flume的基础介绍","date":"2018-05-01T02:30:04.000Z","updated":"2019-09-19T03:45:29.129Z","comments":true,"path":"2018-05-01-Flume学习之路 （一）Flume的基础介绍.html","link":"","permalink":"http://zhangfuxin.cn/2018-05-01-Flume学习之路 （一）Flume的基础介绍.html","excerpt":"** Flume学习之路 （一）Flume的基础介绍：** &lt;Excerpt in index | 首页摘要&gt; ​ Flume学习之路 （一）Flume的基础介绍","text":"** Flume学习之路 （一）Flume的基础介绍：** &lt;Excerpt in index | 首页摘要&gt; ​ Flume学习之路 （一）Flume的基础介绍 &lt;The rest of contents | 余下全文&gt; 一、背景Hadoop业务的整体开发流程： 从Hadoop的业务开发流程图中可以看出，在大数据的业务处理过程中，对于数据的采集是十分重要的一步，也是不可避免的一步. 许多公司的平台每天会产生大量的日志（一般为流式数据，如，搜索引擎的pv，查询等），处理这些日志需要特定的日志系统，一般而言，这些系统需要具有以下特征： （1） 构建应用系统和分析系统的桥梁，并将它们之间的关联解耦； （2） 支持近实时的在线分析系统和类似于Hadoop之类的离线分析系统； （3） 具有高可扩展性。即：当数据量增加时，可以通过增加节点进行水平扩展。 开源的日志系统，包括facebook的scribe，apache的chukwa，linkedin的kafka和cloudera的flume等。 二、Flume的简介 flume 作为 cloudera 开发的实时日志收集系统，受到了业界的认可与广泛应用。Flume 初始的发行版本目前被统称为 Flume OG（original generation），属于 cloudera。 但随着 FLume 功能的扩展，Flume OG 代码工程臃肿、核心组件设计不合理、核心配置不标准等缺点暴露出来，尤其是在 Flume OG 的最后一个发行版本 0.9.4. 中，日 志传输不稳定的现象尤为严重，为了解决这些问题，2011 年 10 月 22 号，cloudera 完成了 Flume-728，对 Flume 进行了里程碑式的改动：重构核心组件、核心配置以 及代码架构，重构后的版本统称为 Flume NG（next generation）；改动的另一原因是将 Flume 纳入 apache 旗下，cloudera Flume 改名为 Apache Flume。 Flume是Apache的顶级项目，官方网站：http://flume.apache.org/ Flume是一个分布式、可靠、高可用的海量日志聚合系统，支持在系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据的简单处理，并写到各种数据接收方的能力。 Flume 在0.9.x and 1.x之间有较大的架构调整，1.x版本之后的改称Flume NG，0.9.x的称为Flume OG。 Flume目前只有Linux系统的启动脚本，没有Windows环境的启动脚本。 三、Flume NG的介绍3.1 Flume特点flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。支持在日志系统中定制各类数据发送方，用于收集数据;同时，Flume提供对数据进行简单处理，并写到各种数据接受方(比如文本、HDFS、Hbase等)的能力 。 flume的数据流由事件(Event)贯穿始终。事件是Flume的基本数据单位，它携带日志数据(字节数组形式)并且携带有头信息，这些Event由Agent外部的Source生成，当Source捕获事件后会进行特定的格式化，然后Source会把事件推入(单个或多个)Channel中。你可以把Channel看作是一个缓冲区，它将保存事件直到Sink处理完该事件。Sink负责持久化日志或者把事件推向另一个Source。 （1）flume的可靠性 当节点出现故障时，日志能够被传送到其他节点上而不会丢失。Flume提供了三种级别的可靠性保障，从强到弱依次分别为：end-to-end（收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送。），Store on failure（这也是scribe采用的策略，当数据接收方crash时，将数据写到本地，待恢复后，继续发送），Besteffort（数据发送到接收方后，不会进行确认）。 （2）flume的可恢复性 还是靠Channel。推荐使用FileChannel，事件持久化在本地文件系统里(性能较差)。 3.2 Flume的一些核心概念 Client：Client生产数据，运行在一个独立的线程。 Event： 一个数据单元，消息头和消息体组成。（Events可以是日志记录、 avro 对象等。） Flow： Event从源点到达目的点的迁移的抽象。 Agent： 一个独立的Flume进程，包含组件Source、 Channel、 Sink。（Agent使用JVM 运行Flume。每台机器运行一个agent，但是可以在一个agent中包含 多个sources和sinks。） Source： 数据收集组件。（source从Client收集数据，传递给Channel） Channel： 中转Event的一个临时存储，保存由Source组件传递过来的Event。（Channel连接 sources 和 sinks ，这个有点像一个队列。） Sink： 从Channel中读取并移除Event， 将Event传递到FlowPipeline中的下一个Agent（如果有的话）（Sink从Channel收集数据，运行在一个独立线程。） 3.3 Flume NG的体系结构 Flume 运行的核心是 Agent。Flume以agent为最小的独立运行单位。一个agent就是一个JVM。它是一个完整的数据收集工具，含有三个核心组件，分别是 source、 channel、 sink。通过这些组件， Event 可以从一个地方流向另一个地方，如下图所示。 3.4 Source Source是数据的收集端，负责将数据捕获后进行特殊的格式化，将数据封装到事件（event） 里，然后将事件推入Channel中。 Flume提供了各种source的实现，包括Avro Source、Exce Source、Spooling Directory Source、NetCat Source、Syslog Source、Syslog TCP Source、Syslog UDP Source、HTTP Source、HDFS Source，etc。如果内置的Source无法满足需要， Flume还支持自定义Source。 3.5 Channel Channel是连接Source和Sink的组件，大家可以将它看做一个数据的缓冲区（数据队列），它可以将事件暂存到内存中也可以持久化到本地磁盘上， 直到Sink处理完该事件。 Flume对于Channel，则提供了Memory Channel、JDBC Chanel、File Channel，etc。 MemoryChannel可以实现高速的吞吐，但是无法保证数据的完整性。 MemoryRecoverChannel在官方文档的建议上已经建义使用FileChannel来替换。 FileChannel保证数据的完整性与一致性。在具体配置不现的FileChannel时，建议FileChannel设置的目录和程序日志文件保存的目录设成不同的磁盘，以便提高效率。 3.6 Sink Flume Sink取出Channel中的数据，进行相应的存储文件系统，数据库，或者提交到远程服务器。 Flume也提供了各种sink的实现，包括HDFS sink、Logger sink、Avro sink、File Roll sink、Null sink、HBase sink，etc。 Flume Sink在设置存储数据时，可以向文件系统中，数据库中，hadoop中储数据，在日志数据较少时，可以将数据存储在文件系中，并且设定一定的时间间隔保存数据。在日志数据较多时，可以将相应的日志数据存储到Hadoop中，便于日后进行相应的数据分析。 四、Flume的部署类型4.1 单一流程 4.2 多代理流程（多个agent顺序连接） 可以将多个Agent顺序连接起来，将最初的数据源经过收集，存储到最终的存储系统中。这是最简单的情况，一般情况下，应该控制这种顺序连接的Agent 的数量，因为数据流经的路径变长了，如果不考虑failover的话，出现故障将影响整个Flow上的Agent收集服务。 4.3 流的合并（多个Agent的数据汇聚到同一个Agent ） 这种情况应用的场景比较多，比如要收集Web网站的用户行为日志， Web网站为了可用性使用的负载集群模式，每个节点都产生用户行为日志，可以为每 个节点都配置一个Agent来单独收集日志数据，然后多个Agent将数据最终汇聚到一个用来存储数据存储系统，如HDFS上。 4.4 多路复用流（多级流） Flume还支持多级流，什么多级流？来举个例子，当syslog， java， nginx、 tomcat等混合在一起的日志流开始流入一个agent后，可以agent中将混杂的日志流分开，然后给每种日志建立一个自己的传输通道。 4.5 load balance功能 下图Agent1是一个路由节点，负责将Channel暂存的Event均衡到对应的多个Sink组件上，而每个Sink组件分别连接到一个独立的Agent上 。 五、Flume的安装5.1 Flume的下载下载地址： http://mirrors.hust.edu.cn/apache/ http://flume.apache.org/download.html 5.2 Flume的安装 Flume框架对hadoop和zookeeper的依赖只是在jar包上，并不要求flume启动时必须将hadoop和zookeeper服务也启动。 （1）将安装包上传到服务器并解压1[hadoop@hadoop1 ~]$ tar -zxvf apache-flume-1.8.0-bin.tar.gz -C apps/ （2）创建软连接1[hadoop@hadoop1 ~]$ ln -s apache-flume-1.8.0-bin/ flume （3）修改配置文件/home/hadoop/apps/apache-flume-1.8.0-bin/conf 1[hadoop@hadoop1 conf]$ cp flume-env.sh.template flume-env.sh （4）配置环境变量1234[hadoop@hadoop1 conf]$ vi ~/.bashrc #FLUMEexport FLUME_HOME=/home/hadoop/apps/flumeexport PATH=$PATH:$FLUME_HOME/bin 保存使其立即生效 1[hadoop@hadoop1 conf]$ source ~/.bashrc （5）查看版本1[hadoop@hadoop1 ~]$ flume-ng version","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"http://zhangfuxin.cn/tags/Flume/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（二十八）MapReduce的API使用（五）","slug":"2018-04-28-Hadoop学习之路（二十八）MapReduce的API使用（五）","date":"2018-04-28T02:30:04.000Z","updated":"2019-09-19T03:31:41.915Z","comments":true,"path":"2018-04-28-Hadoop学习之路（二十八）MapReduce的API使用（五）.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-28-Hadoop学习之路（二十八）MapReduce的API使用（五）.html","excerpt":"** Hadoop学习之路（二十八）MapReduce的API使用（五）：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十八）MapReduce的API使用（五）","text":"** Hadoop学习之路（二十八）MapReduce的API使用（五）：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十八）MapReduce的API使用（五） &lt;The rest of contents | 余下全文&gt; 求所有两两用户之间的共同好友 数据格式 1234567891011121314A:B,C,D,F,E,OB:A,C,E,KC:F,A,D,ID:A,E,F,LE:B,C,D,M,LF:A,B,C,D,E,O,MG:A,C,D,E,FH:A,C,D,E,OI:A,OJ:B,OK:A,C,DL:D,E,FM:E,F,GO:A,H,I,J,K 以上是数据：A:B,C,D,F,E,O表示：B,C,D,E,F,O是A用户的好友。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119 1 public class SharedFriend &#123; 2 /* 3 第一阶段的map函数主要完成以下任务 4 1.遍历原始文件中每行&lt;所有朋友&gt;信息 5 2.遍历“朋友”集合，以每个“朋友”为键，原来的“人”为值 即输出&lt;朋友,人&gt; 6 */ 7 static class SharedFriendMapper01 extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; 8 @Override 9 protected void map(LongWritable key, Text value,Context context) 10 throws IOException, InterruptedException &#123; 11 String line = value.toString(); 12 String[] person_friends = line.split(&quot;:&quot;); 13 String person = person_friends[0]; 14 String[] friends = person_friends[1].split(&quot;,&quot;); 15 16 for(String friend : friends)&#123; 17 context.write(new Text(friend), new Text(person)); 18 &#125; 19 &#125; 20 &#125; 21 22 /* 23 第一阶段的reduce函数主要完成以下任务 24 1.对所有传过来的&lt;朋友，list(人)&gt;进行拼接，输出&lt;朋友,拥有这名朋友的所有人&gt; 25 */ 26 static class SharedFriendReducer01 extends Reducer&lt;Text, Text, Text, Text&gt;&#123; 27 @Override 28 protected void reduce(Text key, Iterable&lt;Text&gt; values,Context context) 29 throws IOException, InterruptedException &#123; 30 StringBuffer sb = new StringBuffer(); 31 for(Text friend : values)&#123; 32 sb.append(friend.toString()).append(&quot;,&quot;); 33 &#125; 34 sb.deleteCharAt(sb.length()-1); 35 context.write(key, new Text(sb.toString())); 36 &#125; 37 &#125; 38 39 /* 40 第二阶段的map函数主要完成以下任务 41 1.将上一阶段reduce输出的&lt;朋友,拥有这名朋友的所有人&gt;信息中的 “拥有这名朋友的所有人”进行排序 ，以防出现B-C C-B这样的重复 42 2.将 “拥有这名朋友的所有人”进行两两配对，并将配对后的字符串当做键，“朋友”当做值输出，即输出&lt;人-人，共同朋友&gt; 43 */ 44 static class SharedFriendMapper02 extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; 45 @Override 46 protected void map(LongWritable key, Text value,Context context) 47 throws IOException, InterruptedException &#123; 48 String line = value.toString(); 49 String[] friend_persons = line.split(&quot;\\t&quot;); 50 String friend = friend_persons[0]; 51 String[] persons = friend_persons[1].split(&quot;,&quot;); 52 Arrays.sort(persons); //排序 53 54 //两两配对 55 for(int i=0;i&lt;persons.length-1;i++)&#123; 56 for(int j=i+1;j&lt;persons.length;j++)&#123; 57 context.write(new Text(persons[i]+&quot;-&quot;+persons[j]+&quot;:&quot;), new Text(friend)); 58 &#125; 59 &#125; 60 &#125; 61 &#125; 62 63 /* 64 第二阶段的reduce函数主要完成以下任务 65 1.&lt;人-人，list(共同朋友)&gt; 中的“共同好友”进行拼接 最后输出&lt;人-人，两人的所有共同好友&gt; 66 */ 67 static class SharedFriendReducer02 extends Reducer&lt;Text, Text, Text, Text&gt;&#123; 68 @Override 69 protected void reduce(Text key, Iterable&lt;Text&gt; values,Context context) 70 throws IOException, InterruptedException &#123; 71 StringBuffer sb = new StringBuffer(); 72 Set&lt;String&gt; set = new HashSet&lt;String&gt;(); 73 for(Text friend : values)&#123; 74 if(!set.contains(friend.toString())) 75 set.add(friend.toString()); 76 &#125; 77 for(String friend : set)&#123; 78 sb.append(friend.toString()).append(&quot;,&quot;); 79 &#125; 80 sb.deleteCharAt(sb.length()-1); 81 82 context.write(key, new Text(sb.toString())); 83 &#125; 84 &#125; 85 86 public static void main(String[] args)throws Exception &#123; 87 Configuration conf = new Configuration(); 88 89 //第一阶段 90 Job job1 = Job.getInstance(conf); 91 job1.setJarByClass(SharedFriend.class); 92 job1.setMapperClass(SharedFriendMapper01.class); 93 job1.setReducerClass(SharedFriendReducer01.class); 94 95 job1.setOutputKeyClass(Text.class); 96 job1.setOutputValueClass(Text.class); 97 98 FileInputFormat.setInputPaths(job1, new Path(&quot;H:/大数据/mapreduce/sharedfriend/input&quot;)); 99 FileOutputFormat.setOutputPath(job1, new Path(&quot;H:/大数据/mapreduce/sharedfriend/output&quot;));100 101 boolean res1 = job1.waitForCompletion(true);102 103 //第二阶段104 Job job2 = Job.getInstance(conf);105 job2.setJarByClass(SharedFriend.class);106 job2.setMapperClass(SharedFriendMapper02.class);107 job2.setReducerClass(SharedFriendReducer02.class);108 109 job2.setOutputKeyClass(Text.class);110 job2.setOutputValueClass(Text.class);111 112 FileInputFormat.setInputPaths(job2, new Path(&quot;H:/大数据/mapreduce/sharedfriend/output&quot;));113 FileOutputFormat.setOutputPath(job2, new Path(&quot;H:/大数据/mapreduce/sharedfriend/output01&quot;));114 115 boolean res2 = job2.waitForCompletion(true);116 117 System.exit(res1?0:1);118 &#125;119 &#125; 第一阶段输出结果 1234567891011121314 1 A F,I,O,K,G,D,C,H,B 2 B E,J,F,A 3 C B,E,K,A,H,G,F 4 D H,C,G,F,E,A,K,L 5 E A,B,L,G,M,F,D,H 6 F C,M,L,A,D,G 7 G M 8 H O 9 I O,C10 J O11 K O,B12 L D,E13 M E,F14 O A,H,I,J,F 第二阶段输出结果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374 1 A-B C,E 2 A-C D,F 3 A-D E,F 4 A-E C,B,D 5 A-F E,O,C,D,B 6 A-G F,C,E,D 7 A-H D,O,C,E 8 A-I O 9 A-J B,O10 A-K C,D11 A-L D,E,F12 A-M E,F13 B-C A14 B-D A,E15 B-E C16 B-F A,C,E17 B-G E,C,A18 B-H A,E,C19 B-I A20 B-K A,C21 B-L E22 B-M E23 B-O K,A24 C-D F,A25 C-E D26 C-F D,A27 C-G D,F,A28 C-H D,A29 C-I A30 C-K A,D31 C-L D,F32 C-M F33 C-O I,A34 D-E L35 D-F A,E36 D-G F,A,E37 D-H A,E38 D-I A39 D-K A40 D-L F,E41 D-M F,E42 D-O A43 E-F C,D,M,B44 E-G C,D45 E-H C,D46 E-J B47 E-K D,C48 E-L D49 F-G C,E,D,A50 F-H D,O,A,E,C51 F-I A,O52 F-J O,B53 F-K D,C,A54 F-L D,E55 F-M E56 F-O A57 G-H E,C,D,A58 G-I A59 G-K D,A,C60 G-L F,E,D61 G-M E,F62 G-O A63 H-I A,O64 H-J O65 H-K C,D,A66 H-L D,E67 H-M E68 H-O A69 I-J O70 I-K A71 I-O A72 K-L D73 K-O A74 L-M F,E","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（二十七）MapReduce的API使用（四）","slug":"2018-04-27-Hadoop学习之路（二十七）MapReduce的API使用（四）","date":"2018-04-27T02:30:04.000Z","updated":"2019-09-19T03:04:58.066Z","comments":true,"path":"2018-04-27-Hadoop学习之路（二十七）MapReduce的API使用（四）.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-27-Hadoop学习之路（二十七）MapReduce的API使用（四）.html","excerpt":"** Hadoop学习之路（二十七）MapReduce的API使用（四）：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十七）MapReduce的API使用（四）","text":"** Hadoop学习之路（二十七）MapReduce的API使用（四）：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十七）MapReduce的API使用（四） &lt;The rest of contents | 余下全文&gt; 第一题 下面是三种商品的销售数据 要求：根据以上数据，用 MapReduce 统计出如下数据： 1、每种商品的销售总金额，并降序排序 2、每种商品销售额最多的三周 第二题：MapReduce 题 现有如下数据文件需要处理: 格式：CSV 数据样例： user_a,location_a,2018-01-01 08:00:00,60 user_a,location_a,2018-01-01 09:00:00,60 user_a,location_b,2018-01-01 10:00:00,60 user_a,location_a,2018-01-01 11:00:00,60 字段：用户 ID，位置 ID，开始时间，停留时长（分钟） 数据意义：某个用户在某个位置从某个时刻开始停留了多长时间 处理逻辑： 对同一个用户，在同一个位置，连续的多条记录进行合并 合并原则：开始时间取最早的，停留时长加和 要求：请编写 MapReduce 程序实现 其他：只有数据样例，没有数据。 UserLocationMR.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139 1 /** 2 测试数据： 3 user_a location_a 2018-01-01 08:00:00 60 4 user_a location_a 2018-01-01 09:00:00 60 5 user_a location_a 2018-01-01 11:00:00 60 6 user_a location_a 2018-01-01 12:00:00 60 7 user_a location_b 2018-01-01 10:00:00 60 8 user_a location_c 2018-01-01 08:00:00 60 9 user_a location_c 2018-01-01 09:00:00 60 10 user_a location_c 2018-01-01 10:00:00 60 11 user_b location_a 2018-01-01 15:00:00 60 12 user_b location_a 2018-01-01 16:00:00 60 13 user_b location_a 2018-01-01 18:00:00 60 14 15 16 结果数据： 17 user_a location_a 2018-01-01 08:00:00 120 18 user_a location_a 2018-01-01 11:00:00 120 19 user_a location_b 2018-01-01 10:00:00 60 20 user_a location_c 2018-01-01 08:00:00 180 21 user_b location_a 2018-01-01 15:00:00 120 22 user_b location_a 2018-01-01 18:00:00 60 23 24 25 */ 26 public class UserLocationMR &#123; 27 28 public static void main(String[] args) throws Exception &#123; 29 // 指定hdfs相关的参数 30 Configuration conf = new Configuration(); 31 // conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop02:9000&quot;); 32 // System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;); 33 34 Job job = Job.getInstance(conf); 35 // 设置jar包所在路径 36 job.setJarByClass(UserLocationMR.class); 37 38 // 指定mapper类和reducer类 39 job.setMapperClass(UserLocationMRMapper.class); 40 job.setReducerClass(UserLocationMRReducer.class); 41 42 // 指定maptask的输出类型 43 job.setMapOutputKeyClass(UserLocation.class); 44 job.setMapOutputValueClass(NullWritable.class); 45 // 指定reducetask的输出类型 46 job.setOutputKeyClass(UserLocation.class); 47 job.setOutputValueClass(NullWritable.class); 48 49 job.setGroupingComparatorClass(UserLocationGC.class); 50 51 // 指定该mapreduce程序数据的输入和输出路径 52 Path inputPath = new Path(&quot;D:\\\\武文\\\\second\\\\input&quot;); 53 Path outputPath = new Path(&quot;D:\\\\武文\\\\second\\\\output2&quot;); 54 FileSystem fs = FileSystem.get(conf); 55 if (fs.exists(outputPath)) &#123; 56 fs.delete(outputPath, true); 57 &#125; 58 FileInputFormat.setInputPaths(job, inputPath); 59 FileOutputFormat.setOutputPath(job, outputPath); 60 61 // 最后提交任务 62 boolean waitForCompletion = job.waitForCompletion(true); 63 System.exit(waitForCompletion ? 0 : 1); 64 &#125; 65 66 private static class UserLocationMRMapper extends Mapper&lt;LongWritable, Text, UserLocation, NullWritable&gt; &#123; 67 68 UserLocation outKey = new UserLocation(); 69 70 /** 71 * value = user_a,location_a,2018-01-01 12:00:00,60 72 */ 73 @Override 74 protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; 75 76 String[] split = value.toString().split(&quot;,&quot;); 77 78 outKey.set(split); 79 80 context.write(outKey, NullWritable.get()); 81 &#125; 82 &#125; 83 84 private static class UserLocationMRReducer extends Reducer&lt;UserLocation, NullWritable, UserLocation, NullWritable&gt; &#123; 85 86 SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); 87 88 UserLocation outKey = new UserLocation(); 89 90 /** 91 * user_a location_a 2018-01-01 08:00:00 60 92 * user_a location_a 2018-01-01 09:00:00 60 93 * user_a location_a 2018-01-01 11:00:00 60 94 * user_a location_a 2018-01-01 12:00:00 60 95 */ 96 @Override 97 protected void reduce(UserLocation key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; 98 99 int count = 0;100 for (NullWritable nvl : values) &#123;101 count++;102 // 如果是这一组key-value中的第一个元素时，直接赋值给outKey对象。基础对象103 if (count == 1) &#123;104 // 复制值105 outKey.set(key);106 &#125; else &#123;107 108 // 有可能连续，有可能不连续， 连续则继续变量， 否则输出109 long current_timestamp = 0;110 long last_timestamp = 0;111 try &#123;112 // 这是新遍历出来的记录的时间戳113 current_timestamp = sdf.parse(key.getTime()).getTime();114 // 这是上一条记录的时间戳 和 停留时间之和115 last_timestamp = sdf.parse(outKey.getTime()).getTime() + outKey.getDuration() * 60 * 1000;116 &#125; catch (ParseException e) &#123;117 e.printStackTrace();118 &#125;119 120 // 如果相等，证明是连续记录，所以合并121 if (current_timestamp == last_timestamp) &#123;122 123 outKey.setDuration(outKey.getDuration() + key.getDuration());124 125 &#125; else &#123;126 127 // 先输出上一条记录128 context.write(outKey, nvl);129 130 // 然后再次记录当前遍历到的这一条记录131 outKey.set(key);132 &#125;133 &#125;134 &#125;135 // 最后无论如何，还得输出最后一次136 context.write(outKey, NullWritable.get());137 &#125;138 &#125;139 &#125; UserLocation.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118 1 public class UserLocation implements WritableComparable&lt;UserLocation&gt; &#123; 2 3 private String userid; 4 private String locationid; 5 private String time; 6 private long duration; 7 8 @Override 9 public String toString() &#123; 10 return userid + &quot;\\t&quot; + locationid + &quot;\\t&quot; + time + &quot;\\t&quot; + duration; 11 &#125; 12 13 public UserLocation() &#123; 14 super(); 15 &#125; 16 17 public void set(String[] split)&#123; 18 this.setUserid(split[0]); 19 this.setLocationid(split[1]); 20 this.setTime(split[2]); 21 this.setDuration(Long.parseLong(split[3])); 22 &#125; 23 24 public void set(UserLocation ul)&#123; 25 this.setUserid(ul.getUserid()); 26 this.setLocationid(ul.getLocationid()); 27 this.setTime(ul.getTime()); 28 this.setDuration(ul.getDuration()); 29 &#125; 30 31 public UserLocation(String userid, String locationid, String time, long duration) &#123; 32 super(); 33 this.userid = userid; 34 this.locationid = locationid; 35 this.time = time; 36 this.duration = duration; 37 &#125; 38 39 public String getUserid() &#123; 40 return userid; 41 &#125; 42 43 public void setUserid(String userid) &#123; 44 this.userid = userid; 45 &#125; 46 47 public String getLocationid() &#123; 48 return locationid; 49 &#125; 50 51 public void setLocationid(String locationid) &#123; 52 this.locationid = locationid; 53 &#125; 54 55 public String getTime() &#123; 56 return time; 57 &#125; 58 59 public void setTime(String time) &#123; 60 this.time = time; 61 &#125; 62 63 public long getDuration() &#123; 64 return duration; 65 &#125; 66 67 public void setDuration(long duration) &#123; 68 this.duration = duration; 69 &#125; 70 71 @Override 72 public void write(DataOutput out) throws IOException &#123; 73 // TODO Auto-generated method stub 74 out.writeUTF(userid); 75 out.writeUTF(locationid); 76 out.writeUTF(time); 77 out.writeLong(duration); 78 &#125; 79 80 @Override 81 public void readFields(DataInput in) throws IOException &#123; 82 // TODO Auto-generated method stub 83 this.userid = in.readUTF(); 84 this.locationid = in.readUTF(); 85 this.time = in.readUTF(); 86 this.duration = in.readLong(); 87 &#125; 88 89 /** 90 * 排序规则 91 * 92 * 按照 userid locationid 和 time 排序 都是 升序 93 */ 94 @Override 95 public int compareTo(UserLocation o) &#123; 96 97 int diff_userid = o.getUserid().compareTo(this.getUserid()); 98 if(diff_userid == 0)&#123; 99 100 int diff_location = o.getLocationid().compareTo(this.getLocationid());101 if(diff_location == 0)&#123;102 103 int diff_time = o.getTime().compareTo(this.getTime());104 if(diff_time == 0)&#123;105 return 0;106 &#125;else&#123;107 return diff_time &gt; 0 ? -1 : 1;108 &#125;109 110 &#125;else&#123;111 return diff_location &gt; 0 ? -1 : 1;112 &#125;113 114 &#125;else&#123;115 return diff_userid &gt; 0 ? -1 : 1;116 &#125;117 &#125;118 &#125; UserLocationGC.java 1234567891011121314151617181920212223242526272829 1 public class UserLocationGC extends WritableComparator&#123; 2 3 public UserLocationGC()&#123; 4 super(UserLocation.class, true); 5 &#125; 6 7 @Override 8 public int compare(WritableComparable a, WritableComparable b) &#123; 9 10 UserLocation ul_a = (UserLocation)a;11 UserLocation ul_b = (UserLocation)b;12 13 int diff_userid = ul_a.getUserid().compareTo(ul_b.getUserid());14 if(diff_userid == 0)&#123;15 16 int diff_location = ul_a.getLocationid().compareTo(ul_b.getLocationid());17 if(diff_location == 0)&#123;18 19 return 0;20 21 &#125;else&#123;22 return diff_location &gt; 0 ? -1 : 1;23 &#125;24 25 &#125;else&#123;26 return diff_userid &gt; 0 ? -1 : 1;27 &#125;28 &#125;29 &#125; 第三题：MapReduce 题–倒排索引 概念： 倒排索引（Inverted Index），也常被称为反向索引、置入档案或反向档案，是一种索引方法， 被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档 检索系统中最常用的数据结构。了解详情可自行百度 有两份数据： mapreduce-4-1.txt 1234huangbo love xuzhenghuangxiaoming love baby huangxiaoming love yangmiliangchaowei love liujialinghuangxiaoming xuzheng huangbo wangbaoqiang mapreduce-4-2.txt 123hello huangbohello xuzhenghello huangxiaoming 题目一：编写 MapReduce 求出以下格式的结果数据：统计每个关键词在每个文档中当中的 第几行出现了多少次 例如，huangxiaoming 关键词的格式： 1huangixaoming mapreduce-4-1.txt:2,2; mapreduce-4-1.txt:4,1;mapreduce-4-2.txt:3,1 以上答案的意义： 123关键词 huangxiaoming 在第一份文档 mapreduce-4-1.txt 中的第 2 行出现了 2 次关键词 huangxiaoming 在第一份文档 mapreduce-4-1.txt 中的第 4 行出现了 1 次关键词 huangxiaoming 在第二份文档 mapreduce-4-2.txt 中的第 3 行出现了 1 次 题目二：编写 MapReduce 程序求出每个关键词在每个文档出现了多少次，并且按照出现次 数降序排序 例如： 1huangixaoming mapreduce-4-1.txt,3;mapreduce-4-2.txt,1 以上答案的含义： 表示关键词 huangxiaoming 在第一份文档 mapreduce-4-1.txt 中出现了 3 次，在第二份文档mapreduce-4-2.txt 中出现了 1 次","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（二十六）MapReduce的API使用（三）","slug":"2018-04-26-Hadoop学习之路（二十六）MapReduce的API使用（三）","date":"2018-04-26T02:30:04.000Z","updated":"2019-09-19T03:02:25.830Z","comments":true,"path":"2018-04-26-Hadoop学习之路（二十六）MapReduce的API使用（三）.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-26-Hadoop学习之路（二十六）MapReduce的API使用（三）.html","excerpt":"** Hadoop学习之路（二十六）MapReduce的API使用（三）：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十六）MapReduce的API使用（三）","text":"** Hadoop学习之路（二十六）MapReduce的API使用（三）：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十六）MapReduce的API使用（三） &lt;The rest of contents | 余下全文&gt; 数据及需求数据格式movies.dat 3884条数据 123456789101::Toy Story (1995)::Animation|Children&apos;s|Comedy2::Jumanji (1995)::Adventure|Children&apos;s|Fantasy3::Grumpier Old Men (1995)::Comedy|Romance4::Waiting to Exhale (1995)::Comedy|Drama5::Father of the Bride Part II (1995)::Comedy6::Heat (1995)::Action|Crime|Thriller7::Sabrina (1995)::Comedy|Romance8::Tom and Huck (1995)::Adventure|Children&apos;s9::Sudden Death (1995)::Action10::GoldenEye (1995)::Action|Adventure|Thriller users.dat 6041条数据 123456789101::F::1::10::480672::M::56::16::700723::M::25::15::551174::M::45::7::024605::M::25::20::554556::F::50::9::551177::M::35::1::068108::M::25::12::114139::M::25::17::6161410::F::35::1::95370 ratings.dat 1000210条数据 123456789101::1193::5::9783007601::661::3::9783021091::914::3::9783019681::3408::4::9783002751::2355::5::9788242911::1197::3::9783022681::1287::5::9783020391::2804::5::9783007191::594::4::9783022681::919::4::978301368 数据解释1、users.dat 数据格式为： 2::M::56::16::70072对应字段为：UserID BigInt, Gender String, Age Int, Occupation String, Zipcode String对应字段中文解释：用户id，性别，年龄，职业，邮政编码 2、movies.dat 数据格式为： 2::Jumanji (1995)::Adventure|Children’s|Fantasy对应字段为：MovieID BigInt, Title String, Genres String对应字段中文解释：电影ID，电影名字，电影类型 3、ratings.dat 数据格式为： 1::1193::5::978300760对应字段为：UserID BigInt, MovieID BigInt, Rating Double, Timestamped String对应字段中文解释：用户ID，电影ID，评分，评分时间戳 用户ID，电影ID，评分，评分时间戳，性别，年龄，职业，邮政编码，电影名字，电影类型userid, movieId, rate, ts, gender, age, occupation, zipcode, movieName, movieType 需求统计（1）求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）（2）分别求男性，女性当中评分最高的10部电影（性别，电影名，评分）（3）求movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，评分）（4）求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（人，电影名，影评）（5）求好片（评分&gt;=4.0）最多的那个年份的最好看的10部电影（6）求1997年上映的电影中，评分最高的10部Comedy类电影（7）该影评库中各种类型电影中评价最高的5部电影（类型，电影名，平均影评分）（8）各年评分最高的电影类型（年份，类型，影评分）（9）每个地区最高评分的电影名，把结果存入HDFS（地区，电影名，电影评分） 代码实现1、求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）分析：此问题涉及到2个文件，ratings.dat和movies.dat，2个文件数据量倾斜比较严重，此处应该使用mapjoin方法，先将数据量较小的文件预先加载到内存中 MovieMR1_1.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151 1 public class MovieMR1_1 &#123; 2 3 public static void main(String[] args) throws Exception &#123; 4 5 if(args.length &lt; 4) &#123; 6 args = new String[4]; 7 args[0] = &quot;/movie/input/&quot;; 8 args[1] = &quot;/movie/output/&quot;; 9 args[2] = &quot;/movie/cache/movies.dat&quot;; 10 args[3] = &quot;/movie/output_last/&quot;; 11 &#125; 12 13 14 Configuration conf1 = new Configuration(); 15 conf1.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000/&quot;); 16 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;); 17 FileSystem fs1 = FileSystem.get(conf1); 18 19 20 Job job1 = Job.getInstance(conf1); 21 22 job1.setJarByClass(MovieMR1_1.class); 23 24 job1.setMapperClass(MoviesMapJoinRatingsMapper1.class); 25 job1.setReducerClass(MovieMR1Reducer1.class); 26 27 job1.setMapOutputKeyClass(Text.class); 28 job1.setMapOutputValueClass(IntWritable.class); 29 30 job1.setOutputKeyClass(Text.class); 31 job1.setOutputValueClass(IntWritable.class); 32 33 34 35 //缓存普通文件到task运行节点的工作目录 36 URI uri = new URI(&quot;hdfs://hadoop1:9000&quot;+args[2]); 37 System.out.println(uri); 38 job1.addCacheFile(uri); 39 40 41 Path inputPath1 = new Path(args[0]); 42 Path outputPath1 = new Path(args[1]); 43 if(fs1.exists(outputPath1)) &#123; 44 fs1.delete(outputPath1, true); 45 &#125; 46 FileInputFormat.setInputPaths(job1, inputPath1); 47 FileOutputFormat.setOutputPath(job1, outputPath1); 48 49 boolean isDone = job1.waitForCompletion(true); 50 System.exit(isDone ? 0 : 1); 51 52 &#125; 53 54 public static class MoviesMapJoinRatingsMapper1 extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; 55 56 //用了存放加载到内存中的movies.dat数据 57 private static Map&lt;String,String&gt; movieMap = new HashMap&lt;&gt;(); 58 //key：电影ID 59 Text outKey = new Text(); 60 //value：电影名+电影类型 61 IntWritable outValue = new IntWritable(); 62 63 64 /** 65 * movies.dat: 1::Toy Story (1995)::Animation|Children&apos;s|Comedy 66 * 67 * 68 * 将小表(movies.dat)中的数据预先加载到内存中去 69 * */ 70 @Override 71 protected void setup(Context context) throws IOException, InterruptedException &#123; 72 73 Path[] localCacheFiles = context.getLocalCacheFiles(); 74 75 76 String strPath = localCacheFiles[0].toUri().toString(); 77 78 BufferedReader br = new BufferedReader(new FileReader(strPath)); 79 String readLine; 80 while((readLine = br.readLine()) != null) &#123; 81 82 String[] split = readLine.split(&quot;::&quot;); 83 String movieId = split[0]; 84 String movieName = split[1]; 85 String movieType = split[2]; 86 87 movieMap.put(movieId, movieName+&quot;\\t&quot;+movieType); 88 &#125; 89 90 br.close(); 91 &#125; 92 93 94 /** 95 * movies.dat: 1 :: Toy Story (1995) :: Animation|Children&apos;s|Comedy 96 * 电影ID 电影名字 电影类型 97 * 98 * ratings.dat: 1 :: 1193 :: 5 :: 978300760 99 * 用户ID 电影ID 评分 评分时间戳100 * 101 * value: ratings.dat读取的数据102 * */103 @Override104 protected void map(LongWritable key, Text value, Context context)105 throws IOException, InterruptedException &#123;106 107 String[] split = value.toString().split(&quot;::&quot;);108 109 String userId = split[0];110 String movieId = split[1];111 String movieRate = split[2];112 113 //根据movieId从内存中获取电影名和类型114 String movieNameAndType = movieMap.get(movieId);115 String movieName = movieNameAndType.split(&quot;\\t&quot;)[0];116 String movieType = movieNameAndType.split(&quot;\\t&quot;)[1];117 118 outKey.set(movieName);119 outValue.set(Integer.parseInt(movieRate));120 121 context.write(outKey, outValue);122 123 &#125;124 125 &#125;126 127 128 public static class MovieMR1Reducer1 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;129 //每部电影评论的次数130 int count;131 //评分次数132 IntWritable outValue = new IntWritable();133 134 @Override135 protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123;136 137 count = 0;138 139 for(IntWritable value : values) &#123;140 count++;141 &#125;142 143 outValue.set(count);144 145 context.write(key, outValue);146 &#125;147 148 &#125;149 150 151 &#125; MovieMR1_2.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788 1 public class MovieMR1_2 &#123; 2 3 public static void main(String[] args) throws Exception &#123; 4 if(args.length &lt; 2) &#123; 5 args = new String[2]; 6 args[0] = &quot;/movie/output/&quot;; 7 args[1] = &quot;/movie/output_last/&quot;; 8 &#125; 9 10 11 Configuration conf1 = new Configuration();12 conf1.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000/&quot;);13 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);14 FileSystem fs1 = FileSystem.get(conf1);15 16 17 Job job = Job.getInstance(conf1);18 19 job.setJarByClass(MovieMR1_2.class);20 21 job.setMapperClass(MoviesMapJoinRatingsMapper2.class);22 job.setReducerClass(MovieMR1Reducer2.class);23 24 25 job.setMapOutputKeyClass(MovieRating.class);26 job.setMapOutputValueClass(NullWritable.class);27 28 job.setOutputKeyClass(MovieRating.class);29 job.setOutputValueClass(NullWritable.class);30 31 32 Path inputPath1 = new Path(args[0]);33 Path outputPath1 = new Path(args[1]);34 if(fs1.exists(outputPath1)) &#123;35 fs1.delete(outputPath1, true);36 &#125;37 //对第一步的输出结果进行降序排序38 FileInputFormat.setInputPaths(job, inputPath1);39 FileOutputFormat.setOutputPath(job, outputPath1);40 41 boolean isDone = job.waitForCompletion(true);42 System.exit(isDone ? 0 : 1);43 44 45 &#125;46 47 //注意输出类型为自定义对象MovieRating，MovieRating按照降序排序48 public static class MoviesMapJoinRatingsMapper2 extends Mapper&lt;LongWritable, Text, MovieRating, NullWritable&gt;&#123;49 50 MovieRating outKey = new MovieRating();51 52 @Override53 protected void map(LongWritable key, Text value, Context context)54 throws IOException, InterruptedException &#123;55 //&apos;Night Mother (1986) 7056 String[] split = value.toString().split(&quot;\\t&quot;);57 58 outKey.setCount(Integer.parseInt(split[1]));;59 outKey.setMovieName(split[0]);60 61 context.write(outKey, NullWritable.get());62 63 &#125;64 65 &#125;66 67 //排序之后自然输出，只取前10部电影68 public static class MovieMR1Reducer2 extends Reducer&lt;MovieRating, NullWritable, MovieRating, NullWritable&gt;&#123;69 70 Text outKey = new Text();71 int count = 0;72 73 @Override74 protected void reduce(MovieRating key, Iterable&lt;NullWritable&gt; values,Context context) throws IOException, InterruptedException &#123;75 76 for(NullWritable value : values) &#123;77 count++;78 if(count &gt; 10) &#123;79 return;80 &#125;81 context.write(key, value);82 83 &#125;84 85 &#125;86 87 &#125;88 &#125; MovieRating.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 1 public class MovieRating implements WritableComparable&lt;MovieRating&gt;&#123; 2 private String movieName; 3 private int count; 4 5 public String getMovieName() &#123; 6 return movieName; 7 &#125; 8 public void setMovieName(String movieName) &#123; 9 this.movieName = movieName;10 &#125;11 public int getCount() &#123;12 return count;13 &#125;14 public void setCount(int count) &#123;15 this.count = count;16 &#125;17 18 public MovieRating() &#123;&#125;19 20 public MovieRating(String movieName, int count) &#123;21 super();22 this.movieName = movieName;23 this.count = count;24 &#125;25 26 27 @Override28 public String toString() &#123;29 return movieName + &quot;\\t&quot; + count;30 &#125;31 @Override32 public void readFields(DataInput in) throws IOException &#123;33 movieName = in.readUTF();34 count = in.readInt();35 &#125;36 @Override37 public void write(DataOutput out) throws IOException &#123;38 out.writeUTF(movieName);39 out.writeInt(count);40 &#125;41 @Override42 public int compareTo(MovieRating o) &#123;43 return o.count - this.count ;44 &#125;45 46 &#125; 2、分别求男性，女性当中评分最高的10部电影（性别，电影名，评分）分析：此问题涉及到3个表的联合查询，需要先将2个小表的数据预先加载到内存中，再进行查询 对三表进行联合 MoviesThreeTableJoin.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126 1 /** 2 * 进行3表的联合查询 3 * 4 * */ 5 public class MoviesThreeTableJoin &#123; 6 7 public static void main(String[] args) throws Exception &#123; 8 9 if(args.length &lt; 4) &#123; 10 args = new String[4]; 11 args[0] = &quot;/movie/input/&quot;; 12 args[1] = &quot;/movie/output2/&quot;; 13 args[2] = &quot;/movie/cache/movies.dat&quot;; 14 args[3] = &quot;/movie/cache/users.dat&quot;; 15 &#125; 16 17 Configuration conf = new Configuration(); 18 conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000/&quot;); 19 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;); 20 FileSystem fs = FileSystem.get(conf); 21 Job job = Job.getInstance(conf); 22 23 job.setJarByClass(MoviesThreeTableJoin.class); 24 job.setMapperClass(ThreeTableMapper.class); 25 26 job.setOutputKeyClass(Text.class); 27 job.setOutputValueClass(NullWritable.class); 28 29 URI uriUsers = new URI(&quot;hdfs://hadoop1:9000&quot;+args[3]); 30 URI uriMovies = new URI(&quot;hdfs://hadoop1:9000&quot;+args[2]); 31 job.addCacheFile(uriUsers); 32 job.addCacheFile(uriMovies); 33 34 Path inputPath = new Path(args[0]); 35 Path outputPath = new Path(args[1]); 36 37 if(fs.exists(outputPath)) &#123; 38 fs.delete(outputPath,true); 39 &#125; 40 41 FileInputFormat.setInputPaths(job, inputPath); 42 FileOutputFormat.setOutputPath(job, outputPath); 43 44 boolean isDone = job.waitForCompletion(true); 45 System.exit(isDone ? 0 : 1); 46 47 &#125; 48 49 50 public static class ThreeTableMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; 51 52 53 //用于缓存movies和users中数据 54 private Map&lt;String,String&gt; moviesMap = new HashMap&lt;&gt;(); 55 private Map&lt;String,String&gt; usersMap = new HashMap&lt;&gt;(); 56 //用来存放读取的ratings.dat中的一行数据 57 String[] ratings; 58 59 60 Text outKey = new Text(); 61 62 @Override 63 protected void setup(Context context) throws IOException, InterruptedException &#123; 64 65 BufferedReader br = null; 66 67 Path[] paths = context.getLocalCacheFiles(); 68 String usersLine = null; 69 String moviesLine = null; 70 71 for(Path path : paths) &#123; 72 String name = path.toUri().getPath(); 73 if(name.contains(&quot;movies.dat&quot;)) &#123; 74 //读取movies.dat文件中的一行数据 75 br = new BufferedReader(new FileReader(name)); 76 while((moviesLine = br.readLine()) != null) &#123; 77 /**对读取的这行数据按照：：进行切分 78 * 2::Jumanji (1995)::Adventure|Children&apos;s|Fantasy 79 * 电影ID，电影名字，电影类型 80 * 81 *电影ID作为key，其余作为value 82 */ 83 String[] split = moviesLine.split(&quot;::&quot;); 84 moviesMap.put(split[0], split[1]+&quot;::&quot;+split[2]); 85 &#125; 86 &#125;else if(name.contains(&quot;users.dat&quot;)) &#123; 87 //读取users.dat文件中的一行数据 88 br = new BufferedReader(new FileReader(name)); 89 while((usersLine = br.readLine()) != null) &#123; 90 /** 91 * 对读取的这行数据按照：：进行切分 92 * 2::M::56::16::70072 93 * 用户id，性别，年龄，职业，邮政编码 94 * 95 * 用户ID作为key，其他的作为value 96 * */ 97 String[] split = usersLine.split(&quot;::&quot;); 98 System.out.println(split[0]+&quot;----&quot;+split[1]); 99 usersMap.put(split[0], split[1]+&quot;::&quot;+split[2]+&quot;::&quot;+split[3]+&quot;::&quot;+split[4]);100 &#125;101 &#125;102 103 &#125;104 105 &#125;106 107 108 @Override109 protected void map(LongWritable key, Text value, Context context)110 throws IOException, InterruptedException &#123;111 112 ratings = value.toString().split(&quot;::&quot;);113 //通过电影ID和用户ID获取用户表和电影表中的其他信息114 String movies = moviesMap.get(ratings[1]);115 String users = usersMap.get(ratings[0]);116 117 //三表信息的联合118 String threeTables = value.toString()+&quot;::&quot;+movies+&quot;::&quot;+users;119 outKey.set(threeTables);120 121 context.write(outKey, NullWritable.get());122 &#125;123 &#125;124 125 126 &#125; 三表联合之后的数据为 123456789101000::1023::5::975041651::Winnie the Pooh and the Blustery Day (1968)::Animation|Children&apos;s::F::25::6::900271000::1029::3::975041859::Dumbo (1941)::Animation|Children&apos;s|Musical::F::25::6::900271000::1036::4::975040964::Die Hard (1988)::Action|Thriller::F::25::6::900271000::1104::5::975042421::Streetcar Named Desire, A (1951)::Drama::F::25::6::900271000::110::5::975040841::Braveheart (1995)::Action|Drama|War::F::25::6::900271000::1196::3::975040841::Star Wars: Episode V - The Empire Strikes Back (1980)::Action|Adventure|Drama|Sci-Fi|War::F::25::6::900271000::1198::5::975040841::Raiders of the Lost Ark (1981)::Action|Adventure::F::25::6::900271000::1200::4::975041125::Aliens (1986)::Action|Sci-Fi|Thriller|War::F::25::6::900271000::1201::5::975041025::Good, The Bad and The Ugly, The (1966)::Action|Western::F::25::6::900271000::1210::5::975040629::Star Wars: Episode VI - Return of the Jedi (1983)::Action|Adventure|Romance|Sci-Fi|War::F::25::6::90027 字段解释 1231000 :: 1036 :: 4 :: 975040964 :: Die Hard (1988) :: Action|Thriller :: F :: 25 :: 6 :: 90027用户ID 电影ID 评分 评分时间戳 电影名字 电影类型 性别 年龄 职业 邮政编码0 1 2 3 4 5 6 7 8 9 要分别求男性，女性当中评分最高的10部电影（性别，电影名，评分） 1、以性别和电影名分组，以电影名+性别为key，以评分为value进行计算； 2、以性别+电影名+评分作为对象，以性别分组，以评分降序进行输出TOP10 业务逻辑：MoviesDemo2.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162 1 public class MoviesDemo2 &#123; 2 3 public static void main(String[] args) throws Exception &#123; 4 5 Configuration conf1 = new Configuration(); 6 Configuration conf2 = new Configuration(); 7 FileSystem fs1 = FileSystem.get(conf1); 8 FileSystem fs2 = FileSystem.get(conf2); 9 Job job1 = Job.getInstance(conf1); 10 Job job2 = Job.getInstance(conf2); 11 12 job1.setJarByClass(MoviesDemo2.class); 13 job1.setMapperClass(MoviesDemo2Mapper1.class); 14 job2.setMapperClass(MoviesDemo2Mapper2.class); 15 job1.setReducerClass(MoviesDemo2Reducer1.class); 16 job2.setReducerClass(MoviesDemo2Reducer2.class); 17 18 job1.setOutputKeyClass(Text.class); 19 job1.setOutputValueClass(DoubleWritable.class); 20 21 job2.setOutputKeyClass(MoviesSexBean.class); 22 job2.setOutputValueClass(NullWritable.class); 23 24 job2.setGroupingComparatorClass(MoviesSexGC.class); 25 26 Path inputPath1 = new Path(&quot;D:\\\\MR\\\\hw\\\\movie\\\\output3he1&quot;); 27 Path outputPath1 = new Path(&quot;D:\\\\MR\\\\hw\\\\movie\\\\output2_1&quot;); 28 Path inputPath2 = new Path(&quot;D:\\\\MR\\\\hw\\\\movie\\\\output2_1&quot;); 29 Path outputPath2 = new Path(&quot;D:\\\\MR\\\\hw\\\\movie\\\\output2_end&quot;); 30 31 if(fs1.exists(outputPath1)) &#123; 32 fs1.delete(outputPath1,true); 33 &#125; 34 if(fs2.exists(outputPath2)) &#123; 35 fs2.delete(outputPath2,true); 36 &#125; 37 38 39 FileInputFormat.setInputPaths(job1, inputPath1); 40 FileOutputFormat.setOutputPath(job1, outputPath1); 41 42 FileInputFormat.setInputPaths(job2, inputPath2); 43 FileOutputFormat.setOutputPath(job2, outputPath2); 44 45 JobControl control = new JobControl(&quot;MoviesDemo2&quot;); 46 47 ControlledJob aJob = new ControlledJob(job1.getConfiguration()); 48 ControlledJob bJob = new ControlledJob(job2.getConfiguration()); 49 50 bJob.addDependingJob(aJob); 51 52 control.addJob(aJob); 53 control.addJob(bJob); 54 55 Thread thread = new Thread(control); 56 thread.start(); 57 58 while(!control.allFinished()) &#123; 59 thread.sleep(1000); 60 &#125; 61 System.exit(0); 62 63 64 &#125; 65 66 67 /** 68 * 数据来源：3个文件关联之后的输出文件 69 * 以电影名+性别为key，以评分为value进行输出 70 * 71 * 1000::1036::4::975040964::Die Hard (1988)::Action|Thriller::F::25::6::90027 72 * 73 * 用户ID::电影ID::评分::评分时间戳::电影名字::电影类型::性别::年龄::职业::邮政编码 74 * 75 * */ 76 public static class MoviesDemo2Mapper1 extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt;&#123; 77 78 Text outKey = new Text(); 79 DoubleWritable outValue = new DoubleWritable(); 80 81 @Override 82 protected void map(LongWritable key, Text value,Context context) 83 throws IOException, InterruptedException &#123; 84 85 String[] split = value.toString().split(&quot;::&quot;); 86 String strKey = split[4]+&quot;\\t&quot;+split[6]; 87 String strValue = split[2]; 88 89 outKey.set(strKey); 90 outValue.set(Double.parseDouble(strValue)); 91 92 context.write(outKey, outValue); 93 &#125; 94 95 &#125; 96 97 /** 98 * 以电影名+性别为key，计算平均分 99 * */100 public static class MoviesDemo2Reducer1 extends Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt;&#123;101 102 DoubleWritable outValue = new DoubleWritable();103 104 @Override105 protected void reduce(Text key, Iterable&lt;DoubleWritable&gt; values,Context context)106 throws IOException, InterruptedException &#123;107 108 int count = 0;109 double sum = 0;110 for(DoubleWritable value : values) &#123;111 count++;112 sum += Double.parseDouble(value.toString());113 &#125;114 double avg = sum / count;115 116 outValue.set(avg);117 context.write(key, outValue);118 &#125;119 &#125;120 121 /**122 * 以电影名+性别+评分作为对象，以性别分组，以评分降序排序123 * */124 public static class MoviesDemo2Mapper2 extends Mapper&lt;LongWritable, Text, MoviesSexBean, NullWritable&gt;&#123;125 126 MoviesSexBean outKey = new MoviesSexBean();127 128 @Override129 protected void map(LongWritable key, Text value,Context context)130 throws IOException, InterruptedException &#123;131 132 String[] split = value.toString().split(&quot;\\t&quot;);133 outKey.setMovieName(split[0]);134 outKey.setSex(split[1]);135 outKey.setScore(Double.parseDouble(split[2]));136 137 context.write(outKey, NullWritable.get());138 139 &#125;140 &#125;141 142 /**143 * 取性别男女各前10名评分最好的电影144 * */145 public static class MoviesDemo2Reducer2 extends Reducer&lt;MoviesSexBean, NullWritable, MoviesSexBean, NullWritable&gt;&#123;146 147 @Override148 protected void reduce(MoviesSexBean key, Iterable&lt;NullWritable&gt; values,Context context)149 throws IOException, InterruptedException &#123;150 151 int count = 0;152 for(NullWritable nvl : values) &#123;153 count++;154 context.write(key, NullWritable.get());155 if(count == 10) &#123;156 return;157 &#125; 158 &#125;159 160 &#125;161 &#125;162 &#125; 对象：MoviesSexBean.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071 1 public class MoviesSexBean implements WritableComparable&lt;MoviesSexBean&gt;&#123; 2 3 private String movieName; 4 private String sex; 5 private double score; 6 7 public MoviesSexBean() &#123; 8 super(); 9 &#125;10 public MoviesSexBean(String movieName, String sex, double score) &#123;11 super();12 this.movieName = movieName;13 this.sex = sex;14 this.score = score;15 &#125;16 public String getMovieName() &#123;17 return movieName;18 &#125;19 public void setMovieName(String movieName) &#123;20 this.movieName = movieName;21 &#125;22 public String getSex() &#123;23 return sex;24 &#125;25 public void setSex(String sex) &#123;26 this.sex = sex;27 &#125;28 public double getScore() &#123;29 return score;30 &#125;31 public void setScore(double score) &#123;32 this.score = score;33 &#125;34 @Override35 public String toString() &#123;36 return movieName + &quot;\\t&quot; + sex + &quot;\\t&quot; + score ;37 &#125;38 @Override39 public void readFields(DataInput in) throws IOException &#123;40 movieName = in.readUTF();41 sex = in.readUTF();42 score = in.readDouble();43 &#125;44 @Override45 public void write(DataOutput out) throws IOException &#123;46 out.writeUTF(movieName);47 out.writeUTF(sex);48 out.writeDouble(score);49 &#125;50 @Override51 public int compareTo(MoviesSexBean o) &#123;52 53 int result = this.getSex().compareTo(o.getSex());54 if(result == 0) &#123;55 double diff = this.getScore() - o.getScore();56 57 if(diff == 0) &#123;58 return 0;59 &#125;else &#123;60 return diff &gt; 0 ? -1 : 1;61 &#125;62 63 &#125;else &#123;64 return result &gt; 0 ? -1 : 1;65 &#125;66 67 &#125;68 69 70 71 &#125; 分组：MoviesSexGC.java 12345678910111213141516 1 public class MoviesSexGC extends WritableComparator&#123; 2 3 public MoviesSexGC() &#123; 4 super(MoviesSexBean.class,true); 5 &#125; 6 7 @Override 8 public int compare(WritableComparable a, WritableComparable b) &#123; 9 10 MoviesSexBean msb1 = (MoviesSexBean)a;11 MoviesSexBean msb2 = (MoviesSexBean)b;12 13 return msb1.getSex().compareTo(msb2.getSex());14 &#125;15 16 &#125; 3、求movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，评分）以第二部三表联合之后的文件进行操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798 1 public class MovieDemo3 &#123; 2 3 public static void main(String[] args) throws Exception &#123; 4 5 Configuration conf = new Configuration(); 6 FileSystem fs = FileSystem.get(conf); 7 Job job = Job.getInstance(conf); 8 9 job.setJarByClass(MovieDemo3.class);10 job.setMapperClass(MovieDemo3Mapper.class);11 job.setReducerClass(MovieDemo3Reducer.class);12 13 job.setOutputKeyClass(Text.class);14 job.setOutputValueClass(DoubleWritable.class);15 16 Path inputPath = new Path(&quot;D:\\\\MR\\\\hw\\\\movie\\\\3he1&quot;);17 Path outputPath = new Path(&quot;D:\\\\MR\\\\hw\\\\movie\\\\outpu3&quot;);18 19 if(fs.exists(outputPath)) &#123;20 fs.delete(outputPath,true);21 &#125;22 23 FileInputFormat.setInputPaths(job, inputPath);24 FileOutputFormat.setOutputPath(job, outputPath);25 26 boolean isDone = job.waitForCompletion(true);27 System.exit(isDone ? 0 : 1);28 29 &#125;30 31 32 /**33 * 1000::1036::4::975040964::Die Hard (1988)::Action|Thriller::F::25::6::9002734 * 35 * 用户ID::电影ID::评分::评分时间戳::电影名字::电影类型::性别::年龄::职业::邮政编码36 * 0 1 2 3 4 5 6 7 8 937 * 38 * key:电影ID+电影名字+年龄段39 * value:评分40 * 求movieid = 2116这部电影各年龄段41 * */42 public static class MovieDemo3Mapper extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt;&#123;43 44 Text outKey = new Text();45 DoubleWritable outValue = new DoubleWritable();46 47 @Override48 protected void map(LongWritable key, Text value, Context context)49 throws IOException, InterruptedException &#123;50 51 String[] split = value.toString().split(&quot;::&quot;);52 int movieID = Integer.parseInt(split[1]);53 54 if(movieID == 2116) &#123;55 String strKey = split[1]+&quot;\\t&quot;+split[4]+&quot;\\t&quot;+split[7];56 String strValue = split[2];57 58 outKey.set(strKey);59 outValue.set(Double.parseDouble(strValue));60 61 context.write(outKey, outValue);62 &#125;63 64 &#125;65 &#125;66 67 68 69 /**70 * 对map的输出结果求平均评分71 * */72 public static class MovieDemo3Reducer extends Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt;&#123;73 74 DoubleWritable outValue = new DoubleWritable();75 76 @Override77 protected void reduce(Text key, Iterable&lt;DoubleWritable&gt; values, Context context)78 throws IOException, InterruptedException &#123;79 80 int count = 0;81 double sum = 0;82 83 for(DoubleWritable value : values) &#123;84 count++;85 sum += Double.parseDouble(value.toString()); 86 &#125;87 88 double avg = sum / count;89 90 outValue.set(avg);91 92 context.write(key, outValue);93 94 &#125;95 96 &#125;97 98 &#125; 4、求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（人，电影名，影评）123451000 :: 1036 :: 4 :: 975040964 :: Die Hard (1988) :: Action|Thriller :: F :: 25 :: 6 :: 90027用户ID 电影ID 评分 评分时间戳 电影名字 电影类型 性别 年龄 职业 邮政编码0 1 2 3 4 5 6 7 8 9 （1）求出评论次数最多的女性ID MoviesDemo4_1.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166 1 public class MoviesDemo4 &#123; 2 3 public static void main(String[] args) throws Exception &#123; 4 5 Configuration conf1 = new Configuration(); 6 FileSystem fs1 = FileSystem.get(conf1); 7 Job job1 = Job.getInstance(conf1); 8 9 job1.setJarByClass(MoviesDemo4.class); 10 job1.setMapperClass(MoviesDemo4Mapper1.class); 11 job1.setReducerClass(MoviesDemo4Reducer1.class); 12 13 14 job1.setMapOutputKeyClass(Text.class); 15 job1.setMapOutputValueClass(Text.class); 16 job1.setOutputKeyClass(Text.class); 17 job1.setOutputValueClass(DoubleWritable.class); 18 19 20 Configuration conf2 = new Configuration(); 21 FileSystem fs2 = FileSystem.get(conf2); 22 Job job2 = Job.getInstance(conf2); 23 24 job2.setJarByClass(MoviesDemo4.class); 25 job2.setMapperClass(MoviesDemo4Mapper2.class); 26 job2.setReducerClass(MoviesDemo4Reducer2.class); 27 28 job2.setMapOutputKeyClass(Moviegoers.class); 29 job2.setMapOutputValueClass(NullWritable.class); 30 job2.setOutputKeyClass(Moviegoers.class); 31 job2.setOutputValueClass(NullWritable.class); 32 33 Path inputPath1 = new Path(&quot;D:\\\\MR\\\\hw\\\\movie\\\\3he1&quot;); 34 Path outputPath1 = new Path(&quot;D:\\\\MR\\\\hw\\\\movie\\\\outpu4_1&quot;); 35 36 if(fs1.exists(outputPath1)) &#123; 37 fs1.delete(outputPath1,true); 38 &#125; 39 40 FileInputFormat.setInputPaths(job1, inputPath1); 41 FileOutputFormat.setOutputPath(job1, outputPath1); 42 43 44 Path inputPath2 = new Path(&quot;D:\\\\MR\\\\hw\\\\movie\\\\outpu4_1&quot;); 45 Path outputPath2 = new Path(&quot;D:\\\\MR\\\\hw\\\\movie\\\\outpu4_2&quot;); 46 47 if(fs2.exists(outputPath2)) &#123; 48 fs2.delete(outputPath2,true); 49 &#125; 50 51 FileInputFormat.setInputPaths(job2, inputPath2); 52 FileOutputFormat.setOutputPath(job2, outputPath2); 53 54 JobControl control = new JobControl(&quot;MoviesDemo4&quot;); 55 56 ControlledJob ajob = new ControlledJob(job1.getConfiguration()); 57 ControlledJob bjob = new ControlledJob(job2.getConfiguration()); 58 59 bjob.addDependingJob(ajob); 60 61 control.addJob(ajob); 62 control.addJob(bjob); 63 64 Thread thread = new Thread(control); 65 thread.start(); 66 67 while(!control.allFinished()) &#123; 68 thread.sleep(1000); 69 &#125; 70 System.exit(0); 71 &#125; 72 73 /** 74 * 1000::1036::4::975040964::Die Hard (1988)::Action|Thriller::F::25::6::90027 75 * 76 * 用户ID::电影ID::评分::评分时间戳::电影名字::电影类型::性别::年龄::职业::邮政编码 77 * 0 1 2 3 4 5 6 7 8 9 78 * 79 * 1、key:用户ID 80 * 2、value：电影名+评分 81 * 82 * */ 83 public static class MoviesDemo4Mapper1 extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; 84 85 Text outKey = new Text(); 86 Text outValue = new Text(); 87 88 @Override 89 protected void map(LongWritable key, Text value, Context context) 90 throws IOException, InterruptedException &#123; 91 92 String[] split = value.toString().split(&quot;::&quot;); 93 94 String strKey = split[0]; 95 String strValue = split[4]+&quot;\\t&quot;+split[2]; 96 97 if(split[6].equals(&quot;F&quot;)) &#123; 98 outKey.set(strKey); 99 outValue.set(strValue);100 context.write(outKey, outValue);101 &#125;102 103 &#125;104 105 &#125;106 107 //统计每位女性的评论总数108 public static class MoviesDemo4Reducer1 extends Reducer&lt;Text, Text, Text, IntWritable&gt;&#123;109 110 IntWritable outValue = new IntWritable();111 112 @Override113 protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context)114 throws IOException, InterruptedException &#123;115 116 int count = 0;117 for(Text value : values) &#123;118 count++;119 &#125;120 outValue.set(count);121 context.write(key, outValue);122 &#125;123 124 &#125;125 126 //对第一次MapReduce的输出结果进行降序排序127 public static class MoviesDemo4Mapper2 extends Mapper&lt;LongWritable, Text,Moviegoers,NullWritable&gt;&#123;128 129 Moviegoers outKey = new Moviegoers();130 131 @Override132 protected void map(LongWritable key, Text value, Context context)133 throws IOException, InterruptedException &#123;134 135 String[] split = value.toString().split(&quot;\\t&quot;);136 137 outKey.setName(split[0]);138 outKey.setCount(Integer.parseInt(split[1]));139 context.write(outKey, NullWritable.get());140 &#125;141 142 &#125;143 144 //排序之后取第一个值（评论最多的女性ID和评论次数）145 public static class MoviesDemo4Reducer2 extends Reducer&lt;Moviegoers,NullWritable, Moviegoers,NullWritable&gt;&#123;146 147 int count = 0;148 149 @Override150 protected void reduce(Moviegoers key, Iterable&lt;NullWritable&gt; values,Context context)151 throws IOException, InterruptedException &#123;152 153 for(NullWritable nvl : values) &#123;154 count++;155 if(count &gt; 1) &#123;156 return;157 &#125;158 context.write(key, nvl); 159 &#125;160 161 &#125;162 163 &#125;164 165 166 &#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（二十四）YARN的资源调度","slug":"2018-04-24-Hadoop学习之路（二十四）YARN的资源调度","date":"2018-04-24T02:30:04.000Z","updated":"2019-09-19T02:53:42.698Z","comments":true,"path":"2018-04-24-Hadoop学习之路（二十四）YARN的资源调度.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-24-Hadoop学习之路（二十四）YARN的资源调度.html","excerpt":"** Hadoop学习之路（二十四）YARN的资源调度：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十四）YARN的资源调度","text":"** Hadoop学习之路（二十四）YARN的资源调度：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十四）YARN的资源调度 &lt;The rest of contents | 余下全文&gt; 1.1、YARN 概述 YARN（Yet Another Resource Negotiator） YARN 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操 作系统平台，而 MapReduce 等运算程序则相当于运行于操作系统之上的应用程序 YARN 是 Hadoop2.x 版本中的一个新特性。它的出现其实是为了解决第一代 MapReduce 编程 框架的不足，提高集群环境下的资源利用率，这些资源包括内存，磁盘，网络，IO等。Hadoop2.X 版本中重新设计的这个 YARN 集群，具有更好的扩展性，可用性，可靠性，向后兼容性，以 及能支持除 MapReduce 以外的更多分布式计算程序 1、YARN 并不清楚用户提交的程序的运行机制 2、YARN 只提供运算资源的调度（用户程序向 YARN 申请资源，YARN 就负责分配资源） 3、YARN 中的主管角色叫 ResourceManager 4、YARN 中具体提供运算资源的角色叫 NodeManager 5、这样一来，YARN 其实就与运行的用户程序完全解耦，就意味着 YARN 上可以运行各种类 型的分布式运算程序（MapReduce 只是其中的一种），比如 MapReduce、Storm 程序，Spark 程序，Tez …… 6、所以，Spark、Storm 等运算框架都可以整合在 YARN 上运行，只要他们各自的框架中有 符合 YARN 规范的资源请求机制即可 7、yarn 就成为一个通用的资源调度平台，从此，企业中以前存在的各种运算集群都可以整 合在一个物理集群上，提高资源利用率，方便数据共享 1.2、原 MapReduce 框架的不足 1、JobTracker 是集群事务的集中处理点，存在单点故障 2、JobTracker 需要完成的任务太多，既要维护 job 的状态又要维护 job 的 task 的状态，造成 过多的资源消耗 3、在 TaskTracker 端，用 Map/Reduce Task 作为资源的表示过于简单，没有考虑到 CPU、内 存等资源情况，当把两个需要消耗大内存的 Task 调度到一起，很容易出现 OOM 4、把资源强制划分为 Map/Reduce Slot，当只有 MapTask 时，TeduceSlot 不能用；当只有 Reduce Task 时，MapSlot 不能用，容易造成资源利用不足。 总结起来就是： 1、扩展性差 2、可靠性低 3、资源利用率低 4、不支持多种计算框架 1.3、新版 YARN 架构的优点 YARN/MRv2 最基本的想法是将原 JobTracker 主要的资源管理和 Job 调度/监视功能分开作为 两个单独的守护进程。有一个全局的 ResourceManager(RM)和每个 Application 有一个 ApplicationMaster(AM)，Application 相当于 MapReduce Job 或者 DAG jobs。ResourceManager 和 NodeManager(NM)组成了基本的数据计算框架。ResourceManager 协调集群的资源利用， 任何 Client 或者运行着的 applicatitonMaster 想要运行 Job 或者 Task 都得向 RM 申请一定的资 源。ApplicatonMaster 是一个框架特殊的库，对于 MapReduce 框架而言有它自己的 AM 实现， 用户也可以实现自己的 AM，在运行的时候，AM 会与 NM 一起来启动和监视 Tasks。 1.4、YARN 的重要概念1.4.1、ResourceManager ResourceManager 是基于应用程序对集群资源的需求进行调度的 YARN 集群主控节点，负责 协调和管理整个集群（所有 NodeManager）的资源，响应用户提交的不同类型应用程序的 解析，调度，监控等工作。ResourceManager 会为每一个 Application 启动一个 MRAppMaster， 并且 MRAppMaster 分散在各个 NodeManager 节点 它主要由两个组件构成：调度器（Scheduler）和应用程序管理器（ApplicationsManager， ASM） YARN 集群的主节点 ResourceManager 的职责： 1、处理客户端请求 2、启动或监控 MRAppMaster 3、监控 NodeManager 4、资源的分配与调度 1.4.2、NodeManager NodeManager 是 YARN 集群当中真正资源的提供者，是真正执行应用程序的容器的提供者， 监控应用程序的资源使用情况（CPU，内存，硬盘，网络），并通过心跳向集群资源调度器 ResourceManager 进行汇报以更新自己的健康状态。同时其也会监督 Container 的生命周期 管理，监控每个 Container 的资源使用（内存、CPU 等）情况，追踪节点健康状况，管理日 志和不同应用程序用到的附属服务（auxiliary service）。 YARN 集群的从节点 NodeManager 的职责： 1、管理单个节点上的资源 2、处理来自 ResourceManager 的命令 3、处理来自 MRAppMaster 的命令 1.4.3、MRAppMaster MRAppMaster 对应一个应用程序，职责是：向资源调度器申请执行任务的资源容器，运行 任务，监控整个任务的执行，跟踪整个任务的状态，处理任务失败以异常情况 1.4.4、Container Container 容器是一个抽象出来的逻辑资源单位。容器是由 ResourceManager Scheduler 服务 动态分配的资源构成，它包括了该节点上的一定量 CPU，内存，磁盘，网络等信息，MapReduce 程序的所有 Task 都是在一个容器里执行完成的，容器的大小是可以动态调整的 1.4.5、ASM 应用程序管理器 ASM 负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协 商资源以启动 MRAppMaster、监控 MRAppMaster 运行状态并在失败时重新启动它等 1.4.6、Scheduler 调度器根据应用程序的资源需求进行资源分配，不参与应用程序具体的执行和监控等工作 资源分配的单位就是 Container，调度器是一个可插拔的组件，用户可以根据自己的需求实 现自己的调度器。YARN 本身为我们提供了多种直接可用的调度器，比如 FIFO，Fair Scheduler 和 Capacity Scheduler 等 1.5、YARN 架构及各角色职责 1.6、YARN 作业执行流程 YARN 作业执行流程： 1、用户向 YARN 中提交应用程序，其中包括 MRAppMaster 程序，启动 MRAppMaster 的命令， 用户程序等。 2、ResourceManager 为该程序分配第一个 Container，并与对应的 NodeManager 通讯，要求 它在这个 Container 中启动应用程序 MRAppMaster。 3、MRAppMaster 首先向 ResourceManager 注册，这样用户可以直接通过 ResourceManager 查看应用程序的运行状态，然后将为各个任务申请资源，并监控它的运行状态，直到运行结束，重复 4 到 7 的步骤。 4、MRAppMaster 采用轮询的方式通过 RPC 协议向 ResourceManager 申请和领取资源。 5、一旦 MRAppMaster 申请到资源后，便与对应的 NodeManager 通讯，要求它启动任务。 6、NodeManager 为任务设置好运行环境（包括环境变量、JAR 包、二进制程序等）后，将 任务启动命令写到一个脚本中，并通过运行该脚本启动任务。 7、各个任务通过某个 RPC 协议向 MRAppMaster 汇报自己的状态和进度，以让 MRAppMaster 随时掌握各个任务的运行状态，从而可以在任务败的时候重新启动任务。 8、应用程序运行完成后，MRAppMaster 向 ResourceManager 注销并关闭自己。","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop源码学习－脚本命令（hadoop fs -ls)执行细节","slug":"2018-04-29-Hadoop源码学习－脚本命令（hadoop fs -ls)执行细节","date":"2018-04-23T02:30:04.000Z","updated":"2019-09-19T03:29:56.568Z","comments":true,"path":"2018-04-29-Hadoop源码学习－脚本命令（hadoop fs -ls)执行细节.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-29-Hadoop源码学习－脚本命令（hadoop fs -ls)执行细节.html","excerpt":"** Hadoop源码学习－脚本命令（hadoop fs -ls)执行细节：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** Hadoop源码学习－脚本命令（hadoop fs -ls)执行细节：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt; Hadoop有提供一些脚本命令，以便于我们对HDFS进行管理，可以通过命令hadoop fs进行查看： 通过以上使用说明可以发现，里面提供了大多数和我们在本地操作文件系统相类似的命令，例如，cat查看文件内容，chgrp改变用户群组权限，chmod改变用户权限，chown改变用户拥有者权限，还有创建目录，查看目录，移动文件，重命名等等。 hadoop fs -ls这里，我们来看看命令hadoop fs -ls： 这个命令大家肯定非常熟悉，在Linux下使用超级频繁，功能就是列出指定目录下的文件及文件夹。那接下来，我们来看看它是怎样查找到此目录下的文件及文件夹的。 在运行hadoop fs -ls /命令之后，真正的命令只是hadoop，后面只是参数，那么，这个hadoop命令到底是哪呢？如果说集群是自己手动搭配的话，那大家肯定知道，这个命令就在${HADOOP_HOME}/bin目录下，但如果集群是自动化部署的时候，你可能一下子找不到这个命令在哪，此时，可以使用以下命令查找： 可以看到，这个命令应该是在目录/usr/bin/之下，使用vim /usr/bin/hadoop查看命令详细： 12345678910#!/bin/bashexport HADOOP_HOME=$&#123;HADOOP_HOME:-/usr/hdp/2.5.0.0-1245/hadoop&#125;export HADOOP_MAPRED_HOME=$&#123;HADOOP_MAPRED_HOME:-/usr/hdp/2.5.0.0-1245/hadoop-mapreduce&#125;export HADOOP_YARN_HOME=$&#123;HADOOP_YARN_HOME:-/usr/hdp/2.5.0.0-1245/hadoop-yarn&#125;export HADOOP_LIBEXEC_DIR=$&#123;HADOOP_HOME&#125;/libexecexport HDP_VERSION=$&#123;HDP_VERSION:-2.5.0.0-1245&#125;export HADOOP_OPTS=\"$&#123;HADOOP_OPTS&#125; -Dhdp.version=$&#123;HDP_VERSION&#125;\"exec /usr/hdp/2.5.0.0-1245//hadoop/bin/hadoop.distro \"$@\" 这个脚本做了两件事，一是export了一些环境变量，使得之后运行的子程序都可以共享这些变量的值；二是执行了命令hadoop.distro命令，并传上了所有参数。 现在，我们来看下命令hadoop.distro做了哪些事，由于代码有点小多，我就不全部贴了，只贴与执行命令hadoop fs -ls /相关的代码。 命令hadoop.distro做的事情是：根据之前传入的参数，然后做一些判断，确定一些变量的值，最后执行的是以下命令： exec “$JAVA” $JAVA_HEAP_MAX $HADOOP_OPTS $CLASS “$@”1这里，我们看到了一堆变量，其中$JAVA：java$JAVA_HEAP_MAX：$HADOOP_OPTS： # Always respect HADOOP_OPTS and HADOOP_CLIENT_OPTS HADOOP_OPTS=&quot;$HADOOP_OPTS $HADOOP_CLIENT_OPTS&quot; #make sure security appender is turned off HADOOP_OPTS=&quot;$HADOOP_OPTS -Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,NullAppender}&quot;$CLASS：org.apache.hadoop.fs.FsShell$@ ： -ls / 注意：这里已经没有参数fs了 因此，命令hadoop.distro也就转换成执行一个JAVA类了，然后继续带上参数。 打开hadoop的源代码，找到类org.apache.hadoop.fs.FsShell，它的main方法如下： 1234567891011121314public static void main(String argv[]) throws Exception &#123; FsShell shell = newShellInstance(); //创建FsShell实例 Configuration conf = new Configuration(); //配置类， conf.setQuietMode(false); //设置成“非安静模式”，默认为“安静模式”，在安静模式下，error和information的信息不会被记录。 shell.setConf(conf); int res; try &#123; res = ToolRunner.run(shell, argv); //ToolRunner就是一个工具类，用于执行实现了接口Tool的类 &#125; finally &#123; shell.close(); &#125; System.exit(res); &#125; ToolRunner类结合GenericOptionsParser类来解析命令行参数，在运行上述ToolRunner.run(shell, argv)代码之后，经过一番解释之后，最后真正执行的仍然是类FsShell的run方法，而且对其参数进行了解析，run方法如下： 12345678910111213141516171819202122232425262728293031 @Override public int run(String argv[]) throws Exception &#123; // initialize FsShell 包括注册命令类 init();int exitCode = -1;if (argv.length &lt; 1) &#123; printUsage(System.err); //打印使用方法&#125; else &#123; String cmd = argv[0]; //取到第一个参数，即 ls Command instance = null; try &#123; // 取得实现了该命令（ls）的命令实例,并且此类已经通过类CommandFactory的addClass方法进行了注册 instance = commandFactory.getInstance(cmd); if (instance == null) &#123; throw new UnknownCommandException(); &#125; exitCode = instance.run(Arrays.copyOfRange(argv, 1, argv.length)); &#125; catch (IllegalArgumentException e) &#123; displayError(cmd, e.getLocalizedMessage()); if (instance != null) &#123; printInstanceUsage(System.err, instance); &#125; &#125; catch (Exception e) &#123; // instance.run catches IOE, so something is REALLY wrong if here LOG.debug(&quot;Error&quot;, e); displayError(cmd, &quot;Fatal internal error&quot;); e.printStackTrace(System.err); &#125;&#125;return exitCode;&#125; 注意：通过查看类Ls的代码，我们可以发现，它有一个静态方法registerCommands，这个方法就是对类Ls进行注册，但是，这只是一个静态方法，那么，到底是在哪进行了此方法的调用呢？ 12345class Ls extends FsCommand &#123; public static void registerCommands(CommandFactory factory) &#123; factory.addClass(Ls.class, &quot;-ls&quot;); factory.addClass(Lsr.class, &quot;-lsr&quot;); &#125; 细心的朋友可能已经发现，就在类FsShell的run方法中，调用了一个init方法，而就在此方法中，有一行注册命令的代码，如下： 123456789101112131415161718192021protected void init() throws IOException &#123; getConf().setQuietMode(true); if (commandFactory == null) &#123; commandFactory = new CommandFactory(getConf()); commandFactory.addObject(new Help(), &quot;-help&quot;); commandFactory.addObject(new Usage(), &quot;-usage&quot;); // 注册，调用registerCommands方法 registerCommands(commandFactory); &#125; &#125; protected void registerCommands(CommandFactory factory) &#123; // TODO: DFSAdmin subclasses FsShell so need to protect the command // registration. This class should morph into a base class for // commands, and then this method can be abstract if (this.getClass().equals(FsShell.class)) &#123; // 调用CommandFactory类的registerCommands方法 // 注意，这里传的参数是类FsCommand factory.registerCommands(FsCommand.class); &#125; &#125; CommandFactory类的registerCommands方法如下： 12345678910public void registerCommands(Class&lt;?&gt; registrarClass) &#123; try &#123; // 这里触发的是类CommandFactory的registerCommands方法 registrarClass.getMethod( &quot;registerCommands&quot;, CommandFactory.class ).invoke(null, this); &#125; catch (Exception e) &#123; throw new RuntimeException(StringUtils.stringifyException(e)); &#125; &#125; 接下来，我拉看看类CommandFactory的registerCommands方法，代码如下： 12345678910111213141516171819202122public static void registerCommands(CommandFactory factory) &#123; factory.registerCommands(AclCommands.class); factory.registerCommands(CopyCommands.class); factory.registerCommands(Count.class); factory.registerCommands(Delete.class); factory.registerCommands(Display.class); factory.registerCommands(Find.class); factory.registerCommands(FsShellPermissions.class); factory.registerCommands(FsUsage.class); // 我们会用到的就是这个类Ls factory.registerCommands(Ls.class); factory.registerCommands(Mkdir.class); factory.registerCommands(MoveCommands.class); factory.registerCommands(SetReplication.class); factory.registerCommands(Stat.class); factory.registerCommands(Tail.class); factory.registerCommands(Test.class); factory.registerCommands(Touch.class); factory.registerCommands(Truncate.class); factory.registerCommands(SnapshotCommands.class); factory.registerCommands(XAttrCommands.class); &#125; 我们再来看看Ls类 12345class Ls extends FsCommand &#123; public static void registerCommands(CommandFactory factory) &#123; factory.addClass(Ls.class, &quot;-ls&quot;); factory.addClass(Lsr.class, &quot;-lsr&quot;); &#125; 也就是，在调用init方法的时候，对这些命令类进行了注册。因此，上面的那个instance，在这里的话，其实就是类Ls的实例。类Ls继承类FsCommand，而类FsCommand是继承类Command,前面instance调用的run方法其实是父类Command的run方法，此方法主要做了两件事，一是处理配置选项，如-d,-R,-h，二是处理参数，如下： 1234567891011121314public int run(String...argv) &#123; LinkedList&lt;String&gt; args = new LinkedList&lt;String&gt;(Arrays.asList(argv)); try &#123; if (isDeprecated()) &#123; displayWarning( &quot;DEPRECATED: Please use &apos;&quot;+ getReplacementCommand() + &quot;&apos; instead.&quot;); &#125; processOptions(args); processRawArguments(args); &#125; catch (IOException e) &#123; displayError(e); &#125;return (numErrors == 0) ? exitCode : exitCodeForError(); &#125; 方法processRawArguments的调用层次关系如下： \\-&gt; processRawArguments(LinkedList) |-&gt; expandArguments(LinkedList) | \\-&gt; expandArgument(String)* \\-&gt; processArguments(LinkedList) |-&gt; processArgument(PathData)* | |-&gt; processPathArgument(PathData) | \\-&gt; processPaths(PathData, PathData...) | \\-&gt; processPath(PathData)* \\-&gt; processNonexistentPath(PathData)从这个层次关系中可以看出，整个方法是先进行展开参数，传入的参数是LinkedList，展开后的参数是LinkedList，PathData类中包含了Path，FileStatus，FileSystem。其实，当程序运行到这里的时候，命令ls的结果就已经可以通过类PathData中的相关方法获取了。 展开参数后，开始进行处理参数，此时的参数就是LinkedList,然后循环处理此List，先是判断目录是否存在，是否需要递归查找，是否只是列出本目录（就是看有没有-R和-d参数），我们来看一下到底是如何输出结果的： 123456789@Override protected void processPaths(PathData parent, PathData ... items) throws IOException &#123; if (parent != null &amp;&amp; !isRecursive() &amp;&amp; items.length != 0) &#123; out.println(&quot;Found &quot; + items.length + &quot; items&quot;); &#125; adjustColumnWidths(items); // 计算列宽，重新构建格式字符串 super.processPaths(parent, items); &#125; 看到这里，大家是不是觉得很面熟？没想起来？我们上个图： 这下看到了吧，最是输出结果的第一行，找到11项。 接下来重新调整了一下列宽，最后调用了父类的processPaths方法，我们继续来看父类的这个方法，它做了哪些事： 123456789101112131415protected void processPaths(PathData parent, PathData ... items) throws IOException &#123; // TODO: this really should be iterative for (PathData item : items) &#123; try &#123; processPath(item); // 真正处理每一项，然后打印出来 if (recursive &amp;&amp; isPathRecursable(item)) &#123; recursePath(item); // 如果有指定参数 -R，则需要进行递归 &#125; postProcessPath(item); // 这个没理解，DFS还有后序DFS么？有知情者，请告知，谢谢。 &#125; catch (IOException e) &#123; displayError(e); &#125; &#125; &#125; 我们来看一下打印具体每行信息的代码： 123456789101112131415@Override protected void processPath(PathData item) throws IOException &#123; FileStatus stat = item.stat; String line = String.format(lineFormat, (stat.isDirectory() ? &quot;d&quot; : &quot;-&quot;), // 文件夹显示d，文件显示- stat.getPermission() + (stat.getPermission().getAclBit() ? &quot;+&quot; : &quot; &quot;), // 获取权限 (stat.isFile() ? stat.getReplication() : &quot;-&quot;), stat.getOwner(), // 获取拥有者 stat.getGroup(), // 获取组 formatSize(stat.getLen()), // 获取大小 dateFormat.format(new Date(stat.getModificationTime())), // 日期 item // 项，即路径 ); out.println(line); // 打印行 &#125; 到这里，命令hadoop fs -ls /的执行过程基本已经结束(关于文件系统内部细节，后续再讲），这就是整个命令执行的过程。最后，我们来总结一下： 执行shell。执行命令hadoop fs -ls /，首先执行的是shell命令，然后转换成执行Java类。执行Java。在执行Java类的时候，使用工具类对其进行配置项解析，并使用反射机制对命令进行了转换，于是后面变成了调用类Ls的run方法。调用类Ls的相关方法。类Ls负责处理路径，并打印详情。 原文链接：https://blog.csdn.net/strongyoung88/article/details/68952248","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（二十五）MapReduce的API使用（二）","slug":"2018-04-25-Hadoop学习之路（二十五）MapReduce的API使用（二）","date":"2018-04-23T02:30:04.000Z","updated":"2019-09-19T02:56:46.607Z","comments":true,"path":"2018-04-25-Hadoop学习之路（二十五）MapReduce的API使用（二）.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-25-Hadoop学习之路（二十五）MapReduce的API使用（二）.html","excerpt":"** Hadoop学习之路（二十五）MapReduce的API使用（二）：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十五）MapReduce的API使用（二）","text":"** Hadoop学习之路（二十五）MapReduce的API使用（二）：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十五）MapReduce的API使用（二） &lt;The rest of contents | 余下全文&gt; 学生成绩—增强版数据信息1234567891011121314151617181920212223242526272829303132 1 computer,huangxiaoming,85,86,41,75,93,42,85 2 computer,xuzheng,54,52,86,91,42 3 computer,huangbo,85,42,96,38 4 english,zhaobenshan,54,52,86,91,42,85,75 5 english,liuyifei,85,41,75,21,85,96,14 6 algorithm,liuyifei,75,85,62,48,54,96,15 7 computer,huangjiaju,85,75,86,85,85 8 english,liuyifei,76,95,86,74,68,74,48 9 english,huangdatou,48,58,67,86,15,33,8510 algorithm,huanglei,76,95,86,74,68,74,4811 algorithm,huangjiaju,85,75,86,85,85,74,8612 computer,huangdatou,48,58,67,86,15,33,8513 english,zhouqi,85,86,41,75,93,42,85,75,55,47,2214 english,huangbo,85,42,96,38,55,47,2215 algorithm,liutao,85,75,85,99,6616 computer,huangzitao,85,86,41,75,93,42,8517 math,wangbaoqiang,85,86,41,75,93,42,8518 computer,liujialing,85,41,75,21,85,96,14,74,8619 computer,liuyifei,75,85,62,48,54,96,1520 computer,liutao,85,75,85,99,66,88,75,9121 computer,huanglei,76,95,86,74,68,74,4822 english,liujialing,75,85,62,48,54,96,1523 math,huanglei,76,95,86,74,68,74,4824 math,huangjiaju,85,75,86,85,85,74,8625 math,liutao,48,58,67,86,15,33,8526 english,huanglei,85,75,85,99,66,88,75,9127 math,xuzheng,54,52,86,91,42,85,7528 math,huangxiaoming,85,75,85,99,66,88,75,9129 math,liujialing,85,86,41,75,93,42,85,7530 english,huangxiaoming,85,86,41,75,93,42,8531 algorithm,huangdatou,48,58,67,86,15,33,8532 algorithm,huangzitao,85,86,41,75,93,42,85,75 数据解释数据字段个数不固定：第一个是课程名称，总共四个课程，computer，math，english，algorithm，第二个是学生姓名，后面是每次考试的分数 统计需求1、统计每门课程的参考人数和课程平均分 2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件，并且按平均分从高到低排序，分数保留一位小数 3、求出每门课程参考学生成绩最高的学生的信息：课程，姓名和平均分 第一题 MRAvgScore1.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146 1 /** 2 * 需求：统计每门课程的参考人数和课程平均分 3 * */ 4 public class MRAvgScore1 &#123; 5 6 public static void main(String[] args) throws Exception &#123; 7 8 Configuration conf1 = new Configuration(); 9 Configuration conf2 = new Configuration(); 10 11 Job job1 = Job.getInstance(conf1); 12 Job job2 = Job.getInstance(conf2); 13 14 job1.setJarByClass(MRAvgScore1.class); 15 job1.setMapperClass(AvgScoreMapper1.class); 16 //job.setReducerClass(MFReducer.class); 17 18 job1.setOutputKeyClass(Text.class); 19 job1.setOutputValueClass(DoubleWritable.class); 20 21 Path inputPath1 = new Path(&quot;D:\\\\MR\\\\hw\\\\work3\\\\input&quot;); 22 Path outputPath1 = new Path(&quot;D:\\\\MR\\\\hw\\\\work3\\\\output_hw1_1&quot;); 23 24 FileInputFormat.setInputPaths(job1, inputPath1); 25 FileOutputFormat.setOutputPath(job1, outputPath1); 26 27 28 job2.setMapperClass(AvgScoreMapper2.class); 29 job2.setReducerClass(AvgScoreReducer2.class); 30 31 job2.setOutputKeyClass(Text.class); 32 job2.setOutputValueClass(DoubleWritable.class); 33 34 Path inputPath2 = new Path(&quot;D:\\\\MR\\\\hw\\\\work3\\\\output_hw1_1&quot;); 35 Path outputPath2 = new Path(&quot;D:\\\\MR\\\\hw\\\\work3\\\\output_hw1_end&quot;); 36 37 FileInputFormat.setInputPaths(job2, inputPath2); 38 FileOutputFormat.setOutputPath(job2, outputPath2); 39 40 JobControl control = new JobControl(&quot;AvgScore&quot;); 41 42 ControlledJob aJob = new ControlledJob(job1.getConfiguration()); 43 ControlledJob bJob = new ControlledJob(job2.getConfiguration()); 44 45 bJob.addDependingJob(aJob); 46 47 control.addJob(aJob); 48 control.addJob(bJob); 49 50 Thread thread = new Thread(control); 51 thread.start(); 52 53 while(!control.allFinished()) &#123; 54 thread.sleep(1000); 55 &#125; 56 System.exit(0); 57 58 &#125; 59 60 61 62 /** 63 * 数据类型：computer,huangxiaoming,85,86,41,75,93,42,85 64 * 65 * 需求：统计每门课程的参考人数和课程平均分 66 * 67 * 分析：以课程名称+姓名作为key，以平均分数作为value 68 * */ 69 public static class AvgScoreMapper1 extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt;&#123; 70 71 @Override 72 protected void map(LongWritable key, Text value,Context context) 73 throws IOException, InterruptedException &#123; 74 75 String[] splits = value.toString().split(&quot;,&quot;); 76 //拼接成要输出的key 77 String outKey = splits[0]+&quot;\\t&quot;+splits[1]; 78 int length = splits.length; 79 int sum = 0; 80 //求出成绩的总和 81 for(int i=2;i&lt;length;i++) &#123; 82 sum += Integer.parseInt(splits[i]); 83 &#125; 84 //求出平均分 85 double outValue = sum / (length - 2); 86 87 context.write(new Text(outKey), new DoubleWritable(outValue)); 88 89 &#125; 90 91 &#125; 92 93 /** 94 * 对第一次MapReduce输出的结果进一步计算，第一步输出结果样式为 95 * math huangjiaju 82.0 96 * math huanglei 74.0 97 * math huangxiaoming 83.0 98 * math liujialing 72.0 99 * math liutao 56.0100 * math wangbaoqiang 72.0101 * math xuzheng 69.0102 * 103 * 需求：统计每门课程的参考人数和课程平均分 104 * 分析：以课程名称作为key，以分数作为value进行 输出105 * 106 * */107 public static class AvgScoreMapper2 extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt;&#123;108 109 @Override110 protected void map(LongWritable key, Text value,Context context)111 throws IOException, InterruptedException &#123;112 113 String[] splits = value.toString().split(&quot;\\t&quot;);114 String outKey = splits[0];115 String outValue = splits[2];116 117 context.write(new Text(outKey), new DoubleWritable(Double.parseDouble(outValue)));118 &#125;119 120 &#125;121 122 /**123 * 针对同一门课程，对values进行遍历计数，看看有多少人参加了考试，并计算出平均成绩124 * */125 public static class AvgScoreReducer2 extends Reducer&lt;Text, DoubleWritable, Text, Text&gt;&#123;126 127 @Override128 protected void reduce(Text key, Iterable&lt;DoubleWritable&gt; values,129 Context context) throws IOException, InterruptedException &#123;130 131 int count = 0;132 double sum = 0;133 for(DoubleWritable value : values) &#123;134 count++;135 sum += value.get();136 &#125;137 138 double avg = sum / count;139 String outValue = count + &quot;\\t&quot; + avg;140 context.write(key, new Text(outValue));141 &#125;142 143 &#125;144 145 146 &#125; 第二题 MRAvgScore2.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061 1 public class MRAvgScore2 &#123; 2 3 public static void main(String[] args) throws Exception &#123; 4 5 Configuration conf = new Configuration(); 6 7 Job job = Job.getInstance(conf); 8 9 job.setJarByClass(MRAvgScore2.class);10 job.setMapperClass(ScoreMapper3.class);11 job.setReducerClass(ScoreReducer3.class);12 13 job.setOutputKeyClass(StudentBean.class);14 job.setOutputValueClass(NullWritable.class);15 16 job.setPartitionerClass(CoursePartitioner.class);17 job.setNumReduceTasks(4);18 19 Path inputPath = new Path(&quot;D:\\\\MR\\\\hw\\\\work3\\\\output_hw1_1&quot;);20 Path outputPath = new Path(&quot;D:\\\\MR\\\\hw\\\\work3\\\\output_hw2_1&quot;);21 22 FileInputFormat.setInputPaths(job, inputPath);23 FileOutputFormat.setOutputPath(job, outputPath);24 boolean isDone = job.waitForCompletion(true);25 System.exit(isDone ? 0 : 1);26 &#125;27 28 29 public static class ScoreMapper3 extends Mapper&lt;LongWritable, Text, StudentBean, NullWritable&gt;&#123;30 31 @Override32 protected void map(LongWritable key, Text value,Context context)33 throws IOException, InterruptedException &#123;34 35 String[] splits = value.toString().split(&quot;\\t&quot;);36 37 double score = Double.parseDouble(splits[2]);38 DecimalFormat df = new DecimalFormat(&quot;#.0&quot;);39 df.format(score);40 41 StudentBean student = new StudentBean(splits[0],splits[1],score);42 43 context.write(student, NullWritable.get());44 45 &#125;46 47 &#125;48 49 public static class ScoreReducer3 extends Reducer&lt;StudentBean, NullWritable, StudentBean, NullWritable&gt;&#123;50 51 @Override52 protected void reduce(StudentBean key, Iterable&lt;NullWritable&gt; values,Context context)53 throws IOException, InterruptedException &#123;54 55 for(NullWritable nvl : values)&#123;56 context.write(key, nvl);57 &#125;58 59 &#125;60 &#125;61 &#125; StudentBean.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061 1 public class StudentBean implements WritableComparable&lt;StudentBean&gt;&#123; 2 private String course; 3 private String name; 4 private double avgScore; 5 6 public String getCourse() &#123; 7 return course; 8 &#125; 9 public void setCourse(String course) &#123;10 this.course = course;11 &#125;12 public String getName() &#123;13 return name;14 &#125;15 public void setName(String name) &#123;16 this.name = name;17 &#125;18 public double getavgScore() &#123;19 return avgScore;20 &#125;21 public void setavgScore(double avgScore) &#123;22 this.avgScore = avgScore;23 &#125;24 public StudentBean(String course, String name, double avgScore) &#123;25 super();26 this.course = course;27 this.name = name;28 this.avgScore = avgScore;29 &#125;30 public StudentBean() &#123;31 super();32 &#125;33 34 @Override35 public String toString() &#123;36 return course + &quot;\\t&quot; + name + &quot;\\t&quot; + avgScore;37 &#125;38 @Override39 public void readFields(DataInput in) throws IOException &#123;40 course = in.readUTF();41 name = in.readUTF();42 avgScore = in.readDouble();43 &#125;44 @Override45 public void write(DataOutput out) throws IOException &#123;46 out.writeUTF(course);47 out.writeUTF(name);48 out.writeDouble(avgScore);49 &#125;50 @Override51 public int compareTo(StudentBean stu) &#123;52 double diffent = this.avgScore - stu.avgScore;53 if(diffent == 0) &#123;54 return 0;55 &#125;else &#123;56 return diffent &gt; 0 ? -1 : 1;57 &#125;58 &#125;59 60 61 &#125; 第三题 MRScore3.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142 1 public class MRScore3 &#123; 2 3 public static void main(String[] args) throws Exception &#123; 4 5 Configuration conf1 = new Configuration(); 6 Configuration conf2 = new Configuration(); 7 8 Job job1 = Job.getInstance(conf1); 9 Job job2 = Job.getInstance(conf2); 10 11 job1.setJarByClass(MRScore3.class); 12 job1.setMapperClass(MRMapper3_1.class); 13 //job.setReducerClass(ScoreReducer3.class); 14 15 16 job1.setMapOutputKeyClass(IntWritable.class); 17 job1.setMapOutputValueClass(StudentBean.class); 18 job1.setOutputKeyClass(IntWritable.class); 19 job1.setOutputValueClass(StudentBean.class); 20 21 job1.setPartitionerClass(CoursePartitioner2.class); 22 23 job1.setNumReduceTasks(4); 24 25 Path inputPath = new Path(&quot;D:\\\\MR\\\\hw\\\\work3\\\\input&quot;); 26 Path outputPath = new Path(&quot;D:\\\\MR\\\\hw\\\\work3\\\\output_hw3_1&quot;); 27 28 FileInputFormat.setInputPaths(job1, inputPath); 29 FileOutputFormat.setOutputPath(job1, outputPath); 30 31 job2.setMapperClass(MRMapper3_2.class); 32 job2.setReducerClass(MRReducer3_2.class); 33 34 job2.setMapOutputKeyClass(IntWritable.class); 35 job2.setMapOutputValueClass(StudentBean.class); 36 job2.setOutputKeyClass(StudentBean.class); 37 job2.setOutputValueClass(NullWritable.class); 38 39 Path inputPath2 = new Path(&quot;D:\\\\MR\\\\hw\\\\work3\\\\output_hw3_1&quot;); 40 Path outputPath2 = new Path(&quot;D:\\\\MR\\\\hw\\\\work3\\\\output_hw3_end&quot;); 41 42 FileInputFormat.setInputPaths(job2, inputPath2); 43 FileOutputFormat.setOutputPath(job2, outputPath2); 44 45 JobControl control = new JobControl(&quot;Score3&quot;); 46 47 ControlledJob aJob = new ControlledJob(job1.getConfiguration()); 48 ControlledJob bJob = new ControlledJob(job2.getConfiguration()); 49 50 bJob.addDependingJob(aJob); 51 52 control.addJob(aJob); 53 control.addJob(bJob); 54 55 Thread thread = new Thread(control); 56 thread.start(); 57 58 while(!control.allFinished()) &#123; 59 thread.sleep(1000); 60 &#125; 61 System.exit(0); 62 63 64 &#125; 65 66 67 68 69 public static class MRMapper3_1 extends Mapper&lt;LongWritable, Text, IntWritable, StudentBean&gt;&#123; 70 71 StudentBean outKey = new StudentBean(); 72 IntWritable outValue = new IntWritable(); 73 List&lt;String&gt; scoreList = new ArrayList&lt;&gt;(); 74 75 protected void map(LongWritable key, Text value, Context context) throws java.io.IOException ,InterruptedException &#123; 76 77 scoreList.clear(); 78 String[] splits = value.toString().split(&quot;,&quot;); 79 long sum = 0; 80 81 for(int i=2;i&lt;splits.length;i++) &#123; 82 scoreList.add(splits[i]); 83 sum += Long.parseLong(splits[i]); 84 &#125; 85 86 Collections.sort(scoreList); 87 outValue.set(Integer.parseInt(scoreList.get(scoreList.size()-1))); 88 89 double avg = sum * 1.0/(splits.length-2); 90 outKey.setCourse(splits[0]); 91 outKey.setName(splits[1]); 92 outKey.setavgScore(avg); 93 94 context.write(outValue, outKey); 95 96 &#125;; 97 &#125; 98 99 100 101 public static class MRMapper3_2 extends Mapper&lt;LongWritable, Text,IntWritable, StudentBean &gt;&#123;102 103 StudentBean outValue = new StudentBean();104 IntWritable outKey = new IntWritable();105 106 protected void map(LongWritable key, Text value, Context context) throws java.io.IOException ,InterruptedException &#123;107 108 String[] splits = value.toString().split(&quot;\\t&quot;);109 outKey.set(Integer.parseInt(splits[0]));110 111 outValue.setCourse(splits[1]);112 outValue.setName(splits[2]);113 outValue.setavgScore(Double.parseDouble(splits[3]));114 115 context.write(outKey, outValue);116 117 118 &#125;;119 &#125;120 121 122 public static class MRReducer3_2 extends Reducer&lt;IntWritable, StudentBean, StudentBean, NullWritable&gt;&#123;123 124 StudentBean outKey = new StudentBean();125 126 @Override127 protected void reduce(IntWritable key, Iterable&lt;StudentBean&gt; values,Context context)128 throws IOException, InterruptedException &#123;129 130 int length = values.toString().length();131 132 for(StudentBean value : values) &#123;133 outKey = value;134 &#125;135 136 context.write(outKey, NullWritable.get());137 138 &#125;139 &#125;140 141 142 &#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（二十三）MapReduce中的shuffle详解","slug":"2018-04-23-Hadoop学习之路（二十三）MapReduce中的shuffle详解","date":"2018-04-23T02:30:04.000Z","updated":"2019-09-19T03:32:24.826Z","comments":true,"path":"2018-04-23-Hadoop学习之路（二十三）MapReduce中的shuffle详解.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-23-Hadoop学习之路（二十三）MapReduce中的shuffle详解.html","excerpt":"** Hadoop学习之路（二十三）MapReduce中的shuffle详解：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十三）MapReduce中的shuffle详解","text":"** Hadoop学习之路（二十三）MapReduce中的shuffle详解：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十三）MapReduce中的shuffle详解 &lt;The rest of contents | 余下全文&gt; 概述1、MapReduce 中，mapper 阶段处理的数据如何传递给 reducer 阶段，是 MapReduce 框架中 最关键的一个流程，这个流程就叫 Shuffle 2、Shuffle: 数据混洗 ——（核心机制：数据分区，排序，局部聚合，缓存，拉取，再合并 排序） 3、具体来说：就是将 MapTask 输出的处理结果数据，按照 Partitioner 组件制定的规则分发 给 ReduceTask，并在分发的过程中，对数据按 key 进行了分区和排序 回到顶部 MapReduce的Shuffle过程介绍Shuffle的本义是洗牌、混洗，把一组有一定规则的数据尽量转换成一组无规则的数据，越随机越好。MapReduce中的Shuffle更像是洗牌的逆过程，把一组无规则的数据尽量转换成一组具有一定规则的数据。 为什么MapReduce计算模型需要Shuffle过程？我们都知道MapReduce计算模型一般包括两个重要的阶段：Map是映射，负责数据的过滤分发；Reduce是规约，负责数据的计算归并。Reduce的数据来源于Map，Map的输出即是Reduce的输入，Reduce需要通过Shuffle来获取数据。 从Map输出到Reduce输入的整个过程可以广义地称为Shuffle。Shuffle横跨Map端和Reduce端，在Map端包括Spill过程，在Reduce端包括copy和sort过程，如图所示： Spill过程Spill过程包括输出、排序、溢写、合并等步骤，如图所示： Collect 每个Map任务不断地以对的形式把数据输出到在内存中构造的一个环形数据结构中。使用环形数据结构是为了更有效地使用内存空间，在内存中放置尽可能多的数据。 这个数据结构其实就是个字节数组，叫Kvbuffer，名如其义，但是这里面不光放置了数据，还放置了一些索引数据，给放置索引数据的区域起了一个Kvmeta的别名，在Kvbuffer的一块区域上穿了一个IntBuffer（字节序采用的是平台自身的字节序）的马甲。数据区域和索引数据区域在Kvbuffer中是相邻不重叠的两个区域，用一个分界点来划分两者，分界点不是亘古不变的，而是每次Spill之后都会更新一次。初始的分界点是0，数据的存储方向是向上增长，索引数据的存储方向是向下增长，如图所示： Kvbuffer的存放指针bufindex是一直闷着头地向上增长，比如bufindex初始值为0，一个Int型的key写完之后，bufindex增长为4，一个Int型的value写完之后，bufindex增长为8。 索引是对在kvbuffer中的索引，是个四元组，包括：value的起始位置、key的起始位置、partition值、value的长度，占用四个Int长度，Kvmeta的存放指针Kvindex每次都是向下跳四个“格子”，然后再向上一个格子一个格子地填充四元组的数据。比如Kvindex初始位置是-4，当第一个写完之后，(Kvindex+0)的位置存放value的起始位置、(Kvindex+1)的位置存放key的起始位置、(Kvindex+2)的位置存放partition的值、(Kvindex+3)的位置存放value的长度，然后Kvindex跳到-8位置，等第二个和索引写完之后，Kvindex跳到-32位置。 Kvbuffer的大小虽然可以通过参数设置，但是总共就那么大，和索引不断地增加，加着加着，Kvbuffer总有不够用的那天，那怎么办？把数据从内存刷到磁盘上再接着往内存写数据，把Kvbuffer中的数据刷到磁盘上的过程就叫Spill，多么明了的叫法，内存中的数据满了就自动地spill到具有更大空间的磁盘。 关于Spill触发的条件，也就是Kvbuffer用到什么程度开始Spill，还是要讲究一下的。如果把Kvbuffer用得死死得，一点缝都不剩的时候再开始Spill，那Map任务就需要等Spill完成腾出空间之后才能继续写数据；如果Kvbuffer只是满到一定程度，比如80%的时候就开始Spill，那在Spill的同时，Map任务还能继续写数据，如果Spill够快，Map可能都不需要为空闲空间而发愁。两利相衡取其大，一般选择后者。 Spill这个重要的过程是由Spill线程承担，Spill线程从Map任务接到“命令”之后就开始正式干活，干的活叫SortAndSpill，原来不仅仅是Spill，在Spill之前还有个颇具争议性的Sort。 Sort先把Kvbuffer中的数据按照partition值和key两个关键字升序排序，移动的只是索引数据，排序结果是Kvmeta中数据按照partition为单位聚集在一起，同一partition内的按照key有序。 SpillSpill线程为这次Spill过程创建一个磁盘文件：从所有的本地目录中轮训查找能存储这么大空间的目录，找到之后在其中创建一个类似于“spill12.out”的文件。Spill线程根据排过序的Kvmeta挨个partition的把数据吐到这个文件中，一个partition对应的数据吐完之后顺序地吐下个partition，直到把所有的partition遍历完。一个partition在文件中对应的数据也叫段(segment)。 所有的partition对应的数据都放在这个文件里，虽然是顺序存放的，但是怎么直接知道某个partition在这个文件中存放的起始位置呢？强大的索引又出场了。有一个三元组记录某个partition对应的数据在这个文件中的索引：起始位置、原始数据长度、压缩之后的数据长度，一个partition对应一个三元组。然后把这些索引信息存放在内存中，如果内存中放不下了，后续的索引信息就需要写到磁盘文件中了：从所有的本地目录中轮训查找能存储这么大空间的目录，找到之后在其中创建一个类似于“spill12.out.index”的文件，文件中不光存储了索引数据，还存储了crc32的校验数据。(spill12.out.index不一定在磁盘上创建，如果内存（默认1M空间）中能放得下就放在内存中，即使在磁盘上创建了，和spill12.out文件也不一定在同一个目录下。) 每一次Spill过程就会最少生成一个out文件，有时还会生成index文件，Spill的次数也烙印在文件名中。索引文件和数据文件的对应关系如下图所示： 在Spill线程如火如荼的进行SortAndSpill工作的同时，Map任务不会因此而停歇，而是一无既往地进行着数据输出。Map还是把数据写到kvbuffer中，那问题就来了：只顾着闷头按照bufindex指针向上增长，kvmeta只顾着按照Kvindex向下增长，是保持指针起始位置不变继续跑呢，还是另谋它路？如果保持指针起始位置不变，很快bufindex和Kvindex就碰头了，碰头之后再重新开始或者移动内存都比较麻烦，不可取。Map取kvbuffer中剩余空间的中间位置，用这个位置设置为新的分界点，bufindex指针移动到这个分界点，Kvindex移动到这个分界点的-16位置，然后两者就可以和谐地按照自己既定的轨迹放置数据了，当Spill完成，空间腾出之后，不需要做任何改动继续前进。分界点的转换如下图所示： Map任务总要把输出的数据写到磁盘上，即使输出数据量很小在内存中全部能装得下，在最后也会把数据刷到磁盘上。 MergeMap任务如果输出数据量很大，可能会进行好几次Spill，out文件和Index文件会产生很多，分布在不同的磁盘上。最后把这些文件进行合并的merge过程闪亮登场。 Merge过程怎么知道产生的Spill文件都在哪了呢？从所有的本地目录上扫描得到产生的Spill文件，然后把路径存储在一个数组里。Merge过程又怎么知道Spill的索引信息呢？没错，也是从所有的本地目录上扫描得到Index文件，然后把索引信息存储在一个列表里。到这里，又遇到了一个值得纳闷的地方。在之前Spill过程中的时候为什么不直接把这些信息存储在内存中呢，何必又多了这步扫描的操作？特别是Spill的索引数据，之前当内存超限之后就把数据写到磁盘，现在又要从磁盘把这些数据读出来，还是需要装到更多的内存中。之所以多此一举，是因为这时kvbuffer这个内存大户已经不再使用可以回收，有内存空间来装这些数据了。（对于内存空间较大的土豪来说，用内存来省却这两个io步骤还是值得考虑的。） 然后为merge过程创建一个叫file.out的文件和一个叫file.out.Index的文件用来存储最终的输出和索引。 一个partition一个partition的进行合并输出。对于某个partition来说，从索引列表中查询这个partition对应的所有索引信息，每个对应一个段插入到段列表中。也就是这个partition对应一个段列表，记录所有的Spill文件中对应的这个partition那段数据的文件名、起始位置、长度等等。 然后对这个partition对应的所有的segment进行合并，目标是合并成一个segment。当这个partition对应很多个segment时，会分批地进行合并：先从segment列表中把第一批取出来，以key为关键字放置成最小堆，然后从最小堆中每次取出最小的输出到一个临时文件中，这样就把这一批段合并成一个临时的段，把它加回到segment列表中；再从segment列表中把第二批取出来合并输出到一个临时segment，把其加入到列表中；这样往复执行，直到剩下的段是一批，输出到最终的文件中。 最终的索引数据仍然输出到Index文件中。 Map端的Shuffle过程到此结束。 CopyReduce任务通过HTTP向各个Map任务拖取它所需要的数据。每个节点都会启动一个常驻的HTTP server，其中一项服务就是响应Reduce拖取Map数据。当有MapOutput的HTTP请求过来的时候，HTTP server就读取相应的Map输出文件中对应这个Reduce部分的数据通过网络流输出给Reduce。 Reduce任务拖取某个Map对应的数据，如果在内存中能放得下这次数据的话就直接把数据写到内存中。Reduce要向每个Map去拖取数据，在内存中每个Map对应一块数据，当内存中存储的Map数据占用空间达到一定程度的时候，开始启动内存中merge，把内存中的数据merge输出到磁盘上一个文件中。 如果在内存中不能放得下这个Map的数据的话，直接把Map数据写到磁盘上，在本地目录创建一个文件，从HTTP流中读取数据然后写到磁盘，使用的缓存区大小是64K。拖一个Map数据过来就会创建一个文件，当文件数量达到一定阈值时，开始启动磁盘文件merge，把这些文件合并输出到一个文件。 有些Map的数据较小是可以放在内存中的，有些Map的数据较大需要放在磁盘上，这样最后Reduce任务拖过来的数据有些放在内存中了有些放在磁盘上，最后会对这些来一个全局合并。 Merge Sort这里使用的Merge和Map端使用的Merge过程一样。Map的输出数据已经是有序的，Merge进行一次合并排序，所谓Reduce端的sort过程就是这个合并的过程。一般Reduce是一边copy一边sort，即copy和sort两个阶段是重叠而不是完全分开的。 Reduce端的Shuffle过程至此结束。","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（二十二）MapReduce的输入和输出","slug":"2018-04-22-Hadoop学习之路（二十二）MapReduce的输入和输出","date":"2018-04-22T02:30:04.000Z","updated":"2019-09-19T03:35:48.887Z","comments":true,"path":"2018-04-22-Hadoop学习之路（二十二）MapReduce的输入和输出.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-22-Hadoop学习之路（二十二）MapReduce的输入和输出.html","excerpt":"** Hadoop学习之路（二十二）MapReduce的输入和输出：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十二）MapReduce的输入和输出","text":"** Hadoop学习之路（二十二）MapReduce的输入和输出：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十二）MapReduce的输入和输出 &lt;The rest of contents | 余下全文&gt; MapReduce的输入 作为一个会编写MR程序的人来说，知道map方法的参数是默认的数据读取组件读取到的一行数据 1、是谁在读取？ 是谁在调用这个map方法? 查看源码Mapper.java知道是run方法在调用map方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 1 /** 2 * 3 * 找出谁在调用Run方法 4 * 5 * 6 * 有一个组件叫做：MapTask 7 * 8 * 就会有对应的方法在调用mapper.run(context); 9 * 10 * 11 * context.nextKeyValue() ====== lineRecordReader.nextKeyValue();12 */13 public void run(Context context) throws IOException, InterruptedException &#123;14 15 /**16 * 在每一个mapTask被初始化出来的时候，就会被调用一次17 */18 setup(context);19 try &#123;20 21 /**22 * 数据读取组件每次读取到一行，都交给map方法执行一次23 * 24 * 25 * context.nextKeyValue()的意义有连点：26 * 27 * 1、读取一个key-value到该context对象中的两个属性中：key-value28 * 2、方法的返回值并不是读取到的key-value，是标志有没有读取到key_value的布尔值29 * 30 * 31 * context.getCurrentKey() ==== key32 * context.getCurrentValue() ==== value33 * 34 * 35 * 36 * 依赖于最底层的 LineRecordReader的实现37 * 38 * 你的nextKeyValue方法的返回结果中，一定要包含 false39 */40 while (context.nextKeyValue()) &#123;41 map(context.getCurrentKey(), context.getCurrentValue(), context);42 &#125;43 44 &#125; finally &#123;45 46 /**47 * 当前这个mapTask在执行完毕所有的该切片数据之后，会执行48 */49 cleanup(context);50 &#125;51 &#125; 此处map方法中有四个重要的方法： 1、context.nextKeyValue(); //负责读取数据，但是方法的返回值却不是读取到的key-value，而是返回了一个标识有没有读取到数据的布尔值 2、context.getCurrentKey(); //负责获取context.nextKeyValue() 读取到的key 3、context.getCurrentValue(); //负责获取context.nextKeyValue() 读取到的value 4、context.write(key,value); //负责输出mapper阶段输出的数据 2、谁在调用run方法？context参数怎么来的，是什么？ 共同答案：找到了谁在调用run方法，那么就能知道这个谁就会给run方法传入一个参数叫做：context 最开始，mapper.run(context)是由mapTask实例对象进行调用 查看源码MapTask.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 1 @Override 2 public void run(final JobConf job, final TaskUmbilicalProtocol umbilical) 3 throws IOException, ClassNotFoundException, InterruptedException &#123; 4 this.umbilical = umbilical; 5 6 if (isMapTask()) &#123; 7 // If there are no reducers then there won&apos;t be any sort. Hence the 8 // map 9 // phase will govern the entire attempt&apos;s progress.10 if (conf.getNumReduceTasks() == 0) &#123;11 mapPhase = getProgress().addPhase(&quot;map&quot;, 1.0f);12 &#125; else &#123;13 // If there are reducers then the entire attempt&apos;s progress will14 // be15 // split between the map phase (67%) and the sort phase (33%).16 mapPhase = getProgress().addPhase(&quot;map&quot;, 0.667f);17 sortPhase = getProgress().addPhase(&quot;sort&quot;, 0.333f);18 &#125;19 &#125;20 TaskReporter reporter = startReporter(umbilical);21 22 boolean useNewApi = job.getUseNewMapper();23 initialize(job, getJobID(), reporter, useNewApi);24 25 // check if it is a cleanupJobTask26 if (jobCleanup) &#123;27 runJobCleanupTask(umbilical, reporter);28 return;29 &#125;30 if (jobSetup) &#123;31 runJobSetupTask(umbilical, reporter);32 return;33 &#125;34 if (taskCleanup) &#123;35 runTaskCleanupTask(umbilical, reporter);36 return;37 &#125;38 39 /**40 * run方法的核心：41 * 42 * 新的API43 */44 45 if (useNewApi) &#123;46 /**47 * jobConf对象， splitMetaInfo 切片信息 umbilical 通信协议48 * reporter就是包含了各种计数器的一个对象49 */50 runNewMapper(job, splitMetaInfo, umbilical, reporter);51 &#125; else &#123;52 runOldMapper(job, splitMetaInfo, umbilical, reporter);53 &#125;54 55 done(umbilical, reporter);56 &#125; 得出伪代码调动新的API 123451 mapTask.run()&#123;2 runNewMapper()&#123;3 mapper.run(mapperContext);4 &#125;5 &#125; 3、查看runNewMapper方法 发现此方法还是在MapTask.java中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202 1 /** 2 * 这就是具体的调用逻辑的核心； 3 * 4 * 5 * mapper.run(context); 6 * 7 * 8 * 9 * @param job 10 * @param splitIndex 11 * @param umbilical 12 * @param reporter 13 * @throws IOException 14 * @throws ClassNotFoundException 15 * @throws InterruptedException 16 */ 17 @SuppressWarnings(&quot;unchecked&quot;) 18 private &lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt; void runNewMapper(final JobConf job, final TaskSplitIndex splitIndex, 19 final TaskUmbilicalProtocol umbilical, TaskReporter reporter) 20 throws IOException, ClassNotFoundException, InterruptedException &#123; 21 // make a task context so we can get the classes 22 org.apache.hadoop.mapreduce.TaskAttemptContext taskContext = new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl( 23 job, getTaskID(), reporter); 24 // make a mapper 25 org.apache.hadoop.mapreduce.Mapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt; mapper = (org.apache.hadoop.mapreduce.Mapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;) ReflectionUtils 26 .newInstance(taskContext.getMapperClass(), job); 27 28 29 30 31 /** 32 * inputFormat.createRecordReader() === RecordReader real 33 * 34 * 35 * inputFormat就是TextInputFormat类的实例对象 36 * 37 * TextInputFormat组件中的createRecordReader方法的返回值就是 LineRecordReader的实例对象 38 */ 39 // make the input format 40 org.apache.hadoop.mapreduce.InputFormat&lt;INKEY, INVALUE&gt; inputFormat = 41 (org.apache.hadoop.mapreduce.InputFormat&lt;INKEY, INVALUE&gt;) 42 ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job); 43 44 45 46 47 48 // rebuild the input split 49 org.apache.hadoop.mapreduce.InputSplit split = null; 50 split = getSplitDetails(new Path(splitIndex.getSplitLocation()), splitIndex.getStartOffset()); 51 LOG.info(&quot;Processing split: &quot; + split); 52 53 /** 54 * NewTrackingRecordReader这个类中一定有三个方法： 55 * 56 * nextKeyValue 57 * getCurrentKey 58 * getCurrentValue 59 * 60 * NewTrackingRecordReader的里面的三个方法的实现 61 * 其实是依赖于于inputFormat对象的createRecordReader方法的返回值的 三个方法的实现 62 * 63 * 默认的InputFormat： TextInputFormat 64 * 默认的RecordReader：LineRecordReader 65 * 66 * 67 * 最终：NewTrackingRecordReader的三个方法的实现是依赖于：LineRecordReader这个类中的三个同名方法的实现 68 */ 69 org.apache.hadoop.mapreduce.RecordReader&lt;INKEY, INVALUE&gt; input = 70 new NewTrackingRecordReader&lt;INKEY, INVALUE&gt;( 71 split, inputFormat, reporter, taskContext); 72 73 job.setBoolean(JobContext.SKIP_RECORDS, isSkipping()); 74 75 76 77 78 79 /** 80 * 声明一个Output对象用来给mapper的key-value进行输出 81 */ 82 org.apache.hadoop.mapreduce.RecordWriter output = null; 83 // get an output object 84 if (job.getNumReduceTasks() == 0) &#123; 85 86 /** 87 * NewDirectOutputCollector 直接输出的一个收集器， 这个类中一定有一个方法 叫做 write 88 */ 89 output = new NewDirectOutputCollector(taskContext, job, umbilical, reporter); 90 &#125; else &#123; 91 92 93 /** 94 * 有reducer阶段了。 95 * 96 * 1、能确定，一定会排序 97 * 98 * 2、能否确定一定会使用Parititioner, 不一定。 在逻辑上可以任务没有起作用。 99 * 100 * NewOutputCollector 这个类当中，一定有一个方法：write方法101 */102 output = new NewOutputCollector(taskContext, job, umbilical, reporter);103 &#125;104 105 106 107 108 109 /**110 * mapContext对象中一定包含三个方法111 * 112 * 找到了之前第一查看源码实现的方法的问题的答案：113 * 114 * 问题：找到谁调用MapContextImpl这个类的构造方法115 * 116 * mapContext就是MapContextImpl的实例对象117 * 118 * MapContextImpl类中一定有三个方法：119 * 120 * input === NewTrackingRecordReader121 * 122 * 123 * 124 * 确定的知识：125 * 126 * 1、mapContext对象中，一定有write方法127 * 128 * 2、通过观看MapContextImpl的组成，发现其实没有write方法129 * 130 * 解决：131 * 132 * 其实mapContext.write方法的调用是来自于MapContextImpl这个类的父类133 * 134 * 135 * 136 * 最底层的write方法： output.write();137 */138 org.apache.hadoop.mapreduce.MapContext&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt; mapContext = 139 new MapContextImpl&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;(140 job, getTaskID(), input, output, committer, reporter, split);141 142 /**143 * mapperContext的内部一定包含是三个犯法：144 * 145 * nextKeyValue146 * getCurrentKey147 * getCurrentValue148 * 149 * mapperContext的具体实现是依赖于new Context(context);150 * context = mapContext151 * 152 * 结论：153 * 154 * mapContext对象的内部一定包含以下三个方法：155 * 156 * nextKeyValue157 * getCurrentKey158 * getCurrentValue159 * 160 * 161 * mapContext 中 也有一个方法叫做：write(key,value)162 */163 org.apache.hadoop.mapreduce.Mapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;.Context mapperContext = 164 new WrappedMapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;()165 .getMapContext(mapContext);166 167 try &#123;168 169 170 171 172 input.initialize(split, mapperContext);173 174 175 176 /**177 * 复杂调用整个mapTask执行的入口178 * 179 * 方法的逻辑构成：180 * 181 * 1、重点方法在最后，或者在try中182 * 2、其他的代码，几乎只有两个任务：一个用来记录记日志或者完善流程。。 一个准备核心方法的参数183 */184 mapper.run(mapperContext);185 186 187 188 mapPhase.complete();189 setPhase(TaskStatus.Phase.SORT);190 statusUpdate(umbilical);191 input.close();192 input = null;193 output.close(mapperContext);194 output = null;195 196 197 198 &#125; finally &#123;199 closeQuietly(input);200 closeQuietly(output, mapperContext);201 &#125;202 &#125; 能确定的是：mapperContext一定有上面说的那四个重要的方法，往上继续查找mapperContext 123456789101112131415161718192021222324 /**143 * mapperContext的内部一定包含是三个犯法：144 * 145 * nextKeyValue146 * getCurrentKey147 * getCurrentValue148 * 149 * mapperContext的具体实现是依赖于new Context(context);150 * context = mapContext151 * 152 * 结论：153 * 154 * mapContext对象的内部一定包含以下三个方法：155 * 156 * nextKeyValue157 * getCurrentKey158 * getCurrentValue159 * 160 * 161 * mapContext 中 也有一个方法叫做：write(key,value)162 */163 org.apache.hadoop.mapreduce.Mapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;.Context mapperContext = 164 new WrappedMapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;()165 .getMapContext(mapContext); 查看WrappedMapper.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233241 /** 2 * Licensed to the Apache Software Foundation (ASF) under one 3 * or more contributor license agreements. See the NOTICE file 4 * distributed with this work for additional information 5 * regarding copyright ownership. The ASF licenses this file 6 * to you under the Apache License, Version 2.0 (the 7 * &quot;License&quot;); you may not use this file except in compliance 8 * with the License. You may obtain a copy of the License at 9 * 10 * http://www.apache.org/licenses/LICENSE-2.0 11 * 12 * Unless required by applicable law or agreed to in writing, software 13 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, 14 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 15 * See the License for the specific language governing permissions and 16 * limitations under the License. 17 */ 18 19 package org.apache.hadoop.mapreduce.lib.map; 20 21 import java.io.IOException; 22 import java.net.URI; 23 24 import org.apache.hadoop.classification.InterfaceAudience; 25 import org.apache.hadoop.classification.InterfaceStability; 26 import org.apache.hadoop.conf.Configuration; 27 import org.apache.hadoop.conf.Configuration.IntegerRanges; 28 import org.apache.hadoop.fs.Path; 29 import org.apache.hadoop.io.RawComparator; 30 import org.apache.hadoop.mapreduce.Counter; 31 import org.apache.hadoop.mapreduce.InputFormat; 32 import org.apache.hadoop.mapreduce.InputSplit; 33 import org.apache.hadoop.mapreduce.JobID; 34 import org.apache.hadoop.mapreduce.MapContext; 35 import org.apache.hadoop.mapreduce.Mapper; 36 import org.apache.hadoop.mapreduce.OutputCommitter; 37 import org.apache.hadoop.mapreduce.OutputFormat; 38 import org.apache.hadoop.mapreduce.Partitioner; 39 import org.apache.hadoop.mapreduce.Reducer; 40 import org.apache.hadoop.mapreduce.TaskAttemptID; 41 import org.apache.hadoop.security.Credentials; 42 43 /** 44 * A &#123;@link Mapper&#125; which wraps a given one to allow custom 45 * &#123;@link Mapper.Context&#125; implementations. 46 */ 47 @InterfaceAudience.Public 48 @InterfaceStability.Evolving 49 public class WrappedMapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; extends Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; &#123; 50 51 /** 52 * Get a wrapped &#123;@link Mapper.Context&#125; for custom implementations. 53 * 54 * @param mapContext 55 * &lt;code&gt;MapContext&lt;/code&gt; to be wrapped 56 * @return a wrapped &lt;code&gt;Mapper.Context&lt;/code&gt; for custom implementations 57 */ 58 public Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt;.Context getMapContext( 59 MapContext&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; mapContext) &#123; 60 return new Context(mapContext); 61 &#125; 62 63 @InterfaceStability.Evolving 64 public class Context extends Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt;.Context &#123; 65 66 protected MapContext&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; mapContext; 67 68 public Context(MapContext&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; mapContext) &#123; 69 this.mapContext = mapContext; 70 &#125; 71 72 /** 73 * Get the input split for this map. 74 */ 75 public InputSplit getInputSplit() &#123; 76 return mapContext.getInputSplit(); 77 &#125; 78 79 @Override 80 public KEYIN getCurrentKey() throws IOException, InterruptedException &#123; 81 return mapContext.getCurrentKey(); 82 &#125; 83 84 @Override 85 public VALUEIN getCurrentValue() throws IOException, InterruptedException &#123; 86 return mapContext.getCurrentValue(); 87 &#125; 88 89 @Override 90 public boolean nextKeyValue() throws IOException, InterruptedException &#123; 91 return mapContext.nextKeyValue(); 92 &#125; 93 94 @Override 95 public Counter getCounter(Enum&lt;?&gt; counterName) &#123; 96 return mapContext.getCounter(counterName); 97 &#125; 98 99 @Override100 public Counter getCounter(String groupName, String counterName) &#123;101 return mapContext.getCounter(groupName, counterName);102 &#125;103 104 @Override105 public OutputCommitter getOutputCommitter() &#123;106 return mapContext.getOutputCommitter();107 &#125;108 109 @Override110 public void write(KEYOUT key, VALUEOUT value) throws IOException, InterruptedException &#123;111 mapContext.write(key, value);112 &#125;113 114 @Override115 public String getStatus() &#123;116 return mapContext.getStatus();117 &#125;118 119 @Override120 public TaskAttemptID getTaskAttemptID() &#123;121 return mapContext.getTaskAttemptID();122 &#125;123 124 @Override125 public void setStatus(String msg) &#123;126 mapContext.setStatus(msg);127 &#125;128 129 @Override130 public Path[] getArchiveClassPaths() &#123;131 return mapContext.getArchiveClassPaths();132 &#125;133 134 @Override135 public String[] getArchiveTimestamps() &#123;136 return mapContext.getArchiveTimestamps();137 &#125;138 139 @Override140 public URI[] getCacheArchives() throws IOException &#123;141 return mapContext.getCacheArchives();142 &#125;143 144 @Override145 public URI[] getCacheFiles() throws IOException &#123;146 return mapContext.getCacheFiles();147 &#125;148 149 @Override150 public Class&lt;? extends Reducer&lt;?, ?, ?, ?&gt;&gt; getCombinerClass() throws ClassNotFoundException &#123;151 return mapContext.getCombinerClass();152 &#125;153 154 @Override155 public Configuration getConfiguration() &#123;156 return mapContext.getConfiguration();157 &#125;158 159 @Override160 public Path[] getFileClassPaths() &#123;161 return mapContext.getFileClassPaths();162 &#125;163 164 @Override165 public String[] getFileTimestamps() &#123;166 return mapContext.getFileTimestamps();167 &#125;168 169 @Override170 public RawComparator&lt;?&gt; getCombinerKeyGroupingComparator() &#123;171 return mapContext.getCombinerKeyGroupingComparator();172 &#125;173 174 @Override175 public RawComparator&lt;?&gt; getGroupingComparator() &#123;176 return mapContext.getGroupingComparator();177 &#125;178 179 @Override180 public Class&lt;? extends InputFormat&lt;?, ?&gt;&gt; getInputFormatClass() throws ClassNotFoundException &#123;181 return mapContext.getInputFormatClass();182 &#125;183 184 @Override185 public String getJar() &#123;186 return mapContext.getJar();187 &#125;188 189 @Override190 public JobID getJobID() &#123;191 return mapContext.getJobID();192 &#125;193 194 @Override195 public String getJobName() &#123;196 return mapContext.getJobName();197 &#125;198 199 @Override200 public boolean getJobSetupCleanupNeeded() &#123;201 return mapContext.getJobSetupCleanupNeeded();202 &#125;203 204 @Override205 public boolean getTaskCleanupNeeded() &#123;206 return mapContext.getTaskCleanupNeeded();207 &#125;208 209 @Override210 public Path[] getLocalCacheArchives() throws IOException &#123;211 return mapContext.getLocalCacheArchives();212 &#125;213 214 @Override215 public Path[] getLocalCacheFiles() throws IOException &#123;216 return mapContext.getLocalCacheFiles();217 &#125;218 219 @Override220 public Class&lt;?&gt; getMapOutputKeyClass() &#123;221 return mapContext.getMapOutputKeyClass();222 &#125;223 224 @Override225 public Class&lt;?&gt; getMapOutputValueClass() &#123;226 return mapContext.getMapOutputValueClass();227 &#125;228 229 @Override230 public Class&lt;? extends Mapper&lt;?, ?, ?, ?&gt;&gt; getMapperClass() throws ClassNotFoundException &#123;231 return mapContext.getMapperClass();232 &#125;233 234 @Override235 public int getMaxMapAttempts() &#123;236 return mapContext.getMaxMapAttempts();237 &#125;238 239 @Override240 public int getMaxReduceAttempts() &#123;241 return mapContext.getMaxReduceAttempts();242 &#125;243 244 @Override245 public int getNumReduceTasks() &#123;246 return mapContext.getNumReduceTasks();247 &#125;248 249 @Override250 public Class&lt;? extends OutputFormat&lt;?, ?&gt;&gt; getOutputFormatClass() throws ClassNotFoundException &#123;251 return mapContext.getOutputFormatClass();252 &#125;253 254 @Override255 public Class&lt;?&gt; getOutputKeyClass() &#123;256 return mapContext.getOutputKeyClass();257 &#125;258 259 @Override260 public Class&lt;?&gt; getOutputValueClass() &#123;261 return mapContext.getOutputValueClass();262 &#125;263 264 @Override265 public Class&lt;? extends Partitioner&lt;?, ?&gt;&gt; getPartitionerClass() throws ClassNotFoundException &#123;266 return mapContext.getPartitionerClass();267 &#125;268 269 @Override270 public Class&lt;? extends Reducer&lt;?, ?, ?, ?&gt;&gt; getReducerClass() throws ClassNotFoundException &#123;271 return mapContext.getReducerClass();272 &#125;273 274 @Override275 public RawComparator&lt;?&gt; getSortComparator() &#123;276 return mapContext.getSortComparator();277 &#125;278 279 @Override280 public boolean getSymlink() &#123;281 return mapContext.getSymlink();282 &#125;283 284 @Override285 public Path getWorkingDirectory() throws IOException &#123;286 return mapContext.getWorkingDirectory();287 &#125;288 289 @Override290 public void progress() &#123;291 mapContext.progress();292 &#125;293 294 @Override295 public boolean getProfileEnabled() &#123;296 return mapContext.getProfileEnabled();297 &#125;298 299 @Override300 public String getProfileParams() &#123;301 return mapContext.getProfileParams();302 &#125;303 304 @Override305 public IntegerRanges getProfileTaskRange(boolean isMap) &#123;306 return mapContext.getProfileTaskRange(isMap);307 &#125;308 309 @Override310 public String getUser() &#123;311 return mapContext.getUser();312 &#125;313 314 @Override315 public Credentials getCredentials() &#123;316 return mapContext.getCredentials();317 &#125;318 319 @Override320 public float getProgress() &#123;321 return mapContext.getProgress();322 &#125;323 &#125;324 &#125; 此类里面一定有那4个重要的方法，发现调用了mapContext，继续往上找 1234567891011121314151617181920212223242526272829303132/**110 * mapContext对象中一定包含三个方法111 * 112 * 找到了之前第一查看源码实现的方法的问题的答案：113 * 114 * 问题：找到谁调用MapContextImpl这个类的构造方法115 * 116 * mapContext就是MapContextImpl的实例对象117 * 118 * MapContextImpl类中一定有三个方法：119 * 120 * input === NewTrackingRecordReader121 * 122 * 123 * 124 * 确定的知识：125 * 126 * 1、mapContext对象中，一定有write方法127 * 128 * 2、通过观看MapContextImpl的组成，发现其实没有write方法129 * 130 * 解决：131 * 132 * 其实mapContext.write方法的调用是来自于MapContextImpl这个类的父类133 * 134 * 135 * 136 * 最底层的write方法： output.write();137 */138 org.apache.hadoop.mapreduce.MapContext&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt; mapContext = 139 new MapContextImpl&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;(140 job, getTaskID(), input, output, committer, reporter, split); mapConext就是这个类MapContextImpl的实例对象 继续确定: 1234567mapConext = new MapContextImpl(input)mapConext.nextKeyVlaue()&#123;LineRecordReader real = input.createRecordReader();real.nextKeyValue();&#125; 查看MapContextImpl.java源码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 1 public class MapContextImpl&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; 2 extends TaskInputOutputContextImpl&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; 3 implements MapContext&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; &#123; 4 5 6 private RecordReader&lt;KEYIN, VALUEIN&gt; reader; 7 private InputSplit split; 8 9 public MapContextImpl(Configuration conf, 10 TaskAttemptID taskid, 11 RecordReader&lt;KEYIN, VALUEIN&gt; reader,12 RecordWriter&lt;KEYOUT, VALUEOUT&gt; writer, 13 OutputCommitter committer, 14 StatusReporter reporter,15 InputSplit split) &#123;16 17 18 19 // 通过super调用父类的构造方法20 super(conf, taskid, writer, committer, reporter);21 22 23 24 this.reader = reader;25 this.split = split;26 &#125;27 28 /**29 * Get the input split for this map.30 */31 public InputSplit getInputSplit() &#123;32 return split;33 &#125;40 @Override41 public KEYIN getCurrentKey() throws IOException, InterruptedException &#123;42 return reader.getCurrentKey();43 &#125;44 45 @Override46 public VALUEIN getCurrentValue() throws IOException, InterruptedException &#123;47 return reader.getCurrentValue();48 &#125;49 50 @Override51 public boolean nextKeyValue() throws IOException, InterruptedException &#123;52 return reader.nextKeyValue();53 &#125;54 55 56 57 58 &#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（二十一）MapReduce实现Reduce Join（多个文件联合查询）","slug":"2018-04-21-Hadoop学习之路（二十一）MapReduce实现Reduce Join（多个文件联合查询）","date":"2018-04-21T02:30:04.000Z","updated":"2019-09-19T02:46:16.339Z","comments":true,"path":"2018-04-21-Hadoop学习之路（二十一）MapReduce实现Reduce Join（多个文件联合查询）.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-21-Hadoop学习之路（二十一）MapReduce实现Reduce Join（多个文件联合查询）.html","excerpt":"** Hadoop学习之路（二十一）MapReduce实现Reduce Join（多个文件联合查询）：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十一）MapReduce实现Reduce Join（多个文件联合查询）","text":"** Hadoop学习之路（二十一）MapReduce实现Reduce Join（多个文件联合查询）：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二十一）MapReduce实现Reduce Join（多个文件联合查询） &lt;The rest of contents | 余下全文&gt; MapReduce Join对两份数据data1和data2进行关键词连接是一个很通用的问题，如果数据量比较小，可以在内存中完成连接。 如果数据量比较大，在内存进行连接操会发生OOM。mapreduce join可以用来解决大数据的连接。 1 思路1.1 reduce join在map阶段, 把关键字作为key输出，并在value中标记出数据是来自data1还是data2。因为在shuffle阶段已经自然按key分组，reduce阶段，判断每一个value是来自data1还是data2,在内部分成2组，做集合的乘积。 这种方法有2个问题： 1, map阶段没有对数据瘦身，shuffle的网络传输和排序性能很低。 2, reduce端对2个集合做乘积计算，很耗内存，容易导致OOM。 1.2 map join两份数据中，如果有一份数据比较小，小数据全部加载到内存，按关键字建立索引。大数据文件作为map的输入文件，对map()函数每一对输入，都能够方便地和已加载到内存的小数据进行连接。把连接结果按key输出，经过shuffle阶段，reduce端得到的就是已经按key分组的，并且连接好了的数据。 这种方法，要使用hadoop中的DistributedCache把小数据分布到各个计算节点，每个map节点都要把小数据库加载到内存，按关键字建立索引。 这种方法有明显的局限性：有一份数据比较小，在map端，能够把它加载到内存，并进行join操作。 1.3 使用内存服务器，扩大节点的内存空间针对map join，可以把一份数据存放到专门的内存服务器，在map()方法中，对每一个&lt;key,value&gt;的输入对，根据key到内存服务器中取出数据，进行连接 1.4 使用BloomFilter过滤空连接的数据对其中一份数据在内存中建立BloomFilter，另外一份数据在连接之前，用BloomFilter判断它的key是否存在，如果不存在，那这个记录是空连接，可以忽略。 1.5 使用mapreduce专为join设计的包在mapreduce包里看到有专门为join设计的包，对这些包还没有学习，不知道怎么使用，只是在这里记录下来，作个提醒。 jar： mapreduce-client-core.jar package： org.apache.hadoop.mapreduce.lib.join 2 实现reduce join两个文件，此处只写出部分数据，测试数据movies.dat数据量为3883条，ratings.dat数据量为1000210条数据 movies.dat 数据格式为：1 :: Toy Story (1995) :: Animation|Children’s|Comedy 对应字段中文解释： 电影ID 电影名字 电影类型 ratings.dat 数据格式为：1 :: 1193 :: 5 :: 978300760 对应字段中文解释： 用户ID 电影ID 评分 评分时间戳 2个文件进行关联实现代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141import java.io.IOException; 2 import java.net.URI; 3 import java.util.ArrayList; 4 import java.util.List; 5 6 import org.apache.hadoop.conf.Configuration; 7 import org.apache.hadoop.fs.FileSystem; 8 import org.apache.hadoop.fs.Path; 9 import org.apache.hadoop.io.IntWritable; 10 import org.apache.hadoop.io.LongWritable; 11 import org.apache.hadoop.io.Text; 12 import org.apache.hadoop.mapreduce.Job; 13 import org.apache.hadoop.mapreduce.Mapper; 14 import org.apache.hadoop.mapreduce.Reducer; 15 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; 16 import org.apache.hadoop.mapreduce.lib.input.FileSplit; 17 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 18 19 public class MovieMR1 &#123; 20 21 public static void main(String[] args) throws Exception &#123; 22 23 Configuration conf1 = new Configuration(); 24 /*conf1.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000/&quot;); 25 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);*/ 26 FileSystem fs1 = FileSystem.get(conf1); 27 28 29 Job job = Job.getInstance(conf1); 30 31 job.setJarByClass(MovieMR1.class); 32 33 job.setMapperClass(MoviesMapper.class); 34 job.setReducerClass(MoviesReduceJoinReducer.class); 35 36 job.setMapOutputKeyClass(Text.class); 37 job.setMapOutputValueClass(Text.class); 38 39 job.setOutputKeyClass(Text.class); 40 job.setOutputValueClass(Text.class); 41 42 Path inputPath1 = new Path(&quot;D:\\\\MR\\\\hw\\\\movie\\\\input\\\\movies&quot;); 43 Path inputPath2 = new Path(&quot;D:\\\\MR\\\\hw\\\\movie\\\\input\\\\ratings&quot;); 44 Path outputPath1 = new Path(&quot;D:\\\\MR\\\\hw\\\\movie\\\\output&quot;); 45 if(fs1.exists(outputPath1)) &#123; 46 fs1.delete(outputPath1, true); 47 &#125; 48 FileInputFormat.addInputPath(job, inputPath1); 49 FileInputFormat.addInputPath(job, inputPath2); 50 FileOutputFormat.setOutputPath(job, outputPath1); 51 52 boolean isDone = job.waitForCompletion(true); 53 System.exit(isDone ? 0 : 1); 54 &#125; 55 56 57 public static class MoviesMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; 58 59 Text outKey = new Text(); 60 Text outValue = new Text(); 61 StringBuilder sb = new StringBuilder(); 62 63 protected void map(LongWritable key, Text value,Context context) throws java.io.IOException ,InterruptedException &#123; 64 65 FileSplit inputSplit = (FileSplit)context.getInputSplit(); 66 String name = inputSplit.getPath().getName(); 67 String[] split = value.toString().split(&quot;::&quot;); 68 sb.setLength(0); 69 70 if(name.equals(&quot;movies.dat&quot;)) &#123; 71 // 1 :: Toy Story (1995) :: Animation|Children&apos;s|Comedy 72 //对应字段中文解释： 电影ID 电影名字 电影类型 73 outKey.set(split[0]); 74 StringBuilder append = sb.append(split[1]).append(&quot;\\t&quot;).append(split[2]); 75 String str = &quot;movies#&quot;+append.toString(); 76 outValue.set(str); 77 //System.out.println(outKey+&quot;---&quot;+outValue); 78 context.write(outKey, outValue); 79 &#125;else&#123; 80 // 1 :: 1193 :: 5 :: 978300760 81 //对应字段中文解释： 用户ID 电影ID 评分 评分时间戳 82 outKey.set(split[1]); 83 StringBuilder append = sb.append(split[0]).append(&quot;\\t&quot;).append(split[2]).append(&quot;\\t&quot;).append(split[3]); 84 String str = &quot;ratings#&quot; + append.toString(); 85 outValue.set(str); 86 //System.out.println(outKey+&quot;---&quot;+outValue); 87 context.write(outKey, outValue); 88 &#125; 89 90 &#125;; 91 92 &#125; 93 94 95 public static class MoviesReduceJoinReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123; 96 //用来存放 电影ID 电影名称 电影类型 97 List&lt;String&gt; moviesList = new ArrayList&lt;&gt;(); 98 //用来存放 电影ID 用户ID 用户评分 时间戳 99 List&lt;String&gt; ratingsList = new ArrayList&lt;&gt;();100 Text outValue = new Text();101 102 @Override103 protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context)104 throws IOException, InterruptedException &#123;105 106 int count = 0;107 108 //迭代集合109 for(Text text : values) &#123;110 111 //将集合中的元素添加到对应的list中112 if(text.toString().startsWith(&quot;movies#&quot;)) &#123;113 String string = text.toString().split(&quot;#&quot;)[1];114 115 moviesList.add(string);116 &#125;else if(text.toString().startsWith(&quot;ratings#&quot;))&#123;117 String string = text.toString().split(&quot;#&quot;)[1];118 ratingsList.add(string);119 &#125;120 &#125;121 122 //获取2个集合的长度123 long moviesSize = moviesList.size();124 long ratingsSize = ratingsList.size();125 126 for(int i=0;i&lt;moviesSize;i++) &#123;127 for(int j=0;j&lt;ratingsSize;j++) &#123;128 outValue.set(moviesList.get(i)+&quot;\\t&quot;+ratingsList.get(j));129 //最后的输出是 电影ID 电影名称 电影类型 用户ID 用户评分 时间戳130 context.write(key, outValue);131 &#125;132 &#125;133 134 moviesList.clear();135 ratingsList.clear();136 137 &#125;138 139 &#125;140 141 &#125; 最后的合并结果： 电影ID 电影名称 电影类型 用户ID 用户评论 时间戳 3 实现map join两个文件，此处只写出部分数据，测试数据movies.dat数据量为3883条，ratings.dat数据量为1000210条数据 movies.dat 数据格式为：1 :: Toy Story (1995) :: Animation|Children’s|Comedy 对应字段中文解释： 电影ID 电影名字 电影类型 ratings.dat 数据格式为：1 :: 1193 :: 5 :: 978300760 对应字段中文解释： 用户ID 电影ID 评分 评分时间戳 需求：求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数） 实现代码 MovieMR1_1.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 1 import java.io.DataInput; 2 import java.io.DataOutput; 3 import java.io.IOException; 4 5 import org.apache.hadoop.io.WritableComparable; 6 7 public class MovieRating implements WritableComparable&lt;MovieRating&gt;&#123; 8 private String movieName; 9 private int count;10 11 public String getMovieName() &#123;12 return movieName;13 &#125;14 public void setMovieName(String movieName) &#123;15 this.movieName = movieName;16 &#125;17 public int getCount() &#123;18 return count;19 &#125;20 public void setCount(int count) &#123;21 this.count = count;22 &#125;23 24 public MovieRating() &#123;&#125;25 26 public MovieRating(String movieName, int count) &#123;27 super();28 this.movieName = movieName;29 this.count = count;30 &#125;31 32 33 @Override34 public String toString() &#123;35 return movieName + &quot;\\t&quot; + count;36 &#125;37 @Override38 public void readFields(DataInput in) throws IOException &#123;39 movieName = in.readUTF();40 count = in.readInt();41 &#125;42 @Override43 public void write(DataOutput out) throws IOException &#123;44 out.writeUTF(movieName);45 out.writeInt(count);46 &#125;47 @Override48 public int compareTo(MovieRating o) &#123;49 return o.count - this.count ;50 &#125;51 52 &#125; MovieMR1_2.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102 1 import java.io.IOException; 2 3 import org.apache.hadoop.conf.Configuration; 4 import org.apache.hadoop.fs.FileSystem; 5 import org.apache.hadoop.fs.Path; 6 import org.apache.hadoop.io.LongWritable; 7 import org.apache.hadoop.io.NullWritable; 8 import org.apache.hadoop.io.Text; 9 import org.apache.hadoop.mapreduce.Job; 10 import org.apache.hadoop.mapreduce.Mapper; 11 import org.apache.hadoop.mapreduce.Reducer; 12 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; 13 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 14 15 public class MovieMR1_2 &#123; 16 17 public static void main(String[] args) throws Exception &#123; 18 if(args.length &lt; 2) &#123; 19 args = new String[2]; 20 args[0] = &quot;/movie/output/&quot;; 21 args[1] = &quot;/movie/output_last/&quot;; 22 &#125; 23 24 25 Configuration conf1 = new Configuration(); 26 conf1.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000/&quot;); 27 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;); 28 FileSystem fs1 = FileSystem.get(conf1); 29 30 31 Job job = Job.getInstance(conf1); 32 33 job.setJarByClass(MovieMR1_2.class); 34 35 job.setMapperClass(MoviesMapJoinRatingsMapper2.class); 36 job.setReducerClass(MovieMR1Reducer2.class); 37 38 39 job.setMapOutputKeyClass(MovieRating.class); 40 job.setMapOutputValueClass(NullWritable.class); 41 42 job.setOutputKeyClass(MovieRating.class); 43 job.setOutputValueClass(NullWritable.class); 44 45 46 Path inputPath1 = new Path(args[0]); 47 Path outputPath1 = new Path(args[1]); 48 if(fs1.exists(outputPath1)) &#123; 49 fs1.delete(outputPath1, true); 50 &#125; 51 //对第一步的输出结果进行降序排序 52 FileInputFormat.setInputPaths(job, inputPath1); 53 FileOutputFormat.setOutputPath(job, outputPath1); 54 55 boolean isDone = job.waitForCompletion(true); 56 System.exit(isDone ? 0 : 1); 57 58 59 &#125; 60 61 //注意输出类型为自定义对象MovieRating，MovieRating按照降序排序 62 public static class MoviesMapJoinRatingsMapper2 extends Mapper&lt;LongWritable, Text, MovieRating, NullWritable&gt;&#123; 63 64 MovieRating outKey = new MovieRating(); 65 66 @Override 67 protected void map(LongWritable key, Text value, Context context) 68 throws IOException, InterruptedException &#123; 69 //&apos;Night Mother (1986) 70 70 String[] split = value.toString().split(&quot;\\t&quot;); 71 72 outKey.setCount(Integer.parseInt(split[1]));; 73 outKey.setMovieName(split[0]); 74 75 context.write(outKey, NullWritable.get()); 76 77 &#125; 78 79 &#125; 80 81 //排序之后自然输出，只取前10部电影 82 public static class MovieMR1Reducer2 extends Reducer&lt;MovieRating, NullWritable, MovieRating, NullWritable&gt;&#123; 83 84 Text outKey = new Text(); 85 int count = 0; 86 87 @Override 88 protected void reduce(MovieRating key, Iterable&lt;NullWritable&gt; values,Context context) throws IOException, InterruptedException &#123; 89 90 for(NullWritable value : values) &#123; 91 count++; 92 if(count &gt; 10) &#123; 93 return; 94 &#125; 95 context.write(key, value); 96 97 &#125; 98 99 &#125;100 101 &#125;102 &#125; MovieRating.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169 1 import java.io.BufferedReader; 2 import java.io.FileReader; 3 import java.io.IOException; 4 import java.net.URI; 5 import java.util.HashMap; 6 import java.util.Map; 7 8 import org.apache.hadoop.conf.Configuration; 9 import org.apache.hadoop.fs.FileSystem; 10 import org.apache.hadoop.fs.Path; 11 import org.apache.hadoop.io.IntWritable; 12 import org.apache.hadoop.io.LongWritable; 13 import org.apache.hadoop.io.Text; 14 import org.apache.hadoop.mapreduce.Job; 15 import org.apache.hadoop.mapreduce.Mapper; 16 import org.apache.hadoop.mapreduce.Reducer; 17 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; 18 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 19 20 21 public class MovieMR1_1 &#123; 22 23 public static void main(String[] args) throws Exception &#123; 24 25 if(args.length &lt; 4) &#123; 26 args = new String[4]; 27 args[0] = &quot;/movie/input/&quot;; 28 args[1] = &quot;/movie/output/&quot;; 29 args[2] = &quot;/movie/cache/movies.dat&quot;; 30 args[3] = &quot;/movie/output_last/&quot;; 31 &#125; 32 33 34 Configuration conf1 = new Configuration(); 35 conf1.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000/&quot;); 36 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;); 37 FileSystem fs1 = FileSystem.get(conf1); 38 39 40 Job job1 = Job.getInstance(conf1); 41 42 job1.setJarByClass(MovieMR1_1.class); 43 44 job1.setMapperClass(MoviesMapJoinRatingsMapper1.class); 45 job1.setReducerClass(MovieMR1Reducer1.class); 46 47 job1.setMapOutputKeyClass(Text.class); 48 job1.setMapOutputValueClass(IntWritable.class); 49 50 job1.setOutputKeyClass(Text.class); 51 job1.setOutputValueClass(IntWritable.class); 52 53 54 55 //缓存普通文件到task运行节点的工作目录 56 URI uri = new URI(&quot;hdfs://hadoop1:9000&quot;+args[2]); 57 System.out.println(uri); 58 job1.addCacheFile(uri); 59 60 Path inputPath1 = new Path(args[0]); 61 Path outputPath1 = new Path(args[1]); 62 if(fs1.exists(outputPath1)) &#123; 63 fs1.delete(outputPath1, true); 64 &#125; 65 FileInputFormat.setInputPaths(job1, inputPath1); 66 FileOutputFormat.setOutputPath(job1, outputPath1); 67 68 boolean isDone = job1.waitForCompletion(true); 69 System.exit(isDone ? 0 : 1); 70 71 &#125; 72 73 public static class MoviesMapJoinRatingsMapper1 extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; 74 75 //用了存放加载到内存中的movies.dat数据 76 private static Map&lt;String,String&gt; movieMap = new HashMap&lt;&gt;(); 77 //key：电影ID 78 Text outKey = new Text(); 79 //value：电影名+电影类型 80 IntWritable outValue = new IntWritable(); 81 82 83 /** 84 * movies.dat: 1::Toy Story (1995)::Animation|Children&apos;s|Comedy 85 * 86 * 87 * 将小表(movies.dat)中的数据预先加载到内存中去 88 * */ 89 @Override 90 protected void setup(Context context) throws IOException, InterruptedException &#123; 91 92 Path[] localCacheFiles = context.getLocalCacheFiles(); 93 94 String strPath = localCacheFiles[0].toUri().toString(); 95 96 BufferedReader br = new BufferedReader(new FileReader(strPath)); 97 String readLine; 98 while((readLine = br.readLine()) != null) &#123; 99 100 String[] split = readLine.split(&quot;::&quot;);101 String movieId = split[0];102 String movieName = split[1];103 String movieType = split[2];104 105 movieMap.put(movieId, movieName+&quot;\\t&quot;+movieType);106 &#125;107 108 br.close();109 &#125;110 111 112 /**113 * movies.dat: 1 :: Toy Story (1995) :: Animation|Children&apos;s|Comedy 114 * 电影ID 电影名字 电影类型115 * 116 * ratings.dat: 1 :: 1193 :: 5 :: 978300760117 * 用户ID 电影ID 评分 评分时间戳118 * 119 * value: ratings.dat读取的数据120 * */121 @Override122 protected void map(LongWritable key, Text value, Context context)123 throws IOException, InterruptedException &#123;124 125 String[] split = value.toString().split(&quot;::&quot;);126 127 String userId = split[0];128 String movieId = split[1];129 String movieRate = split[2];130 131 //根据movieId从内存中获取电影名和类型132 String movieNameAndType = movieMap.get(movieId);133 String movieName = movieNameAndType.split(&quot;\\t&quot;)[0];134 String movieType = movieNameAndType.split(&quot;\\t&quot;)[1];135 136 outKey.set(movieName);137 outValue.set(Integer.parseInt(movieRate));138 139 context.write(outKey, outValue);140 141 &#125;142 143 &#125;144 145 146 public static class MovieMR1Reducer1 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;147 //每部电影评论的次数148 int count;149 //评分次数150 IntWritable outValue = new IntWritable();151 152 @Override153 protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123;154 155 count = 0;156 157 for(IntWritable value : values) &#123;158 count++;159 &#125;160 161 outValue.set(count);162 163 context.write(key, outValue);164 &#125;165 166 &#125;167 168 169 &#125; 最后的结果","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（二十）MapReduce求TopN","slug":"2018-04-20-Hadoop学习之路（二十）MapReduce求TopN","date":"2018-04-20T02:30:04.000Z","updated":"2019-09-19T03:33:28.292Z","comments":true,"path":"2018-04-20-Hadoop学习之路（二十）MapReduce求TopN.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-20-Hadoop学习之路（二十）MapReduce求TopN.html","excerpt":"** Hadoop学习之路（二十）MapReduce求TopN：** &lt;Excerpt in index | 首页摘要&gt; ​ 在Hadoop中，排序是MapReduce的灵魂，MapTask和ReduceTask均会对数据按Key排序，这个操作是MR框架的默认行为，不管你的业务逻辑上是否需要这一操作。","text":"** Hadoop学习之路（二十）MapReduce求TopN：** &lt;Excerpt in index | 首页摘要&gt; ​ 在Hadoop中，排序是MapReduce的灵魂，MapTask和ReduceTask均会对数据按Key排序，这个操作是MR框架的默认行为，不管你的业务逻辑上是否需要这一操作。 &lt;The rest of contents | 余下全文&gt; 技术点MapReduce框架中，用到的排序主要有两种：快速排序和基于堆实现的优先级队列（PriorityQueue）。 Mapper阶段从map输出到环形缓冲区的数据会被排序（这是MR框架中改良的快速排序），这个排序涉及partition和key，当缓冲区容量占用80%，会spill数据到磁盘，生成IFile文件，Map结束后，会将IFile文件排序合并成一个大文件（基于堆实现的优先级队列），以供不同的reduce来拉取相应的数据。 Reducer阶段从Mapper端取回的数据已是部分有序，Reduce Task只需进行一次归并排序即可保证数据整体有序。为了提高效率，Hadoop将sort阶段和reduce阶段并行化，在sort阶段，Reduce Task为内存和磁盘中的文件建立了小顶堆，保存了指向该小顶堆根节点的迭代器，并不断的移动迭代器，以将key相同的数据顺次交给reduce()函数处理，期间移动迭代器的过程实际上就是不断调整小顶堆的过程（建堆→取堆顶元素→重新建堆→取堆顶元素…），这样，sort和reduce可以并行进行。 分组Top N分析在数据处理中，经常会碰到这样一个场景，对表数据按照某一字段分组，然后找出各自组内最大的几条记录情形。针对这种分组Top N问题，我们利用Hive、MapReduce等多种工具实现一下。 场景模拟1234567891011121314151617181920212223242526272829303132computer,huangxiaoming,85,86,41,75,93,42,85computer,xuzheng,54,52,86,91,42computer,huangbo,85,42,96,38english,zhaobenshan,54,52,86,91,42,85,75english,liuyifei,85,41,75,21,85,96,14algorithm,liuyifei,75,85,62,48,54,96,15computer,huangjiaju,85,75,86,85,85english,liuyifei,76,95,86,74,68,74,48english,huangdatou,48,58,67,86,15,33,85algorithm,huanglei,76,95,86,74,68,74,48algorithm,huangjiaju,85,75,86,85,85,74,86computer,huangdatou,48,58,67,86,15,33,85english,zhouqi,85,86,41,75,93,42,85,75,55,47,22english,huangbo,85,42,96,38,55,47,22algorithm,liutao,85,75,85,99,66computer,huangzitao,85,86,41,75,93,42,85math,wangbaoqiang,85,86,41,75,93,42,85computer,liujialing,85,41,75,21,85,96,14,74,86computer,liuyifei,75,85,62,48,54,96,15computer,liutao,85,75,85,99,66,88,75,91computer,huanglei,76,95,86,74,68,74,48english,liujialing,75,85,62,48,54,96,15math,huanglei,76,95,86,74,68,74,48math,huangjiaju,85,75,86,85,85,74,86math,liutao,48,58,67,86,15,33,85english,huanglei,85,75,85,99,66,88,75,91math,xuzheng,54,52,86,91,42,85,75math,huangxiaoming,85,75,85,99,66,88,75,91math,liujialing,85,86,41,75,93,42,85,75english,huangxiaoming,85,86,41,75,93,42,85algorithm,huangdatou,48,58,67,86,15,33,85algorithm,huangzitao,85,86,41,75,93,42,85,75 一、数据解释 数据字段个数不固定：第一个是课程名称，总共四个课程，computer，math，english，algorithm，第二个是学生姓名，后面是每次考试的分数 二、统计需求：1、统计每门课程的参考人数和课程平均分 2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件，并且按平均分从高到低排序，分数保留一位小数 3、求出每门课程参考学生成绩最高的学生的信息：课程，姓名和平均分 第一题 CourseScoreMR1.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122 1 import java.io.IOException; 2 3 import org.apache.hadoop.conf.Configuration; 4 import org.apache.hadoop.fs.FileSystem; 5 import org.apache.hadoop.fs.Path; 6 import org.apache.hadoop.io.DoubleWritable; 7 import org.apache.hadoop.io.LongWritable; 8 import org.apache.hadoop.io.Text; 9 import org.apache.hadoop.mapreduce.Job; 10 import org.apache.hadoop.mapreduce.Mapper; 11 import org.apache.hadoop.mapreduce.Reducer; 12 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; 13 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 14 15 public class CourseScoreMR1 &#123; 16 17 public static void main(String[] args) throws Exception &#123; 18 19 Configuration conf = new Configuration(); 20 FileSystem fs = FileSystem.get(conf); 21 Job job = Job.getInstance(conf); 22 23 24 job.setJarByClass(CourseScoreMR1.class); 25 job.setMapperClass(CourseScoreMR1Mapper.class); 26 job.setReducerClass(CourseScoreMR1Reducer.class); 27 28 job.setMapOutputKeyClass(Text.class); 29 job.setMapOutputValueClass(DoubleWritable.class); 30 job.setOutputKeyClass(Text.class); 31 job.setOutputValueClass(Text.class); 32 33 34 Path inputPath = new Path(&quot;E:\\\\bigdata\\\\cs\\\\input&quot;); 35 Path outputPath = new Path(&quot;E:\\\\bigdata\\\\cs\\\\output_1&quot;); 36 FileInputFormat.setInputPaths(job, inputPath); 37 if(fs.exists(outputPath))&#123; 38 fs.delete(outputPath, true); 39 &#125; 40 FileOutputFormat.setOutputPath(job, outputPath); 41 42 43 boolean isDone = job.waitForCompletion(true); 44 System.exit(isDone ? 0 : 1); 45 &#125; 46 47 public static class CourseScoreMR1Mapper extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt;&#123; 48 49 /** 50 * 数据的三个字段： course , name, score 51 * 52 * value == algorithm,huangzitao,85,86,41,75,93,42,85,75 53 * 54 * 输出的key和value： 55 * 56 * key ： course 57 * 58 * value : avgScore 59 * 60 * 格式化数值相关的操作的API ： NumberFormat 61 * SimpleDateFormat 62 */ 63 64 Text outKey = new Text(); 65 DoubleWritable outValue = new DoubleWritable(); 66 67 @Override 68 protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; 69 70 String[] split = value.toString().split(&quot;,&quot;); 71 72 String course = split[0]; 73 74 int sum = 0; 75 int count = 0; 76 77 for(int i = 2; i&lt;split.length; i++)&#123; 78 int tempScore = Integer.parseInt(split[i]); 79 sum += tempScore; 80 81 count++; 82 &#125; 83 84 double avgScore = 1D * sum / count; 85 86 87 outKey.set(course); 88 outValue.set(avgScore); 89 90 context.write(outKey, outValue); 91 &#125; 92 93 &#125; 94 95 public static class CourseScoreMR1Reducer extends Reducer&lt;Text, DoubleWritable, Text, Text&gt;&#123; 96 97 98 Text outValue = new Text(); 99 /**100 * key : course101 * 102 * values : 98.7 87.6103 */104 @Override105 protected void reduce(Text key, Iterable&lt;DoubleWritable&gt; values, Context context) throws IOException, InterruptedException &#123;106 107 double sum = 0;108 int count = 0;109 110 for(DoubleWritable dw : values)&#123;111 sum += dw.get();112 count ++;113 &#125;114 115 double lastAvgScore = sum / count;116 117 outValue.set(count+&quot;\\t&quot; + lastAvgScore);118 119 context.write(key, outValue);120 &#125;121 &#125;122 &#125; 第二题 CourseScoreMR2.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899 1 import java.io.IOException; 2 3 import org.apache.hadoop.conf.Configuration; 4 import org.apache.hadoop.fs.FileSystem; 5 import org.apache.hadoop.fs.Path; 6 import org.apache.hadoop.io.LongWritable; 7 import org.apache.hadoop.io.NullWritable; 8 import org.apache.hadoop.io.Text; 9 import org.apache.hadoop.mapreduce.Job;10 import org.apache.hadoop.mapreduce.Mapper;11 import org.apache.hadoop.mapreduce.Reducer;12 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;13 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;14 15 import com.ghgj.mr.exercise.pojo.CourseScore;16 import com.ghgj.mr.exercise.ptn.CSPartitioner;17 18 public class CourseScoreMR2&#123;19 20 public static void main(String[] args) throws Exception &#123;21 22 Configuration conf = new Configuration();23 24 FileSystem fs = FileSystem.get(conf);25 Job job = Job.getInstance(conf);26 27 28 job.setJarByClass(CourseScoreMR2.class);29 job.setMapperClass(CourseScoreMR2Mapper.class);30 // job.setReducerClass(CourseScoreMR2Reducer.class);31 32 job.setMapOutputKeyClass(CourseScore.class);33 job.setMapOutputValueClass(NullWritable.class);34 // job.setOutputKeyClass(CourseScore.class);35 // job.setOutputValueClass(NullWritable.class);36 37 38 job.setPartitionerClass(CSPartitioner.class);39 job.setNumReduceTasks(4);40 41 42 Path inputPath = new Path(&quot;E:\\\\bigdata\\\\cs\\\\input&quot;);43 Path outputPath = new Path(&quot;E:\\\\bigdata\\\\cs\\\\output_2&quot;);44 FileInputFormat.setInputPaths(job, inputPath);45 if(fs.exists(outputPath))&#123;46 fs.delete(outputPath, true);47 &#125;48 FileOutputFormat.setOutputPath(job, outputPath);49 50 51 boolean isDone = job.waitForCompletion(true);52 System.exit(isDone ? 0 : 1);53 &#125;54 55 public static class CourseScoreMR2Mapper extends Mapper&lt;LongWritable, Text, CourseScore, NullWritable&gt;&#123;56 57 CourseScore cs = new CourseScore();58 59 /**60 * value = math,huangxiaoming,85,75,85,99,66,88,75,9161 */62 @Override63 protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;64 65 String[] split = value.toString().split(&quot;,&quot;);66 67 String course = split[0];68 String name = split[1];69 70 int sum = 0;71 int count = 0;72 73 for(int i = 2; i&lt;split.length; i++)&#123;74 int tempScore = Integer.parseInt(split[i]);75 sum += tempScore;76 77 count++;78 &#125;79 80 double avgScore = 1D * sum / count;81 82 cs.setCourse(course);83 cs.setName(name);84 cs.setScore(avgScore);85 86 context.write(cs, NullWritable.get());87 &#125;88 89 &#125;90 91 public static class CourseScoreMR2Reducer extends Reducer&lt;CourseScore, NullWritable, CourseScore, NullWritable&gt;&#123;92 93 @Override94 protected void reduce(CourseScore key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123;95 96 97 &#125;98 &#125;99 &#125; CSPartitioner.java 1234567891011121314151617181920212223242526272829 1 import org.apache.hadoop.io.NullWritable; 2 import org.apache.hadoop.mapreduce.Partitioner; 3 4 import com.ghgj.mr.exercise.pojo.CourseScore; 5 6 public class CSPartitioner extends Partitioner&lt;CourseScore,NullWritable&gt;&#123; 7 8 /** 9 * 10 */11 @Override12 public int getPartition(CourseScore key, NullWritable value, int numPartitions) &#123;13 14 String course = key.getCourse();15 if(course.equals(&quot;math&quot;))&#123;16 return 0;17 &#125;else if(course.equals(&quot;english&quot;))&#123;18 return 1;19 &#125;else if(course.equals(&quot;computer&quot;))&#123;20 return 2;21 &#125;else&#123;22 return 3;23 &#125;24 25 26 &#125;27 28 29 &#125; 第三题 CourseScoreMR3.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154 1 import java.io.IOException; 2 3 import org.apache.hadoop.conf.Configuration; 4 import org.apache.hadoop.fs.FileSystem; 5 import org.apache.hadoop.fs.Path; 6 import org.apache.hadoop.io.LongWritable; 7 import org.apache.hadoop.io.NullWritable; 8 import org.apache.hadoop.io.Text; 9 import org.apache.hadoop.mapreduce.Job; 10 import org.apache.hadoop.mapreduce.Mapper; 11 import org.apache.hadoop.mapreduce.Reducer; 12 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; 13 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 14 15 import com.ghgj.mr.exercise.gc.CourseScoreGC; 16 import com.ghgj.mr.exercise.pojo.CourseScore; 17 18 public class CourseScoreMR3&#123; 19 20 private static final int TOPN = 3; 21 22 public static void main(String[] args) throws Exception &#123; 23 24 Configuration conf = new Configuration(); 25 FileSystem fs = FileSystem.get(conf); 26 Job job = Job.getInstance(conf); 27 28 29 job.setJarByClass(CourseScoreMR3.class); 30 job.setMapperClass(CourseScoreMR2Mapper.class); 31 job.setReducerClass(CourseScoreMR2Reducer.class); 32 33 job.setMapOutputKeyClass(CourseScore.class); 34 job.setMapOutputValueClass(NullWritable.class); 35 job.setOutputKeyClass(CourseScore.class); 36 job.setOutputValueClass(NullWritable.class); 37 38 39 // job.setPartitionerClass(CSPartitioner.class); 40 // job.setNumReduceTasks(4); 41 42 43 // 指定分组规则 44 job.setGroupingComparatorClass(CourseScoreGC.class); 45 46 47 Path inputPath = new Path(&quot;E:\\\\bigdata\\\\cs\\\\input&quot;); 48 Path outputPath = new Path(&quot;E:\\\\bigdata\\\\cs\\\\output_3_last&quot;); 49 FileInputFormat.setInputPaths(job, inputPath); 50 if(fs.exists(outputPath))&#123; 51 fs.delete(outputPath, true); 52 &#125; 53 FileOutputFormat.setOutputPath(job, outputPath); 54 55 56 boolean isDone = job.waitForCompletion(true); 57 System.exit(isDone ? 0 : 1); 58 &#125; 59 60 public static class CourseScoreMR2Mapper extends Mapper&lt;LongWritable, Text, CourseScore, NullWritable&gt;&#123; 61 62 CourseScore cs = new CourseScore(); 63 64 /** 65 * value = math,huangxiaoming,85,75,85,99,66,88,75,91 66 */ 67 @Override 68 protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; 69 70 String[] split = value.toString().split(&quot;,&quot;); 71 72 String course = split[0]; 73 String name = split[1]; 74 75 int sum = 0; 76 int count = 0; 77 78 for(int i = 2; i&lt;split.length; i++)&#123; 79 int tempScore = Integer.parseInt(split[i]); 80 sum += tempScore; 81 82 count++; 83 &#125; 84 85 double avgScore = 1D * sum / count; 86 87 cs.setCourse(course); 88 cs.setName(name); 89 cs.setScore(avgScore); 90 91 context.write(cs, NullWritable.get()); 92 &#125; 93 94 &#125; 95 96 public static class CourseScoreMR2Reducer extends Reducer&lt;CourseScore, NullWritable, CourseScore, NullWritable&gt;&#123; 97 98 int count = 0; 99 100 /**101 * reducer阶段的reduce方法的调用参数：key相同的额一组key-vlaue102 * 103 * redcuer阶段，每次遇到一个不同的key的key_value组， 那么reduce方法就会被调用一次。104 * 105 * 106 * values这个迭代器只能迭代一次。107 * values迭代器在迭代的过程中迭代出来的value会变，同时，这个value所对应的key也会跟着变 合理108 * 109 */110 @Override111 protected void reduce(CourseScore key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123;112 113 114 int count = 0;115 116 for(NullWritable nvl : values)&#123;117 System.out.println(&quot;*********************************** &quot; + (++count) + &quot; &quot; + key.toString());118 119 if(count == 3)&#123;120 return;121 &#125;122 &#125;123 124 125 // 原样输出126 /*for(NullWritable nvl : values)&#123;127 context.write(key, nvl);128 &#125;*/129 130 131 // 输出每门课程的最高分数 , 预期结果中，key的显示都是一样的132 // for(NullWritable nvl : values)&#123;133 // System.out.println(key + &quot; - &quot; nvl);134 // 135 // valueList.add(nvl);136 // &#125;137 138 // List&lt;Value&gt; valueList = null;139 // 预期结果中，key的显示都是一样的140 /*int count = 0;141 for(NullWritable nvl : values)&#123;142 count++;143 &#125;144 for(int i = 0; i&lt;count; i++)&#123;145 valueList.get(i) = value146 System.out.println(key + &quot; - &quot;+ value);147 &#125;*/148 149 150 // math hello 1151 // math hi 2152 &#125;153 &#125;154 &#125; CourseScoreGC.java 123456789101112131415161718192021222324252627282930313233343536 1 import org.apache.hadoop.io.WritableComparable; 2 import org.apache.hadoop.io.WritableComparator; 3 4 import com.ghgj.mr.exercise.pojo.CourseScore; 5 6 /** 7 * 分组规则的指定 8 */ 9 public class CourseScoreGC extends WritableComparator&#123;10 11 public CourseScoreGC()&#123;12 super(CourseScore.class, true);13 &#125;14 15 /**16 * 17 * 方法的定义解释：18 * 19 * 方法的意义：一般来说，都可以从方法名找到一些提示20 * 方法的参数：将来你的MR程序中，要作为key的两个对象，是否是相同的对象21 * 方法的返回值： 返回值类型为int 当返回值为0的时候。证明， 两个参数对象，经过比较之后，是同一个对象22 * 23 * 在我们的需求中： 分组规则是 Course24 * 25 */26 @Override27 public int compare(WritableComparable a, WritableComparable b) &#123;28 29 CourseScore cs1 = (CourseScore)a;30 CourseScore cs2 = (CourseScore)b;31 32 int compareTo = cs1.getCourse().compareTo(cs2.getCourse());33 34 return compareTo;35 &#125;36 &#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（十九）MapReduce框架排序","slug":"2018-04-19-Hadoop学习之路（十九）MapReduce框架排序","date":"2018-04-19T02:30:04.000Z","updated":"2019-09-19T03:33:14.213Z","comments":true,"path":"2018-04-19-Hadoop学习之路（十九）MapReduce框架排序.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-19-Hadoop学习之路（十九）MapReduce框架排序.html","excerpt":"** Hadoop学习之路（十九）MapReduce框架排序：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十九）MapReduce框架排序","text":"** Hadoop学习之路（十九）MapReduce框架排序：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十九）MapReduce框架排序 &lt;The rest of contents | 余下全文&gt; 流量统计项目案例样本示例 需求1、 统计每一个用户（手机号）所耗费的总上行流量、总下行流量，总流量 2、 得出上题结果的基础之上再加一个需求：将统计结果按照总流量倒序排序 3、 将流量汇总统计结果按照手机归属地不同省份输出到不同文件中 第一题 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;/** * 第一题：统计每一个用户（手机号）所耗费的总上行流量、总下行流量，总流量 */public class FlowSumMR &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, &quot;FlowSumMR&quot;); job.setJarByClass(FlowSumMR.class); job.setMapperClass(FlowSumMRMapper.class); job.setReducerClass(FlowSumMRReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.setInputPaths(job, new Path(&quot;E:/bigdata/flow/input/&quot;)); FileOutputFormat.setOutputPath(job, new Path(&quot;E:/bigdata/flow/output_sum&quot;)); boolean isDone = job.waitForCompletion(true); System.exit(isDone ? 0 : 1); &#125; public static class FlowSumMRMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; /** * value = 1363157993044 18211575961 94-71-AC-CD-E6-18:CMCC-EASY 120.196.100.99 * iface.qiyi.com 视频网站 15 12 1527 2106 200 */ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] split = value.toString().split(&quot;\\t&quot;); String outkey = split[1]; String outValue = split[8] + &quot;\\t&quot; + split[9]; context.write(new Text(outkey), new Text(outValue)); &#125; &#125; public static class FlowSumMRReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; int upFlow = 0; int downFlow = 0; int sumFlow = 0; for(Text t : values)&#123; String[] split = t.toString().split(&quot;\\t&quot;); int upTempFlow = Integer.parseInt(split[0]); int downTempFlow = Integer.parseInt(split[1]); upFlow+=upTempFlow; downFlow += downTempFlow; &#125; sumFlow = upFlow + downFlow; context.write(key, new Text(upFlow + &quot;\\t&quot; + downFlow + &quot;\\t&quot; + sumFlow)); &#125; &#125;&#125; 第二题 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import comg.ghgj.mr.pojo.FlowBean;/** * 需求： 第二个题目，就是对第一个题目的结果数据，进行按照总流量倒叙排序 * * */public class FlowSortMR &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, &quot;FlowSumMR&quot;); job.setJarByClass(FlowSortMR.class); job.setMapperClass(FlowSortMRMapper.class); job.setReducerClass(FlowSortMRReducer.class); job.setOutputKeyClass(FlowBean.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.setInputPaths(job, new Path(&quot;E:/bigdata/flow/output_sum&quot;)); FileOutputFormat.setOutputPath(job, new Path(&quot;E:/bigdata/flow/output_sort_777&quot;)); boolean isDone = job.waitForCompletion(true); System.exit(isDone ? 0 : 1); &#125; public static class FlowSortMRMapper extends Mapper&lt;LongWritable, Text, FlowBean, NullWritable&gt;&#123; /** * value = 13602846565 26860680 40332600 67193280 */ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] split = value.toString().split(&quot;\\t&quot;); FlowBean fb = new FlowBean(split[0], Long.parseLong(split[1]), Long.parseLong(split[2])); context.write(fb, NullWritable.get()); &#125; &#125; public static class FlowSortMRReducer extends Reducer&lt;FlowBean, NullWritable, FlowBean, NullWritable&gt;&#123; @Override protected void reduce(FlowBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; for(NullWritable nvl : values)&#123; context.write(key, nvl); &#125; &#125; &#125;&#125; FlowBean.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157 1 import java.io.DataInput; 2 import java.io.DataOutput; 3 import java.io.IOException; 4 5 import org.apache.hadoop.io.WritableComparable; 6 7 /** 8 * 第一，定义好属性 9 * 第二，定义好属性的getter 和 setter方法 10 * 第三，定义好构造方法（有参，无参） 11 * 第四：定义好toString(); 12 * 13 * 14 * 详细解释： 15 * 16 * 如果一个自定义对象要作为key 必须要实现 WritableComparable 接口， 而不能实现 Writable, Comparable 17 * 18 * 如果一个自定义对象要作为value，那么只需要实现Writable接口即可 19 */ 20 public class FlowBean implements WritableComparable&lt;FlowBean&gt;&#123; 21 //public class FlowBean implements Comparable&lt;FlowBean&gt;&#123; 22 23 private String phone; 24 private long upFlow; 25 private long downFlow; 26 private long sumFlow; 27 public String getPhone() &#123; 28 return phone; 29 &#125; 30 public void setPhone(String phone) &#123; 31 this.phone = phone; 32 &#125; 33 public long getUpFlow() &#123; 34 return upFlow; 35 &#125; 36 public void setUpFlow(long upFlow) &#123; 37 this.upFlow = upFlow; 38 &#125; 39 public long getDownFlow() &#123; 40 return downFlow; 41 &#125; 42 public void setDownFlow(long downFlow) &#123; 43 this.downFlow = downFlow; 44 &#125; 45 public long getSumFlow() &#123; 46 return sumFlow; 47 &#125; 48 public void setSumFlow(long sumFlow) &#123; 49 this.sumFlow = sumFlow; 50 &#125; 51 public FlowBean(String phone, long upFlow, long downFlow, long sumFlow) &#123; 52 super(); 53 this.phone = phone; 54 this.upFlow = upFlow; 55 this.downFlow = downFlow; 56 this.sumFlow = sumFlow; 57 &#125; 58 public FlowBean(String phone, long upFlow, long downFlow) &#123; 59 super(); 60 this.phone = phone; 61 this.upFlow = upFlow; 62 this.downFlow = downFlow; 63 this.sumFlow = upFlow + downFlow; 64 &#125; 65 public FlowBean() &#123; 66 super(); 67 // TODO Auto-generated constructor stub 68 &#125; 69 @Override 70 public String toString() &#123; 71 return phone + &quot;\\t&quot; + upFlow + &quot;\\t&quot; + downFlow + &quot;\\t&quot; + sumFlow; 72 &#125; 73 74 75 76 77 /** 78 * 把当前这个对象 --- 谁掉用这个write方法，谁就是当前对象 79 * 80 * FlowBean bean = new FlowBean(); 81 * 82 * bean.write(out) 把bean这个对象的四个属性序列化出去 83 * 84 * this = bean 85 */ 86 @Override 87 public void write(DataOutput out) throws IOException &#123; 88 // TODO Auto-generated method stub 89 90 out.writeUTF(phone); 91 out.writeLong(upFlow); 92 out.writeLong(downFlow); 93 out.writeLong(sumFlow); 94 95 &#125; 96 97 98 // 序列化方法中的写出的字段顺序， 一定一定一定要和 反序列化中的 接收顺序一致。 类型也一定要一致 99 100 101 /**102 * bean.readField();103 * 104 * upFlow = 105 */106 @Override107 public void readFields(DataInput in) throws IOException &#123;108 // TODO Auto-generated method stub109 110 phone = in.readUTF();111 upFlow = in.readLong();112 downFlow = in.readLong();113 sumFlow = in.readLong();114 115 &#125;116 117 118 119 /**120 * Hadoop的序列化机制为什么不用 java自带的实现 Serializable这种方式？121 * 122 * 本身Hadoop就是用来解决大数据问题的。123 * 124 * 那么实现Serializable接口这种方式，在进行序列化的时候。除了会序列化属性值之外，还会携带很多跟当前这个对象的类相关的各种信息125 * 126 * Hadoop采取了一种全新的序列化机制；只需要序列化 每个对象的属性值即可。127 */128 129 130 131 /*@Override132 public void readFields(DataInput in) throws IOException &#123;133 value = in.readLong();134 &#125;135 136 @Override137 public void write(DataOutput out) throws IOException &#123;138 out.writeLong(value);139 &#125;*/140 141 142 /**143 * 用来指定排序规则144 */145 @Override146 public int compareTo(FlowBean fb) &#123;147 148 long diff = this.getSumFlow() - fb.getSumFlow();149 150 if(diff == 0)&#123;151 return 0;152 &#125;else&#123;153 return diff &gt; 0 ? -1 : 1;154 &#125;155 156 &#125;157 &#125; 第三题 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package comg.ghgj.mr.flow;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.partition.ProvincePartitioner;public class FlowPartitionerMR &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); Job job = Job.getInstance(conf, &quot;FlowSumMR&quot;); job.setJarByClass(FlowPartitionerMR.class); job.setMapperClass(FlowPartitionerMRMapper.class); job.setReducerClass(FlowPartitionerMRReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); /** * 非常重要的两句代码 */ job.setPartitionerClass(ProvincePartitioner.class); job.setNumReduceTasks(10); FileInputFormat.setInputPaths(job, new Path(&quot;E:\\\\bigdata\\\\flow\\\\input&quot;)); Path outputPath = new Path(&quot;E:\\\\bigdata\\\\flow\\\\output_ptn2&quot;); if(fs.exists(outputPath))&#123; fs.delete(outputPath, true); &#125; FileOutputFormat.setOutputPath(job, outputPath); boolean isDone = job.waitForCompletion(true); System.exit(isDone ? 0 : 1); &#125; public static class FlowPartitionerMRMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; /** * value = 13502468823 101663100 1529437140 1631100240 */ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] split = value.toString().split(&quot;\\t&quot;); String outkey = split[1]; String outValue = split[8] + &quot;\\t&quot; + split[9]; context.write(new Text(outkey), new Text(outValue)); &#125; &#125; public static class FlowPartitionerMRReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; int upFlow = 0; int downFlow = 0; int sumFlow = 0; for(Text t : values)&#123; String[] split = t.toString().split(&quot;\\t&quot;); int upTempFlow = Integer.parseInt(split[0]); int downTempFlow = Integer.parseInt(split[1]); upFlow+=upTempFlow; downFlow += downTempFlow; &#125; sumFlow = upFlow + downFlow; context.write(key, new Text(upFlow + &quot;\\t&quot; + downFlow + &quot;\\t&quot; + sumFlow)); &#125; &#125;&#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（十八）MapReduce框架Combiner分区","slug":"2018-04-18-Hadoop学习之路（十八）MapReduce框架Combiner分区","date":"2018-04-18T02:30:04.000Z","updated":"2019-09-19T02:37:26.783Z","comments":true,"path":"2018-04-18-Hadoop学习之路（十八）MapReduce框架Combiner分区.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-18-Hadoop学习之路（十八）MapReduce框架Combiner分区.html","excerpt":"** Hadoop学习之路（十八）MapReduce框架Combiner分区：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十八）MapReduce框架Combiner分区","text":"** Hadoop学习之路（十八）MapReduce框架Combiner分区：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十八）MapReduce框架Combiner分区 &lt;The rest of contents | 余下全文&gt; 对combiner的理解combiner其实属于优化方案，由于带宽限制，应该尽量map和reduce之间的数据传输数量。它在Map端把同一个key的键值对合并在一起并计算，计算规则与reduce一致，所以combiner也可以看作特殊的Reducer。 执行combiner操作要求开发者必须在程序中设置了combiner(程序中通过job.setCombinerClass(myCombine.class)自定义combiner操作)。 Combiner组件是用来做局部汇总的，就在mapTask中进行汇总；Reducer组件是用来做全局汇总的，最终的，最后一次汇总。 哪里使用combiner?1，map输出数据根据分区排序完成后，在写入文件之前会执行一次combine操作(前提是作业中设置了这个操作); 2，如果map输出比较大，溢出文件个数大于3(此值可以通过属性min.num.spills.for.combine配置)时，在merge的过程(多个spill文件合并为一个大文件)中前还会执行combiner操作; 注意事项不是每种作业都可以做combiner操作的，只有满足以下条件才可以： 1、Combiner 只能对 一个mapTask的中间结果进行汇总 2、如果想使用Reducer直接充当Combiner，那么必须满足： Reducer的输入和输出key-value类型是一致的。 1）处于两个不同节点的mapTask的结果不能combiner到一起 2）处于同一个节点的两个MapTask的结果不能否combiner到一起 3）求最大值、求最小值、求和、去重时可直接使用Reducer充当Combiner，但是求平均值时不能直接使用Reducer充当Combiner。 原因：对2组值求平均值 2 3 4 5 6 == 20 / 5 == 4 4 5 6 == 15 / 3 == 5 *** 20+15 / 5+3 = 35 / 8 4.5","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（十七）MapReduce框架Partitoner分区","slug":"2018-04-17-Hadoop学习之路（十七）MapReduce框架Partitoner分区","date":"2018-04-17T02:30:04.000Z","updated":"2019-09-19T03:35:30.220Z","comments":true,"path":"2018-04-17-Hadoop学习之路（十七）MapReduce框架Partitoner分区.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-17-Hadoop学习之路（十七）MapReduce框架Partitoner分区.html","excerpt":"** Hadoop学习之路（十七）MapReduce框架Partitoner分区：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十七）MapReduce框架Partitoner分区","text":"** Hadoop学习之路（十七）MapReduce框架Partitoner分区：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十七）MapReduce框架Partitoner分区 &lt;The rest of contents | 余下全文&gt; Partitioner分区类的作用是什么？在进行MapReduce计算时，有时候需要把最终的输出数据分到不同的文件中，比如按照省份划分的话，需要把同一省份的数据放到一个文件中；按照性别划分的话，需要把同一性别的数据放到一个文件中。我们知道最终的输出数据是来自于Reducer任务。那么，如果要得到多个文件，意味着有同样数量的Reducer任务在运行。Reducer任务的数据来自于Mapper任务，也就说Mapper任务要划分数据，对于不同的数据分配给不同的Reducer任务运行。Mapper任务划分数据的过程就称作Partition。负责实现划分数据的类称作Partitioner。 Partitoner类的源码如下： 123456789101112131415package org.apache.hadoop.mapreduce.lib.partition;import org.apache.hadoop.mapreduce.Partitioner;/** Partition keys by their &#123;@link Object#hashCode()&#125;. */public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt; &#123; /** Use &#123;@link Object#hashCode()&#125; to partition. */ public int getPartition(K key, V value, int numReduceTasks) &#123; //默认使用key的hash值与上int的最大值，避免出现数据溢出 的情况 return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; getPartition()三个参数分别是什么？ HashPartitioner是处理Mapper任务输出的，getPartition()方法有三个形参，源码中key、value分别指的是Mapper任务的输出，numReduceTasks指的是设置的Reducer任务数量，默认值是1。那么任何整数与1相除的余数肯定是0。也就是说getPartition(…)方法的返回值总是0。也就是Mapper任务的输出总是送给一个Reducer任务，最终只能输出到一个文件中。 据此分析，如果想要最终输出到多个文件中，在Mapper任务中对数据应该划分到多个区中。那么，我们只需要按照一定的规则让getPartition(…)方法的返回值是0,1,2,3…即可。 大部分情况下，我们都会使用默认的分区函数，但有时我们又有一些，特殊的需求，而需要定制Partition来完成我们的业务， 案例如下：按照能否被5除尽去分区 1、如果除以5的余数是0， 放在0号分区2、如果除以5的余数部是0， 放在1分区 123456789101112131415161718192021222324252627import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.mapreduce.Partitioner;public class FivePartitioner extends Partitioner&lt;IntWritable, IntWritable&gt;&#123; /** * 我们的需求：按照能否被5除尽去分区 * * 1、如果除以5的余数是0， 放在0号分区 * 2、如果除以5的余数部是0， 放在1分区 */ @Override public int getPartition(IntWritable key, IntWritable value, int numPartitions) &#123; int intValue = key.get(); if(intValue % 5 == 0)&#123; return 0; &#125;else&#123; if(intValue % 2 == 0)&#123; return 1; &#125;else&#123; return 2; &#125; &#125; &#125;&#125; 在运行Mapreduce程序时，只需在主函数里加入如下两行代码即可： 12job.setPartitionerClass(FivePartitioner.class);job.setNumReduceTasks(3);//设置为3","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（十五）MapReduce的多Job串联和全局计数器","slug":"2018-04-15-Hadoop学习之路（十五）MapReduce的多Job串联和全局计数器","date":"2018-04-15T02:30:04.000Z","updated":"2019-09-19T02:27:56.997Z","comments":true,"path":"2018-04-15-Hadoop学习之路（十五）MapReduce的多Job串联和全局计数器.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-15-Hadoop学习之路（十五）MapReduce的多Job串联和全局计数器.html","excerpt":"** Hadoop学习之路（十五）MapReduce的多Job串联和全局计数器：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十五）MapReduce的多Job串联和全局计数器","text":"** Hadoop学习之路（十五）MapReduce的多Job串联和全局计数器：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十五）MapReduce的多Job串联和全局计数器 &lt;The rest of contents | 余下全文&gt; 正文MapReduce 多 Job 串联 需求 一个稍复杂点的处理逻辑往往需要多个 MapReduce 程序串联处理，多 job 的串联可以借助 MapReduce 框架的 JobControl 实现 实例 以下有两个 MapReduce 任务，分别是 Flow 的 SumMR 和 SortMR，其中有依赖关系：SumMR 的输出是 SortMR 的输入，所以 SortMR 的启动得在 SumMR 完成之后 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657Configuration conf1 = new Configuration(); Configuration conf2 = new Configuration(); Job job1 = Job.getInstance(conf1); Job job2 = Job.getInstance(conf2); job1.setJarByClass(MRScore3.class); job1.setMapperClass(MRMapper3_1.class); //job.setReducerClass(ScoreReducer3.class); job1.setMapOutputKeyClass(IntWritable.class); job1.setMapOutputValueClass(StudentBean.class); job1.setOutputKeyClass(IntWritable.class); job1.setOutputValueClass(StudentBean.class); job1.setPartitionerClass(CoursePartitioner2.class); job1.setNumReduceTasks(4); Path inputPath = new Path(&quot;D:\\\\MR\\\\hw\\\\work3\\\\input&quot;); Path outputPath = new Path(&quot;D:\\\\MR\\\\hw\\\\work3\\\\output_hw3_1&quot;); FileInputFormat.setInputPaths(job1, inputPath); FileOutputFormat.setOutputPath(job1, outputPath); job2.setMapperClass(MRMapper3_2.class); job2.setReducerClass(MRReducer3_2.class); job2.setMapOutputKeyClass(IntWritable.class); job2.setMapOutputValueClass(StudentBean.class); job2.setOutputKeyClass(StudentBean.class); job2.setOutputValueClass(NullWritable.class); Path inputPath2 = new Path(&quot;D:\\\\MR\\\\hw\\\\work3\\\\output_hw3_1&quot;); Path outputPath2 = new Path(&quot;D:\\\\MR\\\\hw\\\\work3\\\\output_hw3_end&quot;); FileInputFormat.setInputPaths(job2, inputPath2); FileOutputFormat.setOutputPath(job2, outputPath2); JobControl control = new JobControl(&quot;Score3&quot;); ControlledJob aJob = new ControlledJob(job1.getConfiguration()); ControlledJob bJob = new ControlledJob(job2.getConfiguration()); // 设置作业依赖关系 bJob.addDependingJob(aJob); control.addJob(aJob); control.addJob(bJob); Thread thread = new Thread(control); thread.start(); while(!control.allFinished()) &#123; thread.sleep(1000); &#125; System.exit(0); MapReduce 全局计数器MapReduce计数器是什么？ 计数器是用来记录job的执行进度和状态的。它的作用可以理解为日志。我们可以在程序的某个位置插入计数器，记录数据或者进度的变化情况。 MapReduce计数器能做什么？ MapReduce 计数器（Counter）为我们提供一个窗口，用于观察 MapReduce Job 运行期的各种细节数据。对MapReduce性能调优很有帮助，MapReduce性能优化的评估大部分都是基于这些 Counter 的数值表现出来的。 MapReduce 都有哪些内置计数器？ MapReduce 自带了许多默认Counter，现在我们来分析这些默认 Counter 的含义，方便大家观察 Job 结果，如输入的字节数、输出的字节数、Map端输入/输出的字节数和条数、Reduce端的输入/输出的字节数和条数等。下面我们只需了解这些内置计数器，知道计数器组名称（groupName）和计数器名称（counterName），以后使用计数器会查找groupName和counterName即可。 1、任务计数器​ 在任务执行过程中，任务计数器采集任务的相关信息，每个作业的所有任务的结果会被聚集起来。例如，MAP_INPUT_RECORDS 计数器统计每个map任务输入记录的总数，并在一个作业的所有map任务上进行聚集，使得最终数字是整个作业的所有输入记录的总数。任务计数器由其关联任务维护，并定期发送给TaskTracker，再由TaskTracker发送给 JobTracker。因此，计数器能够被全局地聚集。下面我们分别了解各种任务计数器。 1）MapReduce 任务计数器 ​ MapReduce 任务计数器的 groupName为org.apache.hadoop.mapreduce.TaskCounter，它包含的计数器如下表所示 计数器名称 说明 map 输入的记录数（MAP_INPUT_RECORDS） 作业中所有 map 已处理的输入记录数。每次 RecorderReader 读到一条记录并将其传给 map 的 map() 函数时，该计数器的值增加。 map 跳过的记录数（MAP_SKIPPED_RECORDS） 作业中所有 map 跳过的输入记录数。 map 输入的字节数（MAP_INPUT_BYTES） 作业中所有 map 已处理的未经压缩的输入数据的字节数。每次 RecorderReader 读到一条记录并 将其传给 map 的 map() 函数时，该计数器的值增加 分片split的原始字节数（SPLIT_RAW_BYTES） 由 map 读取的输入-分片对象的字节数。这些对象描述分片元数据（文件的位移和长度），而不是分片的数据自身，因此总规模是小的 map 输出的记录数（MAP_OUTPUT_RECORDS） 作业中所有 map 产生的 map 输出记录数。每次某一个 map 的Context 调用 write() 方法时，该计数器的值增加 map 输出的字节数（MAP_OUTPUT_BYTES） 作业中所有 map 产生的 未经压缩的输出数据的字节数。每次某一个 map 的 Context 调用 write() 方法时，该计数器的值增加。 map 输出的物化字节数（MAP_OUTPUT_MATERIALIZED_BYTES） map 输出后确实写到磁盘上的字节数；若 map 输出压缩功能被启用，则会在计数器值上反映出来 combine 输入的记录数（COMBINE_INPUT_RECORDS） 作业中所有 Combiner（如果有）已处理的输入记录数。Combiner 的迭代器每次读一个值，该计数器的值增加。 combine 输出的记录数（COMBINE_OUTPUT_RECORDS） 作业中所有 Combiner（如果有）已产生的输出记录数。每当一个 Combiner 的 Context 调用 write() 方法时，该计数器的值增加。 reduce 输入的组（REDUCE_INPUT_GROUPS） 作业中所有 reducer 已经处理的不同的码分组的个数。每当某一个 reducer 的 reduce() 被调用时，该计数器的值增加。 reduce 输入的记录数（REDUCE_INPUT_RECORDS） 作业中所有 reducer 已经处理的输入记录的个数。每当某个 reducer 的迭代器读一个值时，该计数器的值增加。如果所有 reducer 已经处理完所有输入， 则该计数器的值与计数器 “map 输出的记录” 的值相同 reduce 输出的记录数（REDUCE_OUTPUT_RECORDS） 作业中所有 map 已经产生的 reduce 输出记录数。每当某一个 reducer 的 Context 调用 write() 方法时，该计数器的值增加。 reduce 跳过的组数（REDUCE_SKIPPED_GROUPS） 作业中所有 reducer 已经跳过的不同的码分组的个数。 reduce 跳过的记录数（REDUCE_SKIPPED_RECORDS） 作业中所有 reducer 已经跳过输入记录数。 reduce 经过 shuffle 的字节数（REDUCE_SHUFFLE_BYTES） shuffle 将 map 的输出数据复制到 reducer 中的字节数。 溢出的记录数（SPILLED_RECORDS） 作业中所有 map和reduce 任务溢出到磁盘的记录数 CPU 毫秒（CPU_MILLISECONDS） 总计的 CPU 时间，以毫秒为单位，由/proc/cpuinfo获取 物理内存字节数（PHYSICAL_MEMORY_BYTES） 一个任务所用物理内存的字节数，由/proc/cpuinfo获取 虚拟内存字节数（VIRTUAL_MEMORY_BYTES） 一个任务所用虚拟内存的字节数，由/proc/cpuinfo获取 有效的堆字节数（COMMITTED_HEAP_BYTES） 在 JVM 中的总有效内存量（以字节为单位），可由Runtime().getRuntime().totaoMemory()获取。 GC 运行时间毫秒数（GC_TIME_MILLIS） 在任务执行过程中，垃圾收集器（garbage collection）花费的时间（以毫秒为单位）， 可由 GarbageCollector MXBean.getCollectionTime()获取；该计数器并未出现在1.x版本中。 由 shuffle 传输的 map 输出数（SHUFFLED_MAPS） 有 shuffle 传输到 reducer 的 map 输出文件数。 失败的 shuffle 数（SHUFFLE_MAPS） 在 shuffle 过程中，发生拷贝错误的 map 输出文件数，该计数器并没有包含在 1.x 版本中。 被合并的 map 输出数 在 shuffle 过程中，在 reduce 端被合并的 map 输出文件数，该计数器没有包含在 1.x 版本中。 2）文件系统计数器 ​ 文件系统计数器的 groupName为org.apache.hadoop.mapreduce.FileSystemCounter，它包含的计数器如下表所示 计数器名称 说明 文件系统的读字节数（BYTES_READ） 由 map 和 reduce 等任务在各个文件系统中读取的字节数，各个文件系统分别对应一个计数器，可以是 Local、HDFS、S3和KFS等。 文件系统的写字节数（BYTES_WRITTEN） 由 map 和 reduce 等任务在各个文件系统中写的字节数。 3）FileInputFormat 计数器 ​ FileInputFormat 计数器的 groupName为org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter，它包含的计数器如下表所示，计数器名称列的括号（）内容即为counterName 计数器名称 说明 读取的字节数（BYTES_READ） 由 map 任务通过 FileInputFormat 读取的字节数。 4）FileOutputFormat 计数器 ​ FileOutputFormat 计数器的 groupName为org.apache.hadoop.mapreduce.lib.input.FileOutputFormatCounter，它包含的计数器如下表所示 计数器名称 说明 写的字节数（BYTES_WRITTEN） 由 map 任务（针对仅含 map 的作业）或者 reduce 任务通过 FileOutputFormat 写的字节数。 2、作业计数器​ 作业计数器由 JobTracker（或者 YARN）维护，因此无需在网络间传输数据，这一点与包括 “用户定义的计数器” 在内的其它计数器不同。这些计数器都是作业级别的统计量，其值不会随着任务运行而改变。 作业计数器计数器的 groupName为org.apache.hadoop.mapreduce.JobCounter，它包含的计数器如下表所示 计数器名称 说明 启用的map任务数（TOTAL_LAUNCHED_MAPS） 启动的map任务数，包括以“推测执行” 方式启动的任务。 启用的 reduce 任务数（TOTAL_LAUNCHED_REDUCES） 启动的reduce任务数，包括以“推测执行” 方式启动的任务。 失败的map任务数（NUM_FAILED_MAPS） 失败的map任务数。 失败的 reduce 任务数（NUM_FAILED_REDUCES） 失败的reduce任务数。 数据本地化的 map 任务数（DATA_LOCAL_MAPS） 与输入数据在同一节点的 map 任务数。 机架本地化的 map 任务数（RACK_LOCAL_MAPS） 与输入数据在同一机架范围内、但不在同一节点上的 map 任务数。 其它本地化的 map 任务数（OTHER_LOCAL_MAPS） 与输入数据不在同一机架范围内的 map 任务数。由于机架之间的宽带资源相对较少，Hadoop 会尽量让 map 任务靠近输入数据执行，因此该计数器值一般比较小。 map 任务的总运行时间（SLOTS_MILLIS_MAPS） map 任务的总运行时间，单位毫秒。该计数器包括以推测执行方式启动的任务。 reduce 任务的总运行时间（SLOTS_MILLIS_REDUCES） reduce任务的总运行时间，单位毫秒。该值包括以推测执行方式启动的任务。 在保留槽之后，map任务等待的总时间（FALLOW_SLOTS_MILLIS_MAPS） 在为 map 任务保留槽之后所花费的总等待时间，单位是毫秒。 在保留槽之后，reduce 任务等待的总时间（FALLOW_SLOTS_MILLIS_REDUCES） 在为 reduce 任务保留槽之后，花在等待上的总时间，单位为毫秒。 计数器的该如何使用？ 下面我们来介绍如何使用计数器。 1、定义计数器​ 1)枚举声明计数器 12// 自定义枚举变量Enum Counter counter = context.getCounter(Enum enum) 2)自定义计数器 12/ 自己命名groupName和counterName Counter counter = context.getCounter(String groupName,String counterName) 2、为计数器赋值​ 1)初始化计数器 1counter.setValue(long value);// 设置初始值 2)计数器自增 1counter.increment(long incr);// 增加计数 3、获取计数器的值 1) 获取枚举计数器的值 123456Configuration conf = new Configuration(); Job job = new Job(conf, &quot;MyCounter&quot;); job.waitForCompletion(true); Counters counters=job.getCounters(); Counter counter=counters.findCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_LONG);// 查找枚举计数器，假如Enum的变量为BAD_RECORDS_LONG long value=counter.getValue();//获取计数值 2) 获取自定义计数器的值 123456Configuration conf = new Configuration(); Job job = new Job(conf, &quot;MyCounter&quot;); job.waitForCompletion(true); Counters counters = job.getCounters(); Counter counter=counters.findCounter(&quot;ErrorCounter&quot;,&quot;toolong&quot;);// 假如groupName为ErrorCounter，counterName为toolong long value = counter.getValue();// 获取计数值 3) 获取内置计数器的值 1234567Configuration conf = new Configuration(); Job job = new Job(conf, &quot;MyCounter&quot;); job.waitForCompletion(true); Counters counters=job.getCounters(); // 查找作业运行启动的reduce个数的计数器，groupName和counterName可以从内置计数器表格查询（前面已经列举有） Counter counter=counters.findCounter(&quot;org.apache.hadoop.mapreduce.JobCounter&quot;,&quot;TOTAL_LAUNCHED_REDUCES&quot;);// 假如groupName为org.apache.hadoop.mapreduce.JobCounter，counterName为TOTAL_LAUNCHED_REDUCES long value=counter.getValue();// 获取计数值 4) 获取所有计数器的值 12345678Configuration conf = new Configuration(); Job job = new Job(conf, &quot;MyCounter&quot;); Counters counters = job.getCounters(); for (CounterGroup group : counters) &#123; for (Counter counter : group) &#123; System.out.println(counter.getDisplayName() + &quot;: &quot; + counter.getName() + &quot;: &quot;+ counter.getValue()); &#125; &#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（十四）MapReduce的核心运行机制","slug":"2018-04-14-Hadoop学习之路（十四）MapReduce的核心运行机制","date":"2018-04-14T02:30:04.000Z","updated":"2019-09-19T02:25:18.541Z","comments":true,"path":"2018-04-14-Hadoop学习之路（十四）MapReduce的核心运行机制.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-14-Hadoop学习之路（十四）MapReduce的核心运行机制.html","excerpt":"** Hadoop学习之路（十四）MapReduce的核心运行机制：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十四）MapReduce的核心运行机制","text":"** Hadoop学习之路（十四）MapReduce的核心运行机制：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十四）MapReduce的核心运行机制 &lt;The rest of contents | 余下全文&gt; 概述一个完整的 MapReduce 程序在分布式运行时有两类实例进程： 1、MRAppMaster：负责整个程序的过程调度及状态协调 2、Yarnchild：负责 map 阶段的整个数据处理流程 3、Yarnchild：负责 reduce 阶段的整个数据处理流程 以上两个阶段 MapTask 和 ReduceTask 的进程都是 YarnChild，并不是说这 MapTask 和 ReduceTask 就跑在同一个 YarnChild 进行里 MapReduce 套路图 MapReduce 程序的运行1、一个 mr 程序启动的时候，最先启动的是 MRAppMaster，MRAppMaster 启动后根据本次 job 的描述信息，计算出需要的 maptask 实例数量，然后向集群申请机器启动相应数量的 maptask 进程 2、 maptask 进程启动之后，根据给定的数据切片(哪个文件的哪个偏移量范围)范围进行数 据处理，主体流程为： A、利用客户指定的 InputFormat 来获取 RecordReader 读取数据，形成输入 KV 对 B、将输入 KV 对传递给客户定义的 map()方法，做逻辑运算，并将 map()方法输出的 KV 对收 集到缓存 C、将缓存中的 KV 对按照 K 分区排序后不断溢写到磁盘文件 3、 MRAppMaster 监控到所有 maptask 进程任务完成之后（真实情况是，某些 maptask 进 程处理完成后，就会开始启动 reducetask 去已完成的 maptask 处 fetch 数据），会根据客户指 定的参数启动相应数量的 reducetask 进程，并告知 reducetask 进程要处理的数据范围（数据 分区） 4、Reducetask 进程启动之后，根据 MRAppMaster 告知的待处理数据所在位置，从若干台 maptask 运行所在机器上获取到若干个 maptask 输出结果文件，并在本地进行重新归并排序， 然后按照相同 key 的 KV 为一个组，调用客户定义的 reduce()方法进行逻辑运算，并收集运 算输出的结果 KV，然后调用客户指定的 OutputFormat 将结果数据输出到外部存储 mapTask的并行度Hadoop中MapTask的并行度的决定机制。在MapReduce程序的运行中，并不是MapTask越多就越好。需要考虑数据量的多少及机器的配置。如果数据量很少，可能任务启动的时间都远远超过数据的处理时间。同样可不是越少越好。 那么应该如何切分呢？ 假如我们有一个300M的文件，它会在HDFS中被切成3块。0-128M,128-256M,256-300M。并被放置到不同的节点上去了。在MapReduce任务中，这3个Block会被分给3个MapTask。 MapTask在任务切片时实际上也是分配一个范围，只是这个范围是逻辑上的概念，与block的物理划分没有什么关系。但在实践过程中如果MapTask读取的数据不在运行的本机，则必须通过网络进行数据传输，对性能的影响非常大。所以常常采取的策略是就按照块的存储切分MapTask，使得每个MapTask尽可能读取本机的数据。 如果一个Block非常小，也可以把多个小Block交给一个MapTask。 所以MapTask的切分要看情况处理。默认的实现是按照Block大小进行切分。MapTask的切分工作由客户端（我们写的main方法）负责。一个切片就对应一个MapTask实例。 MapTask并行度的决定机制1个job的map阶段并行度由客户端在提交job时决定。 而客户端对map阶段并行度的规划的基本逻辑为： 将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多个split），然后每一个split分配一个mapTask并行实例处理 这段逻辑及形成的切片规划描述文件，由FileInputFormat实现类的getSplits()方法完成，其过程如下图： 切片机制FileInputFormat 中默认的切片机制 1、简单地按照文件的内容长度进行切片 2、切片大小，默认等于 block 大小 3、切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 比如待处理数据有两个文件： File1.txt 200M File2.txt 100M 经过 getSplits()方法处理之后，形成的切片信息是： File1.txt-split1 0-128M File1.txt-split2 129M-200M File2.txt-split1 0-100M FileInputFormat 中切片的大小的参数配置 通过分析源码，在 FileInputFormat 中，计算切片大小的逻辑： long splitSize = computeSplitSize(blockSize, minSize, maxSize)，翻译一下就是求这三个值的中 间值 切片主要由这几个值来运算决定： blocksize：默认是 128M，可通过 dfs.blocksize 修改 minSize：默认是 1，可通过 mapreduce.input.fileinputformat.split.minsize 修改 maxsize：默认是 Long.MaxValue，可通过 mapreduce.input.fileinputformat.split.maxsize 修改 因此，如果 maxsize 调的比 blocksize 小，则切片会小于 blocksize 如果 minsize 调的比 blocksize 大，则切片会大于 blocksize 但是，不论怎么调参数，都不能让多个小文件“划入”一个 split MapTask 并行度经验之谈如果硬件配置为 2*12core + 64G，恰当的 map 并行度是大约每个节点 20-100 个 map，最好 每个 map 的执行时间至少一分钟。 1、如果 job 的每个 map 或者 reduce task 的运行时间都只有 30-40 秒钟，那么就减少该 job 的 map 或者 reduce 数，每一个 task(map|reduce)的 setup 和加入到调度器中进行调度，这个 中间的过程可能都要花费几秒钟，所以如果每个 task 都非常快就跑完了，就会在 task 的开 始和结束的时候浪费太多的时间。 配置 task 的 JVM 重用可以改善该问题： mapred.job.reuse.jvm.num.tasks，默认是 1，表示一个 JVM 上最多可以顺序执行的 task 数目（属于同一个 Job）是 1。也就是说一个 task 启一个 JVM。这个值可以在 mapred-site.xml 中进行更改，当设置成多个，就意味着这多个 task 运行在同一个 JVM 上，但不是同时执行， 是排队顺序执行 2、如果 input 的文件非常的大，比如 1TB，可以考虑将 hdfs 上的每个 blocksize 设大，比如 设成 256MB 或者 512MB ReduceTask 并行度reducetask 的并行度同样影响整个 job 的执行并发度和执行效率，但与 maptask 的并发数由 切片数决定不同，Reducetask 数量的决定是可以直接手动设置： job.setNumReduceTasks(4); 默认值是 1， 手动设置为 4，表示运行 4 个 reduceTask， 设置为 0，表示不运行 reduceTask 任务，也就是没有 reducer 阶段，只有 mapper 阶段 如果数据分布不均匀，就有可能在 reduce 阶段产生数据倾斜 注意：reducetask 数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全 局汇总结果，就只能有 1 个 reducetask 尽量不要运行太多的 reducetask。对大多数 job 来说，最好 rduce 的个数最多和集群中的 reduce 持平，或者比集群的 reduce slots 小。这个对于小集群而言，尤其重要。 ReduceTask 并行度决定机制1、job.setNumReduceTasks(number);2、job.setReducerClass(MyReducer.class);3、job.setPartitioonerClass(MyPTN.class); 分以下几种情况讨论： 1、如果number为1，并且2已经设置为自定义Reducer, reduceTask的个数就是1不管用户编写的MR程序有没有设置Partitioner，那么该分区组件都不会起作用 2、如果number没有设置，并且2已经设置为自定义Reducer, reduceTask的个数就是1在默认的分区组件的影响下，不管用户设置的number，不管是几，只要大于1，都是可以正常执行的。如果在设置自定义的分区组件时，那么就需要注意：你设置的reduceTasks的个数，必须要 ==== 分区编号中的最大值 + 1最好的情况下：分区编号都是连续的。那么reduceTasks = 分区编号的总个数 = 分区编号中的最大值 + 1 3、如果number为 &gt;= 2 并且2已经设置为自定义Reducer reduceTask的个数就是number底层会有默认的数据分区组件在起作用 4、如果你设置了number的个数，但是没有设置自定义的reducer，那么该mapreduce程序不代表没有reducer阶段真正的reducer中的逻辑，就是调用父类Reducer中的默认实现逻辑:原样输出reduceTask的个数 就是 number 5、如果一个MR程序中，不想有reducer阶段。那么只需要做一下操作即可:job.setNumberReudceTasks(0);整个MR程序只有mapper阶段。没有reducer阶段。那么就没有shuffle阶段","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（十三）MapReduce的初识","slug":"2018-04-13-Hadoop学习之路（十三）MapReduce的初识","date":"2018-04-13T02:30:04.000Z","updated":"2019-09-19T03:33:54.871Z","comments":true,"path":"2018-04-13-Hadoop学习之路（十三）MapReduce的初识.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-13-Hadoop学习之路（十三）MapReduce的初识.html","excerpt":"** Hadoop学习之路（十三）MapReduce的初识：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十三）MapReduce的初识","text":"** Hadoop学习之路（十三）MapReduce的初识：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十三）MapReduce的初识 &lt;The rest of contents | 余下全文&gt; MapReduce是什么首先让我们来重温一下 hadoop 的四大组件： HDFS：分布式存储系统 MapReduce：分布式计算系统 YARN：hadoop 的资源调度系统 Common：以上三大组件的底层支撑组件，主要提供基础工具包和 RPC 框架等 MapReduce 是一个分布式运算程序的编程框架，是用户开发“基于 Hadoop 的数据分析应用” 的核心框架 MapReduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布 式运算程序，并发运行在一个 Hadoop 集群上 为什么需要 MapReduce1、海量数据在单机上处理因为硬件资源限制，无法胜任 2、而一旦将单机版程序扩展到集群来分布式运行，将极大增加程序的复杂度和开发难度 3、引入 MapReduce 框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将 分布式计算中的复杂性交由框架来处理 设想一个海量数据场景下的数据计算需求： 单机版：磁盘受限，内存受限，计算能力受限 分布式版：1、 数据存储的问题，hadoop 提供了 hdfs 解决了数据存储这个问题2、 运算逻辑至少要分为两个阶段，先并发计算（map），然后汇总（reduce）结果3、 这两个阶段的计算如何启动？如何协调？4、 运算程序到底怎么执行？数据找程序还是程序找数据？5、 如何分配两个阶段的多个运算任务？6、 如何管理任务的执行过程中间状态，如何容错？7、 如何监控？8、 出错如何处理？抛异常？重试？ 可见在程序由单机版扩成分布式版时，会引入大量的复杂工作。为了提高开发效率，可以将 分布式程序中的公共功能封装成框架，让开发人员可以将精力集中于业务逻辑。 Hadoop 当中的 MapReduce 就是这样的一个分布式程序运算框架，它把大量分布式程序都会 涉及的到的内容都封装进了，让用户只用专注自己的业务逻辑代码的开发。它对应以上问题 的整体结构如下： MRAppMaster：MapReduce Application Master，分配任务，协调任务的运行 MapTask：阶段并发任，负责 mapper 阶段的任务处理 YARNChild ReduceTask：阶段汇总任务，负责 reducer 阶段的任务处理 YARNChild MapReduce做什么 简单地讲，MapReduce可以做大数据处理。所谓大数据处理，即以价值为导向，对大数据加工、挖掘和优化等各种处理。 MapReduce擅长处理大数据，它为什么具有这种能力呢？这可由MapReduce的设计思想发觉。MapReduce的思想就是“分而治之”。 （1）Mapper负责“分”，即把复杂的任务分解为若干个“简单的任务”来处理。“简单的任务”包含三层含义：一是数据或计算的规模相对原任务要大大缩小；二是就近计算原则，即任务会分配到存放着所需数据的节点上进行计算；三是这些小任务可以并行计算，彼此间几乎没有依赖关系。 （2）Reducer负责对map阶段的结果进行汇总。至于需要多少个Reducer，用户可以根据具体问题，通过在mapred-site.xml配置文件里设置参数mapred.reduce.tasks的值，缺省值为1。 MapReduce 程序运行演示 在 MapReduce 组件里，官方给我们提供了一些样例程序，其中非常有名的就是 wordcount 和 pi 程序。这些 MapReduce 程序的代码都在 hadoop-mapreduce-examples-2.7.5.jar 包里，这 个 jar 包在 hadoop 安装目录下的/share/hadoop/mapreduce/目录里 下面我们使用 hadoop 命令来试跑例子程序，看看运行效果 MapReduce 示例 pi 的程序1234[hadoop@hadoop1 ~]$ cd apps/hadoop-2.7.5/share/hadoop/mapreduce/[hadoop@hadoop1 mapreduce]$ pwd/home/hadoop/apps/hadoop-2.7.5/share/hadoop/mapreduce[hadoop@hadoop1 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.5.jar pi 5 5 MapReduce 示例 wordcount 的程序1[hadoop@hadoop1 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.5.jar wordcount /wc/input1/ /wc/output1/ 查看结果 1[hadoop@hadoop1 mapreduce]$ hadoop fs -cat /wc/output1/part-r-00000 其他程序那除了这两个程序以外，还有没有官方提供的其他程序呢，还有就是它们的源码在哪里呢？ 我们打开 mapreduce 的源码工程，里面有一个 hadoop-mapreduce-project 项目： 里面有一个例子程序的子项目：hadoop-mapreduce-examples 其中 src 是例子程序源码目录，pom.xml 是该项目的 maven 管理配置文件，我们打开该文件， 找到第 127 行，它告诉了我们例子程序的主程序入口： 找到src\\main\\java\\org\\apache\\hadoop\\examples目录 打开主入口程序，看源代码： 找到这一步，我们就能知道其实 wordcount 程序的实际程序就是 WordCount.class，这就是我 们想要找的例子程序的源码。 WordCount.java源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788891 /** 2 * Licensed to the Apache Software Foundation (ASF) under one 3 * or more contributor license agreements. See the NOTICE file 4 * distributed with this work for additional information 5 * regarding copyright ownership. The ASF licenses this file 6 * to you under the Apache License, Version 2.0 (the 7 * &quot;License&quot;); you may not use this file except in compliance 8 * with the License. You may obtain a copy of the License at 9 *10 * http://www.apache.org/licenses/LICENSE-2.011 *12 * Unless required by applicable law or agreed to in writing, software13 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,14 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.15 * See the License for the specific language governing permissions and16 * limitations under the License.17 */18 package org.apache.hadoop.examples;19 20 import java.io.IOException;21 import java.util.StringTokenizer;22 23 import org.apache.hadoop.conf.Configuration;24 import org.apache.hadoop.fs.Path;25 import org.apache.hadoop.io.IntWritable;26 import org.apache.hadoop.io.Text;27 import org.apache.hadoop.mapreduce.Job;28 import org.apache.hadoop.mapreduce.Mapper;29 import org.apache.hadoop.mapreduce.Reducer;30 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;31 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;32 import org.apache.hadoop.util.GenericOptionsParser;33 34 public class WordCount &#123;35 36 public static class TokenizerMapper 37 extends Mapper&lt;Object, Text, Text, IntWritable&gt;&#123;38 39 private final static IntWritable one = new IntWritable(1);40 private Text word = new Text();41 42 public void map(Object key, Text value, Context context43 ) throws IOException, InterruptedException &#123;44 StringTokenizer itr = new StringTokenizer(value.toString());45 while (itr.hasMoreTokens()) &#123;46 word.set(itr.nextToken());47 context.write(word, one);48 &#125;49 &#125;50 &#125;51 52 public static class IntSumReducer 53 extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123;54 private IntWritable result = new IntWritable();55 56 public void reduce(Text key, Iterable&lt;IntWritable&gt; values, 57 Context context58 ) throws IOException, InterruptedException &#123;59 int sum = 0;60 for (IntWritable val : values) &#123;61 sum += val.get();62 &#125;63 result.set(sum);64 context.write(key, result);65 &#125;66 &#125;67 68 public static void main(String[] args) throws Exception &#123;69 Configuration conf = new Configuration();70 String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();71 if (otherArgs.length &lt; 2) &#123;72 System.err.println(&quot;Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;&quot;);73 System.exit(2);74 &#125;75 Job job = Job.getInstance(conf, &quot;word count&quot;);76 job.setJarByClass(WordCount.class);77 job.setMapperClass(TokenizerMapper.class);78 job.setCombinerClass(IntSumReducer.class);79 job.setReducerClass(IntSumReducer.class);80 job.setOutputKeyClass(Text.class);81 job.setOutputValueClass(IntWritable.class);82 for (int i = 0; i &lt; otherArgs.length - 1; ++i) &#123;83 FileInputFormat.addInputPath(job, new Path(otherArgs[i]));84 &#125;85 FileOutputFormat.setOutputPath(job,86 new Path(otherArgs[otherArgs.length - 1]));87 System.exit(job.waitForCompletion(true) ? 0 : 1);88 &#125;89 &#125; MapReduce 示例程序编写及编码规范上一步，我们查看了 WordCount 这个 MapReduce 程序的源码编写，可以得出几点结论： 1、 该程序有一个 main 方法，来启动任务的运行，其中 job 对象就存储了该程序运行的必要 信息，比如指定 Mapper 类和 Reducer 类 job.setMapperClass(TokenizerMapper.class); job.setReducerClass(IntSumReducer.class); 2、 该程序中的 TokenizerMapper 类继承了 Mapper 类 3、 该程序中的 IntSumReducer 类继承了 Reducer 类 总结：MapReduce 程序的业务编码分为两个大部分，一部分配置程序的运行信息，一部分 编写该 MapReduce 程序的业务逻辑，并且业务逻辑的 map 阶段和 reduce 阶段的代码分别继 承 Mapper 类和 Reducer 类 编写自己的 Wordcount 程序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158 1 package com.ghgj.mapreduce.wc.demo; 2 3 import java.io.IOException; 4 5 import org.apache.hadoop.conf.Configuration; 6 import org.apache.hadoop.fs.FileSystem; 7 import org.apache.hadoop.fs.Path; 8 import org.apache.hadoop.io.IntWritable; 9 import org.apache.hadoop.io.LongWritable; 10 import org.apache.hadoop.io.Text; 11 import org.apache.hadoop.mapreduce.Job; 12 import org.apache.hadoop.mapreduce.Mapper; 13 import org.apache.hadoop.mapreduce.Reducer; 14 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; 15 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 16 17 /** 18 * 19 * 描述: MapReduce出入门：WordCount例子程序 20 */ 21 public class WordCountMR &#123; 22 23 /** 24 * 该main方法是该mapreduce程序运行的入口，其中用一个Job类对象来管理程序运行时所需要的很多参数： 25 * 比如，指定用哪个组件作为数据读取器、数据结果输出器 指定用哪个类作为map阶段的业务逻辑类，哪个类作为reduce阶段的业务逻辑类 26 * 指定wordcount job程序的jar包所在路径 .... 以及其他各种需要的参数 27 */ 28 public static void main(String[] args) throws Exception &#123; 29 // 指定hdfs相关的参数 30 Configuration conf = new Configuration(); 31 // conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop02:9000&quot;); 32 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;); 33 34 // 这是高可用的集群的配置文件。如果不是高可用集群，请自行替换配置文件 35 // conf.addResource(&quot;hdfs_config/core-site.xml&quot;); 36 // conf.addResource(&quot;hdfs_config/hdfs-site.xml&quot;); 37 38 // conf.set(&quot;mapreduce.framework.name&quot;, &quot;yarn&quot;); 39 // conf.set(&quot;yarn.resourcemanager.hostname&quot;, &quot;hadoop04&quot;); 40 41 // 通过Configuration对象获取Job对象，该job对象会组织所有的该MapReduce程序所有的各种组件 42 Job job = Job.getInstance(conf); 43 44 // 设置jar包所在路径 45 job.setJarByClass(WordCountMR.class); 46 47 // 指定mapper类和reducer类 48 job.setMapperClass(WordCountMapper.class); 49 job.setReducerClass(WordCountReducer.class); 50 51 // Mapper的输入key-value类型，由MapReduce框架决定 52 // 指定maptask的输出类型 53 job.setMapOutputKeyClass(Text.class); 54 job.setMapOutputValueClass(IntWritable.class); 55 // 假如 mapTask的输出key-value类型，跟reduceTask的输出key-value类型一致，那么，以上两句代码可以不用设置 56 57 // reduceTask的输入key-value类型 就是 mapTask的输出key-value类型。所以不需要指定 58 // 指定reducetask的输出类型 59 job.setOutputKeyClass(Text.class); 60 job.setOutputValueClass(IntWritable.class); 61 62 // 为job指定输入数据的组件和输出数据的组件，以下两个参数是默认的，所以不指定也是OK的 63 // job.setInputFormatClass(TextInputFormat.class); 64 // job.setOutputFormatClass(TextOutputFormat.class); 65 66 // 为该mapreduce程序制定默认的数据分区组件。默认是 HashPartitioner.class 67 // job.setPartitionerClass(HashPartitioner.class); 68 69 // 如果MapReduce程序在Eclipse中，运行，也可以读取Windows系统本地的文件系统中的数据 70 Path inputPath = new Path(&quot;D:\\\\bigdata\\\\wordcount\\\\input&quot;); 71 Path outputPath = new Path(&quot;D:\\\\bigdata\\\\wordcount\\\\output33&quot;); 72 73 // 设置该MapReduce程序的ReduceTask的个数 74 // job.setNumReduceTasks(3); 75 76 // 指定该mapreduce程序数据的输入和输出路径 77 // Path inputPath = new Path(&quot;/wordcount/input&quot;); 78 // Path outputPath = new Path(&quot;/wordcount/output&quot;); 79 // 该段代码是用来判断输出路径存在不存在，存在就删除，虽然方便操作，但请谨慎 80 FileSystem fs = FileSystem.get(conf); 81 if (fs.exists(outputPath)) &#123; 82 fs.delete(outputPath, true); 83 &#125; 84 85 // 设置wordcount程序的输入路径 86 FileInputFormat.setInputPaths(job, inputPath); 87 // 设置wordcount程序的输出路径 88 FileOutputFormat.setOutputPath(job, outputPath); 89 90 // job.submit(); 91 // 最后提交任务(verbose布尔值 决定要不要将运行进度信息输出给用户) 92 boolean waitForCompletion = job.waitForCompletion(true); 93 System.exit(waitForCompletion ? 0 : 1); 94 &#125; 95 96 /** 97 * Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; 98 * 99 * KEYIN 是指框架读取到的数据的key的类型，在默认的InputFormat下，读到的key是一行文本的起始偏移量，所以key的类型是Long100 * VALUEIN 是指框架读取到的数据的value的类型,在默认的InputFormat下，读到的value是一行文本的内容，所以value的类型是String101 * KEYOUT 是指用户自定义逻辑方法返回的数据中key的类型，由用户业务逻辑决定，在此wordcount程序中，我们输出的key是单词，所以是String102 * VALUEOUT 是指用户自定义逻辑方法返回的数据中value的类型，由用户业务逻辑决定,在此wordcount程序中，我们输出的value是单词的数量，所以是Integer103 * 104 * 但是，String ，Long等jdk中自带的数据类型，在序列化时，效率比较低，hadoop为了提高序列化效率，自定义了一套序列化框架105 * 所以，在hadoop的程序中，如果该数据需要进行序列化（写磁盘，或者网络传输），就一定要用实现了hadoop序列化框架的数据类型106 * 107 * Long ----&gt; LongWritable 108 * String ----&gt; Text 109 * Integer ----&gt; IntWritable 110 * Null ----&gt; NullWritable111 */112 static class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123;113 114 /**115 * LongWritable key : 该key就是value该行文本的在文件当中的起始偏移量116 * Text value ： 就是MapReduce框架默认的数据读取组件TextInputFormat读取文件当中的一行文本117 */118 @Override119 protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;120 121 // 切分单词122 String[] words = value.toString().split(&quot; &quot;);123 for (String word : words) &#123;124 // 每个单词计数一次，也就是把单词组织成&lt;hello,1&gt;这样的key-value对往外写出125 context.write(new Text(word), new IntWritable(1));126 &#125;127 &#125;128 &#125;129 130 /**131 * 首先，和前面一样，Reducer类也有输入和输出，输入就是Map阶段的处理结果，输出就是Reduce最后的输出132 * reducetask在调我们写的reduce方法,reducetask应该收到了前一阶段（map阶段）中所有maptask输出的数据中的一部分133 * （数据的key.hashcode%reducetask数==本reductask号），所以reducetaks的输入类型必须和maptask的输出类型一样134 * 135 * reducetask将这些收到kv数据拿来处理时，是这样调用我们的reduce方法的： 先将自己收到的所有的kv对按照k分组（根据k是否相同）136 * 将某一组kv中的第一个kv中的k传给reduce方法的key变量，把这一组kv中所有的v用一个迭代器传给reduce方法的变量values137 */138 static class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;139 140 /**141 * Text key : mapTask输出的key值142 * Iterable&lt;IntWritable&gt; values ： key对应的value的集合（该key只是相同的一个key）143 * 144 * reduce方法接收key值相同的一组key-value进行汇总计算145 */146 @Override147 protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123;148 149 // 结果汇总150 int sum = 0;151 for (IntWritable v : values) &#123;152 sum += v.get();153 &#125;154 // 汇总的结果往外输出155 context.write(key, new IntWritable(sum));156 &#125;157 &#125;158 &#125; MapReduce 程序编写规范1、用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行 MR 程序的客户端) 2、Mapper 的输入数据是 KV 对的形式（KV 的类型可自定义） 3、Mapper 的输出数据是 KV 对的形式（KV 的类型可自定义） 4、Mapper 中的业务逻辑写在 map()方法中 5、map()方法（maptask 进程）对每一个&lt;k,v&gt;调用一次 6、Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV 对的形式 7、Reducer 的业务逻辑写在 reduce()方法中 8、Reducetask 进程对每一组相同 k 的&lt;k,v&gt;组调用一次 reduce()方法 9、用户自定义的 Mapper 和 Reducer 都要继承各自的父类 10、整个程序需要一个 Drvier 来进行提交，提交的是一个描述了各种必要信息的 job 对象 WordCount 的业务逻辑1、 maptask 阶段处理每个数据分块的单词统计分析，思路是每遇到一个单词则把其转换成 一个 key-value 对，比如单词 hello，就转换成&lt;’hello’,1&gt;发送给 reducetask 去汇总 2、 reducetask 阶段将接受 maptask 的结果，来做汇总计数 MapReduce 运行方式及 Debug集群运行模式打 jar 包，提交任务到集群运行，适用：生产环境，不适用：测试，调试，开发 要点一：首先要把代码打成 jar 上传到 linux 服务器 要点二：用 hadoop jar 的命令去提交代码到 yarn 集群运行 要点三：处理的数据和输出结果应该位于 hdfs 文件系统 要点四：如果需要在 windows 中的 eclipse 当中直接提交 job 到集群，则需要修改 YarnRunner 类，这个比较复杂，不建议使用 本地运行模式Eclipse 开发环境下本地运行，好处是方便调试和测试 直接在IDE环境中进行环境 ： eclipse 1、直接运行在本地，读取本地数据 2、直接运行在本地，读取远程的文件系统的数据 3、直接在IDE中提交任务给YARN集群运行","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（十二）分布式集群中HDFS系统的各种角色","slug":"2018-04-12-Hadoop学习之路（十二）分布式集群中HDFS系统的各种角色","date":"2018-04-12T02:30:04.000Z","updated":"2019-09-19T02:20:10.573Z","comments":true,"path":"2018-04-12-Hadoop学习之路（十二）分布式集群中HDFS系统的各种角色.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-12-Hadoop学习之路（十二）分布式集群中HDFS系统的各种角色.html","excerpt":"** Hadoop学习之路（十二）分布式集群中HDFS系统的各种角色：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十二）分布式集群中HDFS系统的各种角色","text":"** Hadoop学习之路（十二）分布式集群中HDFS系统的各种角色：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十二）分布式集群中HDFS系统的各种角色 &lt;The rest of contents | 余下全文&gt; NameNode学习目标理解 namenode 的工作机制尤其是元数据管理机制，以增强对 HDFS 工作原理的 理解，及培养 hadoop 集群运营中“性能调优”、“namenode”故障问题的分析解决能力 问题场景1、Namenode 服务器的磁盘故障导致 namenode 宕机，如何挽救集群及数据？ 2、Namenode 是否可以有多个？namenode 内存要配置多大？namenode 跟集群数据存储能 力有关系吗？ 3、文件的 blocksize 究竟调大好还是调小好？结合 mapreduce NameNode的职责1、负责客户端请求（读写数据 请求 ）的响应2、维护目录树结构（ 元数据的管理： 查询，修改 ）3、配置和应用副本存放策略4、管理集群数据块负载均衡问题 NameNode元数据的管理WAL（Write ahead Log）: 预写日志系统 在计算机科学中，预写式日志（Write-ahead logging，缩写 WAL）是关系数据库系统中 用于提供原子性和持久性（ACID 属性中的两个）的一系列技术。在使用 WAL 的系统中，所 有的修改在提交之前都要先写入 log 文件中。 Log 文件中通常包括 redo 和 undo 信息。这样做的目的可以通过一个例子来说明。假设 一个程序在执行某些操作的过程中机器掉电了。在重新启动时，程序可能需要知道当时执行 的操作是成功了还是部分成功或者是失败了。如果使用了 WAL，程序就可以检查 log 文件， 并对突然掉电时计划执行的操作内容跟实际上执行的操作内容进行比较。在这个比较的基础 上，程序就可以决定是撤销已做的操作还是继续完成已做的操作，或者是保持原样。 WAL 允许用 in-place 方式更新数据库。另一种用来实现原子更新的方法是 shadow paging， 它并不是 in-place 方式。用 in-place 方式做更新的主要优点是减少索引和块列表的修改。ARIES 是 WAL 系列技术常用的算法。在文件系统中，WAL 通常称为 journaling。PostgreSQL 也是用 WAL 来提供 point-in-time 恢复和数据库复制特性。 NameNode 对数据的管理采用了两种存储形式：内存和磁盘 首先是内存中存储了一份完整的元数据，包括目录树结构，以及文件和数据块和副本存储地 的映射关系； 1、内存元数据 metadata（全部存在内存中），其次是在磁盘中也存储了一份完整的元数据。 2、磁盘元数据镜像文件 fsimage_0000000000000000555 fsimage_0000000000000000555 等价于 edits_0000000000000000001-0000000000000000018 …… edits_0000000000000000444-0000000000000000555 合并之和 3、数据历史操作日志文件 edits：edits_0000000000000000001-0000000000000000018 （可通过日志运算出元数据，全部存在磁盘中） 4、数据预写操作日志文件 edits_inprogress_0000000000000000556 （存储在磁盘中） metadata = 最新 fsimage_0000000000000000555 + edits_inprogress_0000000000000000556 metadata = 所有的 edits 之和（edits_001_002 + …… + edits_444_555 + edits_inprogress_556） VERSION（存放 hdfs 集群的版本信息）文件解析： 1234567#Sun Jan 06 20:12:30 CST 2017 ## 集群启动时间namespaceID=844434736 ## 文件系统唯一标识符clusterID=CID-5b7b7321-e43f-456e-bf41-18e77c5e5a40 ## 集群唯一标识符cTime=0 ## fsimage 创建的时间，初始为 0，随 layoutVersion 更新storageType=NAME_NODE ##节点类型blockpoolID=BP-265332847-192.168.123.202-1483581570658 ## 数据块池 ID，可以有多个layoutVersion=-60 ## hdfs 持久化数据结构的版本号 查看 edits 文件信息： 12hdfs oev -i edits_0000000000000000482-0000000000000000483 -o edits.xml cat edits.xml 查看 fsimage 镜像文件信息： 12hdfs oiv -i fsimage_0000000000000000348 -p XML -o fsimage.xml cat fsimage.xml NameNode 元数据存储机制A、内存中有一份完整的元数据(内存 metadata) B、磁盘有一个“准完整”的元数据镜像（fsimage）文件(在 namenode 的工作目录中) C、用于衔接内存 metadata 和持久化元数据镜像 fsimage 之间的操作日志（edits 文件） （PS：当客户端对 hdfs 中的文件进行新增或者修改操作，操作记录首先被记入 edits 日志 文件中，当客户端操作成功后，相应的元数据会更新到内存 metadata 中） DataNode问题场景1、集群容量不够，怎么扩容？ 2、如果有一些 datanode 宕机，该怎么办？ 3、datanode 明明已启动，但是集群中的可用 datanode 列表中就是没有，怎么办？ Datanode 工作职责 1、存储管理用户的文件块数据 2、定期向 namenode 汇报自身所持有的 block 信息（通过心跳信息上报） （PS：这点很重要，因为，当集群中发生某些 block 副本失效时，集群如何恢复 block 初始 副本数量的问题） 123456&lt;property&gt; &lt;!—HDFS 集群数据冗余块的自动删除时长，单位 ms，默认一个小时 --&gt;&lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt;&lt;value&gt;3600000&lt;/value&gt;&lt;description&gt;Determines block reporting interval in milliseconds.&lt;/description&gt;&lt;/property&gt; Datanode 掉线判断时限参数datanode 进程死亡或者网络故障造成 datanode 无法与 namenode 通信，namenode 不会立即 把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS 默认的超时时长 为 10 分钟+30 秒。如果定义超时时间为 timeout，则超时时长的计算公式为： t imeout = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval 而默认的 heartbeat.recheck.interval 大小为 5 分钟，dfs.heartbeat.interval 默认为 3 秒。 需要注意的是 hdfs-site.xml 配置文件中的 heartbeat.recheck.interval 的单位为毫秒， dfs.heartbeat.interval 的单位为秒。 所以，举个例子，如果 heartbeat.recheck.interval 设置为 5000（毫秒），dfs.heartbeat.interval 设置为 3（秒，默认），则总的超时时间为 40 秒。 12345678&lt;property&gt; &lt;name&gt;heartbeat.recheck.interval&lt;/name&gt; &lt;value&gt;5000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt; SecondaryNameNodeSecondaryNamenode 工作机制SecondaryNamenode 的作用就是分担 namenode 的合并元数据的压力。所以在配置 SecondaryNamenode 的工作节点时，一定切记，不要和 namenode 处于同一节点。但事实上， 只有在普通的伪分布式集群和分布式集群中才有会 SecondaryNamenode 这个角色，在 HA 或 者联邦集群中都不再出现该角色。在 HA 和联邦集群中，都是有 standby namenode 承担。 元数据的 CheckPoint每隔一段时间，会由 secondary namenode 将 namenode 上积累的所有 edits 和一个最新的 fsimage 下载到本地，并加载到内存进行 merge（这个过程称为 checkpoint） CheckPoint 详细过程图解： CheckPoint 触发配置1234567dfs.namenode.checkpoint.check.period=60 ##检查触发条件是否满足的频率，60 秒dfs.namenode.checkpoint.dir=file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary##以上两个参数做 checkpoint 操作时，secondary namenode 的本地工作目录dfs.namenode.checkpoint.edits.dir=$&#123;dfs.namenode.checkpoint.dir&#125;dfs.namenode.checkpoint.max-retries=3 ##最大重试次数dfs.namenode.checkpoint.period=3600 ##两次 checkpoint 之间的时间间隔 3600 秒dfs.namenode.checkpoint.txns=1000000 ##两次 checkpoint 之间最大的操作记录 CheckPoint 附带作用Namenode 和 SecondaryNamenode 的工作目录存储结构完全相同，所以，当 Namenode 故障 退出需要重新恢复时，可以从SecondaryNamenode的工作目录中将fsimage拷贝到Namenode 的工作目录，以恢复 namenode 的元数据","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（十一）HDFS的读写详解","slug":"2018-04-11-Hadoop学习之路（十一）HDFS的读写详解","date":"2018-04-11T02:30:04.000Z","updated":"2019-09-19T02:18:01.177Z","comments":true,"path":"2018-04-11-Hadoop学习之路（十一）HDFS的读写详解.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-11-Hadoop学习之路（十一）HDFS的读写详解.html","excerpt":"** Hadoop学习之路（十一）HDFS的读写详解：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十一）HDFS的读写详解","text":"** Hadoop学习之路（十一）HDFS的读写详解：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十一）HDFS的读写详解 &lt;The rest of contents | 余下全文&gt; HDFS的写操作《HDFS权威指南》图解HDFS写过程 ​ 详细文字说明（术语）1、使用 HDFS 提供的客户端 Client，向远程的 namenode 发起 RPC 请求 2、namenode 会检查要创建的文件是否已经存在，创建者是否有权限进行操作，成功则会 为文件创建一个记录，否则会让客户端抛出异常； 3、当客户端开始写入文件的时候，客户端会将文件切分成多个 packets，并在内部以数据队列“data queue（数据队列）”的形式管理这些 packets，并向 namenode 申请 blocks，获 取用来存储 replicas 的合适的 datanode 列表，列表的大小根据 namenode 中 replication 的设定而定； 4、开始以 pipeline（管道）的形式将 packet 写入所有的 replicas 中。客户端把 packet 以流的 方式写入第一个 datanode，该 datanode 把该 packet 存储之后，再将其传递给在此 pipeline 中的下一个 datanode，直到最后一个 datanode，这种写数据的方式呈流水线的形式。 5、最后一个 datanode 成功存储之后会返回一个 ack packet（确认队列），在 pipeline 里传递 至客户端，在客户端的开发库内部维护着”ack queue”，成功收到 datanode 返回的 ack packet 后会从”data queue”移除相应的 packet。 6、如果传输过程中，有某个 datanode 出现了故障，那么当前的 pipeline 会被关闭，出现故 障的 datanode 会从当前的 pipeline 中移除，剩余的 block 会继续剩下的 datanode 中继续 以 pipeline 的形式传输，同时 namenode 会分配一个新的 datanode，保持 replicas 设定的 数量。 7、客户端完成数据的写入后，会对数据流调用 close()方法，关闭数据流； 8、只要写入了 dfs.replication.min（最小写入成功的副本数）的复本数（默认为 1），写操作 就会成功，并且这个块可以在集群中异步复制，直到达到其目标复本数（dfs.replication 的默认值为 3），因为 namenode 已经知道文件由哪些块组成，所以它在返回成功前只需 要等待数据块进行最小量的复制。 详细文字说明（口语）1、客户端发起请求：hadoop fs -put hadoop.tar.gz / 客户端怎么知道请求发给那个节点的哪个进程？ 因为客户端会提供一些工具来解析出来你所指定的HDFS集群的主节点是谁，以及端口号等信息，主要是通过URI来确定， url：hdfs://hadoop1:9000 当前请求会包含一个非常重要的信息： 上传的数据的总大小 2、namenode会响应客户端的这个请求 namenode的职责： 1 管理元数据（抽象目录树结构） 用户上传的那个文件在对应的目录如果存在。那么HDFS集群应该作何处理，不会处理 用户上传的那个文件要存储的目录不存在的话，如果不存在不会创建 2、响应请求 真正的操作：做一系列的校验， 1、校验客户端的请求是否合理2、校验客户端是否有权限进行上传 3、如果namenode返回给客户端的结果是 通过， 那就是允许上传 namenode会给客户端返回对应的所有的数据块的多个副本的存放节点列表，如： file1_blk1 hadoop02，hadoop03，hadoop04file1_blk2 hadoop03，hadoop04，hadoop05 4、客户端在获取到了namenode返回回来的所有数据块的多个副本的存放地的数据之后，就可以按照顺序逐一进行数据块的上传操作 5、对要上传的数据块进行逻辑切片 切片分成两个阶段: 1、规划怎么切2、真正的切 物理切片： 1 和 2 逻辑切片： 1 file1_blk1 ： file1:0:128file1_blk2 ： file1:128:256 逻辑切片只是规划了怎么切 6、开始上传第一个数据块 7、客户端会做一系列准备操作 1、依次发送请求去连接对应的datnaode pipline : client - node1 - node2 - node3 按照一个个的数据包的形式进行发送的。 每次传输完一个数据包，每个副本节点都会进行校验，依次原路给客户端 2、在客户端会启动一个服务： 用户就是用来等到将来要在这个pipline数据管道上进行传输的数据包的校验信息 客户端就能知道当前从clinet到写node1,2,3三个节点上去的数据是否都写入正确和成功 8、clinet会正式的把这个快中的所有packet都写入到对应的副本节点 1、block是最大的一个单位，它是最终存储于DataNode上的数据粒度，由dfs.block.size参数决定，2.x版本默认是128M；注：这个参数由客户端配置决定；如：System.out.println(conf.get(“dfs.blocksize”));//结果是134217728 2、packet是中等的一个单位，它是数据由DFSClient流向DataNode的粒度，以dfs.write.packet.size参数为参考值，默认是64K；注：这个参数为参考值，是指真正在进行数据传输时，会以它为基准进行调整，调整的原因是一个packet有特定的结构，调整的目标是这个packet的大小刚好包含结构中的所有成员，同时也保证写到DataNode后当前block的大小不超过设定值； 如：System.out.println(conf.get(“dfs.write.packet.size”));//结果是65536 3、chunk是最小的一个单位，它是DFSClient到DataNode数据传输中进行数据校验的粒度，由io.bytes.per.checksum参数决定，默认是512B；注：事实上一个chunk还包含4B的校验值，因而chunk写入packet时是516B；数据与检验值的比值为128:1，所以对于一个128M的block会有一个1M的校验文件与之对应； 如：System.out.println(conf.get(“io.bytes.per.checksum”));//结果是512 9、clinet进行校验，如果校验通过，表示该数据块写入成功 10、重复7 8 9 三个操作，来继续上传其他的数据块 11、客户端在意识到所有的数据块都写入成功之后，会给namenode发送一个反馈，就是告诉namenode当前客户端上传的数据已经成功。 HDFS读操作《HDFS权威指南》图解HDFS读过程 数据读取1、客户端调用FileSystem 实例的open 方法，获得这个文件对应的输入流InputStream。 2、通过RPC 远程调用NameNode ，获得NameNode 中此文件对应的数据块保存位置，包括这个文件的副本的保存位置( 主要是各DataNode的地址) 。 3、获得输入流之后，客户端调用read 方法读取数据。选择最近的DataNode 建立连接并读取数据。 4、如果客户端和其中一个DataNode 位于同一机器(比如MapReduce 过程中的mapper 和reducer)，那么就会直接从本地读取数据。 5、到达数据块末端，关闭与这个DataNode 的连接，然后重新查找下一个数据块。 6、不断执行第2 - 5 步直到数据全部读完。 7、客户端调用close ，关闭输入流DF S InputStream。","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（十）HDFS API的使用","slug":"2018-04-10-Hadoop学习之路（十）HDFS API的使用","date":"2018-04-10T02:30:04.000Z","updated":"2019-09-19T02:14:28.591Z","comments":true,"path":"2018-04-10-Hadoop学习之路（十）HDFS API的使用.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-10-Hadoop学习之路（十）HDFS API的使用.html","excerpt":"** Hadoop学习之路（十）HDFS API的使用：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十）HDFS API的使用","text":"** Hadoop学习之路（十）HDFS API的使用：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（十）HDFS API的使用 &lt;The rest of contents | 余下全文&gt; HDFS API的高级编程 HDFS的API就两个：FileSystem 和Configuration 1、文件的上传和下载123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657 1 package com.ghgj.hdfs.api; 2 3 import org.apache.hadoop.conf.Configuration; 4 import org.apache.hadoop.fs.FileSystem; 5 import org.apache.hadoop.fs.Path; 6 7 public class HDFS_GET_AND_PUT &#123; 8 9 public static void main(String[] args) throws Exception &#123;10 11 12 Configuration conf = new Configuration();13 conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);14 conf.set(&quot;dfs.replication&quot;, &quot;2&quot;);15 FileSystem fs = FileSystem.get(conf);16 17 18 /**19 * 更改操作用户有两种方式:20 * 21 * 1、直接设置运行换种的用户名为hadoop22 * 23 * VM arguments ; -DHADOOP_USER_NAME=hadoop24 * 25 * 2、在代码中进行声明26 * 27 * System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);28 */29 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);30 31 // 上传32 fs.copyFromLocalFile(new Path(&quot;c:/sss.txt&quot;), new Path(&quot;/a/ggg.txt&quot;));33 34 35 36 /**37 * .crc ： 校验文件38 * 39 * 每个块的元数据信息都只会记录合法数据的起始偏移量： qqq.txt blk_41838 : 0 - 1100byte40 * 41 * 如果进行非法的数据追加。最终是能够下载合法数据。42 * 由于你在数据的中间， 也就是说在 0 -1100 之间的范围进行了数据信息的更改。 造成了采用CRC算法计算出来校验值，和最初存入进HDFS的校验值43 * 不一致。HDFS就认为当前这个文件被损坏了。44 */45 46 47 // 下载 48 fs.copyToLocalFile(new Path(&quot;/a/qqq.txt&quot;), new Path(&quot;c:/qqq3.txt&quot;));49 50 51 /**52 * 上传和下载的API的底层封装其实就是 ： FileUtil.copy(....)53 */54 55 fs.close();56 &#125;57 &#125; 2、配置文件conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172 1 package com.exam.hdfs; 2 3 import java.io.IOException; 4 import java.util.Iterator; 5 import java.util.Map.Entry; 6 7 import org.apache.hadoop.conf.Configuration; 8 import org.apache.hadoop.fs.FileSystem; 9 10 public class TestConf1 &#123;11 12 public static void main(String[] args) throws Exception &#123;13 14 15 /**16 * 底层会加载一堆的配置文件：17 * 18 * core-default.xml19 * hdfs-default.xml20 * mapred-default.xml21 * yarn-default.xml22 */23 Configuration conf = new Configuration();24 // conf.addResource(&quot;hdfs-default.xml&quot;);25 26 /**27 * 当前这个hdfs-site.xml文件就放置在这个项目中的src下。也就是classpath路径下。28 * 所以 FS在初始化的时候，会把hdfs-site.xml这个文件中的name-value对解析到conf中29 * 30 * 31 * 但是：32 * 33 * 1、如果hdfs-site.xml 不在src下， 看是否能加载？？？ 不能34 * 35 * 2、如果文件名不叫做 hdfs-default.xml 或者 hdsf-site.xml 看是否能自动加载？？？ 不能36 * 37 * 得出的结论：38 * 39 * 如果需要项目代码自动加载配置文件中的信息，那么就必须把配置文件改成-default.xml或者-site.xml的名称40 * 而且必须放置在src下41 * 42 * 那如果不叫这个名，或者不在src下，也需要加载这些配置文件中的参数：43 * 44 * 必须使用conf对象提供的一些方法去手动加载45 */46 // conf.addResource(&quot;hdfs-site.xml&quot;);47 conf.set(&quot;dfs.replication&quot;, &quot;1&quot;);48 conf.addResource(&quot;myconfig/hdfs-site.xml&quot;);49 50 51 /**52 * 依次加载的参数信息的顺序是：53 * 54 * 1、加载 core/hdfs/mapred/yarn-default.xml55 * 56 * 2、加载通过conf.addResources()加载的配置文件57 * 58 * 3、加载conf.set(name, value)59 */60 61 FileSystem fs = FileSystem.get(conf);62 63 System.out.println(conf.get(&quot;dfs.replication&quot;));64 65 66 Iterator&lt;Entry&lt;String, String&gt;&gt; iterator = conf.iterator();67 while(iterator.hasNext())&#123;68 Entry&lt;String, String&gt; e = iterator.next();69 System.out.println(e.getKey() + &quot;\\t&quot; + e.getValue());70 &#125;71 &#125;72 &#125; 输出结果123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387 1 log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory). 2 log4j:WARN Please initialize the log4j system properly. 3 log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 4 1 5 hadoop.security.groups.cache.secs 300 6 dfs.datanode.cache.revocation.timeout.ms 900000 7 dfs.namenode.resource.check.interval 5000 8 s3.client-write-packet-size 65536 9 dfs.client.https.need-auth false 10 dfs.replication 1 11 hadoop.security.group.mapping.ldap.directory.search.timeout 10000 12 dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold 10737418240 13 hadoop.work.around.non.threadsafe.getpwuid false 14 dfs.namenode.write-lock-reporting-threshold-ms 5000 15 fs.ftp.host.port 21 16 dfs.namenode.avoid.read.stale.datanode false 17 dfs.journalnode.rpc-address 0.0.0.0:8485 18 hadoop.security.kms.client.encrypted.key.cache.expiry 43200000 19 ipc.client.connection.maxidletime 10000 20 hadoop.registry.zk.session.timeout.ms 60000 21 tfile.io.chunk.size 1048576 22 fs.automatic.close true 23 ha.health-monitor.sleep-after-disconnect.ms 1000 24 io.map.index.interval 128 25 dfs.namenode.https-address 0.0.0.0:50470 26 dfs.mover.max-no-move-interval 60000 27 io.seqfile.sorter.recordlimit 1000000 28 fs.s3n.multipart.uploads.enabled false 29 hadoop.util.hash.type murmur 30 dfs.namenode.replication.min 1 31 dfs.datanode.directoryscan.threads 1 32 dfs.namenode.fs-limits.min-block-size 1048576 33 dfs.datanode.directoryscan.interval 21600 34 fs.AbstractFileSystem.file.impl org.apache.hadoop.fs.local.LocalFs 35 dfs.namenode.acls.enabled false 36 dfs.client.short.circuit.replica.stale.threshold.ms 1800000 37 net.topology.script.number.args 100 38 hadoop.http.authentication.token.validity 36000 39 fs.s3.block.size 67108864 40 dfs.namenode.resource.du.reserved 104857600 41 ha.failover-controller.graceful-fence.rpc-timeout.ms 5000 42 s3native.bytes-per-checksum 512 43 dfs.namenode.datanode.registration.ip-hostname-check true 44 dfs.namenode.path.based.cache.block.map.allocation.percent 0.25 45 dfs.namenode.backup.http-address 0.0.0.0:50105 46 hadoop.security.group.mapping org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback 47 dfs.namenode.edits.noeditlogchannelflush false 48 dfs.datanode.cache.revocation.polling.ms 500 49 dfs.namenode.audit.loggers default 50 hadoop.security.groups.cache.warn.after.ms 5000 51 io.serializations org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization 52 dfs.namenode.lazypersist.file.scrub.interval.sec 300 53 fs.s3a.threads.core 15 54 hadoop.security.crypto.buffer.size 8192 55 hadoop.http.cross-origin.allowed-methods GET,POST,HEAD 56 hadoop.registry.zk.retry.interval.ms 1000 57 dfs.http.policy HTTP_ONLY 58 hadoop.registry.secure false 59 dfs.namenode.replication.interval 3 60 dfs.namenode.safemode.min.datanodes 0 61 dfs.client.file-block-storage-locations.num-threads 10 62 nfs.dump.dir /tmp/.hdfs-nfs 63 dfs.namenode.secondary.https-address 0.0.0.0:50091 64 hadoop.kerberos.kinit.command kinit 65 dfs.block.access.token.lifetime 600 66 dfs.webhdfs.enabled true 67 dfs.client.use.datanode.hostname false 68 dfs.namenode.delegation.token.max-lifetime 604800000 69 fs.trash.interval 0 70 dfs.datanode.drop.cache.behind.writes false 71 dfs.namenode.avoid.write.stale.datanode false 72 dfs.namenode.num.extra.edits.retained 1000000 73 s3.blocksize 67108864 74 ipc.client.connect.max.retries.on.timeouts 45 75 dfs.datanode.data.dir /home/hadoop/data/hadoopdata/data 76 fs.s3.buffer.dir $&#123;hadoop.tmp.dir&#125;/s3 77 fs.s3n.block.size 67108864 78 nfs.exports.allowed.hosts * rw 79 ha.health-monitor.connect-retry-interval.ms 1000 80 hadoop.security.instrumentation.requires.admin false 81 hadoop.registry.zk.retry.ceiling.ms 60000 82 nfs.rtmax 1048576 83 dfs.client.mmap.cache.size 256 84 dfs.datanode.data.dir.perm 700 85 io.file.buffer.size 4096 86 dfs.namenode.backup.address 0.0.0.0:50100 87 dfs.client.datanode-restart.timeout 30 88 dfs.datanode.readahead.bytes 4194304 89 dfs.namenode.xattrs.enabled true 90 io.mapfile.bloom.size 1048576 91 ipc.client.connect.retry.interval 1000 92 dfs.client-write-packet-size 65536 93 dfs.namenode.checkpoint.txns 1000000 94 dfs.datanode.bp-ready.timeout 20 95 dfs.datanode.transfer.socket.send.buffer.size 131072 96 hadoop.security.kms.client.authentication.retry-count 1 97 dfs.client.block.write.retries 3 98 fs.swift.impl org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem 99 ha.failover-controller.graceful-fence.connection.retries 1100 hadoop.registry.zk.connection.timeout.ms 15000101 dfs.namenode.safemode.threshold-pct 0.999f102 dfs.cachereport.intervalMsec 10000103 hadoop.security.java.secure.random.algorithm SHA1PRNG104 ftp.blocksize 67108864105 dfs.namenode.list.cache.directives.num.responses 100106 dfs.namenode.kerberos.principal.pattern *107 file.stream-buffer-size 4096108 dfs.datanode.dns.nameserver default109 fs.s3a.max.total.tasks 1000110 dfs.namenode.replication.considerLoad true111 nfs.allow.insecure.ports true112 dfs.namenode.edits.journal-plugin.qjournal org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager113 dfs.client.write.exclude.nodes.cache.expiry.interval.millis 600000114 dfs.client.mmap.cache.timeout.ms 3600000115 ipc.client.idlethreshold 4000116 io.skip.checksum.errors false117 ftp.stream-buffer-size 4096118 fs.s3a.fast.upload false119 dfs.client.failover.connection.retries.on.timeouts 0120 file.blocksize 67108864121 ftp.replication 3122 dfs.namenode.replication.work.multiplier.per.iteration 2123 hadoop.security.authorization false124 hadoop.http.authentication.simple.anonymous.allowed true125 s3native.client-write-packet-size 65536126 hadoop.rpc.socket.factory.class.default org.apache.hadoop.net.StandardSocketFactory127 file.bytes-per-checksum 512128 dfs.datanode.slow.io.warning.threshold.ms 300129 fs.har.impl.disable.cache true130 rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB org.apache.hadoop.ipc.ProtobufRpcEngine131 io.seqfile.lazydecompress true132 dfs.namenode.reject-unresolved-dn-topology-mapping false133 hadoop.common.configuration.version 0.23.0134 hadoop.security.authentication simple135 dfs.datanode.drop.cache.behind.reads false136 dfs.image.compression.codec org.apache.hadoop.io.compress.DefaultCodec137 dfs.client.read.shortcircuit.streams.cache.size 256138 file.replication 1139 dfs.namenode.top.num.users 10140 dfs.namenode.accesstime.precision 3600000141 dfs.namenode.fs-limits.max-xattrs-per-inode 32142 dfs.image.transfer.timeout 60000143 io.mapfile.bloom.error.rate 0.005144 nfs.wtmax 1048576145 hadoop.security.kms.client.encrypted.key.cache.size 500146 dfs.namenode.edit.log.autoroll.check.interval.ms 300000147 fs.s3a.multipart.purge false148 dfs.namenode.support.allow.format true149 hadoop.hdfs.configuration.version 1150 fs.s3a.connection.establish.timeout 5000151 hadoop.security.group.mapping.ldap.search.attr.member member152 dfs.secondary.namenode.kerberos.internal.spnego.principal $&#123;dfs.web.authentication.kerberos.principal&#125;153 dfs.stream-buffer-size 4096154 hadoop.ssl.client.conf ssl-client.xml155 dfs.namenode.invalidate.work.pct.per.iteration 0.32f156 fs.s3a.multipart.purge.age 86400157 dfs.journalnode.https-address 0.0.0.0:8481158 dfs.namenode.top.enabled true159 hadoop.security.kms.client.encrypted.key.cache.low-watermark 0.3f160 dfs.namenode.max.objects 0161 hadoop.user.group.static.mapping.overrides dr.who=;162 fs.s3a.fast.buffer.size 1048576163 dfs.bytes-per-checksum 512164 dfs.datanode.max.transfer.threads 4096165 dfs.block.access.key.update.interval 600166 ipc.maximum.data.length 67108864167 tfile.fs.input.buffer.size 262144168 ha.failover-controller.new-active.rpc-timeout.ms 60000169 dfs.client.cached.conn.retry 3170 dfs.client.read.shortcircuit false171 hadoop.ssl.hostname.verifier DEFAULT172 dfs.datanode.hdfs-blocks-metadata.enabled false173 dfs.datanode.directoryscan.throttle.limit.ms.per.sec 0174 dfs.image.transfer.chunksize 65536175 hadoop.http.authentication.type simple176 dfs.namenode.list.encryption.zones.num.responses 100177 dfs.client.https.keystore.resource ssl-client.xml178 s3native.blocksize 67108864179 net.topology.impl org.apache.hadoop.net.NetworkTopology180 dfs.client.failover.sleep.base.millis 500181 io.seqfile.compress.blocksize 1000000182 dfs.namenode.path.based.cache.refresh.interval.ms 30000183 dfs.namenode.decommission.interval 30184 dfs.permissions.superusergroup supergroup185 dfs.namenode.fs-limits.max-directory-items 1048576186 hadoop.registry.zk.retry.times 5187 dfs.ha.log-roll.period 120188 fs.AbstractFileSystem.ftp.impl org.apache.hadoop.fs.ftp.FtpFs189 ftp.bytes-per-checksum 512190 dfs.user.home.dir.prefix /user191 dfs.namenode.checkpoint.edits.dir $&#123;dfs.namenode.checkpoint.dir&#125;192 dfs.client.socket.send.buffer.size 131072193 ipc.client.fallback-to-simple-auth-allowed false194 dfs.blockreport.initialDelay 0195 dfs.namenode.inotify.max.events.per.rpc 1000196 dfs.namenode.heartbeat.recheck-interval 300000197 dfs.namenode.safemode.extension 30000198 dfs.client.failover.sleep.max.millis 15000199 dfs.namenode.delegation.key.update-interval 86400000200 dfs.datanode.transfer.socket.recv.buffer.size 131072201 hadoop.rpc.protection authentication202 fs.permissions.umask-mode 022203 fs.s3.sleepTimeSeconds 10204 dfs.namenode.fs-limits.max-xattr-size 16384205 ha.health-monitor.rpc-timeout.ms 45000206 hadoop.http.staticuser.user dr.who207 dfs.datanode.http.address 0.0.0.0:50075208 fs.s3a.connection.maximum 15209 fs.s3a.paging.maximum 5000210 fs.AbstractFileSystem.viewfs.impl org.apache.hadoop.fs.viewfs.ViewFs211 dfs.namenode.blocks.per.postponedblocks.rescan 10000212 fs.ftp.host 0.0.0.0213 dfs.lock.suppress.warning.interval 10s214 hadoop.http.authentication.kerberos.keytab $&#123;user.home&#125;/hadoop.keytab215 fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem216 hadoop.registry.zk.root /registry217 hadoop.jetty.logs.serve.aliases true218 dfs.namenode.fs-limits.max-blocks-per-file 1048576219 dfs.balancer.keytab.enabled false220 dfs.client.block.write.replace-datanode-on-failure.enable true221 hadoop.http.cross-origin.max-age 1800222 io.compression.codec.bzip2.library system-native223 dfs.namenode.checkpoint.dir file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary224 dfs.client.use.legacy.blockreader.local false225 dfs.namenode.top.windows.minutes 1,5,25226 ipc.ping.interval 60000227 net.topology.node.switch.mapping.impl org.apache.hadoop.net.ScriptBasedMapping228 nfs.mountd.port 4242229 dfs.storage.policy.enabled true230 dfs.namenode.list.cache.pools.num.responses 100231 fs.df.interval 60000232 nfs.server.port 2049233 ha.zookeeper.parent-znode /hadoop-ha234 hadoop.http.cross-origin.allowed-headers X-Requested-With,Content-Type,Accept,Origin235 dfs.datanode.block-pinning.enabled false236 dfs.namenode.num.checkpoints.retained 2237 fs.s3a.attempts.maximum 10238 s3native.stream-buffer-size 4096239 io.seqfile.local.dir $&#123;hadoop.tmp.dir&#125;/io/local240 fs.s3n.multipart.copy.block.size 5368709120241 dfs.encrypt.data.transfer.cipher.key.bitlength 128242 dfs.client.mmap.retry.timeout.ms 300000243 dfs.datanode.sync.behind.writes false244 dfs.namenode.fslock.fair true245 hadoop.ssl.keystores.factory.class org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory246 dfs.permissions.enabled true247 fs.AbstractFileSystem.hdfs.impl org.apache.hadoop.fs.Hdfs248 dfs.blockreport.split.threshold 1000000249 dfs.datanode.balance.bandwidthPerSec 1048576250 dfs.block.scanner.volume.bytes.per.second 1048576251 hadoop.security.random.device.file.path /dev/urandom252 fs.s3.maxRetries 4253 hadoop.http.filter.initializers org.apache.hadoop.http.lib.StaticUserWebFilter254 dfs.namenode.stale.datanode.interval 30000255 ipc.client.rpc-timeout.ms 0256 fs.client.resolve.remote.symlinks true257 dfs.default.chunk.view.size 32768258 hadoop.ssl.enabled.protocols TLSv1259 dfs.namenode.decommission.blocks.per.interval 500000260 dfs.namenode.handler.count 10261 dfs.image.transfer.bandwidthPerSec 0262 rpc.metrics.quantile.enable false263 hadoop.ssl.enabled false264 dfs.replication.max 512265 dfs.namenode.name.dir /home/hadoop/data/hadoopdata/name266 dfs.namenode.read-lock-reporting-threshold-ms 5000267 dfs.datanode.https.address 0.0.0.0:50475268 dfs.datanode.failed.volumes.tolerated 0269 ipc.client.kill.max 10270 fs.s3a.threads.max 256271 ipc.server.listen.queue.size 128272 dfs.client.domain.socket.data.traffic false273 dfs.block.access.token.enable false274 dfs.blocksize 134217728275 fs.s3a.connection.timeout 50000276 fs.s3a.threads.keepalivetime 60277 file.client-write-packet-size 65536278 dfs.datanode.address 0.0.0.0:50010279 ha.failover-controller.cli-check.rpc-timeout.ms 20000280 ha.zookeeper.acl world:anyone:rwcda281 ipc.client.connect.max.retries 10282 dfs.encrypt.data.transfer false283 dfs.namenode.write.stale.datanode.ratio 0.5f284 ipc.client.ping true285 dfs.datanode.shared.file.descriptor.paths /dev/shm,/tmp286 dfs.short.circuit.shared.memory.watcher.interrupt.check.ms 60000287 hadoop.tmp.dir /home/hadoop/data/hadoopdata288 dfs.datanode.handler.count 10289 dfs.client.failover.max.attempts 15290 dfs.balancer.max-no-move-interval 60000291 dfs.client.read.shortcircuit.streams.cache.expiry.ms 300000292 dfs.namenode.block-placement-policy.default.prefer-local-node true293 hadoop.ssl.require.client.cert false294 hadoop.security.uid.cache.secs 14400295 dfs.client.read.shortcircuit.skip.checksum false296 dfs.namenode.resource.checked.volumes.minimum 1297 hadoop.registry.rm.enabled false298 dfs.namenode.quota.init-threads 4299 dfs.namenode.max.extra.edits.segments.retained 10000300 dfs.webhdfs.user.provider.user.pattern ^[A-Za-z_][A-Za-z0-9._-]*[$]?$301 dfs.client.mmap.enabled true302 dfs.client.file-block-storage-locations.timeout.millis 1000303 dfs.datanode.block.id.layout.upgrade.threads 12304 dfs.datanode.use.datanode.hostname false305 hadoop.fuse.timer.period 5306 dfs.client.context default307 fs.trash.checkpoint.interval 0308 dfs.journalnode.http-address 0.0.0.0:8480309 dfs.balancer.address 0.0.0.0:0310 dfs.namenode.lock.detailed-metrics.enabled false311 dfs.namenode.delegation.token.renew-interval 86400000312 ha.health-monitor.check-interval.ms 1000313 dfs.namenode.retrycache.heap.percent 0.03f314 ipc.client.connect.timeout 20000315 dfs.reformat.disabled false316 dfs.blockreport.intervalMsec 21600000317 fs.s3a.multipart.threshold 2147483647318 dfs.https.server.keystore.resource ssl-server.xml319 hadoop.http.cross-origin.enabled false320 io.map.index.skip 0321 dfs.balancer.block-move.timeout 0322 io.native.lib.available true323 s3.replication 3324 dfs.namenode.kerberos.internal.spnego.principal $&#123;dfs.web.authentication.kerberos.principal&#125;325 fs.AbstractFileSystem.har.impl org.apache.hadoop.fs.HarFs326 hadoop.security.kms.client.encrypted.key.cache.num.refill.threads 2327 fs.s3n.multipart.uploads.block.size 67108864328 dfs.image.compress false329 dfs.datanode.dns.interface default330 dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction 0.75f331 tfile.fs.output.buffer.size 262144332 fs.du.interval 600000333 dfs.client.failover.connection.retries 0334 dfs.namenode.edit.log.autoroll.multiplier.threshold 2.0335 hadoop.security.group.mapping.ldap.ssl false336 dfs.namenode.top.window.num.buckets 10337 fs.s3a.buffer.dir $&#123;hadoop.tmp.dir&#125;/s3a338 dfs.namenode.checkpoint.check.period 60339 fs.defaultFS hdfs://hadoop1:9000340 fs.s3a.multipart.size 104857600341 dfs.client.slow.io.warning.threshold.ms 30000342 dfs.datanode.max.locked.memory 0343 dfs.namenode.retrycache.expirytime.millis 600000344 hadoop.security.group.mapping.ldap.search.attr.group.name cn345 dfs.client.block.write.replace-datanode-on-failure.best-effort false346 dfs.ha.fencing.ssh.connect-timeout 30000347 dfs.datanode.scan.period.hours 504348 hadoop.registry.zk.quorum localhost:2181349 dfs.namenode.fs-limits.max-component-length 255350 hadoop.http.cross-origin.allowed-origins *351 dfs.namenode.enable.retrycache true352 dfs.datanode.du.reserved 0353 dfs.datanode.ipc.address 0.0.0.0:50020354 hadoop.registry.system.acls sasl:yarn@, sasl:mapred@, sasl:hdfs@355 dfs.namenode.path.based.cache.retry.interval.ms 30000356 hadoop.security.crypto.cipher.suite AES/CTR/NoPadding357 dfs.client.block.write.replace-datanode-on-failure.policy DEFAULT358 dfs.namenode.http-address 0.0.0.0:50070359 hadoop.security.crypto.codec.classes.aes.ctr.nopadding org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec360 dfs.ha.tail-edits.period 60361 hadoop.security.groups.negative-cache.secs 30362 hadoop.ssl.server.conf ssl-server.xml363 hadoop.registry.jaas.context Client364 s3native.replication 3365 hadoop.security.group.mapping.ldap.search.filter.group (objectClass=group)366 hadoop.http.authentication.kerberos.principal HTTP/_HOST@LOCALHOST367 dfs.namenode.startup.delay.block.deletion.sec 0368 hadoop.security.group.mapping.ldap.search.filter.user (&amp;(objectClass=user)(sAMAccountName=&#123;0&#125;))369 dfs.namenode.edits.dir $&#123;dfs.namenode.name.dir&#125;370 dfs.namenode.checkpoint.max-retries 3371 s3.stream-buffer-size 4096372 ftp.client-write-packet-size 65536373 dfs.datanode.fsdatasetcache.max.threads.per.volume 4374 hadoop.security.sensitive-config-keys password$,fs.s3.*[Ss]ecret.?[Kk]ey,fs.azure.account.key.*,dfs.webhdfs.oauth2.[a-z]+.token,hadoop.security.sensitive-config-keys375 dfs.namenode.decommission.max.concurrent.tracked.nodes 100376 dfs.namenode.name.dir.restore false377 ipc.server.log.slow.rpc false378 dfs.heartbeat.interval 3379 dfs.namenode.secondary.http-address hadoop3:50090380 ha.zookeeper.session-timeout.ms 5000381 s3.bytes-per-checksum 512382 fs.s3a.connection.ssl.enabled true383 hadoop.http.authentication.signature.secret.file $&#123;user.home&#125;/hadoop-http-auth-signature-secret384 hadoop.fuse.connection.timeout 300385 dfs.namenode.checkpoint.period 3600386 ipc.server.max.connections 0387 dfs.ha.automatic-failover.enabled false 3、列出指定目录下的文件以及块的信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 1 package com.exam.hdfs; 2 3 import org.apache.hadoop.conf.Configuration; 4 import org.apache.hadoop.fs.BlockLocation; 5 import org.apache.hadoop.fs.FileSystem; 6 import org.apache.hadoop.fs.LocatedFileStatus; 7 import org.apache.hadoop.fs.Path; 8 import org.apache.hadoop.fs.RemoteIterator; 9 10 public class TestHDFS1 &#123;11 12 public static void main(String[] args) throws Exception &#123;13 14 Configuration conf = new Configuration();15 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);16 conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);17 FileSystem fs = FileSystem.get(conf);18 19 /**20 * 列出指定的目录下的所有文件21 */22 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true);23 while(listFiles.hasNext())&#123;24 LocatedFileStatus file = listFiles.next();25 26 27 System.out.println(file.getPath()+&quot;\\t&quot;);28 System.out.println(file.getPath().getName()+&quot;\\t&quot;);29 System.out.println(file.getLen()+&quot;\\t&quot;);30 System.out.println(file.getReplication()+&quot;\\t&quot;);31 32 /**33 * blockLocations的长度是几？ 是什么意义？34 * 35 * 块的数量36 */37 BlockLocation[] blockLocations = file.getBlockLocations();38 System.out.println(blockLocations.length+&quot;\\t&quot;);39 40 for(BlockLocation bl : blockLocations)&#123;41 String[] hosts = bl.getHosts();42 43 System.out.print(hosts[0] + &quot;-&quot; + hosts[1]+&quot;\\t&quot;);44 &#125;45 System.out.println();46 47 &#125;48 49 50 &#125;51 &#125; 输出结果 1234561 hdfs://hadoop1:9000/aa/bb/cc/hadoop.tar.gz 2 hadoop.tar.gz 3 199007110 4 2 5 3 6 hadoop3-hadoop1 hadoop1-hadoop2 hadoop1-hadoop4 4、上传文件123456789101112131415161718192021222324252627282930313233 1 package com.exam.hdfs; 2 3 import java.io.File; 4 import java.io.FileInputStream; 5 import java.io.InputStream; 6 7 import org.apache.hadoop.conf.Configuration; 8 import org.apache.hadoop.fs.FSDataOutputStream; 9 import org.apache.hadoop.fs.FileSystem;10 import org.apache.hadoop.fs.Path;11 import org.apache.hadoop.io.IOUtils;12 13 public class UploadDataByStream &#123;14 15 public static void main(String[] args) throws Exception &#123;16 17 18 Configuration conf = new Configuration();19 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);20 conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);21 FileSystem fs = FileSystem.get(conf);22 23 24 InputStream in = new FileInputStream(new File(&quot;d:/abc.tar.gz&quot;));25 FSDataOutputStream out = fs.create(new Path(&quot;/aa/abc.tar.gz&quot;));26 27 28 IOUtils.copyBytes(in, out, 4096, true);29 30 fs.close();31 32 &#125;33 &#125; 5、下载文件123456789101112131415161718192021222324252627282930313233 1 package com.exam.hdfs; 2 3 import java.io.File; 4 import java.io.FileOutputStream; 5 import java.io.OutputStream; 6 7 import org.apache.hadoop.conf.Configuration; 8 import org.apache.hadoop.fs.FSDataInputStream; 9 import org.apache.hadoop.fs.FileSystem;10 import org.apache.hadoop.fs.Path;11 import org.apache.hadoop.io.IOUtils;12 13 public class DownloadDataByStream &#123;14 15 16 public static void main(String[] args) throws Exception &#123;17 18 Configuration conf = new Configuration();19 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);20 conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);21 FileSystem fs = FileSystem.get(conf);22 23 24 FSDataInputStream in = fs.open(new Path(&quot;/aa/abc.tar.gz&quot;));25 OutputStream out = new FileOutputStream(new File(&quot;D:/abc.sh&quot;));26 27 28 IOUtils.copyBytes(in, out, 4096, true);29 30 fs.close();31 32 &#125;33 &#125; 6、删除某个路径下特定类型的文件，比如class类型文件，比如txt类型文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172 1 package com.exam.hdfs; 2 3 import java.net.URI; 4 5 import org.apache.hadoop.conf.Configuration; 6 import org.apache.hadoop.fs.FileStatus; 7 import org.apache.hadoop.fs.FileSystem; 8 import org.apache.hadoop.fs.Path; 9 10 public class HDFS_DELETE_CLASS &#123;11 12 public static final String FILETYPE = &quot;tar.gz&quot;;13 public static final String DELETE_PATH = &quot;/aa&quot;;14 15 public static void main(String[] args) throws Exception &#123;16 17 new HDFS_DELETE_CLASS().rmrClassFile(new Path(DELETE_PATH));18 &#125;19 20 public void rmrClassFile(Path path) throws Exception&#123;21 22 // 首先获取集群必要的信息，以得到FileSystem的示例对象fs23 Configuration conf = new Configuration();24 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop1:9000&quot;), conf, &quot;hadoop&quot;);25 26 // 首先检查path本身是文件夹还是目录27 FileStatus fileStatus = fs.getFileStatus(path);28 boolean directory = fileStatus.isDirectory();29 30 // 根据该目录是否是文件或者文件夹进行相应的操作31 if(directory)&#123;32 // 如果是目录33 checkAndDeleteDirectory(path, fs);34 &#125;else&#123;35 // 如果是文件，检查该文件名是不是FILETYPE类型的文件36 checkAndDeleteFile(path, fs);37 &#125;38 &#125;39 40 // 处理目录41 public static void checkAndDeleteDirectory(Path path, FileSystem fs) throws Exception&#123;42 // 查看该path目录下一级子目录和子文件的状态43 FileStatus[] listStatus = fs.listStatus(path);44 for(FileStatus fStatus: listStatus)&#123;45 Path p = fStatus.getPath();46 // 如果是文件，并且是以FILETYPE结尾，则删掉，否则继续遍历下一级目录47 if(fStatus.isFile())&#123;48 checkAndDeleteFile(p, fs);49 &#125;else&#123;50 checkAndDeleteDirectory(p, fs);51 &#125;52 &#125;53 &#125;54 55 // 檢查文件是否符合刪除要求，如果符合要求則刪除，不符合要求则不做处理56 public static void checkAndDeleteFile(Path path, FileSystem fs) throws Exception&#123;57 String name = path.getName();58 System.out.println(name);59 /*// 直接判断有没有FILETYPE这个字符串,不是特别稳妥，并且会有误操作，所以得判断是不是以FILETYPE结尾60 if(name.indexOf(FILETYPE) != -1)&#123;61 fs.delete(path, true);62 &#125;*/63 // 判断是不是以FILETYPE结尾64 int startIndex = name.length() - FILETYPE.length();65 int endIndex = name.length();66 // 求得文件后缀名67 String fileSuffix = name.substring(startIndex, endIndex);68 if(fileSuffix.equals(FILETYPE))&#123;69 fs.delete(path, true);70 &#125;71 &#125;72 &#125; 7、删除HDFS集群中的所有空文件和空目录123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127 1 public class DeleteEmptyDirAndFile &#123; 2 3 static FileSystem fs = null; 4 5 public static void main(String[] args) throws Exception &#123; 6 7 initFileSystem(); 8 9 // 创建测试数据 10 // makeTestData(); 11 12 // 删除测试数据 13 // deleteTestData(); 14 15 // 删除指定文件夹下的空文件和空文件夹 16 deleteEmptyDirAndFile(new Path(&quot;/aa&quot;)); 17 &#125; 18 19 /** 20 * 删除指定文件夹下的 空文件 和 空文件夹 21 * @throws Exception 22 */ 23 public static void deleteEmptyDirAndFile(Path path) throws Exception &#123; 24 25 //当是空文件夹时 26 FileStatus[] listStatus = fs.listStatus(path); 27 if(listStatus.length == 0)&#123; 28 fs.delete(path, true); 29 return; 30 &#125; 31 32 // 该方法的结果：包括指定目录的 文件 和 文件夹 33 RemoteIterator&lt;LocatedFileStatus&gt; listLocatedStatus = fs.listLocatedStatus(path); 34 35 while (listLocatedStatus.hasNext()) &#123; 36 LocatedFileStatus next = listLocatedStatus.next(); 37 38 Path currentPath = next.getPath(); 39 // 获取父目录 40 Path parent = next.getPath().getParent(); 41 42 // 如果是文件夹，继续往下遍历，删除符合条件的文件（空文件夹） 43 if (next.isDirectory()) &#123; 44 45 // 如果是空文件夹 46 if(fs.listStatus(currentPath).length == 0)&#123; 47 // 删除掉 48 fs.delete(currentPath, true); 49 &#125;else&#123; 50 // 不是空文件夹，那么则继续遍历 51 if(fs.exists(currentPath))&#123; 52 deleteEmptyDirAndFile(currentPath); 53 &#125; 54 &#125; 55 56 // 如果是文件 57 &#125; else &#123; 58 // 获取文件的长度 59 long fileLength = next.getLen(); 60 // 当文件是空文件时， 删除 61 if(fileLength == 0)&#123; 62 fs.delete(currentPath, true); 63 &#125; 64 &#125; 65 66 // 当空文件夹或者空文件删除时，有可能导致父文件夹为空文件夹， 67 // 所以每次删除一个空文件或者空文件的时候都需要判断一下，如果真是如此，那么就需要把该文件夹也删除掉 68 int length = fs.listStatus(parent).length; 69 if(length == 0)&#123; 70 fs.delete(parent, true); 71 &#125; 72 &#125; 73 &#125; 74 75 /** 76 * 初始化FileSystem对象之用 77 */ 78 public static void initFileSystem() throws Exception&#123; 79 Configuration conf = new Configuration(); 80 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;); 81 conf.addResource(&quot;config/core-site.xml&quot;); 82 conf.addResource(&quot;config/hdfs-site.xml&quot;); 83 fs = FileSystem.get(conf); 84 &#125; 85 86 /** 87 * 创建 测试 数据之用 88 */ 89 public static void makeTestData() throws Exception &#123; 90 91 String emptyFilePath = &quot;D:\\\\bigdata\\\\1704mr_test\\\\empty.txt&quot;; 92 String notEmptyFilePath = &quot;D:\\\\bigdata\\\\1704mr_test\\\\notEmpty.txt&quot;; 93 94 // 空文件夹 和 空文件 的目录 95 String path1 = &quot;/aa/bb1/cc1/dd1/&quot;; 96 fs.mkdirs(new Path(path1)); 97 fs.mkdirs(new Path(&quot;/aa/bb1/cc1/dd2/&quot;)); 98 fs.copyFromLocalFile(new Path(emptyFilePath), new Path(path1)); 99 fs.copyFromLocalFile(new Path(notEmptyFilePath), new Path(path1));100 101 // 空文件 的目录102 String path2 = &quot;/aa/bb1/cc2/dd2/&quot;;103 fs.mkdirs(new Path(path2));104 fs.copyFromLocalFile(new Path(emptyFilePath), new Path(path2));105 106 // 非空文件 的目录107 String path3 = &quot;/aa/bb2/cc3/dd3&quot;;108 fs.mkdirs(new Path(path3));109 fs.copyFromLocalFile(new Path(notEmptyFilePath), new Path(path3));110 111 // 空 文件夹112 String path4 = &quot;/aa/bb2/cc4/dd4&quot;;113 fs.mkdirs(new Path(path4));114 115 System.out.println(&quot;测试数据创建成功&quot;);116 &#125;117 118 /**119 * 删除 指定文件夹120 * @throws Exception 121 */122 public static void deleteTestData() throws Exception &#123;123 boolean delete = fs.delete(new Path(&quot;/aa&quot;), true);124 System.out.println(delete ? &quot;删除数据成功&quot; : &quot;删除数据失败&quot;);125 &#125;126 127 &#125; 8、手动拷贝某个特定的数据块（比如某个文件的第二个数据块）123456789101112131415161718192021222324252627282930313233343536 1 /** 2 * 手动拷贝某个特定的数据块（比如某个文件的第二个数据块） 3 * */ 4 public static void copyBlock(String str,int num) &#123; 5 6 Path path = new Path(str); 7 8 BlockLocation[] localtions = new BlockLocation[0] ; 9 10 try &#123;11 FileStatus fileStatus = fs.getFileStatus(path);12 13 localtions = fs.getFileBlockLocations(fileStatus, 0, fileStatus.getLen());14 15 /*for(int i=0;i&lt;localtions.length;i++) &#123;16 //0,134217728,hadoop1,hadoop317 //134217728,64789382,hadoop3,hadoop118 System.out.println(localtions[i]);19 &#125;*/20 21 /*System.out.println(localtions[num-1].getOffset());22 System.out.println(localtions[num-1].getLength());23 String[] hosts = localtions[num-1].getHosts();*/24 25 FSDataInputStream open = fs.open(path);26 open.seek(localtions[num-1].getOffset());27 OutputStream out = new FileOutputStream(new File(&quot;D:/abc.tar.gz&quot;));28 IOUtils.copyBytes(open, out,4096,true);29 30 31 32 &#125; catch (IOException e) &#123;33 e.printStackTrace();34 &#125;35 36 &#125; 9、编写程序统计出HDFS文件系统中文件大小小于HDFS集群中的默认块大小的文件占比123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657 1 import org.apache.hadoop.conf.Configuration; 2 import org.apache.hadoop.fs.FileSystem; 3 import org.apache.hadoop.fs.LocatedFileStatus; 4 import org.apache.hadoop.fs.Path; 5 import org.apache.hadoop.fs.RemoteIterator; 6 7 /** 8 * 9 * 编写程序统计出HDFS文件系统中文件大小小于HDFS集群中的默认块大小的文件占比10 * 比如：大于等于128M的文件个数为98，小于128M的文件总数为2，所以答案是2%11 */12 public class Exam1_SmallFilePercent &#123;13 14 private static int DEFAULT_BLOCKSIZE = 128 * 1024 * 1024;15 16 public static void main(String[] args) throws Exception &#123;17 18 19 Configuration conf = new Configuration();20 conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);21 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);22 FileSystem fs = FileSystem.get(conf);23 24 25 Path path = new Path(&quot;/&quot;);26 float smallFilePercent = getSmallFilePercent(fs, path);27 System.out.println(smallFilePercent);28 29 30 fs.close();31 &#125;32 33 /**34 * 该方法求出指定目录下的小文件和总文件数的对比35 * @throws Exception 36 */37 private static float getSmallFilePercent(FileSystem fs, Path path) throws Exception &#123;38 // TODO Auto-generated method stub39 40 int smallFile = 0;41 int totalFile = 0;42 43 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(path, false);44 while(listFiles.hasNext())&#123;45 totalFile++;46 LocatedFileStatus next = listFiles.next();47 long len = next.getLen();48 if(len &lt; DEFAULT_BLOCKSIZE)&#123;49 smallFile++;50 &#125;51 &#125;52 System.out.println(smallFile+&quot; : &quot;+totalFile);53 54 return smallFile * 1f /totalFile;55 &#125;56 57 &#125; 10、编写程序统计出HDFS文件系统中的平均数据块数（数据块总数/文件总数）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 1 import org.apache.hadoop.conf.Configuration; 2 import org.apache.hadoop.fs.FileSystem; 3 import org.apache.hadoop.fs.LocatedFileStatus; 4 import org.apache.hadoop.fs.Path; 5 import org.apache.hadoop.fs.RemoteIterator; 6 7 /** 8 * 9 * 编写程序统计出HDFS文件系统中的平均数据块数（数据块总数/文件总数）10 * 比如：一个文件有5个块，一个文件有3个块，那么平均数据块数为411 * 如果还有一个文件，并且数据块就1个，那么整个HDFS的平均数据块数就是312 */13 public class Exam2_HDSFAvgBlocks &#123;14 15 public static void main(String[] args) throws Exception &#123;16 17 18 Configuration conf = new Configuration();19 conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);20 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);21 FileSystem fs = FileSystem.get(conf);22 23 24 Path path = new Path(&quot;/&quot;);25 float avgHDFSBlocks = getHDFSAvgBlocks(fs, path);26 System.out.println(&quot;HDFS的平均数据块个数为：&quot; + avgHDFSBlocks);27 28 29 fs.close();30 &#125;31 32 /**33 * 求出指定目录下的所有文件的平均数据块个数34 */35 private static float getHDFSAvgBlocks(FileSystem fs, Path path) throws Exception &#123;36 // TODO Auto-generated method stub37 38 int totalFiles = 0; // 总文件数39 int totalBlocks = 0; // 总数据块数40 41 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(path, false);42 43 while(listFiles.hasNext())&#123;44 LocatedFileStatus next = listFiles.next();45 int length = next.getBlockLocations().length;46 totalBlocks += length;47 if(next.getLen() != 0)&#123;48 totalFiles++;49 &#125;50 &#125;51 System.out.println(totalBlocks+&quot; : &quot;+totalFiles);52 53 return totalBlocks * 1f / totalFiles;54 &#125;55 56 &#125; 11、编写程序统计出HDFS文件系统中的平均副本数（副本总数/总数据块数）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 1 import org.apache.hadoop.conf.Configuration; 2 import org.apache.hadoop.fs.FileSystem; 3 import org.apache.hadoop.fs.LocatedFileStatus; 4 import org.apache.hadoop.fs.Path; 5 import org.apache.hadoop.fs.RemoteIterator; 6 7 /** 8 * 编写程序统计出HDFS文件系统中的平均副本数（副本总数/总数据块数） 9 * 比如：总共两个文件，一个文件5个数据块，每个数据块3个副本，第二个文件2个数据块，每个文件2个副本，最终的平均副本数 = （3*3 + 2*2）/（3+2）= 2.810 */11 public class Exam3_HDSFAvgBlockCopys &#123;12 13 public static void main(String[] args) throws Exception &#123;14 15 16 Configuration conf = new Configuration();17 conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop02:9000&quot;);18 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);19 FileSystem fs = FileSystem.get(conf);20 21 22 Path path = new Path(&quot;/&quot;);23 float avgHDFSBlockCopys = getHDFSAvgBlockCopys(fs, path);24 System.out.println(&quot;HDFS的平均数据块个数为：&quot; + avgHDFSBlockCopys);25 26 27 fs.close();28 &#125;29 30 /**31 * 求出指定目录下的所有文件的平均数据块个数32 */33 private static float getHDFSAvgBlockCopys(FileSystem fs, Path path) throws Exception &#123;34 // TODO Auto-generated method stub35 36 int totalCopy = 0; // 总副本数37 int totalBlocks = 0; // 总数据块数38 39 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(path, false);40 41 while(listFiles.hasNext())&#123;42 LocatedFileStatus next = listFiles.next();43 44 int length = next.getBlockLocations().length;45 short replication = next.getReplication();46 47 totalBlocks += length;48 totalCopy += length * replication;49 &#125;50 System.out.println(totalCopy+&quot; : &quot;+totalBlocks);51 52 return totalCopy * 1f / totalBlocks;53 &#125;54 55 &#125; 12、统计HDFS整个文件系统中的不足指定数据块大小的数据块的比例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 1 import java.io.IOException; 2 3 import org.apache.hadoop.conf.Configuration; 4 import org.apache.hadoop.fs.BlockLocation; 5 import org.apache.hadoop.fs.FileSystem; 6 import org.apache.hadoop.fs.LocatedFileStatus; 7 import org.apache.hadoop.fs.Path; 8 import org.apache.hadoop.fs.RemoteIterator; 9 10 /**11 * 统计HDFS整个文件系统中的不足指定数据块大小的数据块的比例12 * 比如指定的数据块大小是128M，总数据块有100个，不是大小为完整的128M的数据块有5个，那么不足指定数据块大小的数据块的比例就为5%13 * 注意：千万注意考虑不同文件的指定数据块大小可能不一致。所以千万不能用默认的128M一概而论14 */15 public class Exam4_LTBlockSize &#123;16 17 public static void main(String[] args) throws Exception &#123;18 19 Configuration conf = new Configuration();20 conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop02:9000&quot;);21 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);22 FileSystem fs = FileSystem.get(conf);23 24 Path path = new Path(&quot;/&quot;);25 float avgHDFSBlockCopys = getLessThanBlocksizeBlocks(fs, path);26 System.out.println(&quot;HDFS的不足指定数据块大小的数据块数目为：&quot; + avgHDFSBlockCopys);27 28 fs.close();29 &#125;30 31 private static float getLessThanBlocksizeBlocks(FileSystem fs, Path path) throws Exception &#123;32 // TODO Auto-generated method stub33 34 int totalBlocks = 0; // 总副本数35 int lessThenBlocksizeBlocks = 0; // 总数据块数36 37 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(path, false);38 39 while(listFiles.hasNext())&#123;40 LocatedFileStatus next = listFiles.next();41 42 BlockLocation[] blockLocations = next.getBlockLocations();43 int length = blockLocations.length;44 45 if(length != 0)&#123;46 totalBlocks += length;47 long lastBlockSize = blockLocations[length - 1].getLength();48 long blockSize = next.getBlockSize();49 if(lastBlockSize &lt; blockSize)&#123;50 lessThenBlocksizeBlocks++;51 &#125;52 &#125;53 &#125;54 System.out.println(lessThenBlocksizeBlocks+&quot; : &quot;+totalBlocks);55 56 return lessThenBlocksizeBlocks * 1f / totalBlocks;57 &#125;58 &#125; 13、统计出一个给定数组的蓄水总量（把数组的每个位置的数看是做地势高低）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129 1 /** 2 统计出一个给定数组的蓄水总量（把数组的每个位置的数看是做地势高低） 3 比如：int[] intArray = new int[]&#123;4,3,2,5,6,4,4,7&#125; 4 能蓄水：[0,1,2,0,0,2,2,0] 所以总量是：7 5 6 核心思路：把数组切成很多个 01数组，每一层一个01数组，统计每个01数组中的合法0的总个数（数组的左边第一个1的中间区间中的0的个数）即可 7 */ 8 public class Exam5_WaterStoreOfArray &#123; 9 10 public static void main(String[] args) &#123; 11 12 // int[] intArray = new int[]&#123;4,3,2,5,6,4,4,7&#125;; 13 // int[] intArray = new int[]&#123;1,2,3,4,5,6&#125;; 14 int[] intArray = new int[]&#123;3,1,2,7,3,8,4,9,5,6&#125;; 15 16 int totalWater = getArrayWater(intArray); 17 System.out.println(totalWater); 18 &#125; 19 20 /** 21 * 求出数组中的水数 22 */ 23 private static int getArrayWater(int[] intArray) &#123; 24 25 int findMaxValueOfArray = findMaxValueOfArray(intArray); 26 int findMinValueOfArray = findMinValueOfArray(intArray); 27 int length = intArray.length; 28 29 int totalWater = 0; 30 31 // 循环次数就是最大值和最小值的差 32 for(int i=findMinValueOfArray; i&lt;findMaxValueOfArray; i++)&#123; 33 // 循环构造每一层的01数组 34 int[] tempArray = new int[length]; 35 for(int j=0; j&lt;length; j++)&#123; 36 if(intArray[j] &gt; i)&#123; 37 tempArray[j] = 1; 38 &#125;else&#123; 39 tempArray[j] = 0; 40 &#125; 41 &#125; 42 // 获取每一个01数组的合法0个数 43 int waterOfOneZeroArray = getWaterOfOneZeroArray(tempArray); 44 totalWater += waterOfOneZeroArray; 45 &#125; 46 return totalWater; 47 &#125; 48 49 50 /** 51 * 寻找逻辑是：从左右开始各找一个1，然后这两个1之间的所有0的个数，就是水数 52 */ 53 private static int getWaterOfOneZeroArray(int[] tempArray) &#123; 54 55 int length = tempArray.length; 56 int toatalWater = 0; 57 58 // 找左边的1 59 int i = 0; 60 while(i &lt; length)&#123; 61 if(tempArray[i] == 1)&#123; 62 break; 63 &#125; 64 i++; 65 &#125; 66 67 // 从右边开始找1 68 int j=length-1; 69 while(j &gt;= i)&#123; 70 if(tempArray[j] == 1)&#123; 71 break; 72 &#125; 73 j--; 74 &#125; 75 76 // 找以上两个1之间的0的个数。 77 if(i == j || i + 1 == j)&#123; 78 return 0; 79 &#125;else&#123; 80 for(int k=i+1; k&lt;j; k++)&#123; 81 if(tempArray[k] == 0)&#123; 82 toatalWater++; 83 &#125; 84 &#125; 85 return toatalWater; 86 &#125; 87 &#125; 88 89 /** 90 * 91 * 描述：找出一个数组中的最大值 92 */ 93 public static int findMaxValueOfArray(int[] intArray)&#123; 94 int length = intArray.length; 95 if(length == 0)&#123; 96 return 0; 97 &#125;else if(length == 1)&#123; 98 return intArray[0]; 99 &#125;else&#123;100 int max = intArray[0];101 for(int i=1; i&lt;length; i++)&#123;102 if(intArray[i] &gt; max)&#123;103 max = intArray[i];104 &#125;105 &#125;106 return max;107 &#125;108 &#125;109 110 /**111 * 找出一个数组中的最小值112 */113 public static int findMinValueOfArray(int[] intArray)&#123;114 int length = intArray.length;115 if(length == 0)&#123;116 return 0;117 &#125;else if(length == 1)&#123;118 return intArray[0];119 &#125;else&#123;120 int min = intArray[0];121 for(int i=1; i&lt;length; i++)&#123;122 if(intArray[i] &lt; min)&#123;123 min = intArray[i];124 &#125;125 &#125;126 return min;127 &#125;128 &#125;129 &#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（九）HDFS深入理解","slug":"2018-04-09-Hadoop学习之路（九）HDFS深入理解","date":"2018-04-09T02:30:04.000Z","updated":"2019-09-19T02:06:28.058Z","comments":true,"path":"2018-04-09-Hadoop学习之路（九）HDFS深入理解.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-09-Hadoop学习之路（九）HDFS深入理解.html","excerpt":"** Hadoop学习之路（九）HDFS深入理解：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（九）HDFS深入理解","text":"** Hadoop学习之路（九）HDFS深入理解：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（九）HDFS深入理解 &lt;The rest of contents | 余下全文&gt; HDFS的优点和缺点HDFS的优点1、可构建在廉价机器上 通过多副本提高可靠性，提供了容错和恢复机制 服务器节点的宕机是常态 必须理性对象 2、高容错性 数据自动保存多个副本，副本丢失后，自动恢复 HDFS的核心设计思想： 分散均匀存储 + 备份冗余存储 3、适合批处理 移动计算而非数据，数据位置暴露给计算框架 海量数据的计算 任务 最终是一定要被切分成很多的小任务进行 4、适合大数据处理 GB、TB、甚至 PB 级数据，百万规模以上的文件数量，10K+节点规模 5、流式文件访问 一次性写入，多次读取，保证数据一致性 HDFS的缺点不适合以下操作1、低延迟数据访问 比如毫秒级 低延迟与高吞吐率 2、小文件存取 占用 NameNode 大量内存 150b* 1000W = 15E,1.5G 寻道时间超过读取时间 3、并发写入、文件随机修改 一个文件只能有一个写者 仅支持 append 抛出问题：HDFS文件系统为什么不适用于存储小文件？这是和HDFS系统底层设计实现有关系的，HDFS本身的设计就是用来解决海量大文件数据的存储.，他天生喜欢大数据的处理，大文件存储在HDFS中，会被切分成很多的小数据块，任何一个文件不管有多小，都是一个独立的数据块，而这些数据块的信息则是保存在元数据中的，在之前的博客HDFS基础里面介绍过在HDFS集群的namenode中会存储元数据的信息，这里再说一下，元数据的信息主要包括以下3部分： 1）抽象目录树 2）文件和数据块的映射关系，一个数据块的元数据大小大约是150byte 3）数据块的多个副本存储地 而元数据的存储在磁盘（1和2）和内存中（1、2和3），而服务器中的内存是有上限的，举个例子： 有100个1M的文件存储进入HDFS系统，那么数据块的个数就是100个，元数据的大小就是100*150byte，消耗了15000byte的内存，但是只存储了100M的数据。 有1个100M的文件存储进入HDFS系统，那么数据块的个数就是1个，元数据的大小就是150byte，消耗量150byte的内存，存储量100M的数据。 所以说HDFS文件系统不适用于存储小文件。 HDFS的辅助功能HDFS作为一个文件系统。有两个最主要的功能：上传和下载。而为了保障这两个功能的完美和高效实现，HDFS提供了很多的辅助功能 1.心跳机制普通话讲解1、 Hadoop 是 Master/Slave 结构，Master 中有 NameNode 和 ResourceManager，Slave 中有 Datanode 和 NodeManager 2、 Master 启动的时候会启动一个 IPC（Inter-Process Comunication，进程间通信）server 服 务，等待 slave 的链接 3、 Slave 启动时，会主动链接 master 的 ipc server 服务，并且每隔 3 秒链接一次 master，这 个间隔时间是可以调整的，参数为 dfs.heartbeat.interval，这个每隔一段时间去连接一次 的机制，我们形象的称为心跳。Slave 通过心跳汇报自己的信息给 master，master 也通 过心跳给 slave 下达命令， 4、 NameNode 通过心跳得知 Datanode 的状态 ，ResourceManager 通过心跳得知 NodeManager 的状态 5、 如果 master 长时间都没有收到 slave 的心跳，就认为该 slave 挂掉了。！！！！！ 大白话讲解1、DataNode启动的时候会向NameNode汇报信息，就像钉钉上班打卡一样，你打卡之后，你领导才知道你今天来上班了，同样的道理，DataNode也需要向NameNode进行汇报，只不过每次汇报的时间间隔有点短而已，默认是3秒中，DataNode向NameNode汇报的信息有2点，一个是自身DataNode的状态信息，另一个是自身DataNode所持有的所有的数据块的信息。而DataNode是不会知道他保存的所有的数据块副本到底是属于哪个文件，这些都是存储在NameNode的元数据中。 2、按照规定，每个DataNode都是需要向NameNode进行汇报。那么如果从某个时刻开始，某个DataNode再也不向NameNode进行汇报了。 有可能宕机了。因为只要通过网络传输数据，就一定存在一种可能： 丢失 或者 延迟。 3、HDFS的标准： NameNode如果连续10次没有收到DataNode的汇报。 那么NameNode就会认为该DataNode存在宕机的可能。 4、DataNode启动好了之后，会专门启动一个线程，去负责给NameNode发送心跳数据包，如果说整个DataNode没有任何问题，但是仅仅只是当前负责发送信条数据包的线程挂了。NameNode会发送命令向这个DataNode进行确认。查看这个发送心跳数据包的服务是否还能正常运行，而为了保险起见，NameNode会向DataNode确认2遍，每5分钟确认一次。如果2次都没有返回 结果，那么NameNode就会认为DataNode已经GameOver了！！！ 最终NameNode判断一个DataNode死亡的时间计算公式： timeout = 10 * 心跳间隔时间 + 2 * 检查一次消耗的时间 心跳间隔时间：dfs.heartbeat.interval 心跳时间：3s检查一次消耗的时间：heartbeat.recheck.interval checktime : 5min 最终结果默认是630s。 2.安全模式1、HDFS的启动和关闭都是先启动NameNode，在启动DataNode，最后在启动secondarynamenode。 2、决定HDFS集群的启动时长会有两个因素： 1）磁盘元数据的大小 2）datanode的节点个数 当元数据很大，或者 节点个数很多的时候，那么HDFS的启动，需要一段很长的时间，那么在还没有完全启动的时候HDFS能否对外提供服务？ 在HDFS的启动命令start-dfs.sh执行的时候，HDFS会自动进入安全模式 为了确保用户的操作是可以高效的执行成功的，在HDFS发现自身不完整的时候，会进入安全模式。保护自己。 在正常启动之后，如果HDFS发现所有的数据都是齐全的，那么HDFS会启动的退出安全模式 3、对安全模式进行测试 安全模式常用操作命令： 1234567hdfs dfsadmin -safemode leave //强制 NameNode 退出安全模式hdfs dfsadmin -safemode enter //进入安全模式hdfs dfsadmin -safemode get //查看安全模式状态hdfs dfsadmin -safemode wait //等待，一直到安全模式结束 手工进入安全模式进行测试 1、测试创建文件夹 12345[hadoop@hadoop1 ~]$ hdfs dfsadmin -safemode enterSafe mode is ON[hadoop@hadoop1 ~]$ hadoop fs -mkdir -p /xx/yy/zzmkdir: Cannot create directory /xx/yy/zz. Name node is in safe mode.[hadoop@hadoop1 ~]$ 2、测试下载文件 12345678[hadoop@hadoop1 ~]$ lsapps data[hadoop@hadoop1 ~]$ hdfs dfsadmin -safemode getSafe mode is ON[hadoop@hadoop1 ~]$ hadoop fs -get /aa/1.txt ~/1.txt[hadoop@hadoop1 ~]$ ls1.txt apps data[hadoop@hadoop1 ~]$ 3、测试上传 123[hadoop@hadoop1 ~]$ hadoop fs -put 1.txt /a/xx.txtput: Cannot create file/a/xx.txt._COPYING_. Name node is in safe mode.[hadoop@hadoop1 ~]$ 4、得出结论，在安全模式下： 如果一个操作涉及到元数据的修改的话。都不能进行操作 如果一个操作仅仅只是查询。那是被允许的。 所谓的安全模式，仅仅只是保护namenode，而不是保护datanode 3.副本存放策略第一副本：放置在上传文件的DataNode上；如果是集群外提交，则随机挑选一台磁盘不太慢、CPU不太忙的节点上；第二副本：放置在于第一个副本不同的机架的节点上；第三副本：与第二个副本相同机架的不同节点上；如果还有更多的副本：随机放在节点中； 4.负载均衡负载均衡理想状态：节点均衡、机架均衡和磁盘均衡。 Hadoop的HDFS集群非常容易出现机器与机器之间磁盘利用率不平衡的情况，例如：当集群内新增、删除节点，或者某个节点机器内硬盘存储达到饱和值。当数据不平衡时，Map任务可能会分配到没有存储数据的机器，这将导致网络带宽的消耗，也无法很好的进行本地计算。当HDFS负载不均衡时，需要对HDFS进行数据的负载均衡调整，即对各节点机器上数据的存储分布进行调整。从而，让数据均匀的分布在各个DataNode上，均衡IO性能，防止热点的发生。进行数据的负载均衡调整，必须要满足如下原则： 数据平衡不能导致数据块减少，数据块备份丢失 管理员可以中止数据平衡进程 每次移动的数据量以及占用的网络资源，必须是可控的 数据均衡过程，不能影响namenode的正常工作 负载均衡的原理数据均衡过程的核心是一个数据均衡算法，该数据均衡算法将不断迭代数据均衡逻辑，直至集群内数据均衡为止。该数据均衡算法每次迭代的逻辑如下： 步骤分析如下： 数据均衡服务（Rebalancing Server）首先要求 NameNode 生成 DataNode 数据分布分析报告,获取每个DataNode磁盘使用情况 Rebalancing Server汇总需要移动的数据分布情况，计算具体数据块迁移路线图。数据块迁移路线图，确保网络内最短路径 开始数据块迁移任务，Proxy Source Data Node复制一块需要移动数据块 将复制的数据块复制到目标DataNode上 删除原始数据块 目标DataNode向Proxy Source Data Node确认该数据块迁移完成 Proxy Source Data Node向Rebalancing Server确认本次数据块迁移完成。然后继续执行这个过程，直至集群达到数据均衡标准 DataNode分组在第2步中，HDFS会把当前的DataNode节点,根据阈值的设定情况划分到Over、Above、Below、Under四个组中。在移动数据块的时候，Over组、Above组中的块向Below组、Under组移动。四个组定义如下： Over组：此组中的DataNode的均满足 DataNode_usedSpace_percent &gt; Cluster_usedSpace_percent + threshold Above组：此组中的DataNode的均满足 Cluster_usedSpace_percent + threshold &gt; DataNode_ usedSpace _percent &gt;Cluster_usedSpace_percent Below组：此组中的DataNode的均满足 Cluster_usedSpace_percent &gt; DataNode_ usedSpace_percent &gt; Cluster_ usedSpace_percent – threshold Under组：此组中的DataNode的均满足 Cluster_usedSpace_percent – threshold &gt; DataNode_usedSpace_percent Hadoop HDFS 数据自动平衡脚本使用方法在Hadoop中，包含一个start-balancer.sh脚本，通过运行这个工具，启动HDFS数据均衡服务。该工具可以做到热插拔，即无须重启计算机和 Hadoop 服务。HadoopHome/bin目录下的start−balancer.sh脚本就是该任务的启动脚本。启动命令为：‘HadoopHome/bin目录下的start−balancer.sh脚本就是该任务的启动脚本。启动命令为：‘Hadoop_home/bin/start-balancer.sh –threshold` 影响Balancer的几个参数： -threshold 默认设置：10，参数取值范围：0-100 参数含义：判断集群是否平衡的阈值。理论上，该参数设置的越小，整个集群就越平衡 dfs.balance.bandwidthPerSec 默认设置：1048576（1M/S） 参数含义：Balancer运行时允许占用的带宽 示例如下： 12345678#启动数据均衡，默认阈值为 10%$Hadoop_home/bin/start-balancer.sh#启动数据均衡，阈值 5%bin/start-balancer.sh –threshold 5#停止数据均衡$Hadoop_home/bin/stop-balancer.sh 在hdfs-site.xml文件中可以设置数据均衡占用的网络带宽限制 12345&lt;property&gt;&lt;name&gt;dfs.balance.bandwidthPerSec&lt;/name&gt;&lt;value&gt;1048576&lt;/value&gt;&lt;description&gt; Specifies the maximum bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second. &lt;/description&gt;&lt;/property&gt;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（八）在eclispe上搭建Hadoop开发环境","slug":"2018-04-08-Hadoop学习之路（八）在eclispe上搭建Hadoop开发环境","date":"2018-04-08T02:30:04.000Z","updated":"2019-09-18T16:45:49.847Z","comments":true,"path":"2018-04-08-Hadoop学习之路（八）在eclispe上搭建Hadoop开发环境.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-08-Hadoop学习之路（八）在eclispe上搭建Hadoop开发环境.html","excerpt":"** Hadoop学习之路（八）在eclispe上搭建Hadoop开发环境：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（八）在eclispe上搭建Hadoop开发环境","text":"** Hadoop学习之路（八）在eclispe上搭建Hadoop开发环境：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（八）在eclispe上搭建Hadoop开发环境 &lt;The rest of contents | 余下全文&gt; 一、添加插件将hadoop-eclipse-plugin-2.7.5.jar放入eclipse的plugins文件夹中 二、在Windows上安装Hadoop2.7.5版本最好与Linux集群中的hadoop版本保持一致 1、将hadoop-2.7.5-centos-6.7.tar.gz解压到Windows上的C盘software目录中 2、配置hadoop的环境变量HADOOP_HOME=C:\\software\\hadoop-2.7.5 Path=C:\\software\\hadoop-2.7.5\\bin 3、修改Hadoop安装目录C:\\software\\hadoop-2.7.5\\etc\\hadoop中hadoop-env.cmd 4、查看Hadoop版本 5、添加Windows支持文件因为安装的Hadoop编译的版本是CentOS6.7的版本，在Windows上运行需要添加文件 1）winutils.exe 放在windows平台中你安装的hadoop的bin目录下 2) hadoop.dll 放在windows操作系统的 c:/windows/system32目录下 6、重新启动eclipse三、eclipse中的配置1、重新启动eclipse,打开windows-&gt;Preferences的Hadoop Map/Reduce中设置安装目录 2、打开Windows-&gt;Open Perspective中的Map/Reduce，在此perspective下进行hadoop程序开发 3、打开Windows-&gt;Show View中的Map/Reduce Locations，如下图右键选择New Hadoop location…新建hadoop连接。 4、配置相关信息 5、配置成功之后再右侧显示如下 四、创建HDFS项目1、创建一个java project 2、添加jar包这里使用第二种有三种方式可以往项目中添加jar依赖: 1）直接创建一个lib文件夹，然后放入对应的依赖包，最后add build path 优点：移植方便 缺点：项目臃肿 2）在eclipse中创建user libarary, 然后引入 优点：解决了不同项目中的相同jar的重复依赖问题， 不是直接放入，是引入的方式 缺点：移植不方便 3）最后直接使用maven管理jar依赖 完美解决方案：使用maven 我们在项目中只需要编写好：pom.xml文件即可 目前只是操作HDFS，所以只需要引入common和HDFS相关的jar包即可。 Hadoop的common、hdfs、MapReduce、yarn的相关jar包的位置在安装目录的C:\\software\\hadoop-2.7.5\\share\\hadoop文件夹中，各自文件夹下的jar包是核心jar包，lib下的jar包是核心jar包的依赖jar包，都需要引入 hdfs的jar包用相同的方法引入 这样项目就成功引入了common和hdfs相关的jar包 3、创建测试类 View Code 测试之前 测试之后","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（七）Hadoop集群shell常用命令","slug":"2018-04-07-Hadoop学习之路（七）Hadoop集群shell常用命令","date":"2018-04-07T02:30:04.000Z","updated":"2019-09-18T16:41:13.056Z","comments":true,"path":"2018-04-07-Hadoop学习之路（七）Hadoop集群shell常用命令.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-07-Hadoop学习之路（七）Hadoop集群shell常用命令.html","excerpt":"** Hadoop学习之路（七）Hadoop集群shell常用命令：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（七）Hadoop集群shell常用命令","text":"** Hadoop学习之路（七）Hadoop集群shell常用命令：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（七）Hadoop集群shell常用命令 &lt;The rest of contents | 余下全文&gt; Hadoop常用命令启动HDFS集群12345678910[hadoop@hadoop1 ~]$ start-dfs.shStarting namenodes on [hadoop1]hadoop1: starting namenode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-namenode-hadoop1.outhadoop2: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop2.outhadoop3: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop3.outhadoop4: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop4.outhadoop1: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop1.outStarting secondary namenodes [hadoop3]hadoop3: starting secondarynamenode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-secondarynamenode-hadoop3.out[hadoop@hadoop1 ~]$ 启动YARN集群12345678[hadoop@hadoop4 ~]$ start-yarn.shstarting yarn daemonsstarting resourcemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-resourcemanager-hadoop4.outhadoop2: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop2.outhadoop3: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop3.outhadoop4: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop4.outhadoop1: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop1.out[hadoop@hadoop4 ~]$ 查看HDFS系统根目录12345[hadoop@hadoop1 ~]$ hadoop fs -ls /Found 2 itemsdrwxr-xr-x - hadoop supergroup 0 2018-03-03 11:42 /testdrwx------ - hadoop supergroup 0 2018-03-03 11:42 /tmp[hadoop@hadoop1 ~]$ 创建文件夹1234567[hadoop@hadoop1 ~]$ hadoop fs -mkdir /a[hadoop@hadoop1 ~]$ hadoop fs -ls /Found 3 itemsdrwxr-xr-x - hadoop supergroup 0 2018-03-08 11:09 /adrwxr-xr-x - hadoop supergroup 0 2018-03-03 11:42 /testdrwx------ - hadoop supergroup 0 2018-03-03 11:42 /tmp[hadoop@hadoop1 ~]$ 级联创建文件夹12[hadoop@hadoop1 ~]$ hadoop fs -mkdir -p /aa/bb/cc[hadoop@hadoop1 ~]$ 查看hsdf系统根目录下的所有文件包括子文件夹里面的文件[hadoop@hadoop1 ~]$ hadoop fs -ls -R /aadrwxr-xr-x - hadoop supergroup 0 2018-03-08 11:12 /aa/bbdrwxr-xr-x - hadoop supergroup 0 2018-03-08 11:12 /aa/bb/cc[hadoop@hadoop1 ~]$ 上传文件[hadoop@hadoop1 ~]$ lsapps data words.txt[hadoop@hadoop1 ~]$ hadoop fs -put words.txt /aa[hadoop@hadoop1 ~]$ hadoop fs -copyFromLocal words.txt /aa/bb[hadoop@hadoop1 ~]$ 下载文件1234567[hadoop@hadoop1 ~]$ hadoop fs -get /aa/words.txt ~/newwords.txt[hadoop@hadoop1 ~]$ lsapps data newwords.txt words.txt[hadoop@hadoop1 ~]$ hadoop fs -copyToLocal /aa/words.txt ~/newwords1.txt[hadoop@hadoop1 ~]$ lsapps data newwords1.txt newwords.txt words.txt[hadoop@hadoop1 ~]$ 合并下载12345678910[hadoop@hadoop1 ~]$ hadoop fs -getmerge /aa/words.txt /aa/bb/words.txt ~/2words.txt[hadoop@hadoop1 ~]$ ll总用量 24-rw-r--r--. 1 hadoop hadoop 78 3月 8 12:42 2words.txtdrwxrwxr-x. 3 hadoop hadoop 4096 3月 3 10:30 appsdrwxrwxr-x. 3 hadoop hadoop 4096 3月 3 11:40 data-rw-r--r--. 1 hadoop hadoop 39 3月 8 11:49 newwords1.txt-rw-r--r--. 1 hadoop hadoop 39 3月 8 11:48 newwords.txt-rw-rw-r--. 1 hadoop hadoop 39 3月 3 11:31 words.txt[hadoop@hadoop1 ~]$ 复制从HDFS一个路径拷贝到HDFS另一个路径 123456[hadoop@hadoop1 ~]$ hadoop fs -ls /a[hadoop@hadoop1 ~]$ hadoop fs -cp /aa/words.txt /a[hadoop@hadoop1 ~]$ hadoop fs -ls /aFound 1 items-rw-r--r-- 2 hadoop supergroup 39 2018-03-08 12:46 /a/words.txt[hadoop@hadoop1 ~]$ 移动在HDFS目录中移动文件 123456[hadoop@hadoop1 ~]$ hadoop fs -ls /aa/bb/cc[hadoop@hadoop1 ~]$ hadoop fs -mv /a/words.txt /aa/bb/cc[hadoop@hadoop1 ~]$ hadoop fs -ls /aa/bb/ccFound 1 items-rw-r--r-- 2 hadoop supergroup 39 2018-03-08 12:46 /aa/bb/cc/words.txt[hadoop@hadoop1 ~]$ 删除删除文件或文件夹 12345[hadoop@hadoop1 ~]$ hadoop fs -rm /aa/bb/cc/words.txt18/03/08 12:49:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.Deleted /aa/bb/cc/words.txt[hadoop@hadoop1 ~]$ hadoop fs -ls /aa/bb/cc[hadoop@hadoop1 ~]$ 删除空目录 12345[hadoop@hadoop1 ~]$ hadoop fs -rmdir /aa/bb/cc/[hadoop@hadoop1 ~]$ hadoop fs -ls /aa/bb/Found 1 items-rw-r--r-- 2 hadoop supergroup 39 2018-03-08 11:43 /aa/bb/words.txt[hadoop@hadoop1 ~]$ 强制删除 123456789[hadoop@hadoop1 ~]$ hadoop fs -rm /aa/bb/rm: `/aa/bb&apos;: Is a directory[hadoop@hadoop1 ~]$ hadoop fs -rm -r /aa/bb/18/03/08 12:51:31 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.Deleted /aa/bb[hadoop@hadoop1 ~]$ hadoop fs -ls /aaFound 1 items-rw-r--r-- 2 hadoop supergroup 39 2018-03-08 11:41 /aa/words.txt[hadoop@hadoop1 ~]$ 从本地剪切文件到HDFS上123456[hadoop@hadoop1 ~]$ lsapps data hello.txt[hadoop@hadoop1 ~]$ hadoop fs -moveFromLocal ~/hello.txt /aa[hadoop@hadoop1 ~]$ lsapps data[hadoop@hadoop1 ~]$ 追加文件追加之前hello.txt到words.txt之前 12[hadoop@hadoop1 ~]$ hadoop fs -appendToFile ~/hello.txt /aa/words.txt[hadoop@hadoop1 ~]$ 追加之前hello.txt到words.txt之后 查看文件内容12345[hadoop@hadoop1 ~]$ hadoop fs -cat /aa/hello.txthellohellohello[hadoop@hadoop1 ~]$ chgrp使用方法：hadoop fs -chgrp [-R] GROUP URI [URI …] Change group association of files. With -R, make the change recursively through the directory structure. The user must be the owner of files, or else a super-user. Additional information is in the Permissions User Guide. –&gt; 改变文件所属的组。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。 chmod使用方法：hadoop fs -chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; URI [URI …] 改变文件的权限。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。 chown使用方法：hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ] 改变文件的拥有者。使用-R将使改变在目录结构下递归进行。命令的使用者必须是超级用户。更多的信息请参见HDFS权限用户指南。 du使用方法：hadoop fs -du URI [URI …] 显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。示例：hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://host:port/user/hadoop/dir1返回值：成功返回0，失败返回-1。 dus使用方法：hadoop fs -dus 显示文件的大小。 expunge使用方法：hadoop fs -expunge 清空回收站。请参考HDFS设计文档以获取更多关于回收站特性的信息。 setrep使用方法：hadoop fs -setrep [-R] 改变一个文件的副本系数。-R选项用于递归改变目录下所有文件的副本系数。 示例： hadoop fs -setrep -w 3 -R /user/hadoop/dir1 返回值： 成功返回0，失败返回-1。 tail使用方法：hadoop fs -tail [-f] URI 将文件尾部1K字节的内容输出到stdout。支持-f选项，行为和Unix中一致。 示例： hadoop fs -tail pathname 返回值：成功返回0，失败返回-1。 test使用方法：hadoop fs -test -[ezd] URI 选项：-e 检查文件是否存在。如果存在则返回0。-z 检查文件是否是0字节。如果是则返回0。-d 如果路径是个目录，则返回1，否则返回0。 示例： hadoop fs -test -e filename 查看集群的工作状态123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687[hadoop@hadoop1 ~]$ hdfs dfsadmin -reportConfigured Capacity: 73741402112 (68.68 GB)Present Capacity: 52781039616 (49.16 GB)DFS Remaining: 52780457984 (49.16 GB)DFS Used: 581632 (568 KB)DFS Used%: 0.00%Under replicated blocks: 0Blocks with corrupt replicas: 0Missing blocks: 0Missing blocks (with replication factor 1): 0-------------------------------------------------Live datanodes (4):Name: 192.168.123.102:50010 (hadoop1)Hostname: hadoop1Decommission Status : NormalConfigured Capacity: 18435350528 (17.17 GB)DFS Used: 114688 (112 KB)Non DFS Used: 4298661888 (4.00 GB)DFS Remaining: 13193277440 (12.29 GB)DFS Used%: 0.00%DFS Remaining%: 71.57%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Thu Mar 08 13:05:11 CST 2018Name: 192.168.123.105:50010 (hadoop4)Hostname: hadoop4Decommission Status : NormalConfigured Capacity: 18435350528 (17.17 GB)DFS Used: 49152 (48 KB)Non DFS Used: 4295872512 (4.00 GB)DFS Remaining: 13196132352 (12.29 GB)DFS Used%: 0.00%DFS Remaining%: 71.58%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Thu Mar 08 13:05:13 CST 2018Name: 192.168.123.103:50010 (hadoop2)Hostname: hadoop2Decommission Status : NormalConfigured Capacity: 18435350528 (17.17 GB)DFS Used: 233472 (228 KB)Non DFS Used: 4295700480 (4.00 GB)DFS Remaining: 13196120064 (12.29 GB)DFS Used%: 0.00%DFS Remaining%: 71.58%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Thu Mar 08 13:05:11 CST 2018Name: 192.168.123.104:50010 (hadoop3)Hostname: hadoop3Decommission Status : NormalConfigured Capacity: 18435350528 (17.17 GB)DFS Used: 184320 (180 KB)Non DFS Used: 4296941568 (4.00 GB)DFS Remaining: 13194928128 (12.29 GB)DFS Used%: 0.00%DFS Remaining%: 71.57%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Thu Mar 08 13:05:10 CST 2018[hadoop@hadoop1 ~]$","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（六）HDFS基础","slug":"2018-04-06-Hadoop学习之路（六）HDFS基础","date":"2018-04-06T02:30:04.000Z","updated":"2019-09-18T16:37:11.280Z","comments":true,"path":"2018-04-06-Hadoop学习之路（六）HDFS基础.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-06-Hadoop学习之路（六）HDFS基础.html","excerpt":"** Hadoop学习之路（六）HDFS基础：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（六）HDFS基础","text":"** Hadoop学习之路（六）HDFS基础：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（六）HDFS基础 &lt;The rest of contents | 余下全文&gt; HDFS前言HDFS：Hadoop Distributed File System ，Hadoop分布式文件系统，主要用来解决海量数据的存储问题 设计思想1、分散均匀存储 dfs.blocksize = 128M 2、备份冗余存储 dfs.replication = 3 在大数据系统中作用为各类分布式运算框架（如：mapreduce，spark，tez，……）提供数据存储服务。 重点概念文件切块，副本存放，元数据 HDFS的概念和特性概念首先，它是一个文件系统，用于存储文件，通过统一的命名空间——目录树来定位文件 其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色； 重要特性（1）HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M （2）HDFS文件系统会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data （3）目录结构及文件分块信息**(元数据)**的管理由namenode节点承担 ——namenode是HDFS集群主节点，负责维护整个hdfs文件系统的目录树，以及每一个路径（文件）所对应的block块信息（block的id，及所在的datanode服务器） （4）文件的各个block的存储管理由datanode节点承担 —- datanode是HDFS集群从节点，每一个block都可以在多个datanode上存储多个副本（副本数量也可以通过参数设置dfs.replication） （5）HDFS是设计成适应一次写入，多次读出的场景，且不支持文件的修改 (注：适合用来做数据分析，并不适合用来做网盘应用，因为，不便修改，延迟大，网络开销大，成本太高) 图解HDFS通过上面的描述我们知道，hdfs很多特点： 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复（默认存3份）。 运行在廉价的机器上 适合大数据的处理。HDFS默认会将文件分割成block，，在hadoop2.x以上版本默认128M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。 如上图所示，HDFS也是按照Master和Slave的结构。分NameNode、SecondaryNameNode、DataNode这几个角色。 NameNode：是Master节点，是大领导。管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间； SecondaryNameNode：是一个小弟，分担大哥namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode。 DataNode：Slave节点，奴隶，干活的。负责存储client发来的数据块block；执行数据块的读写操作。 热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作。 冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失。 fsimage:元数据镜像文件（文件系统的目录树。） edits：元数据的操作日志（针对文件系统做的修改操作记录） namenode内存中存储的是=fsimage+edits。 SecondaryNameNode负责定时默认1小时，从namenode上，获取fsimage和edits来进行合并，然后再发送给namenode。减少namenode的工作量。 HDFS的局限性 1）低延时数据访问。在用户交互性的应用中，应用需要在ms或者几个s的时间内得到响应。由于HDFS为高吞吐率做了设计，也因此牺牲了快速响应。对于低延时的应用，可以考虑使用HBase或者Cassandra。 2）大量的小文件。标准的HDFS数据块的大小是64M，存储小文件并不会浪费实际的存储空间，但是无疑会增加了在NameNode上的元数据，大量的小文件会影响整个集群的性能。 前面我们知道，Btrfs为小文件做了优化-inline file，对于小文件有很好的空间优化和访问时间优化。 3）多用户写入，修改文件。HDFS的文件只能有一个写入者，而且写操作只能在文件结尾以追加的方式进行。它不支持多个写入者，也不支持在文件写入后，对文件的任意位置的修改。 但是在大数据领域，分析的是已经存在的数据，这些数据一旦产生就不会修改，因此，HDFS的这些特性和设计局限也就很容易理解了。HDFS为大数据领域的数据分析，提供了非常重要而且十分基础的文件存储功能。 HDFS保证可靠性的措施 1）冗余备份 每个文件存储成一系列数据块（Block）。为了容错，文件的所有数据块都会有副本（副本数量即复制因子，课配置）（dfs.replication） 2）副本存放 采用机架感知（Rak-aware）的策略来改进数据的可靠性、高可用和网络带宽的利用率 3）心跳检测 NameNode周期性地从集群中的每一个DataNode接受心跳包和块报告，收到心跳包说明该DataNode工作正常 4）安全模式 系统启动时，NameNode会进入一个安全模式。此时不会出现数据块的写操作。 5）数据完整性检测 HDFS客户端软件实现了对HDFS文件内容的校验和（Checksum）检查（dfs.bytes-per-checksum）。 单点故障（单点失效）问题单点故障问题 如果NameNode失效，那么客户端或MapReduce作业均无法读写查看文件 解决方案 1）启动一个拥有文件系统元数据的新NameNode（这个一般不采用，因为复制元数据非常耗时间） 2）配置一对活动-备用（Active-Sandby）NameNode，活动NameNode失效时，备用NameNode立即接管，用户不会有明显中断感觉。 共享编辑日志文件（借助NFS、zookeeper等） DataNode同时向两个NameNode汇报数据块信息 客户端采用特定机制处理 NameNode失效问题，该机制对用户透明","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（五）Hadoop集群搭建模式和各模式问题","slug":"2018-04-05-Hadoop学习之路（五）Hadoop集群搭建模式和各模式问题","date":"2018-04-05T02:30:04.000Z","updated":"2019-09-18T16:35:10.592Z","comments":true,"path":"2018-04-05-Hadoop学习之路（五）Hadoop集群搭建模式和各模式问题.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-05-Hadoop学习之路（五）Hadoop集群搭建模式和各模式问题.html","excerpt":"** Hadoop学习之路（五）Hadoop集群搭建模式和各模式问题：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（五）Hadoop集群搭建模式和各模式问题","text":"** Hadoop学习之路（五）Hadoop集群搭建模式和各模式问题：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（五）Hadoop集群搭建模式和各模式问题 &lt;The rest of contents | 余下全文&gt; 分布式集群的通用问题当前的HDFS和YARN都是一主多从的分布式架构，主从节点—管理者和工作者 问题：如果主节点或是管理者宕机了。会出现什么问题？ 群龙无首，整个集群不可用。所以在一主多从的架构中都会有一个通用的问题： 当集群中的主节点宕机之后，整个集群不可用。这个现象叫做：单点故障。SPOF 单点故障讲述的概念有两点1、如果说宕机的那个节点是从节点，那么整个集群能够继续运行，并且对外提供正常的服务。 2、如果说宕机的那个节点是主节点，那么整个集群就处于宕机状态。 通用的解决方案：高可用 概念：当正在对外提供服务器的主从节点宕机，那么备用的主节点立马上位对外提供服务。无缝的瞬时切换。 皇帝驾崩，太子继位。 集群的搭建的集中通用模式1、单机模式 表示所有的分布式系统都是单机的。 2、伪分布式模式（搭建在了只有一个节点的集群中） 表示集群中的所有角色都分配给了一个节点。 表示整个集群被安装在了只有一个节点的集群中的。 主要用于做快速使用，去模拟分布式的效果。 3、分布式模式 表示集群中的节点会被分配成很多种角色，分散在整个集群中。 主要用于学习测试等等一些场景中。 4、高可用模式 表示整个集群中的主节点会有多个 注意区分：能够对外提供服务的主节点还是只有一个。其他的主节点全部处于一个热备的状态。 正在对外提供服务的主节点：active 有且仅有一个 热备的主节点：standby 可以有多个 工作模式：1、在任意时刻，只有一个主节点是active的，active的主节点对外提供服务 2、在任意时刻，都应至少有一个standby的主节点，等待active的宕机来进行接替 架构模式：就是为了解决分布式集群中的通用问题SPOF 不管是分布式架构还是高可用架构，都存在一个问题：主从结构—从节点数量太多了。最直观的的问题：造成主节点的工作压力过载，主节点会宕机，当前的这种现象是一种死循环 5、联邦模式 表示当前集群中的主从节点都可以有很多个。 1）主节点：可以有很多个的意思是说：同时对外提供服务的主节点有很多个。 重点：每一个主节点都是用来管理整个集群中的一部分 2）从节点：一定会有很多个。 在联邦模式下还是会有问题： 虽然这个集群中的一个主节点的压力被分摊到了多个主节点。但是这个多个主节点依然会有一个问题：SOFP 安装Hadoop集群中的一些通用问题1、假如安装不成功，并且不知道应该怎么去解决这个安装错误：重装 需要做的处理：处理安装步骤中不同的部分即可。第一次安装和重装时候的不同步骤： 1）到修改配置文件以前，全部都不用动 2）检查配置文件是否都正确 先检查一个节点上的配置文件是否都正确，如果都正确，重新分发一次即可 3）在安装分布式集群时，所有节点中的安装的安装目录和安装者，需要检查和确定 4）删掉数据目录 A. 删除主节点的工作目录：namenode的数据目录 删除即可，只需要在主节点删除即可 B. 删除从节点的工作目录：datanode的数据目录 删除即可，把每个从节点上的这个对应数据目录都删掉 如果以上两份数据都被删除了之后。整个集群当中就相当于没有存储任何的历史数据。所以就是一个全新的集群 5）在确保数据正常和安装包都正常之后，进行重新初始化 重点强调： hadoop集群的初始化，其实就是初始化HDFS集群， 只能在主节点进行初始化 如果你只需要搭建YARN集群，那么是可以不用做初始化的。 6）启动集群 7）验证集群是否成功 Linux环境变量加载的顺序用户环境变量 ：仅仅只是当前用户使用 ~/.bashrc ~/.bash_profile系统环境变量 ：给当前系统中的所有用户使用 /etc/profile 任何普通用户在进行登录的时候：会同时加载几个环境变量的配置文件： 按顺序：1、/etc/profile2、/.bash_profile3、/.bashrc","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（四）Hadoop集群搭建和简单应用","slug":"2018-04-04-Hadoop学习之路（四）Hadoop集群搭建和简单应用","date":"2018-04-04T02:30:04.000Z","updated":"2019-09-18T16:33:01.086Z","comments":true,"path":"2018-04-04-Hadoop学习之路（四）Hadoop集群搭建和简单应用.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-04-Hadoop学习之路（四）Hadoop集群搭建和简单应用.html","excerpt":"** Hadoop学习之路（四）Hadoop集群搭建和简单应用：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（四）Hadoop集群搭建和简单应用","text":"** Hadoop学习之路（四）Hadoop集群搭建和简单应用：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（四）Hadoop集群搭建和简单应用 &lt;The rest of contents | 余下全文&gt; 概念了解主从结构：在一个集群中，会有部分节点充当主服务器的角色，其他服务器都是从服务器的角色，当前这种架构模式叫做主从结构。 主从结构分类： 1、一主多从 2、多主多从 Hadoop中的HDFS和YARN都是主从结构，主从结构中的主节点和从节点有多重概念方式： 1、主节点 从节点 2、master slave 3、管理者 工作者 4、leader follower Hadoop集群中各个角色的名称： 服务 主节点 从节点 HDFS NameNode DataNode YARN ResourceManager NodeManager 集群服务器规划使用4台CentOS-6.7虚拟机进行集群搭建 软件安装步骤概述1、获取安装包 2、解压缩和安装 3、修改配置文件 4、初始化，配置环境变量，启动，验证 Hadoop安装1、规划规划安装用户：hadoop 规划安装目录：/home/hadoop/apps 规划数据目录：/home/hadoop/data 注：apps和data文件夹需要自己单独创建 2、上传解压缩注：使用hadoop用户 123[hadoop@hadoop1 apps]$ lshadoop-2.7.5-centos-6.7.tar.gz[hadoop@hadoop1 apps]$ tar -zxvf hadoop-2.7.5-centos-6.7.tar.gz 3、修改配置文件配置文件目录：/home/hadoop/apps/hadoop-2.7.5/etc/hadoop A. hadoop-env.sh1[hadoop@hadoop1 hadoop]$ vi hadoop-env.sh 修改JAVA_HOME 1export JAVA_HOME=/usr/local/jdk1.8.0_73 B. core-site.xml 1[hadoop@hadoop1 hadoop]$ vi core-site.xml fs.defaultFS ： 这个属性用来指定namenode的hdfs协议的文件系统通信地址，可以指定一个主机+端口，也可以指定为一个namenode服务（这个服务内部可以有多台namenode实现ha的namenode服务 hadoop.tmp.dir : hadoop集群在工作的时候存储的一些临时文件的目录 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/hadoopdata&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; C. hdfs-site.xml 1[hadoop@hadoop1 hadoop]$ vi hdfs-site.xml dfs.namenode.name.dir：namenode数据的存放地点。也就是namenode元数据存放的地方，记录了hdfs系统中文件的元数据。 dfs.datanode.data.dir： datanode数据的存放地点。也就是block块存放的目录了。 dfs.replication：hdfs的副本数设置。也就是上传一个文件，其分割为block块后，每个block的冗余副本个数，默认配置是3。 dfs.secondary.http.address：secondarynamenode 运行节点的信息，和 namenode 不同节点 12345678910111213141516171819202122232425&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/hadoopdata/name&lt;/value&gt; &lt;description&gt;为了保证元数据的安全一般配置多个不同目录&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/hadoopdata/data&lt;/value&gt; &lt;description&gt;datanode 的数据存储目录&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;description&gt;HDFS 的数据块的副本存储个数, 默认是3&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.http.address&lt;/name&gt; &lt;value&gt;hadoop3:50090&lt;/value&gt; &lt;description&gt;secondarynamenode 运行节点的信息，和 namenode 不同节点&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; D. mapred-site.xml 12[hadoop@hadoop1 hadoop]$ cp mapred-site.xml.template mapred-site.xml[hadoop@hadoop1 hadoop]$ vi mapred-site.xml mapreduce.framework.name：指定mr框架为yarn方式,Hadoop二代MP也基于资源管理系统Yarn来运行 。 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; E. yarn-site.xml 1[hadoop@hadoop1 hadoop]$ vi yarn-site.xml yarn.resourcemanager.hostname：yarn总管理器的IPC通讯地址 yarn.nodemanager.aux-services： 12345678910111213141516&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop4&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;description&gt;YARN 集群为 MapReduce 程序提供的 shuffle 服务&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; F. slaves 12345[hadoop@hadoop1 hadoop]$ vi slaves hadoop1hadoop2hadoop3hadoop4 4、把安装包分别分发给其他的节点重点强调： 每台服务器中的hadoop安装包的目录必须一致， 安装包的配置信息还必须保持一致重点强调： 每台服务器中的hadoop安装包的目录必须一致， 安装包的配置信息还必须保持一致重点强调： 每台服务器中的hadoop安装包的目录必须一致， 安装包的配置信息还必须保持一致 123[hadoop@hadoop1 hadoop]$ scp -r ~/apps/hadoop-2.7.5/ hadoop2:~/apps/[hadoop@hadoop1 hadoop]$ scp -r ~/apps/hadoop-2.7.5/ hadoop3:~/apps/[hadoop@hadoop1 hadoop]$ scp -r ~/apps/hadoop-2.7.5/ hadoop4:~/apps/ 注意：上面的命令等同于下面的命令 1[hadoop@hadoop1 hadoop]$ scp -r ~/apps/hadoop-2.7.5/ hadoop@hadoop2:~/apps/ 5、配置Hadoop环境变量千万注意： 1、如果你使用root用户进行安装。 vi /etc/profile 即可 系统变量 2、如果你使用普通用户进行安装。 vi ~/.bashrc 用户变量 123[hadoop@hadoop1 ~]$ vi .bashrcexport HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin: 使环境变量生效 1[hadoop@hadoop1 bin]$ source ~/.bashrc 6、查看hadoop版本12345678[hadoop@hadoop1 bin]$ hadoop versionHadoop 2.7.5Subversion Unknown -r UnknownCompiled by root on 2017-12-24T05:30ZCompiled with protoc 2.5.0From source with checksum 9f118f95f47043332d51891e37f736e9This command was run using /home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/hadoop-common-2.7.5.jar[hadoop@hadoop1 bin]$ 7、Hadoop初始化注意：HDFS初始化只能在主节点上进行 1[hadoop@hadoop1 ~]$ hadoop namenode -format 8、启动A. 启动HDFS 注意：不管在集群中的那个节点都可以 12345678910[hadoop@hadoop1 ~]$ start-dfs.shStarting namenodes on [hadoop1]hadoop1: starting namenode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-namenode-hadoop1.outhadoop3: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop3.outhadoop2: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop2.outhadoop4: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop4.outhadoop1: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop1.outStarting secondary namenodes [hadoop3]hadoop3: starting secondarynamenode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-secondarynamenode-hadoop3.out[hadoop@hadoop1 ~]$ B. 启动YARN 注意：只能在主节点中进行启动 12345678[hadoop@hadoop4 ~]$ start-yarn.shstarting yarn daemonsstarting resourcemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-resourcemanager-hadoop4.outhadoop2: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop2.outhadoop3: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop3.outhadoop4: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop4.outhadoop1: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop1.out[hadoop@hadoop4 ~]$ 9、查看4台服务器的进程hadoop1 hadoop2 hadoop3 hadoop4 10、启动HDFS和YARN的web管理界面HDFS : http://192.168.123.102:50070YARN ： http://hadoop05:8088 疑惑： fs.defaultFS = hdfs://hadoop02:9000 解答：客户单访问HDFS集群所使用的URL地址 同时，HDFS提供了一个web管理界面 端口：50070 HDFS界面 点击Datanodes可以查看四个节点 YARN界面 点击Nodes可以查看节点 Hadoop的简单使用创建文件夹在HDFS上创建一个文件夹/test/input 1[hadoop@hadoop1 ~]$ hadoop fs -mkdir -p /test/input 查看创建的文件夹1234567[hadoop@hadoop1 ~]$ hadoop fs -ls /Found 1 itemsdrwxr-xr-x - hadoop supergroup 0 2018-03-03 11:33 /test[hadoop@hadoop1 ~]$ hadoop fs -ls /testFound 1 itemsdrwxr-xr-x - hadoop supergroup 0 2018-03-03 11:33 /test/input[hadoop@hadoop1 ~]$ 上传文件创建一个文件words.txt 1234[hadoop@hadoop1 ~]$ vi words.txthello zhangsanhello lisihello wangwu 上传到HDFS的/test/input文件夹中 1[hadoop@hadoop1 ~]$ hadoop fs -put ~/words.txt /test/input 查看是否上传成功 1234[hadoop@hadoop1 ~]$ hadoop fs -ls /test/inputFound 1 items-rw-r--r-- 2 hadoop supergroup 39 2018-03-03 11:37 /test/input/words.txt[hadoop@hadoop1 ~]$ 下载文件将刚刚上传的文件下载到~/data文件夹中 1[hadoop@hadoop1 ~]$ hadoop fs -get /test/input/words.txt ~/data 查看是否下载成功 123[hadoop@hadoop1 ~]$ ls datahadoopdata words.txt[hadoop@hadoop1 ~]$ 运行一个mapreduce的例子程序： wordcount1[hadoop@hadoop1 ~]$ hadoop jar ~/apps/hadoop-2.7.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.5.jar wordcount /test/input /test/output 在YARN Web界面查看 查看结果 12345678910[hadoop@hadoop1 ~]$ hadoop fs -ls /test/outputFound 2 items-rw-r--r-- 2 hadoop supergroup 0 2018-03-03 11:42 /test/output/_SUCCESS-rw-r--r-- 2 hadoop supergroup 35 2018-03-03 11:42 /test/output/part-r-00000[hadoop@hadoop1 ~]$ hadoop fs -cat /test/output/part-r-00000hello 3lisi 1wangwu 1zhangsan 1[hadoop@hadoop1 ~]$","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（三）Hadoop-2.7.5在CentOS-6.7上的编译","slug":"2018-04-02-Hadoop学习之路（三）Hadoop-2.7.5在CentOS-6.7上的编译","date":"2018-04-03T02:30:04.000Z","updated":"2019-09-19T03:36:10.019Z","comments":true,"path":"2018-04-02-Hadoop学习之路（三）Hadoop-2.7.5在CentOS-6.7上的编译.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-02-Hadoop学习之路（三）Hadoop-2.7.5在CentOS-6.7上的编译.html","excerpt":"** Hadoop学习之路（三）Hadoop-2.7.5在CentOS-6.7上的编译：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（三）Hadoop-2.7.5在CentOS-6.7上的编译","text":"** Hadoop学习之路（三）Hadoop-2.7.5在CentOS-6.7上的编译：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（三）Hadoop-2.7.5在CentOS-6.7上的编译 &lt;The rest of contents | 余下全文&gt; 下载Hadoop源码1、登录官网 2、确定你要安装的软件的版本一个选取原则： 不新不旧的稳定版本 几个标准： 1）一般来说，刚刚发布的大版本都是有很多问题 2）应该选择某个大版本中的最后一个小版本 阅读编译文档1、准备一个hadoop源码包，我选择的hadoop的版本是：hadoop-2.7.5-src.tar.gz，在hadoop-2.7.5-src.tar.gz的源码包根目录下有一个文档叫做BUINDING.txt，这其中说明了编译hadoop所需要的一些编译环境相关的东西。不同的hadoop版本的要求都不一样。对应的版本参照BUINDING.txt。 123456789101112Requirements:* Unix System* JDK 1.7+* Maven 3.0 or later* Findbugs 1.3.9 (if running findbugs)* ProtocolBuffer 2.5.0* CMake 2.6 or newer (if compiling native code), must be 3.0 or newer on Mac* Zlib devel (if compiling native code)* openssl devel ( if compiling native hadoop-pipes and to get the best HDFS encryption performance )* Linux FUSE (Filesystem in Userspace) version 2.6 or above ( if compiling fuse_dfs )* Internet connection for first build (to fetch all Maven and Hadoop dependencies) 软件安装对应以上需求，我们准备好所要求版本的这些软件。 JDK的安装选择版本：jdk1.8.0_73 安装依赖包根据编译指导文件BUILDING.txt，安装相关依赖程序包 1[root@master soft]# yum -y install gcc-c++ build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-devua svn openssl-devel ncurses-devel 安装Maven编译要求：Maven 3.0 or later安装软件：apache-maven-3.0.5-bin.tar.gz 1`[root@hadoop1 soft]# ls``apache-maven-3.3.9-bin.tar.gz``[root@hadoop1 soft]# chmod 755 apache-maven-3.3.9-bin.tar.gz``[root@hadoop1 soft]# tar -zxvf apache-maven-3.3.9-bin.tar.gz &lt;br&gt;。。。&lt;br&gt;[root@hadoop1 soft]# mv apache-maven-3.3.9 /opt/&lt;br&gt;[root@hadoop1 soft]# vi /etc/profile` 配置mvn的环境变量 export M2_HOME=/opt/apache-maven-3.3.export PATH=$PATH:$M2_HOME/bin 测试 12345678[root@hadoop1 soft]# mvn -vApache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)Maven home: /opt/apache-maven-3.3.9Java version: 1.8.0_73, vendor: Oracle CorporationJava home: /usr/local/jdk1.8.0_73/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: &quot;linux&quot;, version: &quot;2.6.32-573.el6.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot;[root@hadoop1 soft]# 安装Findbugs编译要求：Findbugs 1.3.9安装软件：findbugs-3.0.1.tar.gz 1234[root@hadoop1 soft]# lsfindbugs-3.0.1.tar.gz[root@hadoop1 soft]# chmod 755 findbugs-3.0.1.tar.gz [root@hadoop1 soft]# tar -zxvf findbugs-3.0.1.tar.gz -C /opt 配置Findbugs环境变量 1[root@hadoop1 soft]# vi /etc/profile export FINDBUGS_HOME=/opt/findbugs-3.0.1export PATH=$PATH:$FINDBUGS_HOME/bin 测试 123[root@hadoop1 soft]# findbugs -version3.0.1[root@hadoop1 soft]# 安装ProtocolBuffer编译要求：ProtocolBuffer 2.5.0安装软件：protobuf-2.5.0.tar.gz，不建议用其它版本 1234[root@hadoop1 soft]# lsprotobuf-2.5.0.tar.gz[root@hadoop1 soft]# chmod 755 protobuf-2.5.0.tar.gz [root@hadoop1 soft]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt[root@hadoop1 soft]# cd /opt/protobuf-2.5.0/[root@hadoop1 protobuf-2.5.0]# ./configure [root@hadoop1 protobuf-2.5.0]# make[root@hadoop1 protobuf-2.5.0]# make install 测试 123[root@hadoop1 protobuf-2.5.0]# protoc --versionlibprotoc 2.5.0[root@hadoop1 protobuf-2.5.0]# 修改maven的配置文件，添加maven的下载源[root@hadoop1 protobuf-2.5.0]# cd /opt/apache-maven-3.3.9/conf/[root@hadoop1 conf]# vi settings.xml 在mirrors中添加alimaven的下载源 123456789101112131415161718192021&lt;mirrors&gt; &lt;!-- mirror | Specifies a repository mirror site to use instead of a given repository. The repository that | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used | for inheritance and direct lookup purposes, and must be unique across the set of mirrors. | &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt; &lt;/mirror&gt; --&gt;&lt;/mirrors&gt; 安装Ant123[root@hadoop1 soft]# lsapache-ant-1.9.4-bin.tar.gz hadoop-2.7.5-src[root@hadoop1 soft]# tar -zxvf apache-ant-1.9.4-bin.tar.gz -C /opt/ 配置环境变量 1234[root@hadoop1 apache-ant-1.9.4]# vi /etc/profile#Antexport ANT_HOME=/opt/apache-ant-1.9.4export PATH=$PATH:$ANT_HOME/bin 检测 123[root@hadoop1 apache-ant-1.9.4]# ant -versionApache Ant(TM) version 1.9.4 compiled on April 29 2014[root@hadoop1 apache-ant-1.9.4]# 安装Snappy解压 123[root@hadoop1 soft]# lshadoop-2.7.5-src snappy-1.1.1.tar.gz[root@hadoop1 soft]# tar -zxvf snappy-1.1.1.tar.gz -C /opt/ 安装 1234[root@hadoop1 soft]# cd /opt/snappy-1.1.1/[root@hadoop1 snappy-1.1.1]# ./configure [root@hadoop1 snappy-1.1.1]# make [root@hadoop1 snappy-1.1.1]# make install 查看snappy文件库 1234567[root@hadoop1 snappy-1.1.1]# ls -lh /usr/local/lib | grep snappy-rw-r--r-- 1 root root 228K 3月 3 09:51 libsnappy.a-rwxr-xr-x 1 root root 953 3月 3 09:51 libsnappy.lalrwxrwxrwx 1 root root 18 3月 3 09:51 libsnappy.so -&gt; libsnappy.so.1.2.0lrwxrwxrwx 1 root root 18 3月 3 09:51 libsnappy.so.1 -&gt; libsnappy.so.1.2.0-rwxr-xr-x 1 root root 145K 3月 3 09:51 libsnappy.so.1.2.0[root@hadoop1 snappy-1.1.1]# 开始编译hadoop12[root@hadoop1 soft]# ls[root@hadoop1 soft]# tar -zxvf hadoop-2.7.5-src.tar.gz 在编译之前防止java.lang.OutOfMemoryError:Java heap space堆栈问题，在centos系统中执行命令 1[root@hadoop1 snappy-1.1.1]# export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot; 进入源码包下，执行命令进行编译 12[root@hadoop1 snappy-1.1.1]# cd /soft/hadoop-2.7.5-src/[root@hadoop1 hadoop-2.7.5-src]# mvn package -Pdist,native,docs -DskipTests -Dtar 如果中途编译失败，并且不要文档的话，请使用这个命令： 1[root@master ~]# mvn clear package -Pdist,native -DskipTests -Dtar -Dsnappy.lib=/usr/local/lib -Dbundle.snappy -Drequire.openssl 编译成功之后，hadoop-2.7.5.tar.gz位于/soft/hadoop-2.7.5-src/hadoop-dist/target目录下，这是编译后文件夹的状态 至此，大功告成！！！","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Hadoop学习之路（二）Hadoop发展背景","slug":"2018-04-01-Hadoop学习之路（二）Hadoop发展背景","date":"2018-04-02T02:30:04.000Z","updated":"2019-09-19T03:36:16.808Z","comments":true,"path":"2018-04-01-Hadoop学习之路（二）Hadoop发展背景.html","link":"","permalink":"http://zhangfuxin.cn/2018-04-01-Hadoop学习之路（二）Hadoop发展背景.html","excerpt":"** Hadoop学习之路（二）Hadoop发展背景：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二）Hadoop发展背景","text":"** Hadoop学习之路（二）Hadoop发展背景：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop学习之路（二）Hadoop发展背景 &lt;The rest of contents | 余下全文&gt; Hadoop产生的背景 HADOOP最早起源于Nutch。Nutch的设计目标是构建一个大型的全网搜索引擎，包括网页抓取、索引、查询等功能，但随着抓取网页数量的增加，遇到了严重的可扩展性问题——如何解决数十亿网页的存储和索引问题。 2003年开始谷歌陆续发表的三篇论文为该问题提供了可行的解决方案。 ——分布式文件系统（GFS），可用于处理海量网页的存储 ——分布式计算框架MAPREDUCE，可用于处理海量网页的索引计算问题。 ——BigTable 数据库：OLTP 联机事务处理 Online Transaction Processing 增删改 OLAP 联机分析处理 Online Analysis Processing 查询 真正的作用：提供了一种可以在超大数据集中进行实时CRUD操作的功能 3.Nutch的开发人员完成了相应的开源实现HDFS和MAPREDUCE，并从Nutch中剥离成为独立项目HADOOP，到2008年1月，HADOOP成为Apache顶级项目，迎来了它的快速发展期。 Hadoop是啥Hadoop的官网：http://hadoop.apache.org/ 1、Hadoop是Apache旗下的一套开源软件平台 2、Hadoop提供的功能：利用服务器集群，根据户自定义业逻辑对海量数进行分布式处理 3、Hadoop的核心组件： 1）Hadoop Common：支持其他Hadoop模块的常用工具。 2) Hadoop分布式文件系统（HDFS™）：一种分布式文件系统，可提供对应用程序数据的高吞吐量访问。 3) Hadoop YARN：作业调度和集群资源管理的框架。 4) Hadoop MapReduce：一种用于并行处理大型数据集的基于YARN的系统。 大数据的处理主要就是存储和计算。 如果说安装hadoop集群，其实就是安装了两个东西： 一个操作系统YARN 和 一个文件系统HDFS。其实MapReduce就是运行在YARN之上的应用。 操作系统 文件系统 应用程序win7 NTFS QQ，WeChatYARN HDFS MapReduce 4、hadoop的概念： 狭义上： 就是apache的一个顶级项目：apahce hadoop 广义上: 就是指以hadoop为核心的整个大数据处理体系 5、Apache的其他Hadoop相关项目包括： Ambari™：一种用于供应，管理和监控Apache Hadoop集群的基于Web的工具，其中包括对Hadoop HDFS，Hadoop MapReduce，Hive，HCatalog，HBase，ZooKeeper，Oozie，Pig和Sqoop的支持。Ambari还提供了一个用于查看群集运行状况的仪表板，例如热图和可以直观地查看MapReduce，Pig和Hive应用程序的功能，以及以用户友好的方式诊断其性能特征的功能。 Avro™：数据序列化系统。 Cassandra™：无单点故障的可扩展多主数据库。 Chukwa™：管理大型分布式系统的数据收集系统。 HBase™：可扩展的分布式数据库，支持大型表格的结构化数据存储。 Hive™：提供数据汇总和即席查询的数据仓库基础架构。 Mahout™：可扩展的机器学习和数据挖掘库。 Pig™：用于并行计算的高级数据流语言和执行框架。 Spark™：用于Hadoop数据的快速和通用计算引擎。Spark提供了一个简单而富有表现力的编程模型，它支持广泛的应用程序，包括ETL，机器学习，流处理和图计算。 Tez™：一种基于Hadoop YARN的通用数据流编程框架，它提供了一个强大且灵活的引擎，可执行任意DAG任务来处理批处理和交互式用例的数据。Hado™，Pig™和Hadoop生态系统中的其他框架以及其他商业软件（例如ETL工具）正在采用Tez来替代Hadoop™MapReduce作为底层执行引擎。 ZooKeeper™：分布式应用程序的高性能协调服务。 HADOOP在大数据、云计算中的位置和关系1、云计算是分布式计算、并行计算、网格计算、多核计算、网络存储、虚拟化、负载均衡等传统计算机技术和互联网技术融合发展的产物。借助IaaS(基础设施即服务)、PaaS(平台即服务)、SaaS（软件即服务）等业务模式，把强大的计算能力提供给终端用户。1、 2、现阶段，云计算的两大底层支撑技术为“虚拟化”和“大数据技术” 3、 而HADOOP则是云计算的PaaS层的解决方案之一，并不等同于PaaS，更不等同于云计算本身。 Hadoop的技术应用HADOOP应用于数据服务基础平台建设 HADOOP用于用户画像 该图是中国电信的用户画像标签体系。 HADOOP用于网站点击流日志**数据**挖掘 总结：hadoop并不会跟某个具体的行业或者某个具体的业务挂钩，它只是一种用来做海量数据分析处理的工具。 HADOOP生态圈以及各组成部分的简介 重点组件： HDFS：Hadoop的分布式文件存储系统。 MapReduce：Hadoop的分布式程序运算框架，也可以叫做一种编程模型。 Hive：基于Hadoop的类SQL数据仓库工具 Hbase：基于Hadoop的列式分布式NoSQL数据库 ZooKeeper：分布式协调服务组件 Mahout：基于MapReduce/Flink/Spark等分布式运算框架的机器学习算法库 Oozie/Azkaban：工作流调度引擎 Sqoop：数据迁入迁出工具 Flume：日志采集工具 获取数据的三种方式1、自己公司收集的数据–日志 或者 数据库中的数据 2、有一些数据可以通过爬虫从网络中进行爬取 3、从第三方机构购买 国内HADOOP的就业情况分析1、HADOOP就业整体情况A. 大数据产业已纳入国家十三五规划 B. 各大城市都在进行智慧城市项目建设，而智慧城市的根基就是大数据综合平台 C. 互联网时代数据的种类，增长都呈现爆发式增长，各行业对数据的价值日益重视 D. 相对于传统JAVAEE技术领域来说，大数据领域的人才相对稀缺 E. 随着现代社会的发展，数据处理和数据挖掘的重要性只会增不会减，因此，大数据技术是一个尚在蓬勃发展且具有长远前景的领域 2、 HADOOP就业职位要求大数据是个复合专业，包括应用开发、软件平台、算法、数据挖掘等，因此，大数据技术领域的就业选择是多样的，但就HADOOP而言，通常都需要具备以下技能或知识： 硬实力 A. HADOOP分布式集群的平台搭建 B. HADOOP分布式文件系统HDFS的原理理解及使用 C. HADOOP分布式运算框架MAPREDUCE的原理理解及编程 D. Hive数据仓库工具的熟练应用 E. Flume、sqoop、oozie等辅助工具的熟练使用 F. Shell/python等脚本语言的开发能力 软实力 A. 解决问题的能力（调试，阅读文档） B. 沟通协调能力（寻求帮助） C. 学习提升自己的能力（自我提高） D. 组织管控能力（管理能力）","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"分布式锁的几种实现方式","slug":"dslock","date":"2018-03-02T14:18:29.000Z","updated":"2019-08-29T03:01:15.917Z","comments":true,"path":"dslock.html","link":"","permalink":"http://zhangfuxin.cn/dslock.html","excerpt":"** 分布式锁的几种实现方式：** &lt;Excerpt in index | 首页摘要&gt;在分布式架构中，由于多线程和多台服务器，何难保证顺序性。如果需要对某一个资源进行限制，比如票务，比如请求幂等性控制等，这个时候分布式锁就排上用处。","text":"** 分布式锁的几种实现方式：** &lt;Excerpt in index | 首页摘要&gt;在分布式架构中，由于多线程和多台服务器，何难保证顺序性。如果需要对某一个资源进行限制，比如票务，比如请求幂等性控制等，这个时候分布式锁就排上用处。 &lt;The rest of contents | 余下全文&gt; 什么是分布式锁分布式锁是控制分布式系统或不同系统之间共同访问共享资源的一种锁实现，如果不同的系统或同一个系统的不同主机之间共享了某个资源时，往往需要互斥来防止彼此干扰来保证一致性。 分布式锁需要解决的问题 互斥性：任意时刻，只能有一个客户端获取锁，不能同时有两个客户端获取到锁。 安全性：锁只能被持有该锁的客户端删除，不能由其它客户端删除。 死锁：获取锁的客户端因为某些原因（如down机等）而未能释放锁，其它客户端再也无法获取到该锁。 容错：当部分节点（redis节点等）down机时，客户端仍然能够获取锁和释放锁。 分布式锁的实现方式 数据库实现 缓存实现，比如redis zookeeper实现","categories":[{"name":"分布式架构","slug":"分布式架构","permalink":"http://zhangfuxin.cn/categories/分布式架构/"}],"tags":[{"name":"java","slug":"java","permalink":"http://zhangfuxin.cn/tags/java/"}],"keywords":[{"name":"分布式架构","slug":"分布式架构","permalink":"http://zhangfuxin.cn/categories/分布式架构/"}]},{"title":"分布式系统理论基础","slug":"dsbasic","date":"2018-02-26T14:31:40.000Z","updated":"2019-08-29T03:00:59.583Z","comments":true,"path":"dsbasic.html","link":"","permalink":"http://zhangfuxin.cn/dsbasic.html","excerpt":"** 分布式系统理论基础：** &lt;Excerpt in index | 首页摘要&gt;分布式系统不是万能，不能解决所有痛点。在高可用，一致性，分区容错性必须有所权衡。","text":"** 分布式系统理论基础：** &lt;Excerpt in index | 首页摘要&gt;分布式系统不是万能，不能解决所有痛点。在高可用，一致性，分区容错性必须有所权衡。 &lt;The rest of contents | 余下全文&gt; CAP理论定理：任何分布式架构都只能同时满足两点，无法三者兼顾。 Consistency（一致性），数据一致更新，所有的数据变动都是同步的。 Availability（可用性），好的响应性能。 Partition tolerance（分区容忍性）可靠性，机器宕机是否影响使用。 关系数据库的ACID模型拥有 高一致性 + 可用性 很难进行分区： Atomicity原子性：一个事务中所有操作都必须全部完成，要么全部不完成。 Consistency一致性. 在事务开始或结束时，数据库应该在一致状态。 Isolation隔离性. 事务将假定只有它自己在操作数据库，彼此不知晓。 Durability持久性 一旦事务完成，就不能返回。跨数据库两段提交事务：2PC (two-phase commit)， 2PC is the anti-scalability pattern (Pat Helland)是反可伸缩模式的，JavaEE中的JTA事务可以支持2PC。因为2PC是反模式，尽量不要使用2PC，使用BASE来回避。 BASE理论 Basically Available 基本可用，支持分区失败 Soft state 软状态，允许状态某个时间短不同步，或者异步 Eventually consistent 最终一致性，要求数据最终结果一致，而不是时刻高度一致。 paxos协议Paxos算法的目的是为了解决分布式环境下一致性的问题。多个节点并发操纵数据，如何保证在读写过程中数据的一致性，并且解决方案要能适应分布式环境下的不可靠性（系统如何就一个值达到统一）。 Paxos的两个组件: Proposer：提议发起者，处理客户端请求，将客户端的请求发送到集群中，以便决定这个值是否可以被批准。 Acceptor:提议批准者，负责处理接收到的提议，他们的回复就是一次投票。会存储一些状态来决定是否接收一个值 Paxos有两个原则 安全原则—保证不能做错的事 a） 针对某个实例的表决只能有一个值被批准，不能出现一个被批准的值被另一个值覆盖的情况；(假设有一个值被多数Acceptor批准了，那么这个值就只能被学习) b） 每个节点只能学习到已经被批准的值，不能学习没有被批准的值。 存活原则—只要有多数服务器存活并且彼此间可以通信，最终都要做到的下列事情： a）最终会批准某个被提议的值； b）一个值被批准了，其他服务器最终会学习到这个值。 zab协议(ZooKeeper Atomic broadcast protocol)ZAB协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。 Phase 0: Leader election（选举阶段）节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。只有到达 Phase 3 准 leader 才会成为真正的 leader。这一阶段的目的是就是为了选出一个准 leader，然后进入下一个阶段。 Phase 1: Discovery（发现阶段）在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。这个一阶段的主要目的是发现当前大多数节点接收的最新提议，并且准 leader 生成新的 epoch，让 followers 接受，更新它们的 acceptedEpoch。一个 follower 只会连接一个 leader，如果有一个节点 f 认为另一个 follower p 是 leader，f 在尝试连接 p 时会被拒绝，f 被拒绝之后，就会进入 Phase 0。 Phase 2: Synchronization（同步阶段）同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。只有当 quorum 都同步完成，准 leader 才会成为真正的 leader。follower 只会接收 zxid 比自己的 lastZxid 大的提议。 Phase 3: Broadcast（广播阶段）到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。 raft协议在Raft中，每个结点会处于下面三种状态中的一种： follower所有结点都以follower的状态开始。如果没收到leader消息则会变成candidate状态。 candidate会向其他结点“拉选票”，如果得到大部分的票则成为leader。这个过程就叫做Leader选举(Leader Election) leader所有对系统的修改都会先经过leader。每个修改都会写一条日志(log entry)。leader收到修改请求后的过程如下，这个过程叫做日志复制(Log Replication)： 1. 复制日志到所有follower结点(replicate entry) 2. 大部分结点响应时才提交日志 3. 通知所有follower结点日志已提交 4. 所有follower也提交日志 5. 现在整个系统处于一致的状态","categories":[{"name":"分布式架构","slug":"分布式架构","permalink":"http://zhangfuxin.cn/categories/分布式架构/"}],"tags":[{"name":"protocol","slug":"protocol","permalink":"http://zhangfuxin.cn/tags/protocol/"}],"keywords":[{"name":"分布式架构","slug":"分布式架构","permalink":"http://zhangfuxin.cn/categories/分布式架构/"}]},{"title":"为什么使用zookeeper？","slug":"zookeeper","date":"2018-02-18T14:15:44.000Z","updated":"2019-08-29T02:22:11.829Z","comments":true,"path":"zookeeper.html","link":"","permalink":"http://zhangfuxin.cn/zookeeper.html","excerpt":"** 为什么使用zookeeper？：** &lt;Excerpt in index | 首页摘要&gt;随着大型互联网的发展，分布式系统应用越来越来越广泛，zookeeper成了分布式系统的标配。集群容错，动态负载均衡，动态扩容，异地多活等架构都依赖于zookeeper而搭建。","text":"** 为什么使用zookeeper？：** &lt;Excerpt in index | 首页摘要&gt;随着大型互联网的发展，分布式系统应用越来越来越广泛，zookeeper成了分布式系统的标配。集群容错，动态负载均衡，动态扩容，异地多活等架构都依赖于zookeeper而搭建。 &lt;The rest of contents | 余下全文&gt; zookeeper是什么？zookeeper是由雅虎创建的，基于google chubby,一个开源的分布式协调服务，是分布式数据一致性的解决方案。 zookeeper的特性 顺序一致性，从同一个客户端发起的事务请求，最终会严格按照顺序被应用到zookeeper中。 原子性，事务请求在所有集群是一致的，要么都成功，要么都失败。 可靠性，一旦服务器成功应用某个事务，那么所有集群中一定同步并保留。 实时性，一个事务被应用，客户端能立即从服务端读取到状态变化。 zookeeper的原理？基于分布式协议pasxo，而实现了自己的zab协议。保证数据的一致性。 zookeeper的数据模型 持久化节点，节点创建后一直存在，直到主动删除。 持久化有序节点，每个节点都会为它的一级子节点维护一个顺序。 临时节点，临时节点的生命周期和客户端会话保持一直。客户端会话失效，节点自动清理。 临时有序节点，临时节点基础上多一个顺序性特性。 zookeeper使用场景有哪些？ 订阅发布 watcher机制 统一配置管理(disconf) 分布式锁（redis也可以） 分布式队列 负载均衡(dubbo) ID生成器 master选举(kafka,hadoop,hbase) 集群角色有哪些？leader 事务请求的唯一调度者和处理者，保证集群事务的处理顺序 集群内部服务的调度者 follower 处理非事务请求，以及转发事务请求到leader 参与事务请求提议的投票 参与leader选举的投票 observer 观察集群中最新状态的变化并同步到observer服务器上 增加observer不影响集群事务处理能力，还能提升非事务请求的处理能力 zookeeper集群为什么是奇数？zookeeper事务请求提议需要超过半数的机器，假如是2(n+1)台,需要n+2台机器同意，由于在增删改操作中需要半数以上服务器通过，来分析以下情况。2台服务器，至少2台正常运行才行（2的半数为1，半数以上最少为2），正常运行1台服务器都不允许挂掉3台服务器，至少2台正常运行才行（3的半数为1.5，半数以上最少为2），正常运行可以允许1台服务器挂掉4台服务器，至少3台正常运行才行（4的半数为2，半数以上最少为3），正常运行可以允许1台服务器挂掉5台服务器，至少3台正常运行才行（5的半数为2.5，半数以上最少为3），正常运行可以允许2台服务器挂掉6台服务器，至少3台正常运行才行（6的半数为3，半数以上最少为4），正常运行可以允许2台服务器挂掉 zookeeper日志管理？leader选举的原理未完待续","categories":[{"name":"分布式架构","slug":"分布式架构","permalink":"http://zhangfuxin.cn/categories/分布式架构/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://zhangfuxin.cn/tags/zookeeper/"}],"keywords":[{"name":"分布式架构","slug":"分布式架构","permalink":"http://zhangfuxin.cn/categories/分布式架构/"}]},{"title":"查找链表倒数第n个元素","slug":"descNode","date":"2018-02-16T13:45:45.000Z","updated":"2019-08-29T03:00:24.260Z","comments":true,"path":"descNode.html","link":"","permalink":"http://zhangfuxin.cn/descNode.html","excerpt":"** 查找链表倒数第n个元素：** &lt;Excerpt in index | 首页摘要&gt;链表应用很广泛，有单向链表，双向链表。单向链表如何查找倒数第n个元素呢？本文以java代码实现链表反向查找。","text":"** 查找链表倒数第n个元素：** &lt;Excerpt in index | 首页摘要&gt;链表应用很广泛，有单向链表，双向链表。单向链表如何查找倒数第n个元素呢？本文以java代码实现链表反向查找。 &lt;The rest of contents | 余下全文&gt; 单向链表的定义单向链表，主要有数据存储，下一个节点的引用这两个元素组成。 12345678public class Node &#123; int value; Node next; Node(int value) &#123; this.value = value; &#125;&#125; 遍历倒数第n个元素在查找过程中，设置两个指针，让其中一个指针比另一个指针先前移k-1步，然后两个指针同时往前移动。循环直到先行的指针指为NULL时，另一个指针所指的位置就是所要找的位置算法复杂度为o（n） 123456789101112131415161718192021222324public Node findDescEle(Node head, int k) &#123; if (k &lt; 1 || head == null) &#123; return null; &#125; Node p1 = head; Node p2 = head; //前移k-1步 int step = 0; for (int i = 0; i &lt; k; i++) &#123; step++; if (p1.next != null) &#123; p1 = p1.next; &#125; else &#123; return null; &#125; &#125; while (p1 != null) &#123; step++; p1 = p1.next; p2 = p2.next; &#125; System.out.println(&quot;o(n)==&quot; + step); return p2;&#125; 总结查找链表倒数第n个元素，复杂度为o(n),使用两个指针即可简单实现。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"sprigmvc项目转为springboot","slug":"sprigmvc2boot","date":"2018-02-06T14:12:55.000Z","updated":"2019-08-29T02:30:40.437Z","comments":true,"path":"sprigmvc2boot.html","link":"","permalink":"http://zhangfuxin.cn/sprigmvc2boot.html","excerpt":"** sprigmvc项目转为springboot：** &lt;Excerpt in index | 首页摘要&gt;是否有老掉牙的springmvc项目，想转成springboot项目，看这个文章就对了。","text":"** sprigmvc项目转为springboot：** &lt;Excerpt in index | 首页摘要&gt;是否有老掉牙的springmvc项目，想转成springboot项目，看这个文章就对了。 &lt;The rest of contents | 余下全文&gt; 说明 如果你的项目连maven项目都不是，请自行转为maven项目，在按照本教程进行。 本教程适用于spring+springmvc+mybatis+shiro的maven项目。 1.修改pom文件依赖 删除之前的spring依赖，添加springboot依赖 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;!-- 这个是剔除掉自带的 tomcat部署的--&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- tomcat容器部署 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;!--&lt;scope&gt;compile&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;!-- 支持 @ConfigurationProperties 注解 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 添加springboot构建插件 12345678910111213141516171819202122&lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt;&lt;/plugins&gt; 2.添加application启动文件注意，如果Application在controller，service，dao的上一层包里，无需配置@ComponentScan,否则，需要指明要扫描的包。 12345678910111213@SpringBootApplication//@ComponentScan(&#123;\"com.cms.controller\",\"com.cms.service\",\"com.cms.dao\"&#125;)public class Application extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) &#123; return application.sources(Application.class); &#125; public static void main(String[] args) throws Exception &#123; SpringApplication.run(Application.class, args); &#125;&#125; 3.添加springboot配置文件 在resources下面添加application.properties文件 添加基本配置1234567891011121314151617#默认前缀server.contextPath=/# 指定环境spring.profiles.active=local# jsp配置spring.mvc.view.prefix=/WEB-INF/jsp/spring.mvc.view.suffix=.jsp#log配置文件logging.config=classpath:logback-cms.xml#log路径logging.path=/Users/mac/work-tommy/cms-springboot/logs/#数据源spring.datasource.name=adminDataSourcespring.datasource.driverClassName = com.mysql.jdbc.Driverspring.datasource.url = jdbc:mysql://localhost:3306/mycms?useUnicode=true&amp;autoReconnect=true&amp;characterEncoding=utf-8spring.datasource.username = rootspring.datasource.password = 123456 4.使用@Configuration注入配置 注入mybatis配置,分页插件请自主选择 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Configuration@MapperScan(basePackages = \"com.kuwo.dao\",sqlSessionTemplateRef = \"adminSqlSessionTemplate\")public class AdminDataSourceConfig &#123; @Bean(name = \"adminDataSource\") @ConfigurationProperties(prefix = \"spring.datasource\") @Primary public DataSource adminDataSource() &#123; return DataSourceBuilder.create().build(); &#125; @Bean(name = \"adminSqlSessionFactory\") @Primary public SqlSessionFactory adminSqlSessionFactory(@Qualifier(\"adminDataSource\") DataSource dataSource) throws Exception &#123; SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dataSource); //分页插件// PageHelper pageHelper = new PageHelper(); PagePlugin pagePlugin = new PagePlugin();// Properties props = new Properties();// props.setProperty(\"reasonable\", \"true\");// props.setProperty(\"supportMethodsArguments\", \"true\");// props.setProperty(\"returnPageInfo\", \"check\");// props.setProperty(\"params\", \"count=countSql\");// pageHelper.setProperties(props); //添加插件 bean.setPlugins(new Interceptor[]&#123;pagePlugin&#125;); // 添加mybatis配置文件 bean.setConfigLocation(new DefaultResourceLoader().getResource(\"classpath:mybatis/mybatis-config.xml\")); // 添加mybatis映射文件 bean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(\"classpath:mybatis/system/*.xml\")); return bean.getObject(); &#125; @Bean(name = \"adminTransactionManager\") @Primary public DataSourceTransactionManager adminTransactionManager(@Qualifier(\"adminDataSource\") DataSource dataSource) &#123; return new DataSourceTransactionManager(dataSource); &#125; @Bean(name = \"adminSqlSessionTemplate\") @Primary public SqlSessionTemplate adminSqlSessionTemplate(@Qualifier(\"adminSqlSessionFactory\") SqlSessionFactory sqlSessionFactory) throws Exception &#123; return new SqlSessionTemplate(sqlSessionFactory); &#125;&#125; 添加Interceptor配置,注意addInterceptor的顺序，不要搞乱了 1234567@Configurationpublic class InterceptorConfiguration extends WebMvcConfigurerAdapter&#123; @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(new LoginHandlerInterceptor()); &#125;&#125; 添加shiro配置文件 注意：本来使用redis做session缓存，但是和shiro集成发现一个问题，user对象存储以后，从shiro中获取后，无法进行类型转换，所以暂时放弃了redis做session缓存。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140@Configurationpublic class ShiroConfiguration &#123; @Value(\"$&#123;spring.redis.host&#125;\") private String host; @Value(\"$&#123;spring.redis.port&#125;\") private int port; @Value(\"$&#123;spring.redis.timeout&#125;\") private int timeout; @Bean public static LifecycleBeanPostProcessor getLifecycleBeanPostProcessor() &#123; return new LifecycleBeanPostProcessor(); &#125; /** * ShiroFilterFactoryBean 处理拦截资源文件问题。 * 注意：单独一个ShiroFilterFactoryBean配置是或报错的，因为在 * 初始化ShiroFilterFactoryBean的时候需要注入：SecurityManager * Filter Chain定义说明 1、一个URL可以配置多个Filter，使用逗号分隔 2、当设置多个过滤器时，全部验证通过，才视为通过 3、部分过滤器可指定参数，如perms，roles * */ @Bean public ShiroFilterFactoryBean shiroFilter(SecurityManager securityManager)&#123; System.out.println(\"ShiroConfiguration.shirFilter()\"); ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean(); // 必须设置 SecurityManager shiroFilterFactoryBean.setSecurityManager(securityManager); // 如果不设置默认会自动寻找Web工程根目录下的\"/login.jsp\"页面 shiroFilterFactoryBean.setLoginUrl(\"/login_toLogin\"); // 登录成功后要跳转的链接 shiroFilterFactoryBean.setSuccessUrl(\"/usersPage\"); //未授权界面; shiroFilterFactoryBean.setUnauthorizedUrl(\"/403\"); //拦截器. Map&lt;String,String&gt; filterChainDefinitionMap = new LinkedHashMap&lt;&gt;(); //配置退出 过滤器,其中的具体的退出代码Shiro已经替我们实现了 filterChainDefinitionMap.put(\"/logout\", \"logout\"); filterChainDefinitionMap.put(\"/login_toLogin\", \"anon\"); filterChainDefinitionMap.put(\"/login_login\", \"anon\"); filterChainDefinitionMap.put(\"/static/login/**\",\"anon\"); filterChainDefinitionMap.put(\"/static/js/**\",\"anon\"); filterChainDefinitionMap.put(\"/uploadFiles/uploadImgs/**\",\"anon\"); filterChainDefinitionMap.put(\"/code.do\",\"anon\"); filterChainDefinitionMap.put(\"/font-awesome/**\",\"anon\"); //&lt;!-- 过滤链定义，从上向下顺序执行，一般将 /**放在最为下边 --&gt;:这是一个坑呢，一不小心代码就不好使了; //&lt;!-- authc:所有url都必须认证通过才可以访问; anon:所有url都都可以匿名访问--&gt; filterChainDefinitionMap.put(\"/**\", \"authc\"); shiroFilterFactoryBean.setFilterChainDefinitionMap(filterChainDefinitionMap); return shiroFilterFactoryBean; &#125; @Bean public SecurityManager securityManager()&#123; DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); //设置realm. securityManager.setRealm(myShiroRealm()); // 自定义缓存实现 使用redis //securityManager.setCacheManager(cacheManager()); // 自定义session管理 使用redis securityManager.setSessionManager(sessionManager()); return securityManager; &#125; @Bean public ShiroRealm myShiroRealm()&#123; ShiroRealm myShiroRealm = new ShiroRealm();// myShiroRealm.setCredentialsMatcher(hashedCredentialsMatcher()); return myShiroRealm; &#125;&#125; /** * 开启shiro aop注解支持. * 使用代理方式;所以需要开启代码支持; * @param securityManager * @return */ @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor(SecurityManager securityManager)&#123; AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor = new AuthorizationAttributeSourceAdvisor(); authorizationAttributeSourceAdvisor.setSecurityManager(securityManager); return authorizationAttributeSourceAdvisor; &#125; /** * 配置shiro redisManager * 使用的是shiro-redis开源插件 * @return */ public RedisManager redisManager() &#123; RedisManager redisManager = new RedisManager(); redisManager.setHost(host); redisManager.setPort(port); redisManager.setExpire(1800); redisManager.setTimeout(timeout); // redisManager.setPassword(password); return redisManager; &#125; /** * cacheManager 缓存 redis实现 * 使用的是shiro-redis开源插件 * @return */ public RedisCacheManager cacheManager() &#123; RedisCacheManager redisCacheManager = new RedisCacheManager(); redisCacheManager.setRedisManager(redisManager()); return redisCacheManager; &#125; /** * RedisSessionDAO shiro sessionDao层的实现 通过redis * 使用的是shiro-redis开源插件 */ @Bean public RedisSessionDAO redisSessionDAO() &#123; RedisSessionDAO redisSessionDAO = new RedisSessionDAO(); redisSessionDAO.setRedisManager(redisManager()); return redisSessionDAO; &#125; @Bean public DefaultWebSessionManager sessionManager() &#123; DefaultWebSessionManager sessionManager = new DefaultWebSessionManager();// sessionManager.setSessionDAO(redisSessionDAO()); return sessionManager; &#125;&#125; 总结搞了一天时间把项目转成springboot，查阅各种资料，希望这篇文章能够为你带来帮助。","categories":[{"name":"项目实战","slug":"项目实战","permalink":"http://zhangfuxin.cn/categories/项目实战/"}],"tags":[{"name":"java","slug":"java","permalink":"http://zhangfuxin.cn/tags/java/"}],"keywords":[{"name":"项目实战","slug":"项目实战","permalink":"http://zhangfuxin.cn/categories/项目实战/"}]},{"title":"Windows连接Linux虚拟机里面的Docker容器","slug":"2018-02-02-Windows连接Linux虚拟机里面的Docker容器","date":"2018-02-02T02:30:04.000Z","updated":"2019-09-19T06:05:10.578Z","comments":true,"path":"2018-02-02-Windows连接Linux虚拟机里面的Docker容器.html","link":"","permalink":"http://zhangfuxin.cn/2018-02-02-Windows连接Linux虚拟机里面的Docker容器.html","excerpt":"** Windows连接Linux虚拟机里面的Docker容器：** &lt;Excerpt in index | 首页摘要&gt; ​ Windows连接Linux虚拟机里面的Docker容器","text":"** Windows连接Linux虚拟机里面的Docker容器：** &lt;Excerpt in index | 首页摘要&gt; ​ Windows连接Linux虚拟机里面的Docker容器 &lt;The rest of contents | 余下全文&gt; 一、Windows、Linux虚拟机、docker关系图 如果此时在Windows宿主机中pingDocker容器是ping不同的，因为在宿主机上没有通往172.17.0.0/24网络的路由，宿主机会将发往172.17.0.0/24网络的数据发往默认路由，这样就无法到达容器。 二、操作2.1 关闭Linux中的防火墙1[root@bigdata ~]# systemctl stop firewalld.service 2.2 在docker容器中安装并启用ssh服务1[root@spark6 redis]# /usr/sbin/sshd -D &amp; 2.3 Windows宿主机与虚拟机CentOS网络互通可通过Xshell连接 2.4 虚拟机CentOS和Docker容器网络互通在CentOS中可以通过docker exec -it /bin/bash命令进入容器内部 2.5 在Windows中添加到docker容器网段的路由1C:\\WINDOWS\\system32&gt;route add 172.17.0.0 mask 255.255.255.0 192.168.123.110 该路由表示通往172.17.0.0/24网络的数据包通过192.168.123.110来转发。 2.6 测试","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"http://zhangfuxin.cn/tags/Flume/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"CentOS7 安装Docker","slug":"2018-02-01-CentOS7 安装Docker","date":"2018-02-01T02:30:04.000Z","updated":"2019-09-19T05:58:49.262Z","comments":true,"path":"2018-02-01-CentOS7 安装Docker.html","link":"","permalink":"http://zhangfuxin.cn/2018-02-01-CentOS7 安装Docker.html","excerpt":"** CentOS7 安装Docker：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** CentOS7 安装Docker：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt; 一、检查系统内核Docker 要求 CentOS 系统的内核版本高于 3.10 ，查看本页面的前提条件来验证你的CentOS 版本是否支持 Docker 。 通过 uname -r 命令查看你当前的内核版本 1[root@bigdata ~]# uname -r 二、安装Docker2.1 安装Docker 软件包和依赖包已经包含在默认的 CentOS-Extras 软件源里，安装命令如下： 1[root@bigdata ~]# yum -y install docker 2.2 查看docker版本1[root@bigdata ~]# docker version 2.3 启动docker方式一： 123[root@bigdata ~]# service docker startRedirecting to /bin/systemctl start docker.service[root@bigdata ~]# 方式二： 12[root@bigdata ~]# systemctl start docker.service[root@bigdata ~]# ps aux | grep docker 三、建立docker用户和组3.1 创建用户及组默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。 12[root@bigdata ~]# groupadd docker[root@bigdata ~]# useradd -g docker docker 3.2 使用新创建的用户运行helloworld1[docker@bigdata ~]$ docker run hello-world 12345678[docker@bigdata ~]$ docker run hello-worldUnable to find image &apos;hello-world:latest&apos; locallyTrying to pull repository docker.io/library/hello-world ... latest: Pulling from docker.io/library/hello-world9bb5a5d4561a: Pulling fs layer /usr/bin/docker-current: error pulling image configuration: Get https://dseasb33srnrn.cloudfront.net/registry-v2/docker/registry/v2/blobs/sha256/e3/e38bc07ac18ee64e6d59cf2eafcdddf9cec2364dfe129fe0af75f1b0194e0c96/data?Expires=1525823399&amp;Signature=SjqbSNVW5X~uDhy9jXvuLqv22jC3auyGRx4JCRE1ceXkdh0Qpsc21VmhIXwAO6XcxwyJ1gGNVQhnJWYozOWXjysL8taJFBCxKNqAD9Cy~TCt-iMi06z9dHX6-WxxIU3WJ4LbCT7RxsWIKArTVKmPvyQdD4Djkgr~rWzoL6eyTfg_&amp;Key-Pair-Id=APKAJECH5M7VWIS5YZ6Q: net/http: TLS handshake timeout.See &apos;/usr/bin/docker-current run --help&apos;.[docker@bigdata ~]$ 3.3 解决报错如上图报错/usr/bin/docker-current: error pulling image configuration。。。 出现这个问题，一般的原因是无法连接到 docker hub通过（使用root用户执行以下命令）： 1[root@bigdata ~]# cat /etc/sysconfig/docker 在文件中添加以下内容： 1--registry-mirror=http://f2d6cb40.m.daocloud.io 重启docker 1[root@bigdata ~]# service docker restart 再次运行helloworld（docker用户） 1[docker@bigdata ~]$ docker run hello-world 由于本地没有hello-world这个镜像，所以会下载一个hello-world的镜像，并在容器内运行。 四、安装centos镜像4.1 下载镜像从 Docker 镜像仓库获取镜像的命令是 docker pull。其命令格式为： 1docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签] 可以直接使用docker pull centos:7命令安装镜像 1[docker@bigdata ~]$ docker pull centos:7 4.2 查看拥有的镜像1[docker@bigdata ~]$ docker image ls 一个是centos镜像，另一个是我们之前使用docker run hello-world命令下载的镜像。 镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 4.3 运行容器有了镜像后，我们就能够以这个镜像为基础启动并运行一个容器。 1[docker@bigdata ~]$ docker run -it --rm centos bash docker run 就是运行容器的命令，说明一下上面用到的参数。 -it：这是两个参数，一个是 -i：交互式操作，一个是 -t 终端。我们这里打算进入 bash 执行一些命令并查看返回结果，因 此我们需要交互式终端。 –rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 –rm 可以避免浪费空间。 centos ：这是指用centos 镜像为基础来启动容器。 bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 bash。 4.4 在容器中运行命令1[root@cb55b5f51685 /]# cat /etc/os-release 进入容器后，我们可以在 Shell 下操作，执行任何所需的命令。这里，我们执行了 cat /etc/os-release，这是 Linux 常用的查看当前系统版本的命令，从返回的结果可以看到容器内是 CentOS Linux 系统。最后我们可以通过 exit 退出了这个容器。 4.5 查看镜像、容器、数据卷所占用的空间1[docker@bigdata ~]$ docker system df 4.6 容器退出再次进入报错1234[docker@bigdata ~]$ docker run -it -v /home/docker/build:/root/build --privileged -h hadoop1 --name hadoop1 centos /bin/bash/usr/bin/docker-current: Error response from daemon: Conflict. The container name &quot;/hadoop1&quot; is already in use by container a094bdef9e1cac62a17022e568fe9b1eb021e13adf8ed2624a71be5a2e42c618. You have to remove (or rename) that container to be able to reuse that name..See &apos;/usr/bin/docker-current run --help&apos;.[docker@bigdata ~]$ docker ps: 查看当前运行的容器 docker ps -a:查看所有容器，包括停止的。 标题含义： CONTAINER ID:容器的唯一表示ID。 IMAGE:创建容器时使用的镜像。 COMMAND:容器最后运行的命令。 CREATED:创建容器的时间。 STATUS:容器状态。 PORTS:对外开放的端口。 NAMES:容器名。可以和容器ID一样唯一标识容器，同一台宿主机上不允许有同名容器存在，否则会冲突。 使用命令停止并删除这个容器就可以 五、运行容器5.1 使用命令运行容器1[docker@bigdata ~]$ docker run -it -v /home/docker/build:/root/build --privileged -h hadoop1 --name hadoop1 centos /bin/bash 以centos镜像启动一个容器，容器名是hadoop1，主机名是hadoop1，并且将基于容器的centos系统的/root/build目录与本地/home/docker/build共享。 参数解释： -v 表示基于容器的centos系统的/root/build目录与本地/home/hadoop/build共享；这可以很方便将本地文件上传到Docker内部的centos系统； -h 指定主机名为hadoop1 –-name 指定容器名 /bin/bash 使用bash命令 六、刚安装的系统非常纯净，需要安装必备的软件6.1 安装vim1[root@hadoop1 /]# yum install vim 6.2 升级及安装sshd6.2.1 安装123[root@hadoop1 /]# yum -y update[root@hadoop1 /]# yum -y install openssh-server[root@hadoop1 /]# yum -y install openssh-clients 编辑sshd的配置文件/etc/ssh/sshd_config，将其中的UsePAM yes改为UsePAM no 1[root@hadoop1 /]# vi /etc/ssh/sshd_config 6.2.2 启动1[root@hadoop1 /]# /usr/sbin/sshd -D 报错如图，解决方案为：创建公私密钥，输入命令后，直接按两次enter键确认就行了 1[root@hadoop1 /]# ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key 1[root@hadoop1 /]# ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key 1[root@hadoop1 /]# ssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key 再次启动SSH服务 12[root@hadoop1 /]# yum -y install lsof[root@hadoop1 /]# lsof -i:22 6.3 修改root密码1[root@hadoop1 /]# passwd 测试 1[root@hadoop1 /]# ssh localhost 上图中可以看到已经登录到本机了，也就说容器中的主机拥有了ssh远程登录其它主机的能力，当然你也可以登录其他主机。要退出的话，输入命令exit即可。 6.4 宿主机能登录本机（容器中的主机）1[root@hadoop1 ~]# vi /etc/hosts 得到容器中的主机的ip地址172.17.0.2（可能和你得到的不一样） 然后在宿主机中开启一个新的终端输入命令 1[docker@bigdata ~]$ ssh root@172.17.0.2 6.5 配置ssh无密码登录1[root@hadoop1 ~]# ssh-keygen -t rsa 12[root@hadoop1 ~]# cd .ssh/[root@hadoop1 .ssh]# cat id_rsa.pub &gt;&gt; authorized_keys 输入完后，这时再输入命令 1[root@hadoop1 .ssh]# ssh localhost 七、上传软件到容器里面将JDK上传到Linux系统，，然后将其移动到/home/docker/build文件夹下面，注意：这里需要使用root用户 1[root@bigdata docker]# mv jdk-8u73-linux-x64.tar.gz build/ 进入容器里面的/root/build文件夹下面进行查看 12[root@hadoop1 /]# cd /root/build/[root@hadoop1 build]# ls 5.2.3 安装JDK在容器/root下面建一个apps文件夹 1[root@hadoop1 ~]# mkdir apps 解压JDK的安装包到apps文件夹下面 1[root@hadoop1 build]# tar -zxvf jdk-8u73-linux-x64.tar.gz -C /root/apps/ 修改环境变量 12345[root@hadoop1 ~]# vi .bashrc#JAVAexport JAVA_HOME=/root/apps/jdk1.8.0_73export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH:$HOME/bin 保存使其立即生效 1[root@hadoop1 ~]# source .bashrc 八、保存镜像基于已有的docker容器，做一新的dokcer image. $ docker commit 另开一个窗口 举例：","categories":[{"name":"Linux","slug":"Linux","permalink":"http://zhangfuxin.cn/categories/Linux/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://zhangfuxin.cn/tags/Docker/"}],"keywords":[{"name":"Linux","slug":"Linux","permalink":"http://zhangfuxin.cn/categories/Linux/"}]},{"title":"mybatis-generator","slug":"mybatis-generator","date":"2018-01-28T09:30:19.000Z","updated":"2019-08-29T02:33:08.554Z","comments":true,"path":"mybatis-generator.html","link":"","permalink":"http://zhangfuxin.cn/mybatis-generator.html","excerpt":"** mybatis-generator：** &lt;Excerpt in index | 首页摘要&gt;mybatis反向生成器，根据数据库表，自动创建pojo，mapper以及mybatis配置文件，能极大的提高开发效率。","text":"** mybatis-generator：** &lt;Excerpt in index | 首页摘要&gt;mybatis反向生成器，根据数据库表，自动创建pojo，mapper以及mybatis配置文件，能极大的提高开发效率。 &lt;The rest of contents | 余下全文&gt; 插件介绍本插件fork自mybatis-generator-gui,在此基础上加了批量生成表。 插件特性 保存数据库配置 根据表生成pojo，mapper以及mybatis配置文件 批量生成 其它功能（待开发） 插件使用要求本工具由于使用了Java 8的众多特性，所以要求JDK 1.8.0.60以上版本，对于JDK版本还没有升级的童鞋表示歉意。 启动本软件 方法一: 自助构建 12345git clone https://github.com/maochunguang/mybatis-generator-guicd mybatis-generator-guimvn jfx:jarcd target/jfx/app/java -jar mybatis-generator-gui.jar 方法二: IDE中运行Eclipse or IntelliJ IDEA中启动, 找到com.zzg.mybatis.generator.MainUI类并运行就可以了 文档更多详细文档请参考本库的Wiki Usage 截图参考","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://zhangfuxin.cn/tags/mysql/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"突破算法第11天-红黑树","slug":"suanfa-11","date":"2017-10-30T14:35:37.000Z","updated":"2019-08-29T02:27:53.213Z","comments":true,"path":"suanfa-11.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-11.html","excerpt":"** 突破算法第11天-红黑树：** &lt;Excerpt in index | 首页摘要&gt;红黑树","text":"** 突破算法第11天-红黑树：** &lt;Excerpt in index | 首页摘要&gt;红黑树 &lt;The rest of contents | 余下全文&gt; 红黑树本文的主要内容： 红黑树的基本概念以及最重要的 5 点规则。 红黑树的左旋转、右旋转、重新着色的原理与 Java 实现； 红黑树的增加结点、删除结点过程解析； 红黑树的基本概念与数据结构表示首先红黑树来个定义： 红黑树定义：红黑树又称红 - 黑二叉树，它首先是一颗二叉树，它具体二叉树所有的特性。同时红黑树更是一颗自平衡的排序二叉树 (平衡二叉树的一种实现方式)。 我们知道一颗基本的二叉排序树他们都需要满足一个基本性质：即树中的任何节点的值大于它的左子节点，且小于它的右子节点。 按照这个基本性质使得树的检索效率大大提高。我们知道在生成二叉排序树的过程是非常容易失衡的，最坏的情况就是一边倒（只有右 / 左子树），这样势必会导致二叉树的检索效率大大降低（O(n)），所以为了维持二叉排序树的平衡，大牛们提出了各种平衡二叉树的实现算法，如：AVL，SBT，伸展树，TREAP ，红黑树等等。 平衡二叉树必须具备如下特性：它是一棵空树或它的左右两个子树的高度差的绝对值不超过 1，并且左右两个子树都是一棵平衡二叉树。也就是说该二叉树的任何一个子节点，其左右子树的高度都相近。下面给出平衡二叉树的几个示意图： 红黑树顾名思义就是结点是红色或者是黑色的平衡二叉树，它通过颜色的约束来维持着二叉树的平衡。对于一棵有效的红黑树而言我们必须增加如下规则，这也是红黑树最重要的 5 点规则： 每个结点都只能是红色或者黑色中的一种。 根结点是黑色的。 每个叶结点（NIL 节点，空节点）是黑色的。 如果一个结点是红的，则它两个子节点都是黑的。也就是说在一条路径上不能出现相邻的两个红色结点。 从任一结点到其每个叶子的所有路径都包含相同数目的黑色结点。 这些约束强制了红黑树的关键性质: 从根到叶子最长的可能路径不多于最短的可能路径的两倍长。结果是这棵树大致上是平衡的。因为操作比如插入、删除和查找某个值的最坏情况时间都要求与树的高度成比例，这个在高度上的理论上限允许红黑树在最坏情况下都是高效的，而不同于普通的二叉查找树。所以红黑树它是复杂而高效的，其检索效率 O(lg n)。下图为一颗典型的红黑二叉树： 上面关于红黑树的概念基本已经说得很清楚了，下面给出红黑树的结点用 Java 表示数据结构： 123456789101112131415161718192021222324252627282930313233343536373839404142434445private static final boolean RED = true;private static final boolean BLACK = false;private Node root;//二叉查找树的根节点//结点数据结构private class Node&#123; private Key key;//键 private Value value;//值 private Node left, right;//指向子树的链接:左子树和右子树. private int N;//以该节点为根的子树中的结点总数 boolean color;//由其父结点指向它的链接的颜色也就是结点颜色. public Node(Key key, Value value, int N, boolean color) &#123; this.key = key; this.value = value; this.N = N; this.color = color; &#125;&#125;/** * 获取整个二叉查找树的大小 * @return */public int size()&#123; return size(root);&#125;/** * 获取某一个结点为根结点的二叉查找树的大小 * @param x * @return */private int size(Node x)&#123; if(x == null)&#123; return 0; &#125; else &#123; return x.N; &#125;&#125;private boolean isRed(Node x)&#123; if(x == null)&#123; return false; &#125; return x.color == RED;&#125; 红黑树的三个基本操作红黑树在插入，删除过程中可能会破坏原本的平衡条件导致不满足红黑树的性质，这时候一般情况下要通过左旋、右旋和重新着色这个三个操作来使红黑树重新满足平衡化条件。 旋转旋转分为左旋和右旋。在我们实现某些操作中可能会出现红色右链接或则两个连续的红链接，这时候就要通过旋转修复。 通常左旋操作用于将一个向右倾斜的红色链接 (这个红色链接链连接的两个结点均是红色结点) 旋转为向左链接。对比操作前后，可以看出，该操作实际上是将红线链接的两个结点中的一个较大的结点移动到根结点上。 左旋的示意图如下： 左旋的 Java 实现如下： 12345678910111213141516171819/** * 左旋转 * @param h * @return */private Node rotateLeft(Node h)&#123; Node x = h.right; //把x的左结点赋值给h的右结点 h.right = x.left; //把h赋值给x的左结点 x.left = h; // x.color = h.color; h.color = RED; x.N = h.N; h.N = 1+ size(h.left) + size(h.right); return x;&#125; 左旋的动画效果如下： 右旋其实就是左旋的逆操作：右旋的代码如下： 12345678910111213141516/** * 右旋转 * @param h * @return */private Node rotateRight(Node h)&#123; Node x = h.left; h.left = x.right; x.right = h; x.color = h.color; h.color = RED; x.N = h.N; h.N = 1+ size(h.left) + size(h.right); return x;&#125; 右旋的动态示意图： 颜色反转当出现一个临时的 4-node 的时候，即一个节点的两个子节点均为红色，如下图：我们需要将 E 提升至父节点，操作方法很简单，就是把 E 对子节点的连线设置为黑色，自己的颜色设置为红色。颜色反转之后颜色如下：实现代码如下： 123456789/** * 颜色转换 * @param h */private void flipColors(Node h)&#123; h.color = RED;//父结点颜色变红 h.left.color = BLACK;//子结点颜色变黑 h.right.color = BLACK;//子结点颜色变黑&#125; 注意：以上的旋转和颜色反转操作都是针对单一结点的，反转或则颜色反转操作之后可能引起其父结点又不满足平衡性质。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第10天-二叉树","slug":"suanfa-10","date":"2017-10-29T13:17:09.000Z","updated":"2019-08-29T02:28:07.188Z","comments":true,"path":"suanfa-10.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-10.html","excerpt":"** 突破算法第10天-二叉树：** &lt;Excerpt in index | 首页摘要&gt;用java实现算法求出二叉树的高度","text":"** 突破算法第10天-二叉树：** &lt;Excerpt in index | 首页摘要&gt;用java实现算法求出二叉树的高度 &lt;The rest of contents | 余下全文&gt; 树 先序遍历：先访问根结点，然后左节点，最后右节点 中序遍历：先访问左结点，然后根节点，最后右节点 后续遍历：先访问左结点，然后右节点，最后根节点 java实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class TreeNode &#123; TreeNode left; TreeNode right; int val; TreeNode(int val) &#123; this.val = val; &#125; public static void main(String[] args) &#123; TreeNode root = new TreeNode(1); TreeNode left1 = new TreeNode(2); TreeNode left2 = new TreeNode(3); TreeNode right1 = new TreeNode(4); //创建一棵树 root.left = left1; left1.right = left2; root.right = right1; scanNodes(root); System.out.println(\"树的深度是：\" + getDepth(root)); System.out.println(\"非递归深度：\" + findDeep2(root)); &#125; // 递归返回二叉树的深度 static int getDepth(TreeNode root) &#123; if (root == null) &#123; return 0; &#125; int left = getDepth(root.left); int right = getDepth(root.right); return left &gt; right ? left + 1 : right + 1; &#125; static void scanNodes(TreeNode root) &#123; if (root == null) &#123; return; &#125;// System.out.println(root.val); //先序遍历 scanNodes(root.left);// System.out.println(root.val); //中序遍历 scanNodes(root.right); System.out.println(root.val); // 后序遍历 &#125; // 非递归求深度 public static int findDeep2(TreeNode root) &#123; if (root == null) return 0; TreeNode current = null; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); int cur, next; int level = 0; while (!queue.isEmpty()) &#123; cur = 0; //当遍历完当前层以后，队列里元素全是下一层的元素，队列的长度是这一层的节点的个数 next = queue.size(); while (cur &lt; next) &#123; current = queue.poll(); cur++; //把当前节点的左右节点入队（如果存在的话） if (current.left != null) &#123; queue.offer(current.left); &#125; if (current.right != null) &#123; queue.offer(current.right); &#125; &#125; level++; &#125; return level; &#125;&#125; 树的变种二叉查找树，平衡二叉查找树，红黑树，b树红黑树和平衡二叉树（AVL树）类似，都是在进行插入和删除操作时通过特定操作保持二叉查找树的平衡，从而获得较高的查找性能。红黑树和AVL树的区别在于它使用颜色来标识结点的高度，它所追求的是局部平衡而不是AVL树中的非常严格的平衡。由于二叉树的效率和深度息息相关，于是出现了多路的B树，B+树等等。b树是叶子为n的平衡树。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第九天-排序算法比较","slug":"suanfa-9","date":"2017-10-28T15:10:58.000Z","updated":"2019-08-29T02:28:19.550Z","comments":true,"path":"suanfa-9.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-9.html","excerpt":"** 突破算法第九天-排序算法比较：** &lt;Excerpt in index | 首页摘要&gt;排序算法个有千秋，有的性能高，有的性能很低。这就要求我们对常用的排序算法要全面了解，不要用错了算法，导致性能问题。","text":"** 突破算法第九天-排序算法比较：** &lt;Excerpt in index | 首页摘要&gt;排序算法个有千秋，有的性能高，有的性能很低。这就要求我们对常用的排序算法要全面了解，不要用错了算法，导致性能问题。 &lt;The rest of contents | 余下全文&gt; 排序算法性能比较·借一张网路上的比较图。特别直观。 \b\b排序算法总结个人看法： 一般的情况还是以快速排序为主， 对于多个有序的数组合并的情况使用归并排序 性能要求快，空间足够，待排序的元素都要在一定的范围内使用桶排序","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第八天-桶排序","slug":"suanfa-8","date":"2017-10-27T14:51:06.000Z","updated":"2019-08-29T02:28:31.888Z","comments":true,"path":"suanfa-8.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-8.html","excerpt":"** 突破算法第八天-桶排序：** &lt;Excerpt in index | 首页摘要&gt;桶排序是个神奇的排序，在某些情况下可以达到O(N)的复杂度，快的离谱。但是桶排序是利用空间换时间，在空间充足的情况下，可以用桶排序进行高效的排序。","text":"** 突破算法第八天-桶排序：** &lt;Excerpt in index | 首页摘要&gt;桶排序是个神奇的排序，在某些情况下可以达到O(N)的复杂度，快的离谱。但是桶排序是利用空间换时间，在空间充足的情况下，可以用桶排序进行高效的排序。 &lt;The rest of contents | 余下全文&gt; 桶排序的基本原理将阵列分到有限数量的桶子里。每个桶子再个别排序（有可能再使用别的排序算法或是以递回方式继续使用桶排序进行排序）。当要被排序的阵列内的数值是均匀分配的时候，桶排序使用线性时间（Θ（n））。但桶排序并不是 比较排序，他不受到 O(nlogn) 下限的影响， 简单来说，就是把数据分组，放在一个个的桶中，然后对每个桶里面的在进行排序 桶排序的java实现12345678910111213141516171819202122232425262728293031323334353637383940414243public static void bucketSort1(int[] arr)&#123; //分桶，这里采用映射函数f(x)=x/10。 int bucketCount =10; Integer[][] bucket = new Integer[bucketCount][arr.length]; for (int i=0; i&lt;arr.length; i++)&#123; int quotient = arr[i]/10; for (int j=0; j&lt;arr.length; j++)&#123; if (bucket[quotient][j]==null)&#123; bucket[quotient][j]=arr[i]; break; &#125; &#125; &#125; //小桶排序 for (int i=0; i&lt;bucket.length; i++)&#123; //insertion sort for (int j=1; j&lt;bucket[i].length; ++j)&#123; if(bucket[i][j]==null)&#123; break; &#125; int value = bucket[i][j]; int position=j; while (position&gt;0 &amp;&amp; bucket[i][position-1]&gt;value)&#123; bucket[i][position] = bucket[i][position-1]; position--; &#125; bucket[i][position] = value; &#125; &#125; //输出 for (int i=0, index=0; i&lt;bucket.length; i++)&#123; for (int j=0; j&lt;bucket[i].length; j++)&#123; if (bucket[i][j]!=null)&#123; arr[index] = bucket[i][j]; index++; &#125; else&#123; break; &#125; &#125; &#125; &#125; 算法复杂度前面说的几大排序算法 ，大部分时间复杂度都是O（n2），也有部分排序算法时间复杂度是O(nlogn)。而桶式排序却能实现O（n）的时间复杂度。但桶排序的缺点是： 首先是空间复杂度比较高，需要的额外开销大。排序有两个数组的空间开销，一个存放待排序数组，一个就是所谓的桶，比如待排序值是从0到m-1，那就需要m个桶，这个桶数组就要至少m个空间。 其次待排序的元素都要在一定的范围内，限制较多。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第七天-堆排序","slug":"suanfa-7","date":"2017-10-26T14:50:57.000Z","updated":"2019-08-29T02:28:44.226Z","comments":true,"path":"suanfa-7.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-7.html","excerpt":"** 突破算法第七天-堆排序：** &lt;Excerpt in index | 首页摘要&gt;堆排序是利用二叉树的原理实现的一种排序，难点在于要构建堆,构建堆一般可以采用下沉或者上浮的算法进行。","text":"** 突破算法第七天-堆排序：** &lt;Excerpt in index | 首页摘要&gt;堆排序是利用二叉树的原理实现的一种排序，难点在于要构建堆,构建堆一般可以采用下沉或者上浮的算法进行。 &lt;The rest of contents | 余下全文&gt; 堆排序的基本原理初始时把要排序的n个数的序列看作是一棵顺序存储的二叉树（一维数组存储二叉树），调整它们的存储序，使之成为一个堆，将堆顶元素输出，得到n 个元素中最小(或最大)的元素，这时堆的根节点的数最小（或者最大）。然后对前面(n-1)个元素重新调整使之成为堆，输出堆顶元素，得到n 个元素中次小(或次大)的元素。依此类推，直到只有两个节点的堆，并对它们作交换，最后得到有n个节点的有序序列。称这个过程为堆排序。因此，实现堆排序需解决两个问题： 如何将n 个待排序的数建成堆； 输出堆顶元素后，怎样调整剩余n-1 个元素，使其成为一个新堆。 堆排序java实现1234567891011121314151617181920212223242526272829303132public static void sort(int[] a) &#123; int n = a.length; for (int k = n / 2; k &gt;= 1; k--) sink(a, k, n); while (n &gt; 1) &#123; swap(a, 1, n--); sink(a, 1, n); &#125; &#125;private static void sink(int[] a, int k, int n) &#123; while (2 * k &lt;= n) &#123; int j = 2 * k; if (j &lt; n &amp;&amp; a[j - 1] &lt; a[j + 1 - 1]) j++; if (a[k - 1] &gt;= a[j - 1]) break; swap(a, k, j); k = j; &#125;&#125;private static void swap(int[] a, int i, int j) &#123; int swap = a[i - 1]; a[i - 1] = a[j - 1]; a[j - 1] = swap;&#125;public static void main(String[] args) &#123; int[] arr = &#123;49, 38, 65, 97, 76, 13, 27, 4, 78, 34, 12, 64, 1, 8&#125;; sort(arr); System.out.println(\"排序之后：\"); System.out.println(Arrays.toString(arr));&#125; 算法复杂度堆排序的平均时间复杂度为Ο(nlogn)","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第六天-冒泡排序","slug":"suanfa-6","date":"2017-10-25T14:06:06.000Z","updated":"2019-08-29T02:28:56.527Z","comments":true,"path":"suanfa-6.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-6.html","excerpt":"** 突破算法第六天-冒泡排序：** &lt;Excerpt in index | 首页摘要&gt;冒泡排序也非常简单，效率比较低。了解即可。","text":"** 突破算法第六天-冒泡排序：** &lt;Excerpt in index | 首页摘要&gt;冒泡排序也非常简单，效率比较低。了解即可。 &lt;The rest of contents | 余下全文&gt; 冒泡排序的原理在要排序的一组数中，对当前还未排好序的范围内的全部数，自上而下对相邻的两个数依次进行比较和调整，让较大的数往下沉，较小的往上冒。即：每当两相邻的数比较后发现它们的排序与排序要求相反时，就将它们互换。 冒泡排序的java实现1234567891011private static void bubbleSort(int a[], int n) &#123; for (int i = 0; i &lt; n - 1; ++i) &#123; for (int j = 0; j &lt; n - i - 1; ++j) &#123; if (a[j] &gt; a[j + 1]) &#123; int tmp = a[j]; a[j] = a[j + 1]; a[j + 1] = tmp; &#125; &#125; &#125;&#125; 算法复杂度冒泡排序的复杂度为O(n^2)","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第五天-选择排序","slug":"suanfa-5","date":"2017-10-24T13:46:33.000Z","updated":"2019-08-29T02:29:09.780Z","comments":true,"path":"suanfa-5.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-5.html","excerpt":"** 突破算法第五天-选择排序：** &lt;Excerpt in index | 首页摘要&gt;选择排序很简单，属于交换排序算法。通过比较找到最大值或最小值，然后进行交换。","text":"** 突破算法第五天-选择排序：** &lt;Excerpt in index | 首页摘要&gt;选择排序很简单，属于交换排序算法。通过比较找到最大值或最小值，然后进行交换。 &lt;The rest of contents | 余下全文&gt; 选择排序的原理首先找到数组中最小的元素，与数组第一个元素交换，然后在剩下的元素中选择最小的，与第二个元素交换，以此类推，直到排序完成。 选择排序的java实现1234567891011121314private static void sort(int[] a) &#123; int len = a.length; for (int i = 1; i &lt; len; i++) &#123; for (int j = i; j &gt; 0 &amp;&amp; (a[j] &lt; a[j - 1]); j--) &#123; swap(a, j, j - 1); &#125; &#125;&#125;private static void swap(int[] a, int i, int j) &#123; int swap = a[i]; a[i] = a[j]; a[j] = swap;&#125; 算法复杂度选择排序的算法复杂度是O(n^2) 改进 每次选择的时候把最大值和最小值都比较出来，双向进行交换排序","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第一天-归并排序","slug":"suanfa-4","date":"2017-10-23T15:56:27.000Z","updated":"2019-08-29T02:29:22.206Z","comments":true,"path":"suanfa-4.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-4.html","excerpt":"** 突破算法第一天-归并排序：** &lt;Excerpt in index | 首页摘要&gt;归并排序是利用分治思想进行排序的典型应用，特别是对几个基本有序的子序列合并时，效率最高。在实际应用中，分布式应用，分布式查询排序会比较多应用到归并排序。","text":"** 突破算法第一天-归并排序：** &lt;Excerpt in index | 首页摘要&gt;归并排序是利用分治思想进行排序的典型应用，特别是对几个基本有序的子序列合并时，效率最高。在实际应用中，分布式应用，分布式查询排序会比较多应用到归并排序。 &lt;The rest of contents | 余下全文&gt; 归并排序的原理归并（Merge）排序法是将两个（或两个以上）有序表合并成一个新的有序表，即把待排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。归并排序分为两种种，第一种是自底向上的归并。 第二种是自顶向下的归并。 自底向上的归并排序java实现1234567891011121314151617181920212223242526272829303132333435public class MergeSortBU &#123; private static void merge(int[] a, int[] aux, int lo, int mid, int hi) &#123; // 复制到aux[] for (int k = lo; k &lt;= hi; k++) &#123; aux[k] = a[k]; &#125; // 合并回 a[] int i = lo, j = mid + 1; for (int k = lo; k &lt;= hi; k++) &#123; if (i &gt; mid) a[k] = aux[j++]; else if (j &gt; hi) a[k] = aux[i++]; else if (aux[j] &lt; aux[i]) a[k] = aux[j++]; else a[k] = aux[i++]; &#125; &#125; public static void mergeSort(int[] a) &#123; int n = a.length; int[] aux = new int[n]; for (int len = 1; len &lt; n; len *= 2) &#123; for (int lo = 0; lo &lt; n - len; lo += len + len) &#123; int mid = lo + len - 1; int hi = Math.min(lo + len + len - 1, n - 1); merge(a, aux, lo, mid, hi); &#125; &#125; &#125; public static void main(String[] args) &#123; int[] arr = &#123;49, 38, 65, 97, 76, 13, 27, 4, 78, 34, 12, 64, 1, 8&#125;; mergeSort(arr); System.out.println(\"排序之后：\"); System.out.println(Arrays.toString(arr)); &#125;&#125; 自顶向下的归并排序java实现1234567891011121314151617181920212223private static void sort(int[] a, int low, int high) &#123; if (high &lt;= low) return; int mid = low + (high - low) / 2; sort(a, low, mid); sort(a, mid + 1, high); merge(a, low, mid, high); &#125; private static void merge(int[] a, int lo, int mid, int hi) &#123; // 复制到aux[] int[] aux = new int[a.length]; for (int k = lo; k &lt;= hi; k++) &#123; aux[k] = a[k]; &#125; // 合并回 a[] int i = lo, j = mid + 1; for (int k = lo; k &lt;= hi; k++) &#123; if (i &gt; mid) a[k] = aux[j++]; else if (j &gt; hi) a[k] = aux[i++]; else if (aux[j] &lt; aux[i]) a[k] = aux[j++]; else a[k] = aux[i++]; &#125; &#125; 算法复杂度归并排序的算法复杂度是nlgn 应用场景 几个基本有序的数组进行排序 部分有序的数组","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第三天-希尔排序","slug":"suanfa-3","date":"2017-10-22T13:51:03.000Z","updated":"2019-08-29T02:29:34.152Z","comments":true,"path":"suanfa-3.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-3.html","excerpt":"** 突破算法第三天-希尔排序：** &lt;Excerpt in index | 首页摘要&gt;希尔排序平常用的比较少，主要是基于插入排序的改进。但是希尔排序的性能很高，数组越大，性能优势越明显。","text":"** 突破算法第三天-希尔排序：** &lt;Excerpt in index | 首页摘要&gt;希尔排序平常用的比较少，主要是基于插入排序的改进。但是希尔排序的性能很高，数组越大，性能优势越明显。 &lt;The rest of contents | 余下全文&gt; 希尔排序的基本原理基本思想：先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录“基本有序”时，再对全体记录进行依次直接插入排序。操作方法： 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1； 按增量序列个数k，对序列进行k 趟排序； 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 希尔排序java实现12345678910111213141516171819public static void shellSort(int[] a) &#123; int n = a.length; int h = 1; while (h &lt; n/3) h = 3*h + 1; while (h &gt;= 1) &#123; // h-sort the array for (int i = h; i &lt; n; i++) &#123; for (int j = i; j &gt;= h &amp;&amp; (a[j]&lt; a[j-h]); j -= h) &#123; swap(a, j, j-h); &#125; &#125; h /= 3; &#125;&#125;private static void swap(int[] a, int i, int j) &#123; int swap = a[i]; a[i] = a[j]; a[j] = swap;&#125; 算法复杂度希尔排序时效分析很难，关键码的比较次数与记录移动次数依赖于增量因子序列d的选取，是一个不稳定排序算法 适用场景","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第二天-插入排序","slug":"suanfa-2","date":"2017-10-21T01:41:27.000Z","updated":"2019-08-29T02:29:46.170Z","comments":true,"path":"suanfa-2.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-2.html","excerpt":"** 突破算法第二天-插入排序：** &lt;Excerpt in index | 首页摘要&gt;今天是突破算法第二天，插入排序，比较简单。效率比较低，但是思想很广泛，应用很广，是很多高级排序算法的一个子过程。","text":"** 突破算法第二天-插入排序：** &lt;Excerpt in index | 首页摘要&gt;今天是突破算法第二天，插入排序，比较简单。效率比较低，但是思想很广泛，应用很广，是很多高级排序算法的一个子过程。 &lt;The rest of contents | 余下全文&gt; 插入排序的原理 将一个记录插入到已排序好的有序表中，从而得到一个新，记录数增1的有序表。即：先将序列的第1个记录 看成是一个有序的子序列，然后从第2个记录逐个进行插入，直至整个序列有序为止。 要点：设立哨兵，作为临时存储和判断数组边界之用 插入排序java实现123456789101112public static void insertSort(int[] a, int n) &#123; for (int i = 0; i &lt; n; i++) &#123; for (int j = i; j &gt; 0 &amp;&amp; (a[j]&lt;a[j-1]); j--) &#123; swap(a, j, j-1); &#125; &#125; &#125; private static void swap(int[] a, int i, int j) &#123; int swap = a[i]; a[i] = a[j]; a[j] = swap; &#125; 算法复杂度插入排序的复杂度为O（n^2） 改进方法希尔排序，其他的插入排序有二分插入排序，2-路插入排序。 适用场景插入排序比较适合部分有序的数组（以下四种数组） 数组中每个元素距离它的最终位置都不远 一个有序的大数组接一个小数组 数组中只有几个位置不正确 数组比较小","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第一天-快速排序","slug":"suanfa-1","date":"2017-10-20T15:46:59.000Z","updated":"2019-08-29T02:29:59.957Z","comments":true,"path":"suanfa-1.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-1.html","excerpt":"** 突破算法第一天-快速排序：** &lt;Excerpt in index | 首页摘要&gt;30天突破算法是我给自己定的一个学习计划，希望在这30天，每天都能完成计划。第一天学习最重要的快速排序。","text":"** 突破算法第一天-快速排序：** &lt;Excerpt in index | 首页摘要&gt;30天突破算法是我给自己定的一个学习计划，希望在这30天，每天都能完成计划。第一天学习最重要的快速排序。 &lt;The rest of contents | 余下全文&gt; 30天突破算法算法种类不计其数，说30天突破只是给自己定的学习计划。目的是通过30天的记录熟悉常见的算法，提高自己的算法能力。对以后的工作来说也是打下夯实的基础。 快速排序的原理快速排序也是分治法思想的一种实现，他的思路是使数组中的每个元素与基准值（Pivot，通常是数组的首个值，A[0]）比较，数组中比基准值小的放在基准值的左边，形成左部；大的放在右边，形成右部；接下来将左部和右部分别递归地执行上面的过程：选基准值，小的放在左边，大的放在右边。重复此过程，直到排序结束。步骤如下： 1.找基准值，设Pivot = a[0] 2.分区（Partition）：比基准值小的放左边，大的放右边，基准值(Pivot)放左部与右部的之间。 3.进行左部（a[0] - a[pivot-1]）的递归，以及右部（a[pivot+1] - a[n-1]）的递归，重复上述步骤。 快速排序java实现（递归版）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class QuickSort &#123; public static void main(String[] args) &#123; int[] a=&#123;49,38,65,97,76,13,27,49,78,34,12,64,1,8&#125;; System.out.println(\"排序之前：\"); for (int i = 0; i &lt; a.length; i++) &#123; System.out.print(a[i]+\" \"); &#125; //快速排序 quick(a); System.out.println(); System.out.println(\"排序之后：\"); for (int i = 0; i &lt; a.length; i++) &#123; System.out.print(a[i]+\" \"); &#125; &#125; private static void quick(int[] a) &#123; if(a.length&gt;0)&#123; quickSort(a,0,a.length-1); &#125; &#125; private static void quickSort(int[] a, int low, int high) &#123; if(low&lt;high)&#123; //如果不加这个判断递归会无法退出导致堆栈溢出异常 int middle = getMiddle(a,low,high); quickSort(a, 0, middle-1); quickSort(a, middle+1, high); &#125; &#125; private static int getMiddle(int[] a, int low, int high) &#123; int temp = a[low];//基准元素 while(low&lt;high)&#123; //找到比基准元素小的元素位置 while(low&lt;high &amp;&amp; a[high]&gt;=temp)&#123; high--; &#125; a[low] = a[high]; while(low&lt;high &amp;&amp; a[low]&lt;=temp)&#123; low++; &#125; a[high] = a[low]; &#125; a[low] = temp; return low; &#125;&#125; 快速排序三向切分法（改进的实现）12345678910111213141516171819private static void quick3Sort(int[] a, int low, int high) &#123; if (low &gt;= high) return; int lt = low, gt = high; int temp = a[low]; int i = low; while (i &lt;= gt) &#123; if (a[i] &lt; temp) swap(a, lt++, i++); else if (a[i] &gt; temp) swap(a, i, gt--); else i++; &#125; quick3Sort(a, low, lt - 1); quick3Sort(a, gt + 1, high);&#125;private static void swap(int[] a, int i, int j) &#123; int swap = a[i]; a[i] = a[j]; a[j] = swap;&#125; 快速排序的复杂度时间复杂度 nlogn,排序方法中平均性能最好的。但若初始序列按关键码有序或基本有序时，快排序反而蜕化为冒泡排序。快速排序是一个不稳定的排序方法。 改进方法 当数组比较小的时候，快速排序比插入排序慢，这个时候用插入排序替换比较好。 通常以“三者取中法”来选取基准记录，即将排序区间的两个端点与中点三个记录关键码居中的调整为支点记录 适用场景 普通的无序集合排序，使用快速排序。 包含很多重复元素的集合排序，使用三向切分的快速排序。 基本有序的集合使用归并排序。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"学习计划-30天突破算法","slug":"study-plan","date":"2017-10-18T11:47:55.000Z","updated":"2019-08-29T02:30:13.270Z","comments":true,"path":"study-plan.html","link":"","permalink":"http://zhangfuxin.cn/study-plan.html","excerpt":"** 学习计划-30天突破算法：** &lt;Excerpt in index | 首页摘要&gt;作为一个非专业出身的程序员，一直对算法的学习赶紧断断续续，终于下定决心对算法做一次详细总结。30天时间把程序员常用算法逐一突破。这次计划更是对自己的一次挑战，希望自己能坚持到最后！","text":"** 学习计划-30天突破算法：** &lt;Excerpt in index | 首页摘要&gt;作为一个非专业出身的程序员，一直对算法的学习赶紧断断续续，终于下定决心对算法做一次详细总结。30天时间把程序员常用算法逐一突破。这次计划更是对自己的一次挑战，希望自己能坚持到最后！ &lt;The rest of contents | 余下全文&gt; 学习排序算法的意义 学会比较算法的性能的方法 相关的排序能解决类似的问题 排序算法很多时候是解决问题的第一步 排序算法 快速排序 插入排序 希尔排序 归并排序 选择排序 冒泡排序 堆排序 桶排序 排序算法比较 树 二叉树高度和二叉树的遍历 红黑树 b树 查找算法 二分查找 二叉查找树 平衡查找树 散列表 算法思想 递归（普通递归，尾递归） 动态规划 贪婪算法 分治法 图的算法 深度优先 广度优先 最小生成树 最短路径 字符串算法 字符串查找 单词查找树 子字符串查找 典型算法分析 拓扑排序 关键路径排序 遗传算法 RSA算法 英语技术文档阅读突破 熟悉常用技术词汇 阅读常见的技术文档（官网文档看一遍） 记住常用的词汇 阅读英文技术书籍","categories":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/categories/算法/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/tags/algorithm/"}],"keywords":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/categories/算法/"}]},{"title":"用java将GBK工程转为uft8","slug":"trandsferProject","date":"2017-08-11T23:43:59.000Z","updated":"2019-08-29T02:27:34.042Z","comments":true,"path":"trandsferProject.html","link":"","permalink":"http://zhangfuxin.cn/trandsferProject.html","excerpt":"** 用java将GBK工程转为uft8：** &lt;Excerpt in index | 首页摘要&gt;windows下的默认编码为GBK还有gb2312，如何把gbk的java工程转为utf8的呢，如果直接修改工程编码，其实里面的java文件中中文是会乱码的，写了个批量转换java工程的程序，消遣一下。","text":"** 用java将GBK工程转为uft8：** &lt;Excerpt in index | 首页摘要&gt;windows下的默认编码为GBK还有gb2312，如何把gbk的java工程转为utf8的呢，如果直接修改工程编码，其实里面的java文件中中文是会乱码的，写了个批量转换java工程的程序，消遣一下。 &lt;The rest of contents | 余下全文&gt; 为什么要转码？有些老的项目，或者朋友的项目之前没注意在windows上不是utf8，而你有需要看注释或者什么，总不能一个文件一个文件的去改编码属性吧。 本程序试用范围gbk的代码，或者gb2312的工程均可以转换 编码转换的思路本来想做成一个通用的会自动检测编码，自动转换的程序。但是由于判断编码类型不准，所以做成了针对GBK的转换。 制定gbk编码把文件流读进来，加载到内存，转为String类型的内容 将String内容转为utf8的String 将String内容写入文件 核心代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class TransferProject &#123; public static void transferFile(String pathName, int depth) throws Exception &#123; File dirFile = new File(pathName); if (!isValidFile(dirFile)) return; //获取此目录下的所有文件名与目录名 String[] fileList = dirFile.list(); int currentDepth = depth + 1; for (int i = 0; i &lt; fileList.length; i++) &#123; String string = fileList[i]; File file = new File(dirFile.getPath(), string); String name = file.getName(); //如果是一个目录，搜索深度depth++，输出目录名后，进行递归 if (file.isDirectory()) &#123; //递归 transferFile(file.getCanonicalPath(), currentDepth); &#125; else &#123; if (name.contains(\".java\") || name.contains(\".properties\") || name.contains(\".xml\")) &#123; readAndWrite(file); System.out.println(name + \" has converted to utf8 \"); &#125; &#125; &#125; &#125; private static boolean isValidFile(File dirFile) throws IOException &#123; if (dirFile.exists()) &#123; System.out.println(\"file exist\"); return true; &#125; if (dirFile.isDirectory()) &#123; if (dirFile.isFile()) &#123; System.out.println(dirFile.getCanonicalFile()); &#125; return true; &#125; return false; &#125; private static void readAndWrite(File file) throws Exception &#123; String content = FileUtils.readFileByEncode(file.getPath(), \"GBK\"); FileUtils.writeByBufferedReader(file.getPath(), new String(content.getBytes(\"UTF-8\"), \"UTF-8\")); &#125; public static void main(String[] args) throws Exception &#123; //程序入口，制定src的path String path = \"/Users/mac/Downloads/unit06_jdbc/src\"; transferFile(path, 1); &#125;&#125; 123456789101112131415161718192021222324252627282930313233public class FileUtils &#123; public static void writeByBufferedReader(String path, String content) &#123; try &#123; File file = new File(path); file.delete(); if (!file.exists()) &#123; file.createNewFile(); &#125; FileWriter fw = new FileWriter(file, false); BufferedWriter bw = new BufferedWriter(fw); bw.write(content); bw.flush(); bw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static String readFileByEncode(String path, String chatSet) throws Exception &#123; InputStream input = new FileInputStream(path); InputStreamReader in = new InputStreamReader(input, chatSet); BufferedReader reader = new BufferedReader(in); StringBuffer sb = new StringBuffer(); String line = reader.readLine(); while (line != null) &#123; sb.append(line); sb.append(\"\\r\\n\"); line = reader.readLine(); &#125; return sb.toString(); &#125;&#125; 总结遇到类似的问题，都可以试着用代码来进行实现，给自己的编码带来一些新的乐趣，也增加自己的信心。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}],"tags":[{"name":"java","slug":"java","permalink":"http://zhangfuxin.cn/tags/java/"}],"keywords":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}]},{"title":"阿拉伯数字转汉字写法","slug":"num2Chinese","date":"2017-07-29T14:25:27.000Z","updated":"2019-08-29T02:31:56.019Z","comments":true,"path":"num2Chinese.html","link":"","permalink":"http://zhangfuxin.cn/num2Chinese.html","excerpt":"** 阿拉伯数字转汉字写法：** &lt;Excerpt in index | 首页摘要&gt;找工作时看到“某团”的题目，把一个int的数字转为汉字的读法，比如123，转成一百二十三，限时20分钟。如果二十分钟做不出来，简历就不要投了。说实话，20分钟能调通的人真的不多，感觉某团还是装逼成分太多！","text":"** 阿拉伯数字转汉字写法：** &lt;Excerpt in index | 首页摘要&gt;找工作时看到“某团”的题目，把一个int的数字转为汉字的读法，比如123，转成一百二十三，限时20分钟。如果二十分钟做不出来，简历就不要投了。说实话，20分钟能调通的人真的不多，感觉某团还是装逼成分太多！ &lt;The rest of contents | 余下全文&gt; 题目要求用java实现，把int的数字转为汉字读音，比如123，转成一百二十三，10020转为一万零二十 思路分析中文计数的特点，以万为小节，万以内的都是以“十百千”为权位单独计数，比如一千百，一千千都是非法的。而“十百千”这样的权位可以与“万”，“亿”进行搭配，二十亿，五千万等等。 中文数字的零中文的零的使用总结起来有三个规则， 以10000为小节，结尾是0，不使用零，比如1020 以10000为小节，小节内两个非0数字之间需要零 小节的千位是0，若小节前无其他数字，不用零，否者用零 完整代码（参考算法的乐趣第四章）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class NumberTransfer &#123; public final String[] chnNumChar = new String[]&#123;\"零\", \"一\", \"二\", \"三\", \"四\", \"五\", \"六\", \"七\", \"八\", \"九\"&#125;; public final String[] chnUnitSection = new String[]&#123;\"\", \"万\", \"亿\", \"万亿\"&#125;; public final String[] chnUnitChar = new String[]&#123;\"\", \"十\", \"百\", \"千\"&#125;; @Test public void testNumberToChinese() &#123; int[] nums = new int[]&#123;304, 4006, 4000, 10003, 10030, 21010011, 101101101&#125;; for (int i = 0; i &lt; nums.length; i++) &#123; System.out.println(numberToChinese(nums[i])); &#125; &#125; public String numberToChinese(int num) &#123; String strIns; String chnStr = \"\"; int unitPos = 0; boolean needZero = false; if (num == 0) return \"零\"; while (num &gt; 0) &#123; strIns = \"\"; int section = num % 10000; if (needZero) &#123; chnStr = chnNumChar[0] + chnStr; &#125; // 添加节权（万，亿） strIns += (section != 0) ? chnUnitSection[unitPos] : chnUnitSection[0]; chnStr = strIns + chnStr; // 以万为单位，求万以内的权位 chnStr = sectionToChinese(section, chnStr); needZero = (section &lt; 1000) &amp;&amp; (section &gt; 0); num = num / 10000; unitPos++; &#125; return chnStr; &#125; private String sectionToChinese(int section, String chnStr) &#123; String strIns; int unitPos = 0; boolean zero = true; while (section &gt; 0) &#123; int v = section % 10; if (v == 0) &#123; if (section == 0 || !zero) &#123; zero = true;// zero确保不会出现多个零 chnStr = chnNumChar[v] + chnStr; &#125; &#125; else &#123; zero = false; strIns = chnNumChar[v]; // 此位置对应等中文数字 strIns += chnUnitChar[unitPos];// 此位置对应的权位 chnStr = strIns + chnStr; &#125; unitPos++; section = section / 10; &#125; return chnStr; &#125;&#125;","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"java面试大全自制版","slug":"java-interview","date":"2017-07-24T13:07:00.000Z","updated":"2019-08-29T02:59:50.120Z","comments":true,"path":"java-interview.html","link":"","permalink":"http://zhangfuxin.cn/java-interview.html","excerpt":"** java面试大全自制版：** &lt;Excerpt in index | 首页摘要&gt;java语言知识点多而杂，面试时很多人找不到重点。这份java面试大全，有部分网络上资源，大多数是从好的文章和书籍里总结出来的知识点。","text":"** java面试大全自制版：** &lt;Excerpt in index | 首页摘要&gt;java语言知识点多而杂，面试时很多人找不到重点。这份java面试大全，有部分网络上资源，大多数是从好的文章和书籍里总结出来的知识点。 &lt;The rest of contents | 余下全文&gt; 本书的目的每个java程序员在面试前都不知该准备什么？或者是随便看几个文章就去面试，这样的结果很容易失败！希望本书能给java程序员一个好的指引，让java程序员没有难找的工作！ 目录gitbook地址","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://zhangfuxin.cn/categories/学习笔记/"}],"tags":[{"name":"java","slug":"java","permalink":"http://zhangfuxin.cn/tags/java/"}],"keywords":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://zhangfuxin.cn/categories/学习笔记/"}]},{"title":"kobo aura one导出笔记高级配置","slug":"kobo-config","date":"2017-06-23T02:22:57.000Z","updated":"2019-08-21T15:13:24.129Z","comments":true,"path":"kobo-config.html","link":"","permalink":"http://zhangfuxin.cn/kobo-config.html","excerpt":"** kobo aura one导出笔记高级配置：** &lt;Excerpt in index | 首页摘要&gt;kobo电子书折腾记，导出笔记，从激活到设置，打补丁实现自定义配置，还是自己折腾起来有意思啊。","text":"** kobo aura one导出笔记高级配置：** &lt;Excerpt in index | 首页摘要&gt;kobo电子书折腾记，导出笔记，从激活到设置，打补丁实现自定义配置，还是自己折腾起来有意思啊。 &lt;The rest of contents | 余下全文&gt; 建议买电子书是为了阅读和学习，不是天天折腾电子书，一天刷一次机，如果只是看书，做笔记，学个英文什么的原生系统是最好的。如果看pdf为主，不建议买这电子书，看pdf首选电脑，平板，sony dsp系列，用普通的电子书阅读器，体验太差。 kobo原生系统的功能（推荐原生系统，打上补丁） 格式支持epub，mobi，cbz漫画，txt，kobo epub格式 高亮，笔记，导出笔记（需要配置一下） 字典（英文，中文，法文等多国字典，可以自己修改） 阅读pocket文章（可以把网页保存到pocket，实用pocket同步到阅读器） 自动亮度（最大的优点） koreader的功能 格式支持epub，mobi，cbz漫画，txt，kobo epub格式 扫描版pdf支持重拍，切边（最大特色） 笔记导出到印象笔记 字典（强大的字典扩展） 激活说明：wifi激活需要翻墙，可以实用笔记连接vpn，然后共享wifi给kobo wifi激活, kobo setup desktop激活，去kobo官网下载软件，然后电脑需要翻墙，电子书连接上电脑，用软件登录激活。这个软件很不好用，bug也多，建议使用wifi激活。 更新固件，打补丁kobo的更新固件，更新补丁都是一个模式，把固件或者补丁放到.kobo文件夹，弹出设备就会自动重启 字体电脑连接kobo，在根目录建立一个fonts文件夹，把需要的字体放进去即可 词典下载网上改好的字典，直接放到.kobo文件夹下的dict目录下，然后重启就可以了 自定义配置 刷新页数（打补丁） 上下页宽（打补丁） 全屏模式（修改配置文件） 字体高级设置（修改配置文件） 导出笔记和高亮（修改配置文件） kobo高级配置文件详解用电脑连接kobo电子书，打开Kobo找到eReader.conf文件，最好用notepad++修改，或者其他文本编辑器。 12345678910111213141516171819202122232425262728[FeatureSettings]#导出笔记ExportHighlightsEnabled=true#显示全书的页码，而不是章节的页码FullBookPageNumbers=true#用在线等维基百科代替词典查询OnlineWikipedia=true#全屏阅读FullScreenReading=true#图片缩放ImageZoom=true#浏览器全屏FullScreenBrowser=true#关机键截图，但是关机键就无法关机了，不要设置这个鸡肋的功能Screenshots=true[Reading]#翻页刷新的页数，20页全刷一次numPartialUpdatePageTurns=20#左边距readingLeftMargin=0#右边距readingRightMargin=0#行高readingLineHeight=1.4[PowerOptions]#自动关机时间AutoOffMinutes=60 博客搬家，请访问新博客地址吧! 我的博客","categories":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/categories/others/"}],"tags":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/tags/开发工具/"}],"keywords":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/categories/others/"}]},{"title":"Illegal mix of collations","slug":"mysql-collation","date":"2017-06-12T03:00:14.000Z","updated":"2019-08-29T02:32:44.417Z","comments":true,"path":"mysql-collation.html","link":"","permalink":"http://zhangfuxin.cn/mysql-collation.html","excerpt":"** mysql排序字符集问题：** &lt;Excerpt in index | 首页摘要&gt;mysql表的每个字段都可以设置单独的排序字符集和文本字符集，如果你创建表的时候不注意，很可能会遇到Illegal mix of collations这个问题。","text":"** mysql排序字符集问题：** &lt;Excerpt in index | 首页摘要&gt;mysql表的每个字段都可以设置单独的排序字符集和文本字符集，如果你创建表的时候不注意，很可能会遇到Illegal mix of collations这个问题。 &lt;The rest of contents | 余下全文&gt; 问题描述用mysql进行两个表的联合查询的时候，出现下面的错误。 1Illegal mix of collations (utf8_unicode_ci,IMPLICIT) and (utf8_general_ci,IMPLICIT) for operation &apos;=&apos; 排查过程 通过google搜索找到原因，这个错误是mysql的排序字符集不一致导致的。 把联合查询的表使用navicat查看字段的设置，发现了有一个关联字段排序字符集的问题，如图： 这两个表中openid的排序规则不一致，导致出现问题。 解决方法将user表中的字符集和排序规则设置为默认，保持一致即可。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://zhangfuxin.cn/tags/mysql/"}],"keywords":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}]},{"title":"mongodb从入门到精通","slug":"mongodb-study","date":"2017-05-26T15:21:23.000Z","updated":"2019-08-21T15:13:24.131Z","comments":true,"path":"mongodb-study.html","link":"","permalink":"http://zhangfuxin.cn/mongodb-study.html","excerpt":"** mongodb从入门到精通** &lt;Excerpt in index | 首页摘要&gt; mongodb日常使用的一些知识，增删改查，索引，分片。","text":"** mongodb从入门到精通** &lt;Excerpt in index | 首页摘要&gt; mongodb日常使用的一些知识，增删改查，索引，分片。 &lt;The rest of contents | 余下全文&gt; mongodb学习1.mongodb特性1）mongo是一个面向文档的数据库，它集合了nosql和sql数据库两方面的特性。 2）所有实体都是在首次使用时创建。 3）没有严格的事务特性，但是它保证任何一次数据变更都是原子性的。 4）也没有固定的数据模型 5）mongo以javascript作为命令行执行引擎，所以利用shell进行复杂的计算和查询时会相当的慢。 6）mongo本身支持集群和数据分片 7）mongo是c++实现的，支持windows mac linux等主流操作系统 8）性能优越，速度快2.mongo常用操作增删操作123456db.user.insert(&#123;name:'aaaa',age:30&#125;);db.user.save(&#123;name:'aaaa',age:30&#125;);db.collection.insertOne(&#123;&#125;);//(3.2新特性)db.collection.deleteOne(&#123;&#125;,&#123;&#125;);//(3.2新特性)db.collection.remove(&#123;name:'aaa'&#125;);db.collection.remove();//(删除全部) 更新操作12345db.users.update(&#123;\"name\": \"joe\"&#125;, joe );//upsert模式db.users.update(&#123;\"name\": \"joe\"&#125;, joe, true );//MULTI模式db.users.update(&#123;\"name\": \"joe\"&#125;, joe, true ，true); update是对文档替换，而不是局部修改默认情况update更新匹配的第一条文档，multi模式更新所有匹配的 查询操作普通查询 123db.user.find();db.user.find(&#123;name:'aaa'&#125;);db.user.findOne(&#123;name:'aaa'&#125;); 模糊查询 12db.UserInfo.find(&#123;userName :'/A/'&#125;) //（名称%A%）db.UserInfo.find(&#123;userName :'/^A/'&#125;) //(名称A%) 操作符 $lt, $lte,$gt, $gte(&lt;, &lt;=, &gt;, &gt;= ) $all 数组中的元素是否完全匹配 db.things.find( { a: { $all: [ 2, 3 ] } } ); $exists 可选：true，false db.things.find( { a : { $exists : true } } ); $mod 取模：a % 10 == 1 db.things.find( { a : { $mod : [ 10 , 1 ] } } ); $ne 取反：即not equals db.things.find( { x : { $ne : 3 } } ); $in 类似于SQL的IN操作 db.things.find({j:{$in: [2,4,6]}}); $nin $in的反操作，即SQL的 NOT IN db.things.find({j:{$nin: [2,4,6]}}); $nor $or的反操作，即不匹配(a或b) db.things.find( { name : “bob”, $nor : [ { a : 1 },{ b : 2 }]}) $or Or子句，注意$or不能嵌套使用 db.things.find( { name : “bob” , $or : [ { a : 1 },{ b : 2 }]}) $size 匹配数组长度 db.things.find( { a : { $size: 1 } } ); $type 匹配子键的数据类型，详情请看 db.things.find( { a : { $type : 2 } } ); 数组查询$size 用来匹配数组长度（即最大下标）// 返回comments包含5个元素的文档db.posts.find({}, {comments:{‘$size’: 5}});// 使用冗余字段来实现db.posts.find({}, {‘commentCount’: { ‘$gt’: 5 }});$slice 操作符类似于子键筛选，只不过它筛选的是数组中的项// 仅返回数组中的前5项db.posts.find({}, {comments:{‘$slice’: 5}});// 仅返回数组中的最后5项db.posts.find({}, {comments:{‘$slice’: -5}});// 跳过数组中的前20项，返回接下来的10项db.posts.find({}, {comments:{‘$slice’: [20, 10]}});// 跳过数组中的最后20项，返回接下来的10项db.posts.find({}, {comments:{‘$slice’: [-20, 10]}});MongoDB 允许在查询中指定数组的下标，以实现更加精确的匹配// 返回comments中第1项的by子键为Abe的所有文档db.posts.find( { “comments.0.by” : “Abe” } ); 3.索引的使用创建索引12345678db.things.ensureIndex(&#123;'j': 1&#125;);//创建子文档 索引db.things.ensureIndex(&#123;'user.Name' : - 1&#125;);//创建 复合 索引db.things.ensureIndex(&#123;'j' : 1 , // 升序'x' : - 1 // 降序&#125;); 如果 您的 find 操作只用到了一个键，那么索引方向是无关紧要的 当创建复合索引的时候，一定要谨慎斟酌每个键的排序方向 修改索引修改索引，只需要重新 运行索引 命令即可如果索引已经存在则会 重建， 不存在的索引会被 添加 1234567891011db.things.ensureIndex (&#123; //原来的索引会 重建 'user.Name ' : - 1 , //新增一个升序 索引 'user.Name ' : 1 , //为 Age 新建降序 索引 'user.Age ' : - 1 //打开后台执行&#125;,&#123; 'background' : true&#125;);//重建索引db.things.reIndex(); 删除索引1234567891011121314//删除集合中的所有 索引db.things.dropIndexes (); //删除指定键的索引 db.things.dropIndex (&#123; x : 1 , y : - 1&#125;); //使用 command 删除指定键的 索引db.runCommand (&#123; dropIndexes : 'foo ' , index:&#123; y : 1 &#125;&#125;); //使用 command 删除所有 索引db.runCommand (&#123;dropIndexes : 'foo ',index: '*'&#125;) 如果是删除集合中所有的文档（remove）则不会影响索引，当有新文档插入时，索引就会重建。 唯一索引创建唯一索引，同时这也是一个符合唯一索引 12345678910db.things.ensureIndex (&#123; 'firstName ' : 1 , 'lastName ' : 1&#125;, &#123;//指定为唯一索引'unique': true ,//删除重复 记录'dropDups': true&#125;); 强制使用索引12345678910111213//强制使用索引 a 和 bdb.collection.find(&#123; 'a' : 4 , 'b' : 5 , 'c' : 6&#125;).hint(&#123; 'a' : 1 , 'b' : 1&#125;);//强制不使用任何 索引db.collection.find().hint(&#123; '$natural' : 1&#125;); 索引总结: 索引可以加速查询； 单个索引无需在意其索引方向； 多键索引需要慎重考虑每个索引的方向； 做海量数据更新时应当先卸载所有索引，待数据更新完成后再重建索引； 不要试图为每个键都创建索引，应考虑实际需要，并不是索引越多越好； 唯一索引可以用来消除重复记录； 地理空间索引是没有单位的，其内部实现是基本的勾股定理算法 4.mongo数据库管理安全与认证 默认为无认证，启动用登录 shell ； 添加账号； 关闭 shell .关闭 MongoDB ； 为 MongoDB 增加 — auth 参数； 重 启 MongoDB ； 登录 shell ，此时就需要认证了 冷备份 关闭MongoDB引擎 拷贝数据库文件夹及文件 恢复时反向操作即可 优点：可以完全保证数据完整性； 缺点：需要数据库引擎离线 热备份 保持MongoDB为运行状态 使用mongodump备份数据 使用mongorestore恢复数据 优点：数据库引擎无须离线 缺点：不能保证数据完整性，操作时会降低MongoDB性能 主从复制备份 创建主从复制机制 配置完成后数据会自动同步 恢复途径很多 优点：可以保持MongoDB处于联机状态，不影响性能 缺点：在数据写入密集的情况下可能无法保证数据完整性 修复db.repairDatabase(); 修复数据库还可以起到压缩数据的作用； 修复数据库的操作相当耗时，万不得已请不要使用； 建议经常做数据备份；5.mongo复制(集群) 主从复制选项 说明 –only 作用是限定仅复制指定的某个数据库–slavedelay 为复制设置操作延迟，单位为秒–fastsync 以主节点的数据快照为基础启动从节点。–autoresync 当主从节点数据不一致时，是否自动重新同步–oplogSize 设定主节点中的oplog的容量，单位是MB 副本集与普通主从复制集群相比，具有自动检测机制需要使用—replSet 选项指定副本同伴任何时候，副本集当中最多只允许有1个活跃节点 读写分离将密集的读取操作分流到从节点上，降低主节点的负载默认情况下，从节点是不允许处理客户端请求的，需要使用—slaveOkay打开不适用于实时性要求非常高的应用 工作原理—— OPLOGoplog保存在local数据库中，oplog就在其中的oplog.$main集合内保存。该集合的每个文档都记录了主节点上执行的一个操作，其键定义如下： ts：操作时间戳，占用4字节 op：操作类型，占用1字节 ns：操作对象的命名空间（或理解为集合全名） o：进一步指定所执行的操作，例如插入 工作原理—— 同步 从节点首次启动时，做完整同步 主节点数据发生变化时，做增量同步 从节点与主节点数据严重不一致时，做完整同步 复制管理—— 诊断db.printReplicationInfo()在主节点上使用 返回信息是oplog的大小以及各种操作的耗时. 空间占用等数据在从节点上使用db.printSlaveReplicationInfo() 返回信息是从节点的数据源列表. 同步延迟时间等 复制管理—— 变更OPLOG 容量在主节点上使用 设定—oplogSize参数 重启MongoDB 复制管理—— 复制认证主从节点皆须配置 存储在local.system.users 优先尝试repl用户 主从节点的用户配置必须保持一致 6.MONGODB分片分片与自动分片分片是指将数据拆分，分散到不同的实例上进行负载分流的做法。我们常说的“分表”、“分库”、“分区”等概念都属于分片的实际体现。传统分片做法是手工分表、分库。自动分片技术是根据指定的“片键”自动拆分数据并维护数据请求路由的过程。 递增片键–连续 不均匀 写入集中 分流较差 随机片键–不连续 均匀 写入分散 分流较好 三个组成部分 片,保存子集数据的容器 mongos,MongoDB的路由器进程 配置服务器,分·片集群的配置信息创建分片 –启动配置服务器,可以创建一个或多个 –添加片,每个片都应该是副本集 –物理服务器,性能、安全和稳定性管理分片12345678910//查询分片db.shards.find();//数据库db.databases.find();//块db.chunks.find();//分片状态db.printShardingStatus();//删除片db.runCommand(&#123; removeshard : 'ip:port' &#125;);","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}],"tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://zhangfuxin.cn/tags/mongodb/"}],"keywords":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}]},{"title":"hexo自用黑色主题","slug":"hexo-theme","date":"2017-05-23T03:32:59.000Z","updated":"2019-08-21T15:13:24.127Z","comments":true,"path":"hexo-theme.html","link":"","permalink":"http://zhangfuxin.cn/hexo-theme.html","excerpt":"** hexo和coding打造静态博客 ：** &lt;Excerpt in index | 首页摘要&gt;使用hexo一年有余，对所有主题都感觉有所缺陷，便修改了一个自用黑色主题，本主题以黑色和蓝色为主，色彩鲜明，主题明确。","text":"** hexo和coding打造静态博客 ：** &lt;Excerpt in index | 首页摘要&gt;使用hexo一年有余，对所有主题都感觉有所缺陷，便修改了一个自用黑色主题，本主题以黑色和蓝色为主，色彩鲜明，主题明确。 &lt;The rest of contents | 余下全文&gt; 主题图片 black-blue主题来源本主题修改自spfk主题，但之前spfk主题有很多问题，本主题改进如下： 压缩js，css提高性能 代码段样式显示更完美 增加本地搜索 设置更合适的字体大小 颜色以黑色和蓝色为主，色彩鲜明 seo适当优化 删除多说，有言，增加畅言评论 删除stylus，全部改用css方便修改 主题地址black-blue 注意：大家使用主题的时候，把主题配置文件_config.yml以下几项必须修改，项目里实用的是我博客的正式代码，请大家修改成自己的！ 12345678910google_analytics: xxxbaidu_analytics: xxxxxxxdisqus: on: false shortname: xxxx# 畅言评论changyan: on: true appid: xxxx conf: xxxxx black-blue主题配置切换主题复制主题到themes目录下cd themes &amp;&amp; git clone https://github.com/maochunguang/black-blue，修改_config.yml theme: black-blue 安装常用插件，建议全部安装123456789## rss插件npm install hexo-generator-feed --save## 站点sitemap生成插件npm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save## 百度url提交npm install hexo-baidu-url-submit --save## 本地搜索插件集成npm install hexo-generator-search --save 博客全局配置，修改根目录下_config.yml插件配置 1234Plugins:- hexo-generator-feed- hexo-generator-sitemap- hexo-generator-baidu-sitemap rss设置 1234feed: type: atom path: atom.xml limit: 20 本地搜索配置 123search: path: search.json field: post 站点地图，seo搜索引擎需要 1234sitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 主题配置菜单配置 123456789## 添加单独的页面:hexo new page about，about是页面的路径，也是名称## Tags Cloud Page: `hexo new page tags`menu: # 主页: /archives/ 所有文章: /archives/ 玩转开发工具: /categories/开发工具/ 玩转数码: /categories/digital 认知提升: /categories/cognition 关于我: /about/ 评论配置 123456789# 是否开启畅言评论，changyan: on: true appid: xxxx conf: xxxxxxxxxxxx# 是否开启disqus，disqus: on: false shortname: mmmmmm 其他配置，详细的配置请下载主题，都有注释1234567# 数学公式支持mathjax: false# Socail Share | 是否开启分享baidushare: true# 谷歌分析，百度分析，seo分析很有用google_analytics: xxxxxxbaidu_analytics: xxcxcxcsdsf 自定义配置（对前端技术有了解即可）显示更多和折叠文章你的md文件格式需要按下面的来： 12345678910title: 突破算法第11天-红黑树date: 2017-10-30 22:35:37tags: 算法categories: algorithm---** &#123;&#123; title &#125;&#125;：** &lt;Excerpt in index | 首页摘要&gt;红黑树&lt;!-- more --&gt;&lt;The rest of contents | 余下全文&gt;正文…… 头像配置在themes/black-blue/source/img/avatar.png,替换此头像即可实现自定义头像 背景图片配置在themes/black-blue/source/background/,替换为自己喜欢的图片，图片名称不能改 添加评论插件比如把畅言替换为有言 先修改themes/black-blue/_config.yml文件 123changyan: on: true uid: xxxxxxx 修改themes/black-blue/layout/_partial/comments/changyan.ejs 12345&lt;section class=\"changyan\" id=\"comments\"&gt;&lt;div id=\"uyan_frame\"&gt;&lt;/div&gt;&lt;script type=\"text/javascript\" src=\"http://v2.uyan.cc/code/uyan.js?uid=&lt;%= uid%&gt;\"&gt;&lt;/script&gt;&lt;/section&gt; 修改themes/black-blue/layout/_partial/article.ejs 123&lt;%- partial('comments/changyan', &#123; uid: theme.changyan.uid&#125;) %&gt; 重新生成页面hexo g","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://zhangfuxin.cn/tags/hexo/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"数码产品选购","slug":"digital-info","date":"2017-05-18T02:35:22.000Z","updated":"2019-08-29T03:00:47.925Z","comments":true,"path":"digital-info.html","link":"","permalink":"http://zhangfuxin.cn/digital-info.html","excerpt":"** 数码产品选购：** &lt;Excerpt in index | 首页摘要&gt;作为一个数码产品控，一出新的的电子产品，我都欣喜若狂。看参数，看评价，感觉合适，就会买。","text":"** 数码产品选购：** &lt;Excerpt in index | 首页摘要&gt;作为一个数码产品控，一出新的的电子产品，我都欣喜若狂。看参数，看评价，感觉合适，就会买。 &lt;The rest of contents | 余下全文&gt; 我喜欢的电子产品 电脑（笔记本，台式机，游戏主机，工作站） 手机（苹果，安卓，其它智能手机） 平板（安卓平板，ios平板） 电子书阅读器 电子手表 选购的原则 产品生态，买电子产品虽然不是随大流，但是用户群体一定程度决定了生态。用的人多，相应的资源会比较丰富，遇到问题很快找到解决方案。 产品价格，性价比在中国，乃至全世界都是很具有吸引力的。物美价廉的都不买的要么是脑残，要么是钱多没地方花。 产品硬件参数，买电子产品不看参数，肯定是买不到物美价廉的产品。 产品外观，现在是看脸的时代，新时代的数码产品对外观要求更高，更时尚。 功能，买电子产品，首要的就是功能，如果功能都不齐全，再漂亮，再便宜都没用。 买电子产品的目的，没有任何需求就是瞎买。 电子产品的使用我见过很多人买电子产品，比如买电子书阅读器，买一个kobo电子书折腾来折腾去，今天刷这个系统，明天改那个设置，书还没读几本，系统刷了几十次，天天刷固件。这真的是得不偿失，捡了芝麻丢了西瓜。第一，买电子产品是为了用的，买回来之后配置好之后，就不要来回折腾系统和配置了，把时间放到核心功能上。第二，买电子产品不要攀比，就跟买苹果手机一样，如果只是为了装B买，真没必要，结果自己还用不习惯。第三，了解自己的需求，需要什么买什么，","categories":[{"name":"digital","slug":"digital","permalink":"http://zhangfuxin.cn/categories/digital/"}],"tags":[{"name":"数码产品","slug":"数码产品","permalink":"http://zhangfuxin.cn/tags/数码产品/"}],"keywords":[{"name":"digital","slug":"digital","permalink":"http://zhangfuxin.cn/categories/digital/"}]},{"title":"如何写一篇好博客？","slug":"bestblog","date":"2017-05-15T15:04:48.000Z","updated":"2019-08-29T03:01:51.712Z","comments":true,"path":"bestblog.html","link":"","permalink":"http://zhangfuxin.cn/bestblog.html","excerpt":"** 提高自己博客的质量：** &lt;Excerpt in index | 首页摘要&gt;写博客陆陆续续也有一年了，但是一直没有多少访问量，仔细看了很多大神的博客，总结了几点，分享一下。","text":"** 提高自己博客的质量：** &lt;Excerpt in index | 首页摘要&gt;写博客陆陆续续也有一年了，但是一直没有多少访问量，仔细看了很多大神的博客，总结了几点，分享一下。 &lt;The rest of contents | 余下全文&gt; 好博客，好文章是什么样的？ 文章名称鲜明，一看名称就知道关于什么的内容 整体结构清晰，把事件或者原理的始末按照‘什么样（what？）’，‘为什么（why）’，‘怎么做（how）’说明 简明扼要。太啰嗦，没人看。 难易适中，太高深也没人看 图文搭配，有句话说的好，一图胜千文，好的图片胜过千言万语 怎么写出好博客？ 定主题和文章名称。如果想写一个关于redis后台启动的文章，名称要准确，就叫redis后台启动，不要起啰嗦的名字，比如redis如何后台启动 准备资料阶段，熟悉redis配置相关资料，做好功课 定文章的结构和提纲。还拿这个redis后台启动为例，你得说明什么是后台启动？为什么要后台启动？如何做到后台启动？ 语言表单，简单直白，不用凑字数 深入主题，比如挖掘更多redis的配置，把参数简要说明 找一个好图片，如果找不到，自己制作一个最契合自己主题的图片 把文章发给好友阅读，提出宝贵的意见 改进博客 坚持写博客","categories":[{"name":"个人随笔","slug":"个人随笔","permalink":"http://zhangfuxin.cn/categories/个人随笔/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"个人随笔","slug":"个人随笔","permalink":"http://zhangfuxin.cn/categories/个人随笔/"}]},{"title":"redis后台启动详细配置","slug":"redis-config","date":"2017-05-15T14:58:07.000Z","updated":"2019-08-29T02:31:44.684Z","comments":true,"path":"redis-config.html","link":"","permalink":"http://zhangfuxin.cn/redis-config.html","excerpt":"** redis后台启动详细配置：** &lt;Excerpt in index | 首页摘要&gt; redis启动的时候有多种模式，后台启动，集群启动等等。","text":"** redis后台启动详细配置：** &lt;Excerpt in index | 首页摘要&gt; redis启动的时候有多种模式，后台启动，集群启动等等。 &lt;The rest of contents | 余下全文&gt; 说明在开发中一般都是在命令行中直接运行redis-server,但是这样命令行关闭，服务就停止了。如果要在后台运行redis服务，需要制定配置文件。这里以ubuntu14为例子 准备配置文件查看‘/etc/redis/redis.conf’,没有可以创建一个，或者下载一个，配置文件位置没有要求 修改配置文件把daemonize设置为yes，然后redis-server /etc/redis/redis.conf启动服务， 查看服务ps -ef|grep redis-server查看是否有redis进程存在 更多配置，在conf文件有说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 是否以后台daemon方式运行，默认是 no，一般我们会改为 yesdaemonize nopidfile /var/run/redis.pid# 只允许本机访问bind 127.0.0.1# 端口设置port 6379tcp-backlog 511timeout 0tcp-keepalive 0loglevel notice# 日志文件logfile &quot;&quot;# 开启数据库的数量，Redis 是有数据库概念的，默认是 16 个，数字从 0 ~ 15databases 16save 900 1save 300 10save 60 10000stop-writes-on-bgsave-error yesrdbcompression yesrdbchecksum yesdbfilename dump.rdbdir ./slave-serve-stale-data yesslave-read-only yesrepl-diskless-sync norepl-diskless-sync-delay 5repl-disable-tcp-nodelay no# 密码设置，需要设置密码打开requirepass 123455slave-priority 100appendonly noappendfilename &quot;appendonly.aof&quot;appendfsync everysecno-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mbaof-load-truncated yeslua-time-limit 5000slowlog-log-slower-than 10000slowlog-max-len 128latency-monitor-threshold 0notify-keyspace-events &quot;&quot;hash-max-ziplist-entries 512hash-max-ziplist-value 64list-max-ziplist-entries 512list-max-ziplist-value 64set-max-intset-entries 512zset-max-ziplist-entries 128zset-max-ziplist-value 64hll-sparse-max-bytes 3000activerehashing yesclient-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60hz 10aof-rewrite-incremental-fsync yes","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://zhangfuxin.cn/tags/redis/"}],"keywords":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}]},{"title":"用koa2.x写下载漫画的爬虫","slug":"spider-koa2","date":"2017-05-13T23:15:38.000Z","updated":"2019-08-29T02:30:56.460Z","comments":true,"path":"spider-koa2.html","link":"","permalink":"http://zhangfuxin.cn/spider-koa2.html","excerpt":"** 用koa2.x写下载漫画的爬虫：** &lt;Excerpt in index | 首页摘要&gt;使用koa2.x的async ，await解决异步问题，写一个下载漫画的爬虫，代码里有惊喜和福利哦！","text":"** 用koa2.x写下载漫画的爬虫：** &lt;Excerpt in index | 首页摘要&gt;使用koa2.x的async ，await解决异步问题，写一个下载漫画的爬虫，代码里有惊喜和福利哦！ &lt;The rest of contents | 余下全文&gt; 项目搭建 安装nodejs&gt;7.6,安装koa-generator 直接koa2 spider,生成项目 安装request,request-promise,cheerio,mkdirp npm install安装依赖 思路图片或者漫画爬虫的思路很简单，首先观察url的规律，把url按规律加入到下载任务，其实就是请求获得html内容，然后对html进行解析，找到下载的图片url（一般都是img标签的src属性值），把url放到数组保存，使用async await控制所有的任务，直到把所有的图片下载完。 难点但是nodejs本身上异步的，如果你直接在for循环里去下载，肯定是不行的，必须控制好异步的执行上关键。爬虫简单，处理好异步难。这里我使用的es7中async，await配合promise解决异步问题，还可以使用async模块，eventproxy，等等异步控制模块来解决。 核心代码,spider.js1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950const fs = require('fs');const request = require(\"request-promise\");const cheerio = require(\"cheerio\");const mkdirp = require('mkdirp');const config = require('../config');exports.download = async function(ctx, next) &#123; const dir = 'images'; // 图片链接地址 let links = []; // 创建目录 mkdirp(dir); var urls = []; let tasks = []; let downloadTask = []; let url = config.url; for (var i = 1; i &lt;= config.size; i++) &#123; let link = url + '_' + i + '.html'; if (i == 1) &#123; link = url + '.html'; &#125; tasks.push(getResLink(i, link)) &#125; links = await Promise.all(tasks) console.log('links==========', links.length); for (var i = 0; i &lt; links.length; i++) &#123; let item = links[i]; let index = item.split('___')[0]; let src = item.split('___')[1]; downloadTask.push(downloadImg(src, dir, index + links[i].substr(-4, 4))); &#125; await Promise.all(downloadTask);&#125;async function downloadImg(url, dir, filename) &#123; console.log('download begin---', url); request.get(url).pipe(fs.createWriteStream(dir + \"/\" + filename)).on('close', function() &#123; console.log('download success', url); &#125;);&#125;async function getResLink(index, url) &#123; const body = await request(url); let urls = []; var $ = cheerio.load(body); $(config.rule).each(function() &#123; var src = $(this).attr('src'); urls.push(src); &#125;); return index + '___' + urls[0];&#125; 基础配置由于爬虫的复杂性基于不同的网站，不同的任务很不一样，这里只是把几个常用的变量抽取到了config.js。 1234567module.exports = &#123; //初始url url: 'http://www.xieet.com/meinv/230', size: 10, // 选中图片img标签的选择器 rule: '.imgbox a img'&#125;; 运行代码 下载我上传的代码koa-spider npm install,npm start即可运行 总结其实无论是写爬虫还是些其他程序，使用nodejs很大一部分都是要处理异步，要学好nodejs必须学好异步处理。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://zhangfuxin.cn/categories/学习笔记/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://zhangfuxin.cn/tags/nodejs/"}],"keywords":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://zhangfuxin.cn/categories/学习笔记/"}]},{"title":"微信公众号开发","slug":"wechat-dev","date":"2017-04-28T04:55:33.000Z","updated":"2019-08-29T02:26:56.748Z","comments":true,"path":"wechat-dev.html","link":"","permalink":"http://zhangfuxin.cn/wechat-dev.html","excerpt":"** 微信公众号开发：** &lt;Excerpt in index | 首页摘要&gt;微信公众号开发的一些注意事项","text":"** 微信公众号开发：** &lt;Excerpt in index | 首页摘要&gt;微信公众号开发的一些注意事项 &lt;The rest of contents | 余下全文&gt; 开发环境搭建 微信公众号开发者配置，url，token， 本地调试，使用内网穿透工具，花生壳，或者netapp，买一个可以自定义域名的，内网映射到制定端口， 项目搭建，express或koa搭建项目，npm有微信的现成包，直接配置 回复 回复和发消息并没有什么特别注意的地方，这里不多说 菜单 微信菜单有自定义菜单，有个性化菜单，但是个性化菜单优先级高于个性化菜单 个性化菜单可以根据用户的tag，sex，group等属性进行区分菜单 注意，我在使用时发现个性化菜单经常会失效，不起作用，偶尔会起作用，如果线上打算使用个性化菜单，请慎重并仔细测试 授权授权有网页授权，js sdk授权，网页授权也有两种，一个上静默授权，一个是点击授权，贴一下js sdk调用前认证的代码，要使用sha1加密 123456789101112131415async getSignConfig(originUrl) &#123; let data = &#123;&#125; const sha1 = crypto.createHash('sha1') const appId = this.app.config.weixin.appID const jsapi_ticket = await this.ctx.service.token.getJSApiTicket() const noncestr = this.app.config.jsapi.noncestr const url = this.app.config.domain + originUrl const timestamp = parseInt(new Date().getTime() / 1000) // sha1加密 const str = `jsapi_ticket=$&#123;jsapi_ticket&#125;&amp;noncestr=$&#123;noncestr&#125;&amp;timestamp=$&#123;timestamp&#125;&amp;url=$&#123;url&#125;` sha1.update(str) const signature = sha1.digest('hex') data = &#123; jsapi_ticket, noncestr, timestamp, url, signature, appId &#125; return data &#125; 调用js sdk页面上代码 12345678910111213wx.config(&#123; debug: false, // 开启调试模式, appId: appId, // 必填，公众号的唯一标识 timestamp: timestamp, // 必填，生成签名的时间戳 nonceStr: nonceStr, // 必填，生成签名的随机串 signature: signature,// 必填，签名，见附录1 jsApiList: ['closeWindow'] // 必填，需要使用的JS接口列表，所有JS接口列表见附录2&#125;);wx.ready(function()&#123; setTimeout(function()&#123; wx.closeWindow(); &#125;,2000);&#125;); 实用的常识 tag不能重复创建，但是给用户可以重复打同一个tag 更改菜单一般五分钟生效，或者重新关注公众号，立马能看到 如果调用js sdk，务必使用https，防止因为安全问题，导致ios下js下载失败。如果你的服务是https，而引用了https的微信js，在ios下肯定会下载失败，这是ios的安全机制导致的。 微信关闭窗口的js接口，不管jsconfig验证是否通过，窗口都可以关闭 微信的token过期时间上2h，但是很多时候30分钟不到可能已经失效，建议把token过期时间设置为10分钟之内 常见报错 创建菜单的时候，菜单长度不合法，仔细检查自己传的json菜单，一般都是json格式问题，而不是长度 redirect_uri不合法，是创建授权菜单的redirect_uri和网页授权域名配置不一样 关注公众号，服务端设置的欢迎消息发不过去，如果自己代码无异常，一般是因为token过期 以后遇到其他问题继续补充","categories":[{"name":"javacript","slug":"javacript","permalink":"http://zhangfuxin.cn/categories/javacript/"}],"tags":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/tags/编程语言/"}],"keywords":[{"name":"javacript","slug":"javacript","permalink":"http://zhangfuxin.cn/categories/javacript/"}]},{"title":"那些年读的书","slug":"mybooks","date":"2017-04-15T06:49:46.000Z","updated":"2019-08-29T02:32:56.582Z","comments":true,"path":"mybooks.html","link":"","permalink":"http://zhangfuxin.cn/mybooks.html","excerpt":"** 那些年读的书：** &lt;Excerpt in index | 首页摘要&gt;人生漫漫，不知不觉读了好多书，此贴只记录自己读过哪些书，不做多余的分析和总结。","text":"** 那些年读的书：** &lt;Excerpt in index | 首页摘要&gt;人生漫漫，不知不觉读了好多书，此贴只记录自己读过哪些书，不做多余的分析和总结。 &lt;The rest of contents | 余下全文&gt; 读过哪些种类的 编程专业类 小说类 励志类 小说 平凡的世界 白鹿原 穆斯林的葬礼 金庸武侠系列 古龙武侠小说 梁羽生武侠小说 余华作品集 雷米小说全集（侦探类） 网络小说： 诛仙， 盗墓笔记， 泡沫之夏， 芈月传， 编程类 java编程思想 effective java java并发编程的艺术 代码整洁之道 黑客与画家 深入浅出nodejs nodejs实战 js高级程序设计 survivejs redux和react中文手册 你不知道的javascript 算法javascript实现 mysql权威指南 mongodb权威指南 mongodb实战第二版 redis入门 经管励志 时间管理 一分钟系列 番茄工作法图解","categories":[{"name":"book","slug":"book","permalink":"http://zhangfuxin.cn/categories/book/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://zhangfuxin.cn/tags/学习笔记/"}],"keywords":[{"name":"book","slug":"book","permalink":"http://zhangfuxin.cn/categories/book/"}]},{"title":"免费的开源书籍","slug":"free-books","date":"2016-11-28T16:15:52.000Z","updated":"2019-08-29T03:01:31.623Z","comments":true,"path":"free-books.html","link":"","permalink":"http://zhangfuxin.cn/free-books.html","excerpt":"** 免费的开源书籍：** &lt;Excerpt in index | 首页摘要&gt;国外程序员在 stackoverflow 推荐的程序员必读书籍，中文版。","text":"** 免费的开源书籍：** &lt;Excerpt in index | 首页摘要&gt;国外程序员在 stackoverflow 推荐的程序员必读书籍，中文版。 &lt;The rest of contents | 余下全文&gt; 目录 语言无关 IDE MySQL NoSQL PostgreSQL Web WEB服务器 其它 函数式概念 分布式系统 在线教育 大数据 操作系统 数据库 智能系统 正则表达式 版本控制 程序员杂谈 管理和监控 编程艺术 编译原理 编辑器 计算机图形学 设计模式 软件开发方法 项目相关 语言相关 Android AWK C C# C++ CoffeeScript Dart Elasticsearch Elixir Erlang Fortran Golang Haskell HTML / CSS HTTP iOS Java JavaScript LaTeX LISP Lua Markdown Node.js Perl PHP Python R reStructuredText Ruby Rust Scala Scheme Shell Swift Vim Visual Prolog 语言无关IDE IntelliJ IDEA 简体中文专题教程 MySQL 21分钟MySQL入门教程 MySQL索引背后的数据结构及算法原理 NoSQL Disque 使用教程 Neo4j .rb 中文資源 Neo4j 简体中文手册 v1.8 Redis 命令参考 Redis 设计与实现 The Little MongoDB Book The Little Redis Book 带有详细注释的 Redis 2.6 代码 带有详细注释的 Redis 3.0 代码 PostgreSQL PostgreSQL 8.2.3 中文文档 PostgreSQL 9.3.1 中文文档 Web 3 Web Designs in 3 Weeks Chrome 开发者工具中文手册 Chrome扩展开发文档 Growth: 全栈增长工程师指南 Grunt中文文档 Gulp 入门指南 gulp中文文档 HTTP 接口设计指北 HTTP/2.0 中文翻译 http2讲解 JSON风格指南 Wireshark用户手册 一站式学习Wireshark 关于浏览器和网络的 20 项须知 前端代码规范 及 最佳实践 前端开发体系建设日记 前端资源分享（一） 前端资源分享（二） 正则表达式30分钟入门教程 浏览器开发工具的秘密 移动Web前端知识库 移动前端开发收藏夹 WEB服务器 Apache 中文手册 Nginx开发从入门到精通 (淘宝团队出品) Nginx教程从入门到精通 (PDF版本，运维生存时间出品) 其它 OpenWrt智能、自动、透明翻墙路由器教程 SAN 管理入门系列 Sketch 中文手册 深入理解并行编程 函数式概念 傻瓜函数编程 分布式系统 走向分布式 (PDF) 在线教育 51CTO学院 Codecademy CodeSchool Coursera Learn X in Y minutes (数十种语言快速入门教程) shiyanlou TeamTreeHouse Udacity xuetangX 慕课网 (丰富的移动端开发、php开发、web前端、html5教程以及css3视频教程等课程资源) 极客学院 计蒜客 大数据 Spark 编程指南简体中文版 大型集群上的快速和通用数据处理架构 大数据/数据挖掘/推荐系统/机器学习相关资源 数据挖掘中经典的算法实现和详细的注释 面向程序员的数据挖掘指南 操作系统 Debian 参考手册 Docker —— 从入门到实践 Docker中文指南 Docker入门实战 FreeBSD 使用手册 FreeRADIUS新手入门 Linux Documentation (中文版) Linux Guide for Complete Beginners Linux 构建指南 Linux 系统高级编程 Linux工具快速教程 Mac 开发配置手册 Operating Systems: Three Easy Pieces The Linux Command Line (中英文版) Ubuntu 参考手册 uCore Lab: Operating System Course in Tsinghua University UNIX TOOLBOX 命令行的艺术 嵌入式 Linux 知识库 (eLinux.org 中文版) 开源世界旅行手册 深入分析Linux内核源码 理解Linux进程 鸟哥的 Linux 私房菜 基础学习篇 鸟哥的 Linux 私房菜 服务器架设篇 数据库 Redis 设计与实现 The Little MongoDB Book 中文版 智能系统 一步步搭建物联网系统 正则表达式 正则表达式30分钟入门教程 版本控制 Git - 简易指南 Git-Cheat-Sheet （感谢 @flyhigher139 翻译了中文版） Git Community Book 中文版 git-flow 备忘清单 Git magic Git Magic Git 参考手册 Github帮助文档 GitHub秘籍 Git教程 （本文由 @廖雪峰 创作，如果觉得本教程对您有帮助，可以去 iTunes 购买） Got GitHub GotGitHub HgInit (中文版) Mercurial 使用教程 Pro Git Pro Git 中文版 (整理在gitbook上) svn 手册 学习 Git 分支 (点击右下角按钮可切换至简体及正体中文) 沉浸式学 Git 猴子都能懂的GIT入门 程序员杂谈 程序员的自我修养 管理和监控 ElasticSearch 权威指南 Elasticsearch 权威指南（中文版） ELKstack 中文指南 Logstash 最佳实践 Mastering Elasticsearch(中文版) Puppet 2.7 Cookbook 中文版 编程艺术 取悦的工序：如何理解游戏 (豆瓣阅读，免费书籍) 每个程序员都应该了解的内存知识(译)【第一部分】 程序员编程艺术 编程入门指南 编译原理 《计算机程序的结构和解释》公开课 翻译项目 编辑器 exvim–vim 改良成IDE项目 Vim中文文档 所需即所获：像 IDE 一样使用 vim 笨方法学Vimscript 中译本 计算机图形学 OpenGL 教程 设计模式 史上最全设计模式导学目录 图说设计模式 软件开发方法 傻瓜函数编程 (《Functional Programming For The Rest of Us》中文版) 硝烟中的 Scrum 和 XP 项目相关 GNU make 指南 Gradle 2 用户指南 Gradle 中文使用文档 Joel谈软件 selenium 中文文档 开源软件架构 持续集成（第二版） (译言网) 約耳談軟體(Joel on Software) 编码规范 让开发自动化系列专栏 追求代码质量 语言相关Android Android Design(中文版) Android Note(开发过程中积累的知识点) Android6.0新特性详解 Android学习之路 Android开发技术前线(android-tech-frontier) Google Android官方培训课程中文版 Google Material Design 正體中文版 (译本一 译本二) Material Design 中文版 Point-of-Android Android 一些重要知识点解析整理 AWK awk中文指南 awk程序设计语言 C C 语言常见问题集 C/C++ 学习教程 Linux C 编程一站式学习 新概念 C 语言教程 C Sharp 精通C#(第6版) C++ 100个gcc小技巧 100个gdb小技巧 C 语言编程透视 C/C++ Primer - @andycai C++ FAQ LITE(中文版) C++ Primer 5th Answers C++ Template 进阶指南 C++ 基础教程 C++ 并发编程(基于C++11) C++ 并发编程指南 CGDB中文手册 Cmake 实践 (PDF版) GNU make 指南 Google C++ 风格指南 QT 教程 ZMQ 指南 像计算机科学家一样思考（C++版) (《How To Think Like a Computer Scientist: C++ Version》中文版) 简单易懂的C魔法 跟我一起写Makefile(PDF) (PDF) CoffeeScript CoffeeScript 中文 CoffeeScript 编程风格指南 Dart Dart 语言导览 Elasticsearch Elasticsearch 权威指南 （《Elasticsearch the definitive guide》中文版） ELKstack 中文指南 Mastering Elasticsearch(中文版) Elixir Elixir Getting Started 中文翻译 Elixir 编程语言教程 (Elixir School) Elixir元编程与DSL 中文翻译 Phoenix 框架中文文档 Erlang Erlang 并发编程 (《Concurrent Programming in Erlang (Part I)》中文版) Fortran Fortran77和90/95编程入门 Golang Effective Go Go Web 编程 Go 入门指南 (《The Way to Go》中文版) Go 官方文档翻译 Go 指南 (《A Tour of Go》中文版) Go 简易教程 (《The Little Go Book》中文版) Go 编程基础 Go 语言标准库 Go命令教程 Go实战开发 Go语言博客实践 Java程序员的Golang入门指南 Network programming with Go 中文翻译版本 Revel 框架手册 学习Go语言 Groovy 实战 Groovy 系列 Haskell Haskell 趣学指南 Real World Haskell 中文版 HTML / CSS CSS3 Tutorial 《CSS3 教程》 CSS参考手册 Emmet 文档 HTML5 教程 HTML和CSS编码规范 Sass Guidelines 中文 前端代码规范 (腾讯 AlloyTeam 团队) 学习CSS布局 通用 CSS 笔记、建议与指导 iOS Apple Watch开发初探 Google Objective-C Style Guide 中文版 iOS7人机界面指南 iOS开发60分钟入门 iPhone 6 屏幕揭秘 网易斯坦福大学公开课：iOS 7应用开发字幕文件 Java Activiti 5.x 用户指南 Apache MINA 2 用户指南 Apache Shiro 用户指南 Google Java编程风格指南 H2 Database 教程 Java Servlet 3.1 规范 Java 编码规范 Jersey 2.x 用户指南 JSSE 参考指南 MyBatis中文文档 Netty 4.x 用户指南 Netty 实战(精髓) REST 实战 Spring Boot参考指南 (翻译中) Spring Framework 4.x参考文档 用jersey构建REST服务 Javascript Airbnb JavaScript 规范 AngularJS AngularJS中译本 AngularJS入门教程 AngularJS最佳实践和风格指南 在Windows环境下用Yeoman构建AngularJS项目 构建自己的AngularJS backbone.js backbone.js中文文档 backbone.js入门教程 (PDF) Backbone.js入门教程第二版 Developing Backbone.js Applications(中文版) Chrome扩展及应用开发 CoffeeScript CoffeeScript 编码风格指南 D3.js D3.js 入门系列 (还有进阶、高级等系列) 官方API文档 张天旭的D3教程 楚狂人的D3教程 ECMAScript 6 入门 (作者：阮一峰) ExtJS Ext4.1.0 中文文档 Google JavaScript 代码风格指南 Google JSON 风格指南 impress.js impress.js的中文教程 JavaScript Promise迷你书 Javascript 原理 JavaScript 标准参考教程（alpha） 《JavaScript 模式》 “JavaScript patterns”中译本 javascript 的 12 个怪癖 JavaScript 秘密花园 JavaScript核心概念及实践 (PDF) (此书已由人民邮电出版社出版发行，但作者依然免费提供PDF版本，希望开发者们去购买，支持作者) Javascript编程指南 (源码) jQuery How to write jQuery plugin 简单易懂的JQuery魔法 Meteor Discover Meteor Node.js express.js 中文文档 Express框架 koa 中文文档 Learn You The Node.js For Much Win! (中文版) Node debug 三法三例 Node.js Fullstack《從零到一的進撃》 Node.js 包教不包会 Nodejs Wiki Book (繁体中文) nodejs中文文档 Node入门 七天学会NodeJS 使用 Express + MongoDB 搭建多人博客 React.js Learn React &amp; Webpack by building the Hacker News front page React Native 中文文档(含最新Android内容) React webpack-cookbook React 入门教程 React.js 中文文档 underscore.js Underscore.js中文文档 You-Dont-Know-JS (深入JavaScript语言核心机制的系列图书) Zepto.js Zepto.js 中文文档 命名函数表达式探秘 (注:原文由为之漫笔 翻译，原始地址无法打开，所以此处地址为我博客上的备份) 学用 JavaScript 设计模式 (开源中国) 深入理解JavaScript系列 LaTeX LaTeX 笔记 一份不太简短的 LaTeX2ε 介绍 大家來學 LaTeX (PDF) LISP ANSI Common Lisp 中文翻译版 Common Lisp 高级编程技术 (《On Lisp》中文版) Lua Lua 5.3 参考手册 Markdown Markdown 快速入门 Markdown 简明教程 Markdown 语法说明 献给写作者的 Markdown 新手指南 Node.js Node 入门 The NodeJS 中文文档（社区翻译） 七天学会NodeJS 阿里出品，很好的入门资料 Perl Master Perl Today 《Modern Perl》中文版 Perl 5 教程 Perl 教程 PHP PHP 之道 PHP5中文手册 PHP扩展开发及内核应用 Symfony2 实例教程 深入理解 PHP 内核 Python Django book 2.0 Python 3 文档(简体中文) 3.2.2 documentation Python 中文学习大本营 深入 Python 3 笨办法学 Python R 153分钟学会 R (PDF) 《R for beginners》中文版 (PDF) R 导论 (《An Introduction to R》中文版) (PDF) 用 R 构建 Shiny 应用程序 (《Building ‘Shiny’ Applications with R》中文版) 统计学与 R 读书笔记 (PDF) reStructuredText reStructuredText 入门 reStructuredText 简明教程 Ruby Rails 风格指南 Ruby on Rails Tutorial 原书第 2 版 Ruby on Rails 实战圣经 Ruby 风格指南 笨方法学 Ruby Rust Rust 官方教程 Rust 语言学习笔记 RustPrimer 通过例子学习 Rust Scala Effective Scala Scala 初学者指南 (The Neophyte’s Guide to Scala) Scala 课堂 (Twitter的Scala中文教程) Scheme Scheme 入门教程 (《Yet Another Scheme Tutorial》中文版) Shell Shell 编程基础 Shell 脚本编程30分钟入门 The Linux Command Line 中文版 Swift 《The Swift Programming Language》中文版 Vim Vim Manual(中文版) 大家來學 VIM Visual Prolog Visual Prolog 7初学指南 Visual Prolog 7边练边学","categories":[{"name":"资源分享","slug":"资源分享","permalink":"http://zhangfuxin.cn/categories/资源分享/"}],"tags":[{"name":"book","slug":"book","permalink":"http://zhangfuxin.cn/tags/book/"}],"keywords":[{"name":"资源分享","slug":"资源分享","permalink":"http://zhangfuxin.cn/categories/资源分享/"}]},{"title":"hexo配置和优化记录","slug":"hexo-config","date":"2016-11-28T15:07:12.000Z","updated":"2019-08-29T03:00:02.297Z","comments":true,"path":"hexo-config.html","link":"","permalink":"http://zhangfuxin.cn/hexo-config.html","excerpt":"** hexo配置和优化高级篇：** &lt;Excerpt in index | 首页摘要&gt;本文章不讲解hexo的基础配置，只针对hexo的高级配置，性能优化，seo配置进行讲解。","text":"** hexo配置和优化高级篇：** &lt;Excerpt in index | 首页摘要&gt;本文章不讲解hexo的基础配置，只针对hexo的高级配置，性能优化，seo配置进行讲解。 &lt;The rest of contents | 余下全文&gt; 前言仔细想想，使用hexo搭建博客也有半年多了，但是发现访问量一直几乎没有，特别是经历几次迁移之后，之前从github到coding，现在迁移到了云服务器，研究了一下如何进行seo和网站性能优化，便有了这篇文章。 实用的功能 站内搜索（百度的） 本地搜索（本地插件） 网站统计 留言功能 rss订阅功能 性能优化 html压缩 css压缩 js压缩· img压缩 nginx代理，开启gzip压缩 cdn代理css和图·片 删除主题无用的js和css seo优化 sitemap 对于没有价值的外链a标签添加rel=&quot;external nofollow&quot; 使用meta标签 使用robots文件 主动提交sitemap到搜索引擎 添加外链和内链","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://zhangfuxin.cn/tags/hexo/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"ubuntu服务器详细配置","slug":"server-config","date":"2016-11-28T12:36:03.000Z","updated":"2019-08-29T02:31:18.936Z","comments":true,"path":"server-config.html","link":"","permalink":"http://zhangfuxin.cn/server-config.html","excerpt":"** ubuntu服务器私人定制：** &lt;Excerpt in index | 首页摘要&gt;把ubuntu服务器打造成自己的个性服务器，装逼必备！！！","text":"** ubuntu服务器私人定制：** &lt;Excerpt in index | 首页摘要&gt;把ubuntu服务器打造成自己的个性服务器，装逼必备！！！ &lt;The rest of contents | 余下全文&gt; ## 说明此教程针对Ubuntu14,其他版本仅作参考 ## 用户密码管理sudo passwd root 添加一个用户组并指定id为1002sudo groupadd －g 1002 www 添加一个用户到www组并指定id为1003sudo useradd wyx -g 1002 -u 1003 -m 修改用户的密码sudo passwd wyx 删除一个用户sudo userdel wyx 为该用户添加sudo权限 12sudo usermod -a -G adm wyxsudo usermod -a -G sudo wyx 查看所有用户和用户组：12cat /etc/passwdcat /etc/group 安装nodejs 安装nvmcurl -o- https://raw.githubusercontent.com/creationix/nvm/v0.32.1/install.sh | bash 安装nodenvm install v4.4.4,安装nvm install v6.9.1 设置默认的node版本nvm alias default v4.4.4 安装npm3 npm install -g npm@3 设置淘宝的cnpm源 npm install -g cnpm --registry=https://registry.npm.taobao.org 验证安装node -v,npm -v,cnpm -v安装node常用包 安装pm2cnpm install -g pm2 安装hexo博客cnpm install -g hexo-cli 安装同步插件rsynccnpm install -g rsync 安装docker apt安装 1234sudo apt-get updatesudo apt-get install -y docker.iosudo ln -sf /usr/bin/docker.io /usr/local/bin/dockersudo sed -i '$acomplete -F _docker docker' /etc/bash_completion.d/docker.io 源码安装最新版本 12345sudo apt-get install apt-transport-httpssudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9sudo bash -c \"echo deb https://get.docker.io/ubuntu docker main &gt; /etc/apt/sources.list.d/docker.list\"sudo apt-get updatesudo apt-get install lxc-docker 验证安装版本docker -v 安装nginxsudo apt-get install nginx启动和配置nginx 安装redissudo apt-get install redis-server启动和配置文件: 安装mongodb 安装3.0 1234apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10echo \"deb http://repo.mongodb.org/apt/debian wheezy/mongodb-org/3.0 main\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.0.listapt-get update apt-get install mongodb-org 安装3.2最新版 1234sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927echo \"deb http://repo.mongodb.org/apt/ubuntu \"$(lsb_release -sc)\"/mongodb-org/3.2 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb.listsudo apt-get updatesudo apt-get install mongodb-org 制定版本apt-get install mongodb-org=3.2.0 mongodb-org-server=3.2.0 mongodb-org-shell=3.2.0 mongodb-org-mongos=3.2.0 mongodb-org-tools=3.2.0 启动服务 12sudo service mongod startsudo service mongod stop 验证安装mongod --version 配置 安装jdk安装jdk1.7sudo apt-get install openjdk-7-jdk源码安装 1234567891011sudo mkdir /usr/lib/jvmsudo tar zxvf jdk-7u21-linux-i586.tar.gz -C /usr/lib/jvmcd /usr/lib/jvmsudo mv jdk1.7.0_21 javasudo vim ~/.bashrcexport JAVA_HOME=/usr/lib/jvm/javaexport JRE_HOME=$&#123;JAVA_HOME&#125;/jre export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 安装mysql实用ubuntu自带的工具下载sudo apt-get install mysql-server 环境变量常见的方法有两种。 在用户主目录下有一个 .bashrc 文件，可以在此文件中加入 PATH 的设置如下：export PATH=”$PATH:/your path1/:/your path2/…..” 在 /etc/profile中增加 12PATH=\"$PATH:/home/zhengb66/bin\" export PATH 开机自启动 方法一，编辑rc.loacl脚本Ubuntu开机之后会执行/etc/rc.local文件中的脚本，所以我们可以直接在/etc/rc.local中添加启动脚本。当然要添加到语句：exit 0 前面才行。代码如下:sudo vi /etc/rc.local然后在 exit 0 前面添加好脚本代码。 方法二，添加一个Ubuntu的开机启动服务。如果要添加为开机启动执行的脚本文件，可先将脚本复制或者软连接到/etc/init.d/目录下，然后用：update-rc.d xxx defaults NN命令(NN为启动顺序)，将脚本添加到初始化执行的队列中去。注意如果脚本需要用到网络，则NN需设置一个比较大的数字，如99。1) 将你的启动脚本复制到 /etc/init.d目录下以下假设你的脚本文件名为 test。2) 设置脚本文件的权限 代码如下:sudo chmod 755 /etc/init.d/test3) 执行如下命令将脚本放到启动脚本中去：代码如下:cd /etc/init.d sudo update-rc.d test defaults 95 注：其中数字95是脚本启动的顺序号，按照自己的需要相应修改即可。在你有多个启动脚本，而它们之间又有先后启动的依赖关系时你就知道这个数字的具体作用了。该命令的输出信息参考如下：卸载启动脚本的方法：代码如下:cd /etc/init.dsudo update-rc.d -f test remove 定时任务在Ubuntu下，cron是被默认安装并启动的。通过查看/etc/crontab推荐使用crontab -e命令添加自定义的任务（编辑的是/var/spool/cron下对应用户的cron文件，在/var/spool/cron下的crontab文件 不可以直接创建或者直接修改，crontab文件是通过crontab命令得到的）。crontab -e 直接执行命令行每2分钟打印一个字符串“Hello World”，保存至文件/home/laigw/cron/HelloWorld.txt中，cron 格式如下：*/2 * * * * echo “Hello World.” &gt;&gt; /home/HelloWorld.txt shell 文件每3分钟调用一次 /home/laigw/cron/test.sh 文件，cron 格式如下：*/3 * * * * /home/laigw/cron/test.sh ftp和rsync配置 持续集成环境 jenkens配置 gitlab配置 git服务器","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://zhangfuxin.cn/tags/linux/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"mac开发环境配置","slug":"mac-dev","date":"2016-11-27T07:52:38.000Z","updated":"2019-08-29T02:58:55.017Z","comments":true,"path":"mac-dev.html","link":"","permalink":"http://zhangfuxin.cn/mac-dev.html","excerpt":"** mac开发环境配置：** &lt;Excerpt in index | 首页摘要&gt;工欲善其事，必先利其器，做好开发者，先搞好开发环境啊。针对mac开发者的开发配置，把mac打造成最具生产力工具！","text":"** mac开发环境配置：** &lt;Excerpt in index | 首页摘要&gt;工欲善其事，必先利其器，做好开发者，先搞好开发环境啊。针对mac开发者的开发配置，把mac打造成最具生产力工具！ &lt;The rest of contents | 余下全文&gt; 软件下载说明下面所提到的软件，有很多需要付费或者破解版，为了方便大家使用，会在网盘分享给大家，只需在评论的地方留下自己的百度云账号！！！ 软件分类说明 通用（开发者必备的软件） java类（java开发者必不可少） 前端类（偏前端和nodejs） python类 数据库类 其他（php，ruby等等） 通用软件 Alfred dash homebrew zsh（oh my zsh） sublime text3, vscode paste(剪切板工具) BetterSnapTool(分屏软件) cornerstone(svn) tower(git) alternote() paw chrome firefox pdf expert CheatSheet snippetslab java软件 jdk idea eclipse maven zookeeper,dubbo tomcat apache 前端必备 nvm(nodejs,npm,cnpm) webpack yo webstorm python必备 pycharm sublime text（插件） 数据库类 mysql mongodb sqllite navicate robomongo redis 其他软件 office keynote,pages,number photoshop 文章长期更新，请收藏","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"mac","slug":"mac","permalink":"http://zhangfuxin.cn/tags/mac/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"atom中最好的js代码补全","slug":"best-js-snippet","date":"2016-07-21T22:11:30.000Z","updated":"2019-08-29T02:21:45.014Z","comments":true,"path":"best-js-snippet.html","link":"","permalink":"http://zhangfuxin.cn/best-js-snippet.html","excerpt":"** atom中最好的js代码补全：** &lt;Excerpt in index | 首页摘要&gt; 这或许是atom中最好的js代码补全,包含了express,nodejs,es6,目前仍在继续更新","text":"** atom中最好的js代码补全：** &lt;Excerpt in index | 首页摘要&gt; 这或许是atom中最好的js代码补全,包含了express,nodejs,es6,目前仍在继续更新 &lt;The rest of contents | 余下全文&gt; best-js-snippets这个package的名字就叫 best-js-snippets ,用atom的可以下载使用一下,提出建议,我会尽快修改 特性 express补全 es6补全 js补全(string,dom操作) nodejs补全(fs,event,util,module,class,assert) 如何安装 atom编辑器中找到设置,搜索package,安装即可. 重启atom,享受吧!","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"常用排序算法学习","slug":"sort-study","date":"2016-07-08T14:39:24.000Z","updated":"2019-08-21T15:13:24.138Z","comments":true,"path":"sort-study.html","link":"","permalink":"http://zhangfuxin.cn/sort-study.html","excerpt":"** 常用排序算法学习：** &lt;Excerpt in index | 首页摘要&gt; 程序员各种排序算法，算法的实现和分析","text":"** 常用排序算法学习：** &lt;Excerpt in index | 首页摘要&gt; 程序员各种排序算法，算法的实现和分析 &lt;The rest of contents | 余下全文&gt; 排序算法的分类 排序分内排序和外排序。 内排序:指在排序期间数据对象全部存放在内存的排序。 外排序:指在排序期间全部对象个数太多,不能同时存放在内存,必须根据排序过程的要求,不断在内、外存之间移动的排序。 内排序的方法有许多种,按所用策略不同,可归纳为五类:插入排序、选择排序、交换排序、归并排序、分配排序和计数排序。 插入排序主要包括直接插入排序，折半插入排序和希尔排序两种; 选择排序主要包括直接选择排序和堆排序; 交换排序主要包括冒泡排序和快速排序; 归并排序主要包括二路归并(常用的归并排序)和自然归并。 分配排序主要包括箱排序和基数排序 冒泡排序 冒泡排序就是把小的元素往前调或者把大的元素往后调。比较是相邻的两个元素比较，交换也发生在这两个元素之间。所以，如果两个元素相等，是不用交换的；如果两个相等的元素没有相邻，那么即使通过前面的两两交换把两个相邻起来，这时候也不会交换，所以相同元素的前后顺序并没有改变，所以冒泡排序是一种稳定排序算法1234567891011121314151617// js代码function sort(arr) &#123;if (arr.length == 0) &#123; return [];&#125;var length = arr.length;for (var i = 0; i &lt; length; i++) &#123; for (var j = 0; j &lt; length - i - 1; j++) &#123; if (arr[j] &gt; arr[j + 1]) &#123; var temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; console.log(arr); &#125; &#125; &#125;&#125; 快速排序 快速排序是对冒泡排序的一种改进。它的基本思想是：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列. 时间复杂度：O（nlgn）最坏：O（n^2）空间复杂度：O（nlgn） 1234567891011121314151617181920212223// js递归实现function quickSort(arr) &#123; if (arr.length == 0) &#123; return []; &#125; var left = []; var right = []; var pivot = arr[0]; for (var i = 1; i &lt; arr.length; i++) &#123; if (arr[i] &lt; pivot) &#123; left.push(arr[i]); &#125; else &#123; right.push(arr[i]); &#125; &#125; return quickSort(left).concat(pivot, quickSort(right));&#125;var a = [];for (var i = 0; i &lt; 10; ++i) &#123; a[i] = Math.floor((Math.random() * 100) + 1);&#125;console.log(a);console.log(quickSort(a)); 直接插入排序 直接插入排序(straight insertion sort)的作法是：每次从无序表中取出第一个元素，把它插入到有序表的合适位置，使有序表仍然有序. 12345678910111213141516171819function insertionSort(arr) &#123; var temp, inner; for (var outer = 1; outer &lt;= arr.length - 1; ++outer) &#123; temp = arr[outer]; inner = outer; while (inner &gt; 0 &amp;&amp; (arr[inner - 1] &gt;= temp)) &#123; arr[inner] = arr[inner - 1]; --inner; &#125; arr[inner] = temp; &#125; return arr;&#125;var a = [];for (var i = 0; i &lt; 10; ++i) &#123; a[i] = Math.floor((Math.random() * 100) + 1);&#125;console.log(a);console.log(insertionSort(a)); 折半插入排序 折半插入排序算法的具体操作为：在将一个新元素插入已排好序的数组的过程中，寻找插入点时，将待插入区域的首元素设置为a[low],末元素设置为 a[high]，则轮比较时将待插入元素与a[m],其中m=(low+high)/2相比较,如果比参考元素小，则选择a[low]到a[m-1]为新 的插入区域(即high=m-1)，否则选择a[m+1]到a[high]为新的插入区域（即low=m+1），如此直至low&lt;=high不成 立，即将此位置之后所有元素后移一位，并将新元素插入a[high+1] 希尔排序 先取一个小于n的整数d1作为第一个增量，把文件的全部记录分成d1个组。所有距离为dl的倍数的记录放在同一个组中。先在各组内进行直接插入 排序；然后，取第二个增量d2&lt;d1重复上述的分组和排序，直至所取的增量dt=1(dt&lt;dt-l&lt;…&lt;d2&lt;d1)， 即所有记录放在同一组中进行直接插入排序为止。 该方法实质上是一种分组插入方法。插入排序（Insertion Sort）的一个重要的特点是，如果原始数据的大部分元素已经排序，那么插入排序的速度很快（因为需要移动的元素很少）。从这个事实我们可以想到，如果原 始数据只有很少元素，那么排序的速度也很快。－－希尔排序就是基于这两点对插入排序作出了改进。 直接选择排序 直接选择排序是给每个位置选择当前元素最小的，比如给第一个位置选择最小的，在剩余元素里面给第二个元素选择第二小的，依次类推，直到第n-1个元素，第n个 元素不用选择了，因为只剩下它一个最大的元素了。那么，在一趟选择，如果当前元素比一个元素小，而该小的元素又出现在一个和当前元素相等的元素后面，那么 交换后稳定性就被破坏了。比较拗口，举个例子，序列5 8 5 2 9，我们知道第一遍选择第1个元素5会和2交换，那么原序列中2个5的相对前后顺序就被破坏了，所以选择排序不是一个稳定的排序算法。时间复杂度是O(n^2) 堆排序 我们知道堆的结构是节点i的孩子为2i和2i+1节点，大顶堆要求父节点大于等于其2个子节点，小顶堆要求父节点小于等于其2个子节点。在一个长为n 的序列，堆排序的过程是从第n/2开始和其子节点共3个值选择最大(大顶堆)或者最小(小顶堆),这3个元素之间的选择当然不会破坏稳定性。但当为n /2-1, n/2-2, …1这些个父节点选择元素时，就会破坏稳定性。有可能第n/2个父节点交换把后面一个元素交换过去了，而第n/2-1个父节点把后面一个相同的元素没 有交换，那么这2个相同的元素之间的稳定性就被破坏了。所以，堆排序不是稳定的排序算法。 二路归并排序 归并排序是把序列递归地分成短序列，递归出口是短序列只有1个元素(认为直接有序)或者2个序列(1次比较和交换),然后把各个有序的段序列合并成一个有 序的长序列，不断合并直到原序列全部排好序。可以发现，在1个或2个元素时，1个元素不会交换，2个元素如果大小相等也没有人故意交换，这不会破坏稳定 性。那么，在短的有序序列合并的过程中，稳定是是否受到破坏？没有，合并过程中我们可以保证如果两个当前元素相等时，我们把处在前面的序列的元素保存在结 果序列的前面，这样就保证了稳定性。所以，归并排序也是稳定的排序算法。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"mysql优化的常用方法","slug":"mysql-optimize","date":"2016-06-10T23:25:13.000Z","updated":"2019-08-29T02:32:33.395Z","comments":true,"path":"mysql-optimize.html","link":"","permalink":"http://zhangfuxin.cn/mysql-optimize.html","excerpt":"** mysql优化：** &lt;Excerpt in index | 首页摘要&gt; mysql的优化措施，从sql优化做起","text":"** mysql优化：** &lt;Excerpt in index | 首页摘要&gt; mysql的优化措施，从sql优化做起 &lt;The rest of contents | 余下全文&gt; 优化sql的一般步骤 通过show status了解各种sql的执行频率 定位执行效率低的sql语句 通过explain分析效率低的sql 通过show profile分析sql 通过trace分析优化器如何选择执行计划 确定问题，采取措施优化 索引优化措施 mysql中使用索引的典型场景 匹配全值，条件所有列都在索引中而且是等值匹配 匹配值的范围查找，字段必须在索引中 匹配最左前缀，复合索引只会根据最左列进行查找 仅仅对索引进行查询，即查询的所有字段都在索引上 匹配列前缀，比如like ‘ABC%’,如果是like ‘%aaa’就不可以 如果列名是索引，使用column is null会使用索引 存在索引但不会使用索引的典型场景 以%开头的like查询不能使用b树索引 数据类型出现隐式转换不能使用索引 复合索引，查询条件不符合最左列原则 用or分割的条件，如果前面的条件有索引，而后面的条件没有索引 查看索引使用的情况 1show status like &apos;Handler_read%&apos;; 如果Handler_read_rnd_next的值比较高，说明索引不正确或者查询没有使用到索引 简单实用的优化方法 定期检查表和分析表分析表语法：1analyze table 表名； 检查表语法： 1check table 表名； 定期优化表 对于字节大小不固定的字段，数据更新和删除会造成磁盘空间不释放，这时候就行优化表，可以整理磁盘碎片，提高性能语法如下：1optimize table user(表名)；","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://zhangfuxin.cn/tags/mysql/"}],"keywords":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}]},{"title":"mac下mysql5.6字符集设置","slug":"mac-mysql-unicode","date":"2016-05-28T15:10:37.000Z","updated":"2019-08-29T02:33:30.478Z","comments":true,"path":"mac-mysql-unicode.html","link":"","permalink":"http://zhangfuxin.cn/mac-mysql-unicode.html","excerpt":"** mac下mysql5.6字符集设置：** &lt;Excerpt in index | 首页摘要&gt; 在mac下设置mysql5.6字符集时踩过的坑，百分百保证有效","text":"** mac下mysql5.6字符集设置：** &lt;Excerpt in index | 首页摘要&gt; 在mac下设置mysql5.6字符集时踩过的坑，百分百保证有效 &lt;The rest of contents | 余下全文&gt; 为什么要设置字符集 设置字符集主要是解决乱码问题，由于中文和英文编码不同导致，中文出现乱码，所以一般都设置为utf8格式 不同的字符集和编码占用的字节不同，选择适合的编码会提高数据库性能 mac下设置 在/etc/my.cnf文件进行设置，如果没有此文件可以从/usr/local/mysql/support-files/拷贝，命令如下12cd /usr/local/mysql/support-filessudo cp my.cnf /etc/my.cnf 查看文件的读写权限，如果为644（rw- r– r–）则改为(664) (rw- rw- r–)如果改为(666)(rw- rw- rw-)则修改以后配置文件不会生效 1sudo chmod 664 /etc/my.cnf my.cnf设置如下：12345678[client]default-character-set=utf8[mysqld]collation-server = utf8_unicode_ciinit-connect=&apos;SET NAMES utf8&apos;character-set-server = utf8[mysql]default-character-set=utf8 查看设置是否成功在命令行输入mysql，如果提示没有命令的话，在bash或者zsh的文件里修改，我用的是zsh，设置~/.zshrc, 12export MYSQL=&quot;/usr/local/mysql/bin/&quot;export PATH=&quot;/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:$MYSQL&quot; 在命令行输入mysql,进入mysql命令行后，输入status;或者show variables like &#39;%char%&#39;; 12345678| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/local/mysql-5.6.30-osx10.11-x86_64/share/charsets/","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://zhangfuxin.cn/tags/mysql/"}],"keywords":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}]},{"title":"mysql学习笔记","slug":"mysql-study","date":"2016-05-28T14:24:56.000Z","updated":"2019-08-21T15:13:24.133Z","comments":true,"path":"mysql-study.html","link":"","permalink":"http://zhangfuxin.cn/mysql-study.html","excerpt":"** mysql学习笔记：** &lt;Excerpt in index | 首页摘要&gt; mysql学习，基础的增删改查，数据库优化，索引，分片，集群搭建等等。","text":"** mysql学习笔记：** &lt;Excerpt in index | 首页摘要&gt; mysql学习，基础的增删改查，数据库优化，索引，分片，集群搭建等等。 &lt;The rest of contents | 余下全文&gt; mysql的特点 关系型数据库，免费使用， 插入式存储引擎， 性能高， 基础的增删改查 ddl语句，数据定义语句 123456789101112create database test1;drop database test1;use test1;create table emp(ename varchar(10),hiredate date,sal decimal(10,2),deptno int(2));drop table emp;alter table emp modify ename varchar(20);alter table emp add column age int(3);alter table emp drop column age;alter table emp change age age1 int(4);alter table emp add birth date after ename;alter table emp modify age int(3) first;alter table emp rename emp1; dml语句，数据操纵语句 123456789101112insert into emp(ename,hiredate,sal,deptno) values(&apos;zzx1&apos;,&apos;2000-10-11&apos;,2000,1);insert into emp values(&apos;lisa&apos;,&apos;2004-05-09&apos;,3000,2);insert into dept values(5,&apos;dept5&apos;),(6,&apos;dept6&apos;);update emp set sal=4000 where ename=&apos;lisa&apos;;update emp a,dept b set a.sal=a.sal*b.deptno,b.deptname=a.ename where a.deptno=b.deptno;delete from emp where ename=&apos;dony&apos;;delete a,b from emp a,dept b where a.deptno=b.deptno and a.deptno=3;select * from emp where ename=&apos;lisa&apos;;select distinct deptno from emp;select * from emp order by sal(desc);select * from emp order by sal limit 5;select * from emp order by sal limit 1,5;ss dcl语句，数据控制语句 sql优化 尽量使用 prepareStatement(java)，利用预处理功能。 在进行多条记录的增加、修改、删除时，建议使用批处理功能，批处理的次数以整个 SQL 语句不超过相应数据库的 SQL 语句大小的限制为准。 建议每条 SQL 语句中 in 中的元素个数在 200 以下，如果个数超过时，应拆分为多条 SQL 语句。禁止使用 xx in(‘’,’’….) or xx in(‘’,’’,’’)。 ★ 禁止使用 or 超过 200，如 xx =’123’ or xx=’456’。 ★ 尽量不使用外连接。 禁止使用 not in 语句，建议用 not exist。 ★ 禁止使用 Union, 如果有业务需要，请拆分为两个查询。 ★ 禁止在一条 SQL 语句中使用 3 层以上的嵌套查询，如果有，请考虑使用临时表或中间结果集。 尽量避免在一条 SQL 语句中从&gt;= 4 个表中同时取数， 对于仅是作为过滤条件关联，但不涉及取数的表，不参与表个数计算 查询条件里任何对列的操作都将导致表扫描，所以应尽量将数据库函数、计算表达式写在逻辑操作符右边。 在对 char 类型比较时,建议不要使用 rtrim()函数,应该在程序中将不足的长度补齐。 用多表连接代替 EXISTS 子句。 如果有多表连接时， 应该有主从之分， 并尽量从一个表取数， 如 select a.col1, a.col2from a join b on a.col3=b.col4 where b.col5 = ‘a’。 在使用 Like 时，建议 Like 的一边是字符串，表列在一边出现。 不允许将 where 子句的条件放到 having 中。 将更新操作放到事务的最后执行。如 一个事务需更新多个对象时，需保证更新的顺序一致以避免死锁的发生。如总是先更新子表再更新主表，根据存货档案批量更新现存量时，对传入的存货档案 PK 进行排序，再做更新处理等。 禁止随意使用临时表，在临时数据不超过 200 行的情况下禁止使用临时表。 禁止随意使用 distinct，避免造成不必要的排序。 索引优化 创建索引，删除索引 12create index cityname on city(city(10));drop index cityname on city; 搜索的索引列最好在where的字句或者连接子句 使用唯一索引 使用短索引，对于较长的字段，使用其前缀做索引 不要过度使用索引，索引引起额外的性能开销和维护 高级优化措施集群搭建","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://zhangfuxin.cn/tags/mysql/"}],"keywords":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}]},{"title":"nodejs开发规范","slug":"node-develop","date":"2016-05-23T06:18:02.000Z","updated":"2019-08-21T15:13:24.134Z","comments":true,"path":"node-develop.html","link":"","permalink":"http://zhangfuxin.cn/node-develop.html","excerpt":"** nodejs开发规范：** &lt;Excerpt in index | 首页摘要&gt; nodejs开发中应当遵循的规范，以及最佳实践","text":"** nodejs开发规范：** &lt;Excerpt in index | 首页摘要&gt; nodejs开发中应当遵循的规范，以及最佳实践 &lt;The rest of contents | 余下全文&gt; node开发需要编程规范吗？ js的灵活性非常大，如果开发人员每个人都按自己的习惯随意编写，js的代码会非常混乱不堪。js程序员需要更强的自律性和规范，才能写出易读性，易维护的代码。 随着前端mvc的崛起，前端的js代码会更加庞大难以管理，如果没有统一的规范，后期维护会比登天还难。 编码规范 缩进采用两个空格缩进，在编辑器中设置tab为两个空格 变量声明 用var声明变量var assert = require(‘assert’);var fork = require(‘child_process’).fork;var net = require(‘net’); 错误实例：var assert = require(‘assert’), fork = require(‘child_process’).fork, net = require(‘net’)； 用字面量声明方式var num = 123;var aaa = {};var arr = [];var isAdmin = true; 避免使用：var obj =new Object();var arr = new Array();var test =new String(“”);var size = new Number(); 不要在for循环等循环里声明var变量首先var是函数作用域，在循环声明以后只有等函数声明周期结束这些资源才会释放 空格在操作符前后需要加上空格,= 、% 、* 、- 、+ 前后都应该加一个空格比如：var foo = ‘bar’ + baz;错误实例：var foo=’bar’+baz; 单双引号的使用在node中尽量使用单引号，var html = ‘CNode‘;在json中使用双引号 分号给表达式结尾加分号，尽管js会自动在行尾加上分号，但是会产生一些误解 命名规范在编码中，命名是重头戏。好的命名可以使代码赏心悦目，具有良好的维护性。 变量命名变量名采用小驼峰命名，单词之间没有任何符号如：var adminUser = {};var callNum = 2134323; 方法命名也是采用小驼峰命名，与变量不同的是采用动词或判断行词汇，如：var getUser = function(){};var isAdmin = function(){};var findUser = function(){}; 类命名类名采用大驼峰，所有单词首字母大写，如：function User{} 常量命名作为常量，单词所有字母大写，用下划线分割，如：var PINK_COLOR = “PINK”; 文件命名命名文件时，尽量使用下划线分割单词，比如child_process.js和string_decode.js 包名在包名中尽量不要包含js和node的字样，应当适当短并且有意义 其它要点 作用域慎用with和eval（），容易引起作用域混乱 比较操作尽量使用===代替==,否则会遇到下面的情况，’0’==0;//true;‘’==0;//true;‘0’===’’//false; 严格模式在node后台中尽量全使用严格模式‘use strict’; 对象和数组遍历数组遍历使用普通for循环，避免使用for in对数组遍历，对象的遍历使用for in 项目中实践 sublime和webstorm都有JSLint,JSHint这样的代码质量工具，在配置文件中制定好模板规范即可 在版本控制工具中设置hook，在precommit的脚本中设置，如果代码不符合标准，就无法提交 参考文献 深入浅出nodejs js秘密花园 js高级编程","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}],"tags":[{"name":"node","slug":"node","permalink":"http://zhangfuxin.cn/tags/node/"}],"keywords":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}]},{"title":"redis学习笔记","slug":"redis-study","date":"2016-05-23T00:25:57.000Z","updated":"2019-08-21T15:13:24.136Z","comments":true,"path":"redis-study.html","link":"","permalink":"http://zhangfuxin.cn/redis-study.html","excerpt":"** redis学习笔记：** &lt;Excerpt in index | 首页摘要&gt; redis数据库的基本操作，增删改查","text":"** redis学习笔记：** &lt;Excerpt in index | 首页摘要&gt; redis数据库的基本操作，增删改查 &lt;The rest of contents | 余下全文&gt; keysredis本质上是一个key-value数据库 设置：set key value 获取：get key 判断存在：exists key 删除：del key del test:fan:age 重命名：rename oldkey newkey 数量：dbsize 返回数据 获取所有key（通配符）：Keys test:*:ageKeys test:?:age 清空：flushdb flushall 设置有效时间：expire test:fan:age 30 查询有效时间：ttl test:fan:age String类型 设置： set key value setnx ky value(nx是not exist) mset key1 value1 keyN valueN msetnx key1 value1 keyN valueN 获取： get 不存在返回nil getset 设置key的值，并返回key的旧值，不存在返回nil mget 自增减： incr key 对key的值进行++操作，返回新的值 decr key incrby key integer 对key加上一个数值 decrby key integer 截取： substr key indexStart indexEnd 下标从0开始 追加： append key value list类型redis的list其实就是一个每个元素都是string 的双向链表，所以push和pop的时间复杂度都是O（1） 添加 lpush key string 在头部添加 rpush key string 在尾部添加 修改 lset key index value 修改指定下标的key的值 删除 lpop key 从头部返回删除 rpop key 从尾部 lrem key count value 删除count个相同的value，count为0删除全部 blpop key …keyN timeout brpop 从尾部删除 获取 lrange key indexStart indexEnd 数量 llen key 返回key对应的list长度 截取 ltrim key start end 转移 rpoplpush key1 key2 从key1尾部移到key2头部 set集合redis的set就是String的无序集合，通过hashtable实现 添加 sadd key member 删除 srem key member 移除指定的元素 spop key 删除并返回一个随机的 获取 smembers key 返回所有 srandmember 随机取一个不删除 判断存在 sismember key member 数量 scard key 返回元素个数 转移 smove srckey dstkey member 取交集 sinter key1 key2 keyN sinterstore dstkey key1 keyN 将交集存在dstkey 取并集 sunion key1 key2 keyN sunionstore dstkey key1 keyN 将并集存在dstkey 取差集 sdiff key1 key2 keyN sdiffstore dstkey key1 keyN 将差集存在dstkey 有序set类型和set一样，不同的是每个元素关联一个double类型的score，根据score排序，sorted set的实现由skip list和hashtable 添加 zadd key score member 删除 zrem key member zremrangebyrank key min max zremrangebyscore key min max 删除集合score在给定区间的元素 获取 zrange key start end zrevrange key start end 按score的逆序 zrangebyscore key min max 判断存在 zrank key member 返回下标 zrerank key member 返回逆序的下标 数量 zcard key 总数 zcount key min max 区间的数量 修改 zincrby key incr member 增加member的score值并排序 hash类型redis的hash是一个string类型的field和value的映射表，hash特别适合存储对象， 设置： hset key field value hmset key field1 value1 field2 value2 获取： hget key field hmget key field1 field2 判断存在 hexists key field 删除 hdel key field 查找 hkeys key 返回所有 field hvals key 返回所有的value hgetall key 返回所有field和value 数量 hlen key 值加减 hincrby key field integer 将指定的hash field加上定值","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://zhangfuxin.cn/tags/redis/"}],"keywords":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}]},{"title":"git比svn的优势","slug":"git-svn","date":"2016-05-22T03:13:00.000Z","updated":"2019-08-21T15:13:24.126Z","comments":true,"path":"git-svn.html","link":"","permalink":"http://zhangfuxin.cn/git-svn.html","excerpt":"** git比svn的优势：** &lt;Excerpt in index | 首页摘要&gt; 主要介绍svn和git在使用的时候一些区别","text":"** git比svn的优势：** &lt;Excerpt in index | 首页摘要&gt; 主要介绍svn和git在使用的时候一些区别 &lt;The rest of contents | 余下全文&gt; 合并操作时对提交过程的保留 git:合并操作保留原有的提交过程 svn:多个提交合并为一个提交 不用因为合并操作而导致追踪的困难 修正提交 git：可以修正提交。使用功能分支工作流，在自己的分支可以方便修正提交而不会影响大家。 svn：一旦提交就到服务器上，实际使用中就是不能修改（svn可以在服务器上修改，因为过程复杂需要权限实际上从不会这样做） 本地分支 git可以方便的创建本地分支,创建时间极短,分支可以是本地的,不会存在svn中目录权限的问题 强大的合并能力 git：重命名（无论文件还有目录）提交 可以合并上 文件重命名前的这些文件的提交 svn：重命名（无论文件还有目录）提交后，你本地/或是分支上 有文件重命名前的这些文件的修改或提交，在做合并操作时,你会碰上传说中难搞的树冲突！ 这就导致在调整目录名称和类名调整的时候比较繁琐,需要告诉大家,我修改完以后你再修改 tag的支持 svn在模型上是没有分支和tag的。tag是通过目录权限限制（对开发只读）来保证不变。 git模型上一等公民支持tag，保证只读。 速度优势 git的提交是个本地提交,相对svn来说如闪电一般 git提供了暂存区,可以方便制定提交内容,而不是全部内容 日志查看 git：本地包含了完整的日志，闪电的速度（并且无需网络) svn：需要从服务拉取。 一旦用了git后，等待svn日志过程简直让我发狂","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"java和javascript日期详解","slug":"java-date","date":"2016-05-13T13:48:00.000Z","updated":"2019-08-21T15:13:24.128Z","comments":true,"path":"java-date.html","link":"","permalink":"http://zhangfuxin.cn/java-date.html","excerpt":"** java，js日期转换：** &lt;Excerpt in index | 首页摘要&gt; java的各种日期转换","text":"** java，js日期转换：** &lt;Excerpt in index | 首页摘要&gt; java的各种日期转换 &lt;The rest of contents | 余下全文&gt; 日期表示类型 获取long类型的日期格式 1234long time = System.currentTimeMillis();System.out.printf(time+\"\");Date date =new Date();System.out.println(date.getTime()); 获取制定格式的日期 123SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd hh:mm:ss\");Date date =new Date();System.out.println(sdf.format(date) ); 把制定格式的日期转为date或者毫秒值 123SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd hh:mm:ss\");Date date = sdf.parse(\"2016-05-22 10:15:21\");long mills = date.getTime(); 说明:System.currentTimeMillis()并不能精确到1ms的级别,它取决于运行的系统,你再windows,mac,linux精确的范围都有差异,对于有高精度时间的要求,不能使用这个 日期计算 最方便的方式是将时间转为毫秒值进行计算1234Date from =new Date();Thread.sleep(200);//线程休眠2msDate to =new Date();System.out.println(to.getTime()-from.getTime()); 高精度时间12long time1 =System.nanoTime();System.out.printf(time1+\"\"); 说明:System.nanoTime()提高了ns级别的精度,1ms=1000000ns, javascript日期 获取时间的毫秒值，获取月份，时间 1234567891011121314var myDate = new Date();myDate.getYear(); //获取当前年份(2位)myDate.getFullYear(); //获取完整的年份(4位,1970-????)myDate.getMonth(); //获取当前月份(0-11,0代表1月)myDate.getDate(); //获取当前日(1-31)myDate.getDay(); //获取当前星期X(0-6,0代表星期天)myDate.getTime(); //获取当前时间(从1970.1.1开始的毫秒数)myDate.getHours(); //获取当前小时数(0-23)myDate.getMinutes(); //获取当前分钟数(0-59)myDate.getSeconds(); //获取当前秒数(0-59)myDate.getMilliseconds(); //获取当前毫秒数(0-999)myDate.toLocaleDateString(); //获取当前日期var mytime=myDate.toLocaleTimeString(); //获取当前时间myDate.toLocaleString( ); //获取日期与时间 时间戳获取注意，java，php等生成的时间戳是秒，不是毫秒，所以需要签名时间戳的时候，需要转为秒时间戳 12var time = new Date();var timestamp = parseInt(time.getTime()/1000); 格式化时间 12345678910111213141516//获取当前时间，格式YYYY-MM-DDfunction getNowFormatDate() &#123; var date = new Date(); var seperator1 = \"-\"; var year = date.getFullYear(); var month = date.getMonth() + 1; var strDate = date.getDate(); if (month &gt;= 1 &amp;&amp; month &lt;= 9) &#123; month = \"0\" + month; &#125; if (strDate &gt;= 0 &amp;&amp; strDate &lt;= 9) &#123; strDate = \"0\" + strDate; &#125; var currentdate = year + seperator1 + month + seperator1 + strDate; return currentdate;&#125;","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}],"tags":[{"name":"java","slug":"java","permalink":"http://zhangfuxin.cn/tags/java/"}],"keywords":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}]},{"title":"制定学习目标和计划","slug":"study-goals","date":"2016-05-06T02:20:45.000Z","updated":"2019-08-21T15:13:24.140Z","comments":true,"path":"study-goals.html","link":"","permalink":"http://zhangfuxin.cn/study-goals.html","excerpt":"** 制定学习目标和计划：** &lt;Excerpt in index | 首页摘要&gt; 近期的学习目标和学习重点,提高自己的能力","text":"** 制定学习目标和计划：** &lt;Excerpt in index | 首页摘要&gt; 近期的学习目标和学习重点,提高自己的能力 &lt;The rest of contents | 余下全文&gt; 找到自己的兴趣 自己主动学习一定要基于自己的兴趣,不要看什么框架流行,什么语言火,就去学,学的不温不火,然后放弃. 一定看自己的兴趣,比如你对色彩,对布局,对特效比较痴迷,那你去css3,html5做出特酷的效果,肯定能让你肯定自己,收获知识和自信. 没有兴趣的时候,可以适当的多接触一些东西,在最短的时间多接触一些领域,让自己的心去做选择, 制定目标 为什么要制定目标? 制定目标是对自己学习能力的检验,同时也是提高学习效率的关键,而不是自己没有目的的瞎看, 如何制定目标? 结合自身的能力,定制比自己能力稍高的目标,这样自己通过一定程度的努力可以实现目标.这样自己的能力能一次一次提高. 及时反馈 古人说的好,吾日三省吾身,对待学习目标也是一样,要时不时的看自己的目标完成的如何,进度如何,是不是需要调整,不能闷着头蛮干,方向错了,再多的努力也是白搭了. 总结 我在刚开始学编程的时候,每天都给自己定制了目标,一天完成多少课时,完成多少练习,都是按量完成,在最初的几个月收到了立竿见影的效果,让我也在短短三个月的时间学会了java,所以,目标的制定对于结果的影响是非常大.","categories":[{"name":"个人随笔","slug":"个人随笔","permalink":"http://zhangfuxin.cn/categories/个人随笔/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"个人随笔","slug":"个人随笔","permalink":"http://zhangfuxin.cn/categories/个人随笔/"}]},{"title":"使用ghost搭建个人博客","slug":"ghost-blog","date":"2016-05-03T23:59:22.000Z","updated":"2019-08-21T15:13:24.125Z","comments":true,"path":"ghost-blog.html","link":"","permalink":"http://zhangfuxin.cn/ghost-blog.html","excerpt":"** 使用ghost搭建个人博客：** &lt;Excerpt in index | 首页摘要&gt; 使用ghost搭建个人博客","text":"** 使用ghost搭建个人博客：** &lt;Excerpt in index | 首页摘要&gt; 使用ghost搭建个人博客 &lt;The rest of contents | 余下全文&gt; ghost简介 ghost是轻量级的博客建站工具,使用起来简单,功能强大,适合个人搭建小型网站,个人博客,或者个人展示的网站 ghost基于nodejs,对于熟悉js的前端小伙伴来说,入手起来也是简单不少. 准备工作 安装nodejs 安转git 配置ssh 下载ghost 购买域名 搭建博客定制个人博客享受吧","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"git学习笔记","slug":"git-config-study","date":"2016-05-01T00:24:45.000Z","updated":"2019-08-21T15:13:24.125Z","comments":true,"path":"git-config-study.html","link":"","permalink":"http://zhangfuxin.cn/git-config-study.html","excerpt":"** git学习笔记：** &lt;Excerpt in index | 首页摘要&gt; git的常用操作，高级技巧都要哦","text":"** git学习笔记：** &lt;Excerpt in index | 首页摘要&gt; git的常用操作，高级技巧都要哦 &lt;The rest of contents | 余下全文&gt; 安装git 下载安装包 ￼下载地址￼ 安装git 进入命令行,输入git看看是否成功 配置git 配置全局用户名和密码 `git config –global user.name “John Doe” git config –global user.email johndoe@example.com ` 配置ssh公钥 cd ~/.ssh 然后ls 如果没有,直接生成,一路点击enter ``` ssh-keygen cat ~/.ssh/id_rsa.pub ``` 把公钥配置到github的个人设置 常用的命令 repository操作 检出（clone）仓库代码：git clone repository-url / git clone repository-url local-directoryname 例如，clone jquery 仓库到本地： git clone git://github.com/jquery/jquery.git clone jquery 仓库到本地，并且重命名为 my-jquery ：git clone git://github.com/jquery/jquery.git my-jquery 查看远程仓库：git remote -v 添加远程仓库：git remote add [name] [repository-url] 删除远程仓库：git remote rm [name] 修改远程仓库地址：git remote set-url origin new-repository-url 拉取远程仓库： git pull [remoteName] [localBranchName] 推送远程仓库： git push [remoteName] [localBranchName] 提交/拉取/合并/删除 添加文件到暂存区（staged）：git add filename / git stage filename 将所有修改文件添加到暂存区（staged）： git add --all / git add -A 提交修改到暂存区（staged）：git commit -m &#39;commit message&#39; / git commit -a -m &#39;commit message&#39; 注意理解 -a 参数的意义 从Git仓库中删除文件：git rm filename 从Git仓库中删除文件，但本地文件保留：git rm --cached filename 重命名某个文件：git mv filename newfilename 或者直接修改完毕文件名 ，进行git add -A &amp;&amp; git commit -m &#39;commit message&#39; Git会自动识别是重命名了文件 获取远程最新代码到本地：git pull (origin branchname) 可以指定分支名，也可以忽略。pull 命令自动 fetch 远程代码并且 merge，如果有冲突，会显示在状态栏，需要手动处理。更推荐使用：git fetch 之后 git merge --no-ff origin branchname 拉取最新的代码到本地仓库，并手动 merge 。 日志查看 查看日志：git log 查看日志，并查看每次的修改内容：git log -p 查看日志，并查看每次文件的简单修改状态：git log --stat 一行显示日志：git log --pretty=oneline / git log --pretty=&#39;format:&quot;%h - %an, %ar : %s&#39; 查看日志范围： 查看最近10条日志：git log -10 查看2周前：git log --until=2week 或者指定2周的明确日期，比如：git log --until=2015-08-12 查看最近2周内：git log --since=2week 或者指定2周明确日志，比如：git log --since=2015-08-12 只查看某个用户的提交：git log --committer=user.name / git log --author=user.name 取消操作 上次提交msg错误/有未提交的文件应该同上一次一起提交，需要重新提交备注：git commit --amend -m &#39;new msg&#39; 一次git add -A后，需要将某个文件撤回到工作区，即：某个文件不应该在本次commit中：git reset HEAD filename 撤销某些文件的修改内容：git checkout -- filename 注意：一旦执行，所有的改动都没有了，谨慎！谨慎！谨慎！ 将工作区内容回退到远端的某个版本：git reset --hard &lt;sha1-of-commit&gt; --hard：reset stage and working directory , 以来所有的变更全部丢弃，并将 HEAD 指向 --soft：nothing changed to stage and working directory ,仅仅将HEAD指向 ，所有变更显示在”changed to be committed”中 --mixed：default,reset stage ,nothing to working directory ，这也就是第二个例子的原因 比较差异 查看工作区（working directory）和暂存区（staged）之间差异：git diff 查看工作区（working directory）与当前仓库版本（repository）HEAD版本差异：git diff HEAD 查看暂存区（staged）与当前仓库版本（repository）差异：git diff --cached / git diff --staged 合并操作 解决冲突后/获取远程最新代码后合并代码：git merge branchname 保留该存在版本合并log：git merge --no-ff branchname 参数--no-ff防止 fast-forward 的提交","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://zhangfuxin.cn/categories/学习笔记/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://zhangfuxin.cn/categories/学习笔记/"}]},{"title":"ubuntu16服务器环境配置","slug":"ubuntu-dev-config","date":"2016-04-26T10:48:11.000Z","updated":"2019-08-21T15:13:24.147Z","comments":true,"path":"ubuntu-dev-config.html","link":"","permalink":"http://zhangfuxin.cn/ubuntu-dev-config.html","excerpt":"** ubuntu开发环境配置：** &lt;Excerpt in index | 首页摘要&gt; ubuntu16下node,java开发环境配置","text":"** ubuntu开发环境配置：** &lt;Excerpt in index | 首页摘要&gt; ubuntu16下node,java开发环境配置 &lt;The rest of contents | 余下全文&gt; ubuntu14升级到ubuntu16 终端下执行命令sudo apt-get update &amp;&amp; sudo apt-get dist-upgrade 重启系统以完成更新的安装sudo init 6 用命令安装更新管理器核心update-manager-core，如果服务器已安装则可以跳过sudo apt-get install update-manager-core 编辑/etc/update-manager/release-upgrades配置文件，设置Prompt=ltssudo vi /etc/update-manager/release-upgrades 启动升级进程sudo do-release-upgrade -d 安装系统软件 更新系统和软件 12sudo apt-get updatesudo apt-get upgade 谷歌浏览器，火狐浏览器，atom编辑器，sublime编辑器，webstome,idea,eclipse 安装搜狗输入法（官网），安装fcitx配置搜狗输入法 安装jdk 下载jdk并新建一个文件夹 1sudo mkdir /usr/lib/jvm 解压文件 1sudo tar zxvf jdk-7u71-linux-x64.tar.gz -C /usr/lib/jvm/jdk1.7 设置环境变量,设置~/.zshrc文件,或者编辑/etc/profile（全局）文件 1234export JAVA_HOME=/usr/lib/jvm/jdk1.7export JRE_HOME=$&#123;JAVA_HOME&#125;/jre export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 检查是否安装成功 打开shell, 1java --version 安装nodejs nodejs版本迭代较快，有时候需要检查在不同版本下的兼容性问题，用nvm来控制版本 安装nvm,source的时候根据自己的shell版本，~/.bashrc, ~/.profile, 或者 ~/.zshrc 1234curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.31.0/install.sh | bashexport NVM_DIR=&quot;$HOME/.nvm&quot;[ -s &quot;$NVM_DIR/nvm.sh&quot; ] &amp;&amp; . &quot;$NVM_DIR/nvm.sh&quot; # This loads nvmsource ~/.profile 安装不同版本的nodejs 12345nvm ls-remotenvm install v0.12.9nvm install 5.0nvm use 0.12.9nvm alias default 0.12.9 安装mongodb 配置公钥 12sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10echo \"deb http://repo.mongodb.org/apt/ubuntu \"$(lsb_release -sc)\"/mongodb-org/3.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.0.list 更新软件列表 12sudo apt-get updatesudo apt-get install -y mongodb-org 完成上面的安装步骤配置mongodb的数据库的位置 1sudo mongod --dbpath /data/db 启动mongod 123sudo service mongod startsudo service mongod stopsudo service mongod restart 安装redis 下载软件 1wget http://download.redis.io/releases/redis-2.8.11.tar.gz 解压安装 12tar xvfz redis-2.8.11.tar.gzcd redis-2.8.11 &amp;&amp; sudo make &amp;&amp; sudo make install 配置使用 下载配置文件和init启动脚本 12345wget https://github.com/ijonas/dotfiles/raw/master/etc/init.d/redis-serverwget https://github.com/ijonas/dotfiles/raw/master/etc/redis.confsudo mv redis-server /etc/init.d/redis-serversudo chmod +x /etc/init.d/redis-serversudo mv redis.conf /etc/redis.conf 初始化用户和日志路径 12345sudo useradd redissudo mkdir -p /var/lib/redissudo mkdir -p /var/log/redissudo chown redis.redis /var/lib/redissudo chown redis.redis /var/log/redis 设置开机自动启动，关机自动关闭 1sudo update-rc.d redis-server defaults 环境变量配置 认识环境变量相关的文件 /etc/profile —— 此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行.并从/etc/profile.d目录的配置文件中搜集shell的设置； /etc/environment —— 在登录时操作系统使用的第二个文件,系统在读取你自己的profile前,设置环境文件的环境变量； /etc/bashrc —— 为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取； ~/.profile —— 每个用户都可使用该文件输入专用于自己使用的shell信息，当用户登录时，该文件仅仅执行一次！默认情况下,它设置一些环境变量,执行用户的.bashrc文件； ~/.bashrc —— 该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该文件被读取； 配置环境变量 在Ubuntu14.04的~/.bashrc中添加的环境变量,在文件添加 1export PATH=$PATH:/home/qtcreator-2.6.1/bin 修改profile文件,vim编辑/etc/profile 12sudo vim /etc/profilesource /etc/profile 安装开发工具 zsh命令行工具 mysql客户端workbench，mongo客户端工具robomongo 安装git,svn版本控制工具12sudo apt-get install gitsudo apt-get install subversion","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://zhangfuxin.cn/tags/linux/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"我的梦想","slug":"dream","date":"2016-04-24T14:07:27.000Z","updated":"2019-08-21T15:13:24.122Z","comments":true,"path":"dream.html","link":"","permalink":"http://zhangfuxin.cn/dream.html","excerpt":"** 我的梦想：** &lt;Excerpt in index | 首页摘要&gt; 一个人如果活着没有梦想,那和咸鱼有什么区别?","text":"** 我的梦想：** &lt;Excerpt in index | 首页摘要&gt; 一个人如果活着没有梦想,那和咸鱼有什么区别? 请问你的梦想是什么? &lt;The rest of contents | 余下全文&gt; 我的梦想是什么? 刚开始接触编程的时候,感觉代码是个神器的世界,在这里你可以为所欲为,然后看到很多大神的框架,软件,在使用别人好的框架,好的软件,那一刻我感觉 “我的梦想就是用代码改变世界!” 感觉自己迷失了好久,找不到方向,曾经的激情不知道去了哪里? 开始追梦 有了梦想,我开始了疯狂的奋斗,每天休息4,5个小时,全身心去学习编程,努力还是很快得到了回报,我用了3个月就入门学好了java,然后找了java程序员的工作,就这样开始了我程序员的追梦之旅! 初级程序员 虽然入门了,但是刚开始的工作并不是一帆风顺的.我还记得第一份任务,老大让我写一个稍微复杂的接口,客户专用的接口,使用springmvc,还要提交到git上,对我而言,这一切都是新东西,经过我几天的努力,还是搞砸了,就这样第一个任务以失败告终! 虽然第一个任务失败了,但是工作还在继续,我还是继续努力的工作,我必须承认我不是编程的天才,可能别人一个小时完成的任务,我需要一个半小时,但是我必须做好,因为我有梦想! 中级程序员 在工作的时候就感觉时间飞逝,一天天很快过去.晚上睡觉的时候,我就会问自己,我今天到底做了什么功能?我收获了哪些技能?曾经有段时间每天都是该页面,我几乎烦的崩溃,感觉每天都在做无用的东西,后来发现,无论是前段后端,其实都是必不可少的技能,我的心态应该调整,让自己去喜欢前段,同时保持后端的热情. 一个成熟的程序员和菜鸟最大的区别应该是心态! 高级程序员 不再是代码搬运工，根据业务和需求自己随便造个轮子什么的。强大的代码能力，考虑事情应该全面，深刻 架构师 未完待续","categories":[{"name":"个人随笔","slug":"个人随笔","permalink":"http://zhangfuxin.cn/categories/个人随笔/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"个人随笔","slug":"个人随笔","permalink":"http://zhangfuxin.cn/categories/个人随笔/"}]},{"title":"程序员入门指南","slug":"coder-study","date":"2016-04-17T04:37:19.000Z","updated":"2019-08-21T15:13:24.120Z","comments":true,"path":"coder-study.html","link":"","permalink":"http://zhangfuxin.cn/coder-study.html","excerpt":"** 程序员入门指南 ：** &lt;Excerpt in index | 首页摘要&gt; 程序员入门必须了解的一些知识，个人经验，不喜勿喷！","text":"** 程序员入门指南 ：** &lt;Excerpt in index | 首页摘要&gt; 程序员入门必须了解的一些知识，个人经验，不喜勿喷！ &lt;The rest of contents | 余下全文&gt; 程序员的入门规划1.我该学习什么语言？ 这个问题困扰了几乎所有的程序员，比如java应用广好就业，比如php入门简单，ios待遇高， python是万能语言，HTML和js前端缺人才等等 个人见解：先学习难度小，大众化的编程语言，比如java，php，python，javascript,c/c++,这几个学哪一种其实差不多，入门以后看自己兴趣在进行其它语言的学习。 2.我该怎么学习编程？这个问题是所有的程序员都有的，我也经常会疑问，到底该怎么学习呢？ 个人见解： 先了解语言的特性，适用的范围场景，比如是适合web开发，还是适合客户端程序，有的适合并发多线程，有的适合异步，还有的比较稳定，适合构建大型项目，有的开发效率高，等等。 了解语言的语法和常用api的使用，比如变量的声明，循环的使用，io的读取，http服务的创建，把这些基本的语法搞清楚，在进行下一步的学习。 学习web开发之前的准备，数据库的学习，http协议的学习，html，css和javacript的常用知识了解 学习常用框架，比如java学习常用的ssh三大框架，node的学习express，一定要做2个项目练习，把自己的之前学习的知识都巩固一下， 总结一下自己学习的过程，明白编程的思想在哪里，思路在哪里，学习编程，首先应该培养的是编程的思维和思想，有个正确的思维后面都简单多了。 养成写博客或者学习笔记的习惯，推荐写博客， 熟悉项目管理工具，svn，git之类的必须要会，工作中这些都是必须的 准备面试，通过面试题进一步巩固自己的知识，夯实基础。 3.我应该去哪里学习编程？其实这个看个人，如果自学能力强，自控能力强，自学挺好的，下面我列举几个程序员常用的网站 网易云课堂，很多免费的视频课程，适合入门学习 慕课网，很多it入门教学视频，资源也不错 极客学院，和前两个网站差不多， 北风网，类似的教学网站，其它的就不说了 4.编程遇到问题怎么办？ 百度或者谷歌看看网上有没有类似的问题，一回生，二回熟，很快就明白了 去官网查看api文档查找原因 自己要学会debug代码，查找原因 去各大论坛逛逛，说不定早有人提问此类问题了 5.我想看编程的书籍去哪找呢？经典书籍还是买纸质的，买正版的，支持正版！ 新浪微盘，非常多的it书籍 脚本之家，非常多的pdf书籍，可惜大多数不是文字版pdf 英文原版书籍，都是高清文字版pdf，强烈推荐，都是英文原版的 计算机书控，都是免费的pdf文档，大多数不是文字版pdf 6.代码资源 最好的代码仓库 github csdn代码仓库 gist 代码片段之家 7.学习心态 不要老是折腾工具，ide工具和文本编辑器一样一个就够了 不要自满，编程的东西学一辈子也学不会，要谦虚好学 不要急躁，既然知识学不完，我们应该掌握学习方法，指定计划去学习 要持之以恒，学习是一辈子的事，如果你没有这个打算，还是不要做程序员的好 切忌眼高手低，必须要敲代码才能达到效果 8.编程进阶之路当有了一定的编程基础之后,最大的问题是确定自己的方向,这个时候最容易迷茫和困惑,学习什么技术?怎么去学,这些真的很难 个人建议如下:1.技术型方向:提高自己的编程能力和语言造诣,最有效的是”造轮子”,量变引起质变 写插件,写框架,写爬虫,写数据库,自制编程语言,等等.2.业务型方向:提高自己的业务能力,和客户的沟通能力,分析需求,解决客户的难题 多出去见客户,去现场,了解需求,分析需求,","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}]},{"title":"hexo和github打造个人博客","slug":"hexo-githup-blog","date":"2015-12-20T14:35:04.000Z","updated":"2019-08-21T15:13:24.127Z","comments":true,"path":"hexo-githup-blog.html","link":"","permalink":"http://zhangfuxin.cn/hexo-githup-blog.html","excerpt":"** hexo和github打造个人博客 ：** &lt;Excerpt in index | 首页摘要&gt; 使用hexo和github打造属于自己的静态博客，展示自己的作品，思想……","text":"** hexo和github打造个人博客 ：** &lt;Excerpt in index | 首页摘要&gt; 使用hexo和github打造属于自己的静态博客，展示自己的作品，思想…… &lt;The rest of contents | 余下全文&gt; ##说明 自己在使用hexo搭建静态博客的时候踩了许多坑,最终去官网看教程搞定了, 建议用hexo搭建个人博客的时候,最好看清教程的日期和使用的版本,这样就 不会因为版本的不同导致的问题了.建议先去hexo官网了解一下 hexo官网 1.准备工作 安装nodejs 去官网下载nodejs安装(推荐安装4.x),安装之后在命令行 node -v,如果成功说明node环境ok,不成功就去环境变量配置一下. 安装hexo 使用命令 npm install hexo -g,执行hexo -v 查看版本,本教程适合3.1.1以上版本 安装git 去官网下载git安装,不会自行百度 配置git 配置ssh私钥,上传到github上 2.github-pages的说明 github有两种主页,一种是github-page(个人主页),一种是项目主页,本教程针对个人主页 github-page需要将hexo博客发布到repository的master(主干)即可 github的个人主页要求repository的名称和username一致，加入username是tom，则repository的名称为tom.github.io 3.使用hexo写博客- 新建一个文件夹myblog, - 右键git bash here使用git的shell - 在shell中输入hexo init,回车执行 - 在shell中输入hexo g ,回车 - 在shell中hexo s,回车 - 去浏览器访问http://localhost:4000,访问到主页,然后在shell中ctrl c停止 - 在shell中hexo new &quot;first-blog&quot;,回车 - 在shell中hexo g ,回车 - 在shell中hexo s ,回车,在访问 - ok,在本地测试就没问题了4.发布到github打开项目根部录下的.config.yml,找到deploy,修改如下: 123deploy: - type: git repo: git@github.com:yourname/yourname.github.io.git,master 12345deploy: type: git repo: &lt;repository url&gt; branch: [branch] message: [message] 访问地址就是 http://tom.github.io/ 5.常用命令命令的简写为： 12345hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deployhexo clean 删除public文件夹6.常见问题 部署时出现git not found npm install hexo-deployer-git –save 安装依赖包 7.详细设置每个人对自己的博客都有不一样的要求，比如主题，分类，标签，评论插件的选择， 这些对程序员的你来说，都是小菜一碟，下面是官网教程： hexo官方文档 博客效果可以看我的个人博客 我的个人博客","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://zhangfuxin.cn/tags/hexo/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"node学习","slug":"node-study","date":"2015-12-19T10:58:56.000Z","updated":"2019-08-21T15:13:24.134Z","comments":true,"path":"node-study.html","link":"","permalink":"http://zhangfuxin.cn/node-study.html","excerpt":"** node学习： ** &lt;Excerpt in index | 首页摘要&gt; nodejs学习的方法，进阶路线","text":"** node学习： ** &lt;Excerpt in index | 首页摘要&gt; nodejs学习的方法，进阶路线 &lt;The rest of contents | 余下全文&gt; 一 学习内容 node的常用模块,buffer,fs,http,net等. node常用框架express,mongoose,koa,mocha,should 部署上线,pm2,grunt, 二 学习要点 了解node的特性和语法 编写扩展node模块 用异步的思想编程 常用框架的使用 回调的解决方案(promise) 三 入门实战 参照nodejs实战上的微博系统,使用express4.x+ mongoose实现 使用socket.io实现一个简单的即时聊天的系统 使用mongoose+express+node开发一个论坛系统 使用koa+mongoose做一个简单的cms或者权限系统 四 学习方法 建议有基础的直接开始入门实战,在练习中熟悉node的api,做完一个项目再去看书 不要一直看书,没什么效果的,实战永远是最有效的","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}],"tags":[{"name":"node","slug":"node","permalink":"http://zhangfuxin.cn/tags/node/"}],"keywords":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}]},{"title":"模板","slug":"2015-08-15-模板","date":"2015-08-15T01:13:14.000Z","updated":"2019-09-19T06:56:47.476Z","comments":true,"path":"2015-08-15-模板.html","link":"","permalink":"http://zhangfuxin.cn/2015-08-15-模板.html","excerpt":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt;","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://zhangfuxin.cn/tags/Scala/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]}]}