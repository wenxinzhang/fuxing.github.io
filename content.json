{"meta":{"title":"福星","subtitle":null,"description":null,"author":"福 星","url":"http://zhangfuxin.cn"},"pages":[],"posts":[{"title":"数据集网站汇总","slug":"2019-09-17-数据集网站汇总","date":"2019-09-17T02:30:04.000Z","updated":"2019-09-17T01:29:13.803Z","comments":true,"path":"2019-09-17-数据集网站汇总.html","link":"","permalink":"http://zhangfuxin.cn/2019-09-17-数据集网站汇总.html","excerpt":"** 数据集网站汇总：** &lt;Excerpt in index | 首页摘要&gt; ​ 如果用一个句子总结学习数据科学的本质，那就是： 学习数据科学的最佳方法就是应用数据科学。 如果你是一个初学者，你每完成一个新项目后自身能力都会有极大的提高，如果你是一个有经验的数据科学专家，你已经知道这里所蕴含的价值。","text":"** 数据集网站汇总：** &lt;Excerpt in index | 首页摘要&gt; ​ 如果用一个句子总结学习数据科学的本质，那就是： 学习数据科学的最佳方法就是应用数据科学。 如果你是一个初学者，你每完成一个新项目后自身能力都会有极大的提高，如果你是一个有经验的数据科学专家，你已经知道这里所蕴含的价值。 &lt;The rest of contents | 余下全文&gt; 一.如何使用这些资源? 如何使用这些数据源是没有限制的，应用和使用只受到您的创造力和实际应用。使用它们最简单的方法是进行数据项目并在网站上发布它们。这不仅能提高你的数据和可视化技能，还能改善你的结构化思维。 另一方面，如果你正在考虑/处理基于数据的产品，这些数据集可以通过提供额外的/新的输入数据来增加您的产品的功能。所以，继续在这些项目上工作吧，与更大的世界分享它们，以展示你的数据能力!我们已经在不同的部分中划分了这些数据源，以帮助你根据应用程序对数据源进行分类。 我们从简单、通用和易于处理数据集开始，然后转向大型/行业相关数据集。然后，我们为特定的目的——文本挖掘、图像分类、推荐引擎等提供数据集的链接。这将为您提供一个完整的数据资源列表。如果你能想到这些数据集的任何应用，或者知道我们漏掉了什么流行的资源，请在下面的评论中与我们分享。(部分可能需要翻墙) 二.由简单和通用的数据集开始 1.data.gov ( https://www.data.gov/ ) 这是美国政府公开数据的所在地，该站点包含了超过19万的数据点。这些数据集不同于气候、教育、能源、金融和更多领域的数据。 2.data.gov.in ( https://data.gov.in/ ) 这是印度政府公开数据的所在地，通过各种行业、气候、医疗保健等来寻找数据，你可以在这里找到一些灵感。根据你居住的国家的不同，你也可以从其他一些网站上浏览类似的网站。 3.World Bank( http://data.worldbank.org/ ) 世界银行的开放数据。该平台提供 Open Data Catalog，世界发展指数，教育指数等几个工具。 4.RBI ( https://rbi.org.in/Scripts/Statistics.aspx ) 印度储备银行提供的数据。这包括了货币市场操作、收支平衡、银行使用和一些产品的几个指标。 5.Five Thirty Eight Datasets ( https://github.com/fivethirtyeight/data ) Five Thirty Eight，亦称作 538，专注与民意调查分析，政治，经济与体育的博客。该数据集为 Five Thirty Eight Datasets 使用的数据集。每个数据集包括数据，解释数据的字典和Five Thirty Eight 文章的链接。如果你想学习如何创建数据故事，没有比这个更好。 三.大型数据集 1.Amazon Web Services(AWS)datasets ( https://aws.amazon.com/cn/datasets/ )Amazon提供了一些大数据集，可以在他们的平台上使用，也可以在本地计算机上使用。您还可以通过EMR使用EC2和Hadoop来分析云中的数据。在亚马逊上流行的数据集包括完整的安然电子邮件数据集，Google Books n-gram，NASA NEX 数据集，百万歌曲数据集等。 2.Google datasets ( https://cloud.google.com/bigquery/public-data/ ) Google 提供了一些数据集作为其 Big Query 工具的一部分。包括 GitHub 公共资料库的数据，Hacker News 的所有故事和评论。 3.Youtube labeled Video Dataset ( https://research.google.com/youtube8m/ ) 几个月前，谷歌研究小组发布了YouTube上的“数据集”，它由800万个YouTube视频id和4800个视觉实体的相关标签组成。它来自数十亿帧的预先计算的，最先进的视觉特征。 四.预测建模与机器学习数据集 1.UCI Machine Learning Repository ( https://archive.ics.uci.edu/ml/datasets.html )UCI机器学习库显然是最著名的数据存储库。如果您正在寻找与机器学习存储库相关的数据集，通常是首选的地方。这些数据集包括了各种各样的数据集，从像Iris和泰坦尼克这样的流行数据集到最近的贡献，比如空气质量和GPS轨迹。存储库包含超过350个与域名类似的数据集(分类/回归)。您可以使用这些过滤器来确定您需要的数据集。 2.Kaggle ( https://www.kaggle.com/datasets ) Kaggle提出了一个平台，人们可以贡献数据集，其他社区成员可以投票并运行内核/脚本。他们总共有超过350个数据集——有超过200个特征数据集。虽然一些最初的数据集通常出现在其他地方，但我在平台上看到了一些有趣的数据集，而不是在其他地方出现。与新的数据集一起，界面的另一个好处是，您可以在相同的界面上看到来自社区成员的脚本和问题。 3.Analytics Vidhya (https://datahack.analyticsvidhya.com/contest/all/ ) 您可以从我们的实践问题和黑客马拉松问题中参与和下载数据集。问题数据集基于真实的行业问题，并且相对较小，因为它们意味着2 - 7天的黑客马拉松。 4.Quandl ( https://www.quandl.com/ ) Quandl 通过起网站、API 或一些工具的直接集成提供了不同来源的财务、经济和替代数据。他们的数据集分为开放和付费。所有开放数据集为免费，但高级数据集需要付费。通过搜索仍然可以在平台上找到优质数据集。例如，来自印度的证券交易所数据是免费的。 5.Past KDD Cups ( http://www.kdd.org/kdd-cup ) KDD Cup 是 ACM Special Interest Group 组织的年度数据挖掘和知识发现竞赛。 6.Driven Data ( https://www.drivendata.org/ ) Driven Data 发现运用数据科学带来积极社会影响的现实问题。然后，他们为数据科学家组织在线模拟竞赛，从而开发出最好的模型来解决这些问题。 五.图像分类数据集 1.The MNIST Database ( http://yann.lecun.com/exdb/mnist/ ) 最流行的图像识别数据集，使用手写数字。它包括6万个示例和1万个示例的测试集。这通常是第一个进行图像识别的数据集。 2.Chars74K (http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/ ) 这里是下一阶段的进化，如果你已经通过了手写的数字。该数据集包括自然图像中的字符识别。数据集包含74,000个图像，因此数据集的名称。 3.Frontal Face Images (http://vasc.ri.cmu.edu//idb/html/face/frontal_images/index.html ) 如果你已经完成了前两个项目，并且能够识别数字和字符，这是图像识别中的下一个挑战级别——正面人脸图像。这些图像是由CMU &amp; MIT收集的，排列在四个文件夹中。 4.ImageNet ( http://image-net.org/ ) 现在是时候构建一些通用的东西了。根据WordNet层次结构组织的图像数据库(目前仅为名词)。层次结构的每个节点都由数百个图像描述。目前，该集合平均每个节点有超过500个图像(而且还在增加)。 六.文本分类数据集 1.Spam – Non Spam (http://www.esp.uem.es/jmgomez/smsspamcorpus/) 区分短信是否为垃圾邮件是一个有趣的问题。你需要构建一个分类器将短信进行分类。 2.Twitter Sentiment Analysis (http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/) 该数据集包含 1578627 个分类推文，每行被标记为1的积极情绪，0位负面情绪。数据依次基于 Kaggle 比赛和 Nick Sanders 的分析。 3.Movie Review Data (http://www.cs.cornell.edu/People/pabo/movie-review-data/) 这个网站提供了一系列的电影评论文件，这些文件标注了他们的总体情绪极性(正面或负面)或主观评价(例如，“两个半明星”)和对其主观性地位(主观或客观)或极性的标签。 七.推荐引擎数据集 1.MovieLens ( https://grouplens.org/ ) MovieLens 是一个帮助人们查找电影的网站。它有成千上万的注册用户。他们进行自动内容推荐，推荐界面，基于标签的推荐页面等在线实验。这些数据集可供下载，可用于创建自己的推荐系统。 2.Jester (http://www.ieor.berkeley.edu/~goldberg/jester-data/) 在线笑话推荐系统。 八.各种来源的数据集网站 1.KDNuggets (http://www.kdnuggets.com/datasets/index.html) KDNuggets 的数据集页面一直是人们搜索数据集的参考。列表全面，但是某些来源不再提供数据集。因此，需要谨慎选择数据集和来源。 2.Awesome Public Datasets (https://github.com/caesar0301/awesome-public-datasets) 一个GitHub存储库，它包含一个由域分类的完整的数据集列表。数据集被整齐地分类在不同的领域，这是非常有用的。但是，对于存储库本身的数据集没有描述，这可能使它非常有用。 3.Reddit Datasets Subreddit (https://www.reddit.com/r/datasets/) 由于这是一个社区驱动的论坛，它可能会遇到一些麻烦(与之前的两个来源相比)。但是，您可以通过流行/投票来对数据集进行排序，以查看最流行的数据集。另外，它还有一些有趣的数据集和讨论。 九.结尾的话 我们希望这一资源清单对于那些想项目的人来说是非常有用的。这绝对是一个金矿，好好加以利用吧! 转自：https://mp.weixin.qq.com/s?__biz=MzI2MjM2MDEzNQ==&amp;mid=2247489072&amp;idx=1&amp;sn=2ac46ef358be4eef43f3de8670086746&amp;chksm=ea4d0b18dd3a820ef82122648806c8516970e8e7323efb5475aa0db1da1752d22ee8c38ec604&amp;mpshare=1&amp;scene=23&amp;srcid=042625ULmfK6xU66wcmkCf1G#rd","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"CM+CDH离线安装","slug":"CDH-hadoop","date":"2019-09-05T17:30:04.000Z","updated":"2019-09-06T00:12:24.698Z","comments":true,"path":"CDH-hadoop.html","link":"","permalink":"http://zhangfuxin.cn/CDH-hadoop.html","excerpt":"** CM+CDH离线安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Cloudera Manager可以轻松管理任何生产规模的Hadoop部署。通过直观的用户界面快速部署，配置和监控群集 - 完成滚动升级，备份和灾难恢复以及可定制警报。","text":"** CM+CDH离线安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Cloudera Manager可以轻松管理任何生产规模的Hadoop部署。通过直观的用户界面快速部署，配置和监控群集 - 完成滚动升级，备份和灾难恢复以及可定制警报。 &lt;The rest of contents | 余下全文&gt; 1.1 Cloudera 简介1.1.1Cloudera 简介官网：https://www.cloudera.com/ 文档：https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_intro.html ​ CDH是Apache Hadoop和相关项目中最完整，经过测试和最流行的发行版。CDH提供了Hadoop的核心元素 - 可扩展存储和分布式计算 - 以及基于Web的用户界面和重要的企业功能。CDH是Apache许可的开源软件，是唯一提供统一批处理，交互式SQL和交互式搜索以及基于角色的访问控制的Hadoop解决方案。 CDH提供： 灵活性 - 存储任何类型的数据并使用各种不同的计算框架对其进行操作，包括批处理，交互式SQL，自由文本搜索，机器学习和统计计算。 集成 - 在完整的Hadoop平台上快速启动和运行，该平台可与各种硬件和软件解决方案配合使用。 安全 - 处理和控制敏感数据。 可扩展性 - 支持广泛的应用程序，并扩展和扩展它们以满足您的要求。 高可用性 - 充满信心地执行任务关键型业务任务。 兼容性 - 利用您现有的IT基础架构和投资。 1.1.2Hadoop起源​ 2004年，Cutting和同为程序员出身的Mike Cafarella决定开发一款可以代替当时的主流搜索产品的开源搜索引擎，这个项目被命名为Nutch。Nutch基于Lucence实现的搜索引擎，能够从互联网上抓取网页数据。抓取来的海量数据的存储问题。但是，这些海量数据都是非结构化数据，不能存在关系型数据库里。如果连数据的存储和管理都解决不了化，就谈不上后续为用户提供搜索服务，包括通过算法去优化检索速度。那时的Cutting正处在Freelancer的生涯 中，所以他希望自己的项目能通过一种低开销的方式来构建网页中的大量算法。幸运的是，Google这时正好发布了一项研究报告，报告中介绍了两款Google为支持自家的搜索引擎而开发的软件平台。这两个平台一个是GFS（Google File System），用于存储不同设备所产生的海量数据；另一个是MapReduce，它运行在GFS之上，负责分布式大规模数据。 ​ 2006年cutting根据《Google File System》设计了Nutch的HDFS,hadoop distributed file system。 Hadoop最开始是nutch的子项目，目的是解决nutch的海量数据存储问题。在nutch 0.8版本之后，Hadoop独立处理，成为一个独立的项目。后来，又根据《Google MapReduce》设计了基于HDFS的MapRedce计算框架。 2006年之后，cutting带着Hadoop去了雅虎，当时有100多人的团队共同帮cutting完善hadoop。后来yahoo把Hadoop贡献了Apache。所以，现在Hadoop是Apache的顶级项目。 2.1Hadoop搭建Hadoop的三种运行模式 ： 独立（本地）运行模式：无需任何守护进程，所有的程序都运行在同一个JVM上执行。在独立模式下调试MR程序非常高效方便。所以一般该模式主要是在学习或者开发阶段调试使用 。 伪分布式模式： Hadoop守护进程运行在本地机器上，模拟一个小规模的集群，换句话说，可以配置一台机器的Hadoop集群,伪分布式是完全分布式的一个特例。 完全分布式模式：Hadoop守护进程运行在一个集群上。 3.1 单机伪分布模式​ 只支持MapReduce，不支持HDFS。这种模式一般用于调试MapReduce任务用的。 3.1.1 准备Linux环境，最低的工作内存1G内容详见：Vmware安装Centos6.9文档 3.1.2 关闭防火墙临时关闭防火墙：service iptables stop 1service iptables stop 永久关闭防火墙：chkconfig iptables off 1chkconfig iptables off 注意：永久修改防火墙需要重启，永久和临时同时执行，可以不用重启。 3.1.3 配置主机名查询主机名称：hostname 1hostname 临时修改主机名：hostname 1hostname &lt;name&gt; 永久修改主机名：vim /etc/sysconfig/network 1vim /etc/sysconfig/network 注意： 1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。 2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。 注意： 1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。 2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。 3.1.4 配置hosts文件执行: vim /etc/hosts 1vim /etc/hosts 注意： 不要删除前两行内容。 IP在前，主机名在后。 3.1.5 配置免密码登录3.1.5.1 免密登陆原理 A机器生成公钥和私钥 机器A发送自己的公钥到机器B，这个时候机器B有了机器A的公钥 机器B发送一个随机的字符串向机器A 机器A利用自己的私钥把字符串加密 机器A把加密后的字符串再次发送给机器B 机器B利用公钥解密字符串，如果和原来的一样，则OK。 3.1.5.1 免密登陆实现 生成自己的公钥和私钥 ssh-keygen 1ssh-keygen 把生成的公钥copy到远程机器上 ssh-copy-id root@hadoop01 1ssh-copy-id root@hadoop01 注意：如果是单机的伪分布式环境，自己节点也需要配置免密登录。 3.1.6 安装和配置jdk 执行： 1vim /etc/profile 在尾行添加 12345#Set Java ENVJAVA_HOME=/home/jdk1.8.0_65PATH=$JAVA_HOME/bin:$PATHCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JAVA_HOME PATH CLASS_PATH 保存退出 :wq 命令行执行： 1source /etc/profile java -version 查看JDK版本信息。 1java -version 3.1.7 上传和安装hadoop下载地址：http://hadoop.apache.org/releases.html 注意： source表示源码 binary表示二级制包（安装包） 3.1.7.1 解压Hadoop文件包执行：tar -zxvf hadoop-2.7.1_64bit.tar.gz 1tar -zxvf hadoop-2.7.1_64bit.tar.gz 3.1.7.2 Hadoop目录说明bin目录：命令脚本 etc/hadoop:存放hadoop的配置文件 lib目录：hadoop运行的依赖jar包 sbin目录：启动和关闭hadoop等命令都在这里 libexec目录：存放的也是hadoop命令，但一般不常用 注意：最常用的就是bin和etc目录。 3.1.8 配置hadoop配置文件Hadoop目录下/home/hadoop-2.7.1/etc/hadoop/目录下6个文件 3.1.8.1 hadoop-env.sh执行：vim hadoop-env.sh 1vim hadoop-env.sh 修改：修改java_home路径和hadoop_conf_dir 路径 25行 33行 1234#25行export JAVA_HOME=/home/jdk1.8.0_65#33行export HADOOP_CONF_DIR=/home/hadoop-2.7.1/etc/hadoop 然后执行：source hadoop-env.sh编译文件。 1source hadoop-env.sh 3.1.8.2 core-site.xml命令行执行：vim core-site.xml 1vim core-site.xml 123456789101112&lt;configuration&gt;&lt;!--用来指定hdfs的老大，namenode的地址--&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://tedu:9000&lt;/value&gt;&lt;/property&gt;&lt;!--用来指定hadoop运行时产生文件的存放目录--&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/home/hadoop-2.7.1/tmp&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.3 hdfs-site .xml命令行执行：vim hdfs-site.xml 1vim hdfs-site.xml 12345678910111213&lt;configuration&gt;&lt;!--指定hdfs保存数据副本的数量，包括自己，默认值是3--&gt;&lt;!--如果是伪分布模式，此值是1--&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;!--设置hdfs的操作权限，false表示任何用户都可以在hdfs上操作文件--&gt;&lt;property&gt;&lt;name&gt;dfs.permissions&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.4 mapred-site.xml命令行执行： 123cp mapred-site.xml.template mapred-site.xmlvim mapred-site.xml 1234567&lt;configuration&gt;&lt;property&gt;&lt;!--指定mapreduce运行在yarn上--&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.5 yarn-site.xml命令行执行：vim yarn-site.xml 1vim yarn-site.xml 12345678910111213&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt;&lt;property&gt;&lt;!--指定yarn的老大 resoucemanager的地址--&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;tedu&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!--NodeManager获取数据的方式--&gt;&lt;name&gt;yarn.nodemanager.aux- services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.6 slaves命令行执行： 1vim slaves 修改主机名 3.1.9 配置hadoop的环境变量 文件最后追加文件 HADOOP_HOME=/home/hadoop-2.7.1 export HADOOP_HOME source /etc/profile 使更改的配置立即生效。 123456#Set Java ENVJAVA_HOME=/home/jdk1.8.0_65HADOOP_HOME=/home/hadoop-2.7.1PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JAVA_HOME PATH CLASSPATH HADOOP_HOME 3.1.10 格式化Namenode执行：hdfs namenode -format 1hdfs namenode -format 如果不好使，可以重启linux 当出现：successfully，证明格式化成功。 3.1.11 启动Hadoop在/home/hadoop-2.7.1/sbin目录下 执行:./start-all.sh 1./start-all.sh 3.1.12 验证启动成功可以访问网址： http://192.168.220.128:50070","categories":[{"name":"CDH","slug":"CDH","permalink":"http://zhangfuxin.cn/categories/CDH/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"http://zhangfuxin.cn/tags/CDH/"}],"keywords":[{"name":"CDH","slug":"CDH","permalink":"http://zhangfuxin.cn/categories/CDH/"}]},{"title":"Hadoop伪分布式搭建","slug":"hadoop-single","date":"2019-08-29T17:30:04.000Z","updated":"2019-08-29T17:28:47.951Z","comments":true,"path":"hadoop-single.html","link":"","permalink":"http://zhangfuxin.cn/hadoop-single.html","excerpt":"** Hadoop伪分布式搭建：** &lt;Excerpt in index | 首页摘要&gt; ​ 大数据（Big Data），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。​ 大数据的定义是4Vs：数据量大、处理速度快、数据源多样、真实性。用中文简单描述就是大、快、多、真。","text":"** Hadoop伪分布式搭建：** &lt;Excerpt in index | 首页摘要&gt; ​ 大数据（Big Data），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。​ 大数据的定义是4Vs：数据量大、处理速度快、数据源多样、真实性。用中文简单描述就是大、快、多、真。 &lt;The rest of contents | 余下全文&gt; 1.1Hadoop简介1.1.1Hadoop创始人​ 1985年，Doug Cutting毕业于美国斯坦福大学。他并不是一开始就决心投身IT行业的，在大学时代的头两年，Cutting学习了诸如物理、地理等常规课程。因为学费的压力，Cutting开始意识到，自己必须学习一些更加实用、有趣的技能。这样，一方面可以帮助自己还清贷款，另一方面，也是为自己未来的生活做打算。因为斯坦福大学座落在IT行业的“圣地”硅谷，所以学习软件对年轻人来说是再自然不过的事情了。 1997年底，Cutting开始以每周两天的时间投入，在家里试着用Java把这个想法变成现实，不久之后，Lucene诞生了。作为第一个提供全文文本搜索的开源函数库，Lucene的伟大自不必多言。 Doug Cutting是Lucence,Nutch,Hadoop的创始人。 1.1.2Hadoop起源​ 2004年，Cutting和同为程序员出身的Mike Cafarella决定开发一款可以代替当时的主流搜索产品的开源搜索引擎，这个项目被命名为Nutch。Nutch基于Lucence实现的搜索引擎，能够从互联网上抓取网页数据。抓取来的海量数据的存储问题。但是，这些海量数据都是非结构化数据，不能存在关系型数据库里。如果连数据的存储和管理都解决不了化，就谈不上后续为用户提供搜索服务，包括通过算法去优化检索速度。那时的Cutting正处在Freelancer的生涯 中，所以他希望自己的项目能通过一种低开销的方式来构建网页中的大量算法。幸运的是，Google这时正好发布了一项研究报告，报告中介绍了两款Google为支持自家的搜索引擎而开发的软件平台。这两个平台一个是GFS（Google File System），用于存储不同设备所产生的海量数据；另一个是MapReduce，它运行在GFS之上，负责分布式大规模数据。 ​ 2006年cutting根据《Google File System》设计了Nutch的HDFS,hadoop distributed file system。 Hadoop最开始是nutch的子项目，目的是解决nutch的海量数据存储问题。在nutch 0.8版本之后，Hadoop独立处理，成为一个独立的项目。后来，又根据《Google MapReduce》设计了基于HDFS的MapRedce计算框架。 2006年之后，cutting带着Hadoop去了雅虎，当时有100多人的团队共同帮cutting完善hadoop。后来yahoo把Hadoop贡献了Apache。所以，现在Hadoop是Apache的顶级项目。 2.1Hadoop搭建Hadoop的三种运行模式 ： 独立（本地）运行模式：无需任何守护进程，所有的程序都运行在同一个JVM上执行。在独立模式下调试MR程序非常高效方便。所以一般该模式主要是在学习或者开发阶段调试使用 。 伪分布式模式： Hadoop守护进程运行在本地机器上，模拟一个小规模的集群，换句话说，可以配置一台机器的Hadoop集群,伪分布式是完全分布式的一个特例。 完全分布式模式：Hadoop守护进程运行在一个集群上。 3.1 单机伪分布模式​ 只支持MapReduce，不支持HDFS。这种模式一般用于调试MapReduce任务用的。 3.1.1 准备Linux环境，最低的工作内存1G内容详见：Vmware安装Centos6.9文档 3.1.2 关闭防火墙临时关闭防火墙：service iptables stop 1service iptables stop 永久关闭防火墙：chkconfig iptables off 1chkconfig iptables off 注意：永久修改防火墙需要重启，永久和临时同时执行，可以不用重启。 3.1.3 配置主机名查询主机名称：hostname 1hostname 临时修改主机名：hostname 1hostname &lt;name&gt; 永久修改主机名：vim /etc/sysconfig/network 1vim /etc/sysconfig/network 注意： 1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。 2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。 注意： 1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。 2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。 3.1.4 配置hosts文件执行: vim /etc/hosts 1vim /etc/hosts 注意： 不要删除前两行内容。 IP在前，主机名在后。 3.1.5 配置免密码登录3.1.5.1 免密登陆原理 A机器生成公钥和私钥 机器A发送自己的公钥到机器B，这个时候机器B有了机器A的公钥 机器B发送一个随机的字符串向机器A 机器A利用自己的私钥把字符串加密 机器A把加密后的字符串再次发送给机器B 机器B利用公钥解密字符串，如果和原来的一样，则OK。 3.1.5.1 免密登陆实现 生成自己的公钥和私钥 ssh-keygen 1ssh-keygen 把生成的公钥copy到远程机器上 ssh-copy-id root@hadoop01 1ssh-copy-id root@hadoop01 注意：如果是单机的伪分布式环境，自己节点也需要配置免密登录。 3.1.6 安装和配置jdk 执行： 1vim /etc/profile 在尾行添加 12345#Set Java ENVJAVA_HOME=/home/jdk1.8.0_65PATH=$JAVA_HOME/bin:$PATHCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JAVA_HOME PATH CLASS_PATH 保存退出 :wq 命令行执行： 1source /etc/profile java -version 查看JDK版本信息。 1java -version 3.1.7 上传和安装hadoop下载地址：http://hadoop.apache.org/releases.html 注意： source表示源码 binary表示二级制包（安装包） 3.1.7.1 解压Hadoop文件包执行：tar -zxvf hadoop-2.7.1_64bit.tar.gz 1tar -zxvf hadoop-2.7.1_64bit.tar.gz 3.1.7.2 Hadoop目录说明bin目录：命令脚本 etc/hadoop:存放hadoop的配置文件 lib目录：hadoop运行的依赖jar包 sbin目录：启动和关闭hadoop等命令都在这里 libexec目录：存放的也是hadoop命令，但一般不常用 注意：最常用的就是bin和etc目录。 3.1.8 配置hadoop配置文件Hadoop目录下/home/hadoop-2.7.1/etc/hadoop/目录下6个文件 3.1.8.1 hadoop-env.sh执行：vim hadoop-env.sh 1vim hadoop-env.sh 修改：修改java_home路径和hadoop_conf_dir 路径 25行 33行 1234#25行export JAVA_HOME=/home/jdk1.8.0_65#33行export HADOOP_CONF_DIR=/home/hadoop-2.7.1/etc/hadoop 然后执行：source hadoop-env.sh编译文件。 1source hadoop-env.sh 3.1.8.2 core-site.xml命令行执行：vim core-site.xml 1vim core-site.xml 123456789101112&lt;configuration&gt;&lt;!--用来指定hdfs的老大，namenode的地址--&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://tedu:9000&lt;/value&gt;&lt;/property&gt;&lt;!--用来指定hadoop运行时产生文件的存放目录--&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/home/hadoop-2.7.1/tmp&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.3 hdfs-site .xml命令行执行：vim hdfs-site.xml 1vim hdfs-site.xml 12345678910111213&lt;configuration&gt;&lt;!--指定hdfs保存数据副本的数量，包括自己，默认值是3--&gt;&lt;!--如果是伪分布模式，此值是1--&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;!--设置hdfs的操作权限，false表示任何用户都可以在hdfs上操作文件--&gt;&lt;property&gt;&lt;name&gt;dfs.permissions&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.4 mapred-site.xml命令行执行： 123cp mapred-site.xml.template mapred-site.xmlvim mapred-site.xml 1234567&lt;configuration&gt;&lt;property&gt;&lt;!--指定mapreduce运行在yarn上--&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.5 yarn-site.xml命令行执行：vim yarn-site.xml 1vim yarn-site.xml 12345678910111213&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt;&lt;property&gt;&lt;!--指定yarn的老大 resoucemanager的地址--&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;tedu&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!--NodeManager获取数据的方式--&gt;&lt;name&gt;yarn.nodemanager.aux- services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 3.1.8.6 slaves命令行执行： 1vim slaves 修改主机名 3.1.9 配置hadoop的环境变量 文件最后追加文件 HADOOP_HOME=/home/hadoop-2.7.1 export HADOOP_HOME source /etc/profile 使更改的配置立即生效。 123456#Set Java ENVJAVA_HOME=/home/jdk1.8.0_65HADOOP_HOME=/home/hadoop-2.7.1PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JAVA_HOME PATH CLASSPATH HADOOP_HOME 3.1.10 格式化Namenode执行：hdfs namenode -format 1hdfs namenode -format 如果不好使，可以重启linux 当出现：successfully，证明格式化成功。 3.1.11 启动Hadoop在/home/hadoop-2.7.1/sbin目录下 执行:./start-all.sh 1./start-all.sh 3.1.12 验证启动成功可以访问网址： http://192.168.220.128:50070","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"如何选购合适的电脑","slug":"buy-computer","date":"2019-08-28T16:30:04.000Z","updated":"2019-08-28T17:10:49.787Z","comments":true,"path":"buy-computer.html","link":"","permalink":"http://zhangfuxin.cn/buy-computer.html","excerpt":"** 购买合适的电脑：** &lt;Excerpt in index | 首页摘要&gt;随着时代的发展，人们生活水平的提高，计算机也成为了我们工作必不可少的生产力，办公，打游戏，看电影，购物等等。市面上电脑种类根据用途又可以分为很多类，台式机、笔记本、IPAD（平板）。购买一台适合自己的电脑工作时可以如虎添翼，电脑用着舒服，心情也会舒畅许多。","text":"** 购买合适的电脑：** &lt;Excerpt in index | 首页摘要&gt;随着时代的发展，人们生活水平的提高，计算机也成为了我们工作必不可少的生产力，办公，打游戏，看电影，购物等等。市面上电脑种类根据用途又可以分为很多类，台式机、笔记本、IPAD（平板）。购买一台适合自己的电脑工作时可以如虎添翼，电脑用着舒服，心情也会舒畅许多。 &lt;The rest of contents | 余下全文&gt; 买电脑主要需求 看电影，上网（购物） 打游戏 办公（移动办公） 平面设计（CAD） UI(影视剪辑) 编程 其他 电脑配置说明目前电脑配置的CPU（绝对过剩），内存Win10最低要8个G，显卡要根据自己需求一般显卡基本够用，电脑最大的瓶颈都是在硬盘上，所以现在买电脑带不带固态硬盘是我首选的配置（我对固态硬盘定义最低要128G,512G固态才是标配，毕竟固态大小会影响到一定的读写速率，还有为了保证固态寿命做系统时会留出10%的空间不划分到分区中），至于买笔记本还是台式机需要根据不同应用场景来定。台式机性能肯定远超同价位笔记本，这个是毋庸置疑的。 看电影，上网（购物）对于这方面需求的一般一女生居多，看电影上网，对电脑配置要求比较低的，一般普通双核CPU，AMD、酷睿i3都可以（最好是i5），内存8G就够了（win7的话4G就够，但是Win7现在不支持更新了）。要是女生最重要的是漂亮，这里推荐DELL或者HP相对性价比会比较合适。毕竟要是要以轻薄、美观为主。要是资金充足可以考虑各家品牌的超级本。要是父母的需求的话其实买笔记本或者台式机都可以。这里不推荐苹果笔记本，因为用苹果看电影会容易热，要是妹子是苹果控或者周边产品都是苹果产品，苹果笔记本也可在考虑之列。 打游戏游戏主机两个最主要的要求配置和扩展性，主要是CPU和显卡，我们又称之为“双烧”，建议买台式机。要是需要便携的话，外星人品牌是一个不错的考虑，笔记本显卡最好不要超过GTX2070以上，也许你会问为什么不买笔记本GTX2080的本子，一方面是贵，价格会差很多。还有就是散热问题。为了更好体验还是台式机加水冷。 一般的主流网游：i5或i7处理器，内存16G，中端显卡就可以了，硬盘128G固态+1T机械起 大型单机：i7或i9处理器（水冷），内存16-32G，，显卡中高端GTX1060起，要是玩刺客信条奥德赛GTX2080Ti不用犹豫，硬盘512三星固态+1T机械（最好在配置1T的固态，毕竟游戏不小）起 发烧友：i9处理器（水冷），内存32G-64G，显卡高端GTX2080或者是多显交火，硬盘512G（三星固态PRO系列）+1T固态 办公用于办公的大多是商务人士，对笔记本的性能要求一般，最主要的是便携性，各大品牌的超极本都很合适，还能衬托气质，最推荐的还是联想的thinkpad系列，没钱买个E系类（基本三年就会坏），要是有资金充裕T系列或者X系列是首选配置（尤其是X系列）。 平面设计（CAD）这个是专业领域的需求，对CPU、显卡和内存、显示器都较高，能好一点就好一点。 UI(影视剪辑)苹果的Macbookpro 16G，512SSD（固态太小用久了会后悔的），i7处理器 最为合适。没有比苹果更适合做平面设计的电脑。Windows系统和苹果系统没得比。 编程苹果的Macbookpro 16G、512SSD、i7处理器。个人推荐MAC的笔记本做编程，一用就停不下来，会上瘾。Windows系统用来打游戏就好了。推荐配置：Macbookpro 16G、i7处理器（i9也是阉割版没意义）、512SSD（固态真的不能太小，512G就不大，考虑到价格没办法）、最好是能带键盘灯、Air pods耳机还是要有一个的，用了就知道不亏。经济允许最好是配置一个IPAD PRO做分屏开发可以调高效率。 其他IPAD我对它的定义就是一台游戏机，不建议用IPAD看电影（用久了手会麻）。因为我不做UI我也没有体会到那只笔的好处。 还有一个设备一点光要说一下就是亚马逊的Kindle，要是你经常看小说，或者是看英文，建议有一个（前期是你不是必须要纸质书）还是很方便的，尤其是书多了的时候。IPAD优势在于pdf文档做笔记。用了就会知道两个不一样。Kindle看电子书是生活品质提升的表现。","categories":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/categories/others/"}],"tags":[{"name":"数码产品","slug":"数码产品","permalink":"http://zhangfuxin.cn/tags/数码产品/"}],"keywords":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/categories/others/"}]},{"title":"Spark学习之路 （十九）SparkSQL的自定义函数UDF","slug":"2019-06-19-Spark学习之路 （十九）SparkSQL的自定义函数UDF","date":"2019-06-19T02:30:04.000Z","updated":"2019-09-17T04:15:17.246Z","comments":true,"path":"2019-06-19-Spark学习之路 （十九）SparkSQL的自定义函数UDF.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-19-Spark学习之路 （十九）SparkSQL的自定义函数UDF.html","excerpt":"** Spark学习之路 （十九）SparkSQL的自定义函数UDF：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十九）SparkSQL的自定义函数UDF","text":"** Spark学习之路 （十九）SparkSQL的自定义函数UDF：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十九）SparkSQL的自定义函数UDF &lt;The rest of contents | 余下全文&gt; 在Spark中，也支持Hive中的自定义函数。自定义函数大致可以分为三种： UDF(User-Defined-Function)，即最基本的自定义函数，类似to_char,to_date等 UDAF（User- Defined Aggregation Funcation），用户自定义聚合函数，类似在group by之后使用的sum,avg等 UDTF(User-Defined Table-Generating Functions),用户自定义生成函数，有点像stream里面的flatMap 自定义一个UDF函数需要继承UserDefinedAggregateFunction类，并实现其中的8个方法 示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import org.apache.spark.sql.Rowimport org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;import org.apache.spark.sql.types.&#123;DataType, StringType, StructField, StructType&#125;object GetDistinctCityUDF extends UserDefinedAggregateFunction&#123; /** * 输入的数据类型 * */ override def inputSchema: StructType = StructType( StructField(\"status\",StringType,true) :: Nil ) /** * 缓存字段类型 * */ override def bufferSchema: StructType = &#123; StructType( Array( StructField(\"buffer_city_info\",StringType,true) ) ) &#125;/** * 输出结果类型 * */ override def dataType: DataType = StringType/** * 输入类型和输出类型是否一致 * */ override def deterministic: Boolean = true/** * 对辅助字段进行初始化 * */ override def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer.update(0,\"\") &#125;/** *修改辅助字段的值 * */ override def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; //获取最后一次的值 var last_str = buffer.getString(0) //获取当前的值 val current_str = input.getString(0) //判断最后一次的值是否包含当前的值 if(!last_str.contains(current_str))&#123; //判断是否是第一个值，是的话走if赋值，不是的话走else追加 if(last_str.equals(\"\"))&#123; last_str = current_str &#125;else&#123; last_str += \",\" + current_str &#125; &#125; buffer.update(0,last_str) &#125;/** *对分区结果进行合并 * buffer1是机器hadoop1上的结果 * buffer2是机器Hadoop2上的结果 * */ override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; var buf1 = buffer1.getString(0) val buf2 = buffer2.getString(0) //将buf2里面存在的数据而buf1里面没有的数据追加到buf1 //buf2的数据按照，进行切分 for(s &lt;- buf2.split(\",\"))&#123; if(!buf1.contains(s))&#123; if(buf1.equals(\"\"))&#123; buf1 = s &#125;else&#123; buf1 += s &#125; &#125; &#125; buffer1.update(0,buf1) &#125;/** * 最终的计算结果 * */ override def evaluate(buffer: Row): Any = &#123; buffer.getString(0) &#125;&#125; 注册自定义的UDF函数为临时函数 123456789101112131415161718def main(args: Array[String]): Unit = &#123; /** * 第一步 创建程序入口 */ val conf = new SparkConf().setAppName(\"AralHotProductSpark\") val sc = new SparkContext(conf) val hiveContext = new HiveContext(sc) //注册成为临时函数 hiveContext.udf.register(\"get_distinct_city\",GetDistinctCityUDF) //注册成为临时函数 hiveContext.udf.register(\"get_product_status\",(str:String) =&gt;&#123; var status = 0 for(s &lt;- str.split(\",\"))&#123; if(s.contains(\"product_status\"))&#123; status = s.split(\":\")(1).toInt &#125; &#125; &#125;)&#125;","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"RDD、DataFrame和DataSet的区别是什么","slug":"2019-06-18-DataSet和DataFrame区别和转换","date":"2019-06-18T03:30:04.000Z","updated":"2019-09-17T04:14:51.186Z","comments":true,"path":"2019-06-18-DataSet和DataFrame区别和转换.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-18-DataSet和DataFrame区别和转换.html","excerpt":"** RDD、DataFrame和DataSet的区别是什么：** &lt;Excerpt in index | 首页摘要&gt; ​ RDD、DataFrame和DataSet是容易产生混淆的概念，必须对其相互之间对比，才可以知道其中异同：DataFrame多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合。DataFrame是分布式的Row对象的集合。","text":"** RDD、DataFrame和DataSet的区别是什么：** &lt;Excerpt in index | 首页摘要&gt; ​ RDD、DataFrame和DataSet是容易产生混淆的概念，必须对其相互之间对比，才可以知道其中异同：DataFrame多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合。DataFrame是分布式的Row对象的集合。 &lt;The rest of contents | 余下全文&gt; RDD、DataFrame和DataSet是容易产生混淆的概念，必须对其相互之间对比，才可以知道其中异同。 RDD和DataFrame RDD-DataFrame 上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解 Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。 提升执行效率RDD API是函数式的，强调不变性，在大部分场景下倾向于创建新对象而不是修改老对象。这一特点虽然带来了干净整洁的API，却也使得Spark应用程序在运行期倾向于创建大量临时对象，对GC造成压力。在现有RDD API的基础之上，我们固然可以利用mapPartitions方法来重载RDD单个分片内的数据创建方式，用复用可变对象的方式来减小对象分配和GC的开销，但这牺牲了代码的可读性，而且要求开发者对Spark运行时机制有一定的了解，门槛较高。另一方面，Spark SQL在框架内部已经在各种可能的情况下尽量重用对象，这样做虽然在内部会打破了不变性，但在将数据返回给用户时，还会重新转为不可变数据。利用 DataFrame API进行开发，可以免费地享受到这些优化效果。 减少数据读取分析大数据，最快的方法就是 ——忽略它。这里的“忽略”并不是熟视无睹，而是根据查询条件进行恰当的剪枝。 上文讨论分区表时提到的分区剪 枝便是其中一种——当查询的过滤条件中涉及到分区列时，我们可以根据查询条件剪掉肯定不包含目标数据的分区目录，从而减少IO。 对于一些“智能”数据格 式，Spark SQL还可以根据数据文件中附带的统计信息来进行剪枝。简单来说，在这类数据格式中，数据是分段保存的，每段数据都带有最大值、最小值、null值数量等 一些基本的统计信息。当统计信息表名某一数据段肯定不包括符合查询条件的目标数据时，该数据段就可以直接跳过(例如某整数列a某段的最大值为100，而查询条件要求a &gt; 200)。 此外，Spark SQL也可以充分利用RCFile、ORC、Parquet等列式存储格式的优势，仅扫描查询真正涉及的列，忽略其余列的数据。 执行优化 人口数据分析示例 为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter 下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。 得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。 对于普通开发者而言，查询优化 器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。 RDD和DataSetDataSet以Catalyst逻辑执行计划表示，并且数据以编码的二进制形式被存储，不需要反序列化就可以执行sorting、shuffle等操作。 DataSet创立需要一个显式的Encoder，把对象序列化为二进制，可以把对象的scheme映射为SparkSQl类型，然而RDD依赖于运行时反射机制。 通过上面两点，DataSet的性能比RDD的要好很多。 DataFrame和DataSetDataSet跟DataFrame还是有挺大区别的，DataFrame开发都是写sql，但是DataSet是使用类似RDD的API。主要区别是Dataset每一个record存储的是一个强类型值而不是一个Row。 (1)相同点：都是分布式数据集 DataFrame底层是RDD，但是DataSet不是，不过他们最后都是转换成RDD运行 DataSet和DataFrame的相同点都是有数据特征、数据类型的分布式数据集(schema) (2)不同点：(a)schema信息： RDD中的数据是没有数据类型的 DataFrame中的数据是弱数据类型，不会做数据类型检查 虽然有schema规定了数据类型，但是编译时是不会报错的，运行时才会报错 DataSet中的数据类型是强数据类型 (b)序列化机制： RDD和DataFrame默认的序列化机制是java的序列化，可以修改为Kyro的机制 DataSet使用自定义的数据编码器进行序列化和反序列化 创建方式：(1)要使用toDS之前1import sqlContext.implicits._ (2)将内存中的数据转换成DataSet123456// Encoders for most common types are automatically provided by importingsqlContext.implicits._val ds = Seq(1, 2, 3).toDS()ds.map(_ + 1).collect() // Returns: Array(2, 3, 4) 其中： collect()：返回一个Array，包含所有行信息 Returns an array that contains all rows in this Dataset. (3)可以直接把case class对象转化成DataSet1234// Encoders are also created for case classes.case class Person(name: String, age: Long)val ds = Seq(Person(\"Andy\", 32)).toDS() (4)将DataFrame转换成DataSet，不过要求是DataFrame的数据类型必须是case class并且要求DataFrame的数据类型必须和case class一致(顺序也必须一致) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package _0729DFimport org.apache.sparkimport org.apache.spark.sql.&#123;Dataset, SparkSession&#125;//import org.apache.sparkobject Dataset extends App&#123;// import spark.implicits._// val ds = Seq(1, 2, 3).toDS()// ds.map(_ + 1).collect() // Returns: Array(2, 3, 4)// // Encoders are also created for case classes.// case class Person(name: String, age: Long)// val ds = Seq(Person(\"Andy\", 32)).toDS()// ds.showval session = SparkSession.builder().appName(\"app\").master(\"local\").getOrCreate()val sqlContext = session.sqlContextval wcDs = sqlContext.read.textFile(\"datas/halibote.txt\")// 导入隐式转换import session.implicits._val wordData=wcDs.flatMap(_.split(\" \"))wordData.createTempView(\"t_word\")wordData.show() //wordData.printSchema()// Encoders for most common types are automatically provided by importing sqlContext.implicits._val ds=Seq(1,2,3).toDS()ds.map(_ + 1).collect() // Returns: Array(2, 3, 4)// // Encoders are also created for case classes.// case class Person(name: String, age: Long)// val ds = Seq(Person(\"Andy\", 32)).toDS()// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name.case class Person(age:Long,name:String)val path = \"datas/people.json\"val people: Dataset[Person] = sqlContext.read.json(path).as[Person]people.show()&#125; 用wordcount举例： 12345678910// DataFrame// Load a text file and interpret each line as a java.lang.Stringval ds = sqlContext.read.text(\"/home/spark/1.6/lines\").as[String]val result = ds .flatMap(_.split(\" \")) // Split on whitespace .filter(_ != \"\") // Filter empty words .toDF() // Convert to DataFrame to perform aggregation / sorting .groupBy($\"value\") // Count number of occurences of each word .agg(count(\"*\") as \"numOccurances\") .orderBy($\"numOccurances\" desc) // Show most common words first 后面版本DataFrame会继承DataSet，DataFrame是面向Spark SQL的接口。 123456// DataSet,完全使用scala编程，不要切换到DataFrameval wordCount = ds.flatMap(.split(\" \")) .filter( != \"\") .groupBy(_.toLowerCase()) // Instead of grouping on a column expression (i.e. $\"value\") we pass a lambda function .count() DataFrame和DataSet可以相互转化， df.as[ElementType] 这样可以把DataFrame转化为DataSet， ds.toDF() 这样可以把DataSet转化为DataFrame。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十八）SparkSQL简单使用","slug":"2019-06-18-Spark学习之路 （十八）SparkSQL简单使用","date":"2019-06-18T02:30:04.000Z","updated":"2019-09-17T03:39:50.782Z","comments":true,"path":"2019-06-18-Spark学习之路 （十八）SparkSQL简单使用.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-18-Spark学习之路 （十八）SparkSQL简单使用.html","excerpt":"** Spark学习之路 （十八）SparkSQL简单使用：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十八）SparkSQL简单使用","text":"** Spark学习之路 （十八）SparkSQL简单使用：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十八）SparkSQL简单使用 &lt;The rest of contents | 余下全文&gt; 一、SparkSQL的进化之路1.0以前： Shark 1.1.x： SparkSQL(只是测试性的) SQL 1.3.x: SparkSQL(正式版本)+Dataframe 1.5.x: SparkSQL 钨丝计划 1.6.x： SparkSQL+DataFrame+DataSet(测试版本) 2.x : SparkSQL+DataFrame+DataSet(正式版本) ​ SparkSQL:还有其他的优化 ​ StructuredStreaming(DataSet) 二、认识SparkSQL2.1 什么是SparkSQL?spark SQL是spark的一个模块，主要用于进行结构化数据的处理。它提供的最核心的编程抽象就是DataFrame。 2.2 SparkSQL的作用提供一个编程抽象（DataFrame） 并且作为分布式 SQL 查询引擎 DataFrame：它可以根据很多源进行构建，包括：结构化的数据文件，hive中的表，外部的关系型数据库，以及RDD 2.3 运行原理将 Spark SQL 转化为 RDD， 然后提交到集群执行 2.4 特点（1）容易整合 （2）统一的数据访问方式 （3）兼容 Hive （4）标准的数据连接 2.5 SparkSessionSparkSession是Spark 2.0引如的新概念。SparkSession为用户提供了统一的切入点，来让用户学习spark的各项功能。 在spark的早期版本中，SparkContext是spark的主要切入点，由于RDD是主要的API，我们通过sparkcontext来创建和操作RDD。对于每个其他的API，我们需要使用不同的context。例如，对于Streming，我们需要使用StreamingContext；对于sql，使用sqlContext；对于Hive，使用hiveContext。但是随着DataSet和DataFrame的API逐渐成为标准的API，就需要为他们建立接入点。所以在spark2.0中，引入SparkSession作为DataSet和DataFrame API的切入点，SparkSession封装了SparkConf、SparkContext和SQLContext。为了向后兼容，SQLContext和HiveContext也被保存下来。 SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。 特点： —- 为用户提供一个统一的切入点使用Spark 各项功能 ​ —- 允许用户通过它调用 DataFrame 和 Dataset 相关 API 来编写程序 ​ —- 减少了用户需要了解的一些概念，可以很容易的与 Spark 进行交互 ​ —- 与 Spark 交互之时不需要显示的创建 SparkConf, SparkContext 以及 SQlContext，这些对象已经封闭在 SparkSession 中 2.7 DataFrames在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。 三、RDD转换成为DataFrame使用spark1.x版本的方式 测试数据目录：/home/hadoop/apps/spark/examples/src/main/resources（spark的安装目录里面） people.txt 3.1 方式一：通过 case class 创建 DataFrames（反射）12345678910111213141516171819//定义case class，相当于表结构case class People(var name:String,var age:Int)object TestDataFrame1 &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"RDDToDataFrame\").setMaster(\"local\") val sc = new SparkContext(conf) val context = new SQLContext(sc) // 将本地的数据读入 RDD， 并将 RDD 与 case class 关联 val peopleRDD = sc.textFile(\"E:\\\\666\\\\people.txt\") .map(line =&gt; People(line.split(\",\")(0), line.split(\",\")(1).trim.toInt)) import context.implicits._ // 将RDD 转换成 DataFrames val df = peopleRDD.toDF //将DataFrames创建成一个临时的视图 df.createOrReplaceTempView(\"people\") //使用SQL语句进行查询 context.sql(\"select * from people\").show() &#125;&#125; 运行结果 3.2 方式二：通过 structType 创建 DataFrames（编程接口）1234567891011121314151617181920212223242526object TestDataFrame2 &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"TestDataFrame2\").setMaster(\"local\") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val fileRDD = sc.textFile(\"E:\\\\666\\\\people.txt\") // 将 RDD 数据映射成 Row，需要 import org.apache.spark.sql.Row val rowRDD: RDD[Row] = fileRDD.map(line =&gt; &#123; val fields = line.split(\",\") Row(fields(0), fields(1).trim.toInt) &#125;) // 创建 StructType 来定义结构 val structType: StructType = StructType( //字段名，字段类型，是否可以为空 StructField(\"name\", StringType, true) :: StructField(\"age\", IntegerType, true) :: Nil ) /** * rows: java.util.List[Row], * schema: StructType * */ val df: DataFrame = sqlContext.createDataFrame(rowRDD,structType) df.createOrReplaceTempView(\"people\") sqlContext.sql(\"select * from people\").show() &#125;&#125; 运行结果 3.3 方式三：通过 json 文件创建 DataFrames12345678910object TestDataFrame3 &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"TestDataFrame2\").setMaster(\"local\") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val df: DataFrame = sqlContext.read.json(\"E:\\\\666\\\\people.json\") df.createOrReplaceTempView(\"people\") sqlContext.sql(\"select * from people\").show() &#125;&#125; 四、DataFrame的read和save和savemode4.1 数据的读取123456789101112131415object TestRead &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"TestDataFrame2\").setMaster(\"local\") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) //方式一 val df1 = sqlContext.read.json(\"E:\\\\666\\\\people.json\") val df2 = sqlContext.read.parquet(\"E:\\\\666\\\\users.parquet\") //方式二 val df3 = sqlContext.read.format(\"json\").load(\"E:\\\\666\\\\people.json\") val df4 = sqlContext.read.format(\"parquet\").load(\"E:\\\\666\\\\users.parquet\") //方式三，默认是parquet格式 val df5 = sqlContext.load(\"E:\\\\666\\\\users.parquet\") &#125;&#125; 4.2 数据的保存1234567891011121314151617object TestSave &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"TestDataFrame2\").setMaster(\"local\") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val df1 = sqlContext.read.json(\"E:\\\\666\\\\people.json\") //方式一 df1.write.json(\"E:\\\\111\") df1.write.parquet(\"E:\\\\222\") //方式二 df1.write.format(\"json\").save(\"E:\\\\333\") df1.write.format(\"parquet\").save(\"E:\\\\444\") //方式三 df1.write.save(\"E:\\\\555\") &#125;&#125; 4.3 数据的保存模式使用mode 1df1.write.format(&quot;parquet&quot;).mode(SaveMode.Ignore).save(&quot;E:\\\\444&quot;) 五、数据源5.1 数据源只json参考4.1 5.2 数据源之parquet参考4.1 5.3 数据源之Mysql123456789101112131415161718object TestMysql &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"TestMysql\").setMaster(\"local\") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val url = \"jdbc:mysql://192.168.123.102:3306/hivedb\" val table = \"dbs\" val properties = new Properties() properties.setProperty(\"user\",\"root\") properties.setProperty(\"password\",\"root\") //需要传入Mysql的URL、表明、properties（连接数据库的用户名密码） val df = sqlContext.read.jdbc(url,table,properties) df.createOrReplaceTempView(\"dbs\") sqlContext.sql(\"select * from dbs\").show() &#125;&#125; 运行结果 5.4 数据源之Hive（1）准备工作在pom.xml文件中添加依赖 123456&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; 开发环境则把resource文件夹下添加hive-site.xml文件，集群环境把hive的配置文件要发到$SPARK_HOME/conf目录下 12345678910111213141516171819202122232425262728&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hivedb?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;!-- 如果 mysql 和 hive 在同一个服务器节点，那么请更改 hadoop02 为 localhost --&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/hive/warehouse&lt;/value&gt; &lt;description&gt;hive default warehouse, if nessecory, change it&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; （2）测试代码12345678object TestHive &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setMaster(\"local\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(conf) val sqlContext = new HiveContext(sc) sqlContext.sql(\"select * from myhive.student\").show() &#125;&#125; 运行结果","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十七）Spark分区","slug":"2019-06-17-Spark学习之路 （十七）Spark分区","date":"2019-06-17T02:30:04.000Z","updated":"2019-09-17T03:30:58.175Z","comments":true,"path":"2019-06-17-Spark学习之路 （十七）Spark分区.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-17-Spark学习之路 （十七）Spark分区.html","excerpt":"** Spark学习之路 （十七）Spark分区：** &lt;Excerpt in index | 首页摘要&gt; 分区是RDD内部并行计算的一个计算单元，RDD的数据集在逻辑上被划分为多个分片，每一个分片称为分区，分区的格式决定了并行计算的粒度，而每个分区的数值计算都是在一个任务中进行的，因此任务的个数，也是由RDD(准确来说是作业最后一个RDD)的分区数决定。","text":"** Spark学习之路 （十七）Spark分区：** &lt;Excerpt in index | 首页摘要&gt; 分区是RDD内部并行计算的一个计算单元，RDD的数据集在逻辑上被划分为多个分片，每一个分片称为分区，分区的格式决定了并行计算的粒度，而每个分区的数值计算都是在一个任务中进行的，因此任务的个数，也是由RDD(准确来说是作业最后一个RDD)的分区数决定。 &lt;The rest of contents | 余下全文&gt; 一、为什么要进行分区 数据分区，在分布式集群里，网络通信的代价很大，减少网络传输可以极大提升性能。mapreduce框架的性能开支主要在io和网络传输，io因为要大量读写文件，它是不可避免的，但是网络传输是可以避免的，把大文件压缩变小文件， 从而减少网络传输，但是增加了cpu的计算负载。 Spark里面io也是不可避免的，但是网络传输spark里面进行了优化： Spark把rdd进行分区（分片），放在集群上并行计算。同一个rdd分片100个，10个节点，平均一个节点10个分区，当进行sum型的计算的时候，先进行每个分区的sum，然后把sum值shuffle传输到主程序进行全局sum，所以进行sum型计算对网络传输非常小。但对于进行join型的计算的时候，需要把数据本身进行shuffle，网络开销很大。 spark是如何优化这个问题的呢？ Spark把key－value rdd通过key的hashcode进行分区，而且保证相同的key存储在同一个节点上，这样对改rdd进行key聚合时，就不需要shuffle过程，我们进行mapreduce计算的时候为什么要进行shuffle？，就是说mapreduce里面网络传输主要在shuffle阶段，shuffle的根本原因是相同的key存在不同的节点上，按key进行聚合的时候不得不进行shuffle。shuffle是非常影响网络的，它要把所有的数据混在一起走网络，然后它才能把相同的key走到一起。要进行shuffle是存储决定的。 Spark从这个教训中得到启发，spark会把key进行分区，也就是key的hashcode进行分区，相同的key，hashcode肯定是一样的，所以它进行分区的时候100t的数据分成10分，每部分10个t，它能确保相同的key肯定在一个分区里面，而且它能保证存储的时候相同的key能够存在同一个节点上。比如一个rdd分成了100份，集群有10个节点，所以每个节点存10份，每一分称为每个分区，spark能保证相同的key存在同一个节点上，实际上相同的key存在同一个分区。 key的分布不均决定了有的分区大有的分区小。没法分区保证完全相等，但它会保证在一个接近的范围。所以mapreduce里面做的某些工作里边，spark就不需要shuffle了，spark解决网络传输这块的根本原理就是这个。 进行join的时候是两个表，不可能把两个表都分区好，通常情况下是把用的频繁的大表事先进行分区，小表进行关联它的时候小表进行shuffle过程。 大表不需要shuffle。 需要在工作节点间进行数据混洗的转换极大地受益于分区。这样的转换是 cogroup，groupWith，join，leftOuterJoin，rightOuterJoin，groupByKey，reduceByKey，combineByKey 和lookup。 分区是可配置的，只要RDD是基于键值对的即可。 二、Spark分区原则及方法RDD分区的一个分区原则：尽可能是得分区的个数等于集群核心数目 无论是本地模式、Standalone模式、YARN模式或Mesos模式，我们都可以通过spark.default.parallelism来配置其默认分区个数，若没有设置该值，则根据不同的集群环境确定该值 2.1 本地模式（1）默认方式以下这种默认方式就一个分区 结果 （2）手动设置设置了几个分区就是几个分区 结果 （3）跟local[n] 有关n等于几默认就是几个分区 如果n=* 那么分区个数就等于cpu core的个数 结果 本机电脑查看cpu core，我的电脑–》右键管理–》设备管理器–》处理器 （4）参数控制 结果 2.2 YARN模式 进入defaultParallelism方法 继续进入defaultParallelism方法 这个一个trait，其实现类是（Ctrl+h） 进入TaskSchedulerImpl类找到defaultParallelism方法 继续进入defaultParallelism方法，又是一个trait，看其实现类 Ctrl+h看SchedulerBackend类的实现类 进入CoarseGrainedSchedulerBackend找到defaultParallelism totalCoreCount.get()是所有executor使用的core总数，和2比较去较大值 如果正常的情况下，那你设置了多少就是多少 四、分区器（1）如果是从HDFS里面读取出来的数据，不需要分区器。因为HDFS本来就分好区了。 分区数我们是可以控制的，但是没必要有分区器。 （2）非key-value RDD分区，没必要设置分区器 1234al testRDD = sc.textFile(\"C:\\\\Users\\\\Administrator\\\\IdeaProjects\\\\myspark\\\\src\\\\main\\\\hello.txt\") .flatMap(line =&gt; line.split(\",\")) .map(word =&gt; (word, 1)).partitionBy(new HashPartitioner(2)) 没必要设置，但是非要设置也行。 （3）Key-value形式的时候，我们就有必要了。 HashPartitioner 1234val resultRDD = testRDD.reduceByKey(new HashPartitioner(2),(x:Int,y:Int) =&gt; x+ y)//如果不设置默认也是HashPartitoiner，分区数跟spark.default.parallelism一样println(resultRDD.partitioner)println(\"resultRDD\"+resultRDD.getNumPartitions) RangePartitioner 12345val resultRDD = testRDD.reduceByKey((x:Int,y:Int) =&gt; x+ y)val newresultRDD=resultRDD.partitionBy(new RangePartitioner[String,Int](3,resultRDD))println(newresultRDD.partitioner)println(\"newresultRDD\"+newresultRDD.getNumPartitions)注：按照范围进行分区的，如果是字符串，那么就按字典顺序的范围划分。如果是数字，就按数据自的范围划分。 自定义分区 需要实现2个方法 123456789101112131415161718192021222324252627282930313233343536class MyPartitoiner(val numParts:Int) extends Partitioner&#123; override def numPartitions: Int = numParts override def getPartition(key: Any): Int = &#123; val domain = new URL(key.toString).getHost val code = (domain.hashCode % numParts) if (code &lt; 0) &#123; code + numParts &#125; else &#123; code &#125; &#125;&#125;object DomainNamePartitioner &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(\"word count\").setMaster(\"local\") val sc = new SparkContext(conf) val urlRDD = sc.makeRDD(Seq((\"http://baidu.com/test\", 2), (\"http://baidu.com/index\", 2), (\"http://ali.com\", 3), (\"http://baidu.com/tmmmm\", 4), (\"http://baidu.com/test\", 4))) //Array[Array[(String, Int)]] // = Array(Array(), // Array((http://baidu.com/index,2), (http://baidu.com/tmmmm,4), // (http://baidu.com/test,4), (http://baidu.com/test,2), (http://ali.com,3))) val hashPartitionedRDD = urlRDD.partitionBy(new HashPartitioner(2)) hashPartitionedRDD.glom().collect() //使用spark-shell --jar的方式将这个partitioner所在的jar包引进去，然后测试下面的代码 // spark-shell --master spark://master:7077 --jars spark-rdd-1.0-SNAPSHOT.jar val partitionedRDD = urlRDD.partitionBy(new MyPartitoiner(2)) val array = partitionedRDD.glom().collect() &#125;&#125;","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本","slug":"2019-06-16-Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本","date":"2019-06-16T02:30:04.000Z","updated":"2019-09-17T03:17:31.537Z","comments":true,"path":"2019-06-16-Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-16-Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本.html","excerpt":"** Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本","text":"** Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本 &lt;The rest of contents | 余下全文&gt; 一、概述上一篇主要是介绍了spark启动的一些脚本，这篇主要分析一下Spark源码中提交任务脚本的处理逻辑，从spark-submit一步步深入进去看看任务提交的整体流程,首先看一下整体的流程概要图： 二、源码解读2.1 spark-submit 12345678910111213141516# -z是检查后面变量是否为空（空则真） shell可以在双引号之内引用变量，单引号不可#这一步作用是检查SPARK_HOME变量是否为空，为空则执行then后面程序#source命令： source filename作用在当前bash环境下读取并执行filename中的命令#$0代表shell脚本文件本身的文件名，这里即使spark-submit#dirname用于取得脚本文件所在目录 dirname $0取得当前脚本文件所在目录#$(命令)表示返回该命令的结果#故整个if语句的含义是：如果SPARK_HOME变量没有设置值，则执行当前目录下的find-spark-home脚本文件，设置SPARK_HOME值if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then source \"$(dirname \"$0\")\"/find-spark-homefi# disable randomized hash for string in Python 3.3+export PYTHONHASHSEED=0#执行spark-class脚本，传递参数org.apache.spark.deploy.SparkSubmit 和\"$@\"#这里$@表示之前spark-submit接收到的全部参数exec \"$&#123;SPARK_HOME&#125;\"/bin/spark-class org.apache.spark.deploy.SparkSubmit \"$@\" 所以spark-submit脚本的整体逻辑就是：首先 检查SPARK_HOME是否设置；if 已经设置 执行spark-class文件 否则加载执行find-spark-home文件 2.2 find-spark-home1234567891011121314151617181920212223#定义一个变量用于后续判断是否存在定义SPARK_HOME的python脚本文件FIND_SPARK_HOME_PYTHON_SCRIPT=\"$(cd \"$(dirname \"$0\")\"; pwd)/find_spark_home.py\"# Short cirtuit if the user already has this set.##如果SPARK_HOME为不为空值，成功退出程序if [ ! -z \"$&#123;SPARK_HOME&#125;\" ]; then exit 0# -f用于判断这个文件是否存在并且是否为常规文件，是的话为真，这里不存在为假，执行下面语句，给SPARK_HOME变量赋值elif [ ! -f \"$FIND_SPARK_HOME_PYTHON_SCRIPT\" ]; then # If we are not in the same directory as find_spark_home.py we are not pip installed so we don't # need to search the different Python directories for a Spark installation. # Note only that, if the user has pip installed PySpark but is directly calling pyspark-shell or # spark-submit in another directory we want to use that version of PySpark rather than the # pip installed version of PySpark. export SPARK_HOME=\"$(cd \"$(dirname \"$0\")\"/..; pwd)\"else # We are pip installed, use the Python script to resolve a reasonable SPARK_HOME # Default to standard python interpreter unless told otherwise if [[ -z \"$PYSPARK_DRIVER_PYTHON\" ]]; then PYSPARK_DRIVER_PYTHON=\"$&#123;PYSPARK_PYTHON:-\"python\"&#125;\" fi export SPARK_HOME=$($PYSPARK_DRIVER_PYTHON \"$FIND_SPARK_HOME_PYTHON_SCRIPT\")fi 可以看到，如果事先用户没有设定SPARK_HOME的值，这里程序也会自动设置并且将其注册为环境变量，供后面程序使用 当SPARK_HOME的值设定完成之后，就会执行Spark-class文件，这也是我们分析的重要部分，源码如下： 2.3 spark-class1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#!/usr/bin/env bash#依旧是检查设置SPARK_HOME的值if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then source \"$(dirname \"$0\")\"/find-spark-homefi#执行load-spark-env.sh脚本文件，主要目的在于加载设定一些变量值#设定spark-env.sh中的变量值到环境变量中，供后续使用#设定scala版本变量值. \"$&#123;SPARK_HOME&#125;\"/bin/load-spark-env.sh# Find the java binary#检查设定java环境值#-n代表检测变量长度是否为0，不为0时候为真#如果已经安装Java没有设置JAVA_HOME,command -v java返回的值为$&#123;JAVA_HOME&#125;/bin/javaif [ -n \"$&#123;JAVA_HOME&#125;\" ]; then RUNNER=\"$&#123;JAVA_HOME&#125;/bin/java\"else if [ \"$(command -v java)\" ]; then RUNNER=\"java\" else echo \"JAVA_HOME is not set\" &gt;&amp;2 exit 1 fifi# Find Spark jars.#-d检测文件是否为目录，若为目录则为真#设置一些关联Class文件if [ -d \"$&#123;SPARK_HOME&#125;/jars\" ]; then SPARK_JARS_DIR=\"$&#123;SPARK_HOME&#125;/jars\"else SPARK_JARS_DIR=\"$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars\"fiif [ ! -d \"$SPARK_JARS_DIR\" ] &amp;&amp; [ -z \"$SPARK_TESTING$SPARK_SQL_TESTING\" ]; then echo \"Failed to find Spark jars directory ($SPARK_JARS_DIR).\" 1&gt;&amp;2 echo \"You need to build Spark with the target \\\"package\\\" before running this program.\" 1&gt;&amp;2 exit 1else LAUNCH_CLASSPATH=\"$SPARK_JARS_DIR/*\"fi# Add the launcher build dir to the classpath if requested.if [ -n \"$SPARK_PREPEND_CLASSES\" ]; then LAUNCH_CLASSPATH=\"$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH\"fi# For testsif [[ -n \"$SPARK_TESTING\" ]]; then unset YARN_CONF_DIR unset HADOOP_CONF_DIRfi# The launcher library will print arguments separated by a NULL character, to allow arguments with# characters that would be otherwise interpreted by the shell. Read that in a while loop, populating# an array that will be used to exec the final command.## The exit code of the launcher is appended to the output, so the parent shell removes it from the# command array and checks the value to see if the launcher succeeded.#执行类文件org.apache.spark.launcher.Main，返回解析后的参数build_command() &#123; \"$RUNNER\" -Xmx128m -cp \"$LAUNCH_CLASSPATH\" org.apache.spark.launcher.Main \"$@\" printf \"%d\\0\" $?&#125;# Turn off posix mode since it does not allow process substitution#将build_command方法解析后的参数赋给CMDset +o posixCMD=()while IFS= read -d '' -r ARG; do CMD+=(\"$ARG\")done &lt; &lt;(build_command \"$@\")COUNT=$&#123;#CMD[@]&#125;LAST=$((COUNT - 1))LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;# Certain JVM failures result in errors being printed to stdout (instead of stderr), which causes# the code that parses the output of the launcher to get confused. In those cases, check if the# exit code is an integer, and if it's not, handle it as a special error case.if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then echo \"$&#123;CMD[@]&#125;\" | head -n-1 1&gt;&amp;2 exit 1fiif [ $LAUNCHER_EXIT_CODE != 0 ]; then exit $LAUNCHER_EXIT_CODEfiCMD=(\"$&#123;CMD[@]:0:$LAST&#125;\")#执行CMD中的某个参数类org.apache.spark.deploy.SparkSubmitexec \"$&#123;CMD[@]&#125;\" spark-class文件的执行逻辑稍显复杂，总体上应该是这样的： 检查SPARK_HOME的值—-》执行load-spark-env.sh文件，设定一些需要用到的环境变量，如scala环境值，这其中也加载了spark-env.sh文件——-》检查设定java的执行路径变量值——-》寻找spark jars,设定一些引用相关类的位置变量——》执行类文件org.apache.spark.launcher.Main，返回解析后的参数给CMD——-》判断解析参数是否正确（代表了用户设置的参数是否正确）——–》正确的话执行org.apache.spark.deploy.SparkSubmit这个类 2.4 SparkSubmit2.1最后提交语句，D:\\src\\spark-2.3.0\\core\\src\\main\\scala\\org\\apache\\spark\\deploy\\SparkSubmit.scala 1exec \"$&#123;SPARK_HOME&#125;\"/bin/spark-class org.apache.spark.deploy.SparkSubmit \"$@\" 123456789101112131415161718192021override def main(args: Array[String]): Unit = &#123; // Initialize logging if it hasn't been done yet. Keep track of whether logging needs to // be reset before the application starts. val uninitLog = initializeLogIfNecessary(true, silent = true) //拿到submit脚本传入的参数 val appArgs = new SparkSubmitArguments(args) if (appArgs.verbose) &#123; // scalastyle:off println printStream.println(appArgs) // scalastyle:on println &#125; //根据传入的参数匹配对应的执行方法 appArgs.action match &#123; //根据传入的参数提交命令 case SparkSubmitAction.SUBMIT =&gt; submit(appArgs, uninitLog) //只有standalone和mesos集群模式才触发 case SparkSubmitAction.KILL =&gt; kill(appArgs) //只有standalone和mesos集群模式才触发 case SparkSubmitAction.REQUEST_STATUS =&gt; requestStatus(appArgs) &#125; &#125; 2.4.1 submit十分关键，主要分为两步骤（1）调用prepareSubmitEnvironment （2）调用doRunMain","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本","slug":"2019-06-15-Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本","date":"2019-06-15T02:30:04.000Z","updated":"2019-09-17T04:21:07.772Z","comments":true,"path":"2019-06-15-Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-15-Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本.html","excerpt":"** Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本","text":"** Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本 &lt;The rest of contents | 余下全文&gt; 一、启动脚本分析独立部署模式下，主要由master和slaves组成，master可以利用zk实现高可用性，其driver，work，app等信息可以持久化到zk上；slaves由一台至多台主机构成。Driver通过向Master申请资源获取运行环境。 启动master和slaves主要是执行/usr/dahua/spark/sbin目录下的start-master.sh和start-slaves.sh，或者执行 start-all.sh，其中star-all.sh本质上就是调用start-master.sh和start-slaves.sh 1.1 start-all.sh12345678910111213#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi#2.执行$&#123;SPARK_HOME&#125;/sbin/spark-config.sh，见以下分析. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"#3.执行\"$&#123;SPARK_HOME&#125;/sbin\"/start-master.sh，见以下分析\"$&#123;SPARK_HOME&#125;/sbin\"/start-master.sh#4.执行\"$&#123;SPARK_HOME&#125;/sbin\"/start-slaves.sh，见以下分析\"$&#123;SPARK_HOME&#125;/sbin\"/star` t-slaves.sh 其中start-master.sh和start-slave.sh分别调用的是 org.apache.spark.deploy.master.Master和org.apache.spark.deploy.worker.Worker 1.2 start-master.shstart-master.sh调用了spark-daemon.sh，注意这里指定了启动的类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi# NOTE: This exact class name is matched downstream by SparkSubmit.# Any changes need to be reflected there.#2.设置CLASS=\"org.apache.spark.deploy.master.Master\"CLASS=\"org.apache.spark.deploy.master.Master\"#3.如果参数结尾包含--help或者-h则打印帮助信息，并退出if [[ \"$@\" = *--help ]] || [[ \"$@\" = *-h ]]; then echo \"Usage: ./sbin/start-master.sh [options]\" pattern=\"Usage:\" pattern+=\"\\|Using Spark's default log4j profile:\" pattern+=\"\\|Registered signal handlers for\" \"$&#123;SPARK_HOME&#125;\"/bin/spark-class $CLASS --help 2&gt;&amp;1 | grep -v \"$pattern\" 1&gt;&amp;2 exit 1fi#4.设置ORIGINAL_ARGS为所有参数ORIGINAL_ARGS=\"$@\"#5.执行$&#123;SPARK_HOME&#125;/sbin/spark-config.sh. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"#6.执行$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh. \"$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh\"#7.SPARK_MASTER_PORT为空则赋值7077if [ \"$SPARK_MASTER_PORT\" = \"\" ]; then SPARK_MASTER_PORT=7077fi#8.SPARK_MASTER_HOST为空则赋值本主机名(hostname)if [ \"$SPARK_MASTER_HOST\" = \"\" ]; then case `uname` in (SunOS) SPARK_MASTER_HOST=\"`/usr/sbin/check-hostname | awk '&#123;print $NF&#125;'`\" ;; (*) SPARK_MASTER_HOST=\"`hostname -f`\" ;; esacfi#9.SPARK_MASTER_WEBUI_PORT为空则赋值8080if [ \"$SPARK_MASTER_WEBUI_PORT\" = \"\" ]; then SPARK_MASTER_WEBUI_PORT=8080fi#10.执行脚本\"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start $CLASS 1 \\ --host $SPARK_MASTER_HOST --port $SPARK_MASTER_PORT --webui-port $SPARK_MASTER_WEBUI_PORT \\ $ORIGINAL_ARGS 其中10肯定是重点，分析之前我们看看5，6都干了些啥，最后直译出最后一个脚本 1.3 spark-config.sh(1.2的第5步)123456789101112#判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi#SPARK_CONF_DIR存在就用此目录，不存在用$&#123;SPARK_HOME&#125;/confexport SPARK_CONF_DIR=\"$&#123;SPARK_CONF_DIR:-\"$&#123;SPARK_HOME&#125;/conf\"&#125;\"# Add the PySpark classes to the PYTHONPATH:if [ -z \"$&#123;PYSPARK_PYTHONPATH_SET&#125;\" ]; then export PYTHONPATH=\"$&#123;SPARK_HOME&#125;/python:$&#123;PYTHONPATH&#125;\" export PYTHONPATH=\"$&#123;SPARK_HOME&#125;/python/lib/py4j-0.10.6-src.zip:$&#123;PYTHONPATH&#125;\" export PYSPARK_PYTHONPATH_SET=1fi 1.4 load-spark-env.sh(1.2的第6步)12345678910111213141516171819202122232425262728293031323334353637#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then source \"$(dirname \"$0\")\"/find-spark-homefi#2.判断SPARK_ENV_LOADED是否有值，没有将其设置为1if [ -z \"$SPARK_ENV_LOADED\" ]; then export SPARK_ENV_LOADED=1#3.设置user_conf_dir为SPARK_CONF_DIR或SPARK_HOME/conf export SPARK_CONF_DIR=\"$&#123;SPARK_CONF_DIR:-\"$&#123;SPARK_HOME&#125;\"/conf&#125;\"#4.执行\"$&#123;user_conf_dir&#125;/spark-env.sh\" [注：set -/+a含义再做研究] if [ -f \"$&#123;SPARK_CONF_DIR&#125;/spark-env.sh\" ]; then # Promote all variable declarations to environment (exported) variables set -a . \"$&#123;SPARK_CONF_DIR&#125;/spark-env.sh\" set +a fifi# Setting SPARK_SCALA_VERSION if not already set.#5.选择scala版本，2.11和2.12都存在的情况下，优先选择2.11if [ -z \"$SPARK_SCALA_VERSION\" ]; then ASSEMBLY_DIR2=\"$&#123;SPARK_HOME&#125;/assembly/target/scala-2.11\" ASSEMBLY_DIR1=\"$&#123;SPARK_HOME&#125;/assembly/target/scala-2.12\" if [[ -d \"$ASSEMBLY_DIR2\" &amp;&amp; -d \"$ASSEMBLY_DIR1\" ]]; then echo -e \"Presence of build for multiple Scala versions detected.\" 1&gt;&amp;2 echo -e 'Either clean one of them or, export SPARK_SCALA_VERSION in spark-env.sh.' 1&gt;&amp;2 exit 1 fi if [ -d \"$ASSEMBLY_DIR2\" ]; then export SPARK_SCALA_VERSION=\"2.11\" else export SPARK_SCALA_VERSION=\"2.12\" fifi 1.5 spark-env.sh列举很多种模式的选项配置 1.6 spark-daemon.sh回过头来看看1.2第10步中需要直译出的最后一个脚本,如下： 1sbin/spark-daemon.sh start org.apache.spark.deploy.master.Master 1 --host hostname --port 7077 --webui-port 8080 上面搞了半天只是设置了变量，最终才进入主角，继续分析spark-daemon.sh脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223#1.参数个数小于等于1，打印帮助if [ $# -le 1 ]; then echo $usage exit 1fi#2.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi#3.执行$&#123;SPARK_HOME&#125;/sbin/spark-config.sh，见上述分析 [类似脚本是否有重复？原因是有的人是直接用spark-daemon.sh启动的服务，反正重复设置下变量不需要什么代价]. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"# get arguments# Check if --config is passed as an argument. It is an optional parameter.# Exit if the argument is not a directory.#4.判断第一个参数是否是--config,如果是取空格后一个字符串，然后判断该目录是否存在，不存在则打印错误信息并退出，存在设置SPARK_CONF_DIR为该目录,shift到下一个参数#[注：--config只能用在第一参数上]if [ \"$1\" == \"--config\" ]then shift conf_dir=\"$1\" if [ ! -d \"$conf_dir\" ] then echo \"ERROR : $conf_dir is not a directory\" echo $usage exit 1 else export SPARK_CONF_DIR=\"$conf_dir\" fi shiftfi#5.分别设置option、command、instance为后面的三个参数(如：option=start,command=org.apache.spark.deploy.master.Master,instance=1)#[注：很多人用spark-daemon.sh启动服务不成功的原因是名字不全]option=$1shiftcommand=$1shiftinstance=$1shift#6.日志回滚函数，主要用于更改日志名，如log--&gt;log.1等，略过spark_rotate_log ()&#123; log=$1; num=5; if [ -n \"$2\" ]; then num=$2 fi if [ -f \"$log\" ]; then # rotate logs while [ $num -gt 1 ]; do prev=`expr $num - 1` [ -f \"$log.$prev\" ] &amp;&amp; mv \"$log.$prev\" \"$log.$num\" num=$prev done mv \"$log\" \"$log.$num\"; fi&#125;#7.执行$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh\"#8.判断SPARK_IDENT_STRING是否有值，没有将其设置为$USER(linux用户)if [ \"$SPARK_IDENT_STRING\" = \"\" ]; then export SPARK_IDENT_STRING=\"$USER\"fi#9.设置SPARK_PRINT_LAUNCH_COMMAND=1export SPARK_PRINT_LAUNCH_COMMAND=\"1\"# get log directory#10.判断SPARK_LOG_DIR是否有值，没有将其设置为$&#123;SPARK_HOME&#125;/logs，并创建改目录，测试创建文件，修改权限if [ \"$SPARK_LOG_DIR\" = \"\" ]; then export SPARK_LOG_DIR=\"$&#123;SPARK_HOME&#125;/logs\"fimkdir -p \"$SPARK_LOG_DIR\"touch \"$SPARK_LOG_DIR\"/.spark_test &gt; /dev/null 2&gt;&amp;1TEST_LOG_DIR=$?if [ \"$&#123;TEST_LOG_DIR&#125;\" = \"0\" ]; then rm -f \"$SPARK_LOG_DIR\"/.spark_testelse chown \"$SPARK_IDENT_STRING\" \"$SPARK_LOG_DIR\"fi#11.判断SPARK_PID_DIR是否有值，没有将其设置为/tmpif [ \"$SPARK_PID_DIR\" = \"\" ]; then SPARK_PID_DIR=/tmpfi# some variables#12.设置log和pidlog=\"$SPARK_LOG_DIR/spark-$SPARK_IDENT_STRING-$command-$instance-$HOSTNAME.out\"pid=\"$SPARK_PID_DIR/spark-$SPARK_IDENT_STRING-$command-$instance.pid\"# Set default scheduling priority#13.判断SPARK_NICENESS是否有值，没有将其设置为0 [注：调度优先级，见后面]if [ \"$SPARK_NICENESS\" = \"\" ]; then export SPARK_NICENESS=0fi#14.execute_command()函数，暂且略过，调用时再作分析execute_command() &#123; if [ -z $&#123;SPARK_NO_DAEMONIZE+set&#125; ]; then nohup -- \"$@\" &gt;&gt; $log 2&gt;&amp;1 &lt; /dev/null &amp; newpid=\"$!\" echo \"$newpid\" &gt; \"$pid\" # Poll for up to 5 seconds for the java process to start for i in &#123;1..10&#125; do if [[ $(ps -p \"$newpid\" -o comm=) =~ \"java\" ]]; then break fi sleep 0.5 done sleep 2 # Check if the process has died; in that case we'll tail the log so the user can see if [[ ! $(ps -p \"$newpid\" -o comm=) =~ \"java\" ]]; then echo \"failed to launch: $@\" tail -10 \"$log\" | sed 's/^/ /' echo \"full log in $log\" fi else \"$@\" fi&#125;#15.进入case语句，判断option值，进入该分支，我们以start为例# 执行run_command class \"$@\"，其中$@此时为空，经验证，启动带上此参数后，关闭也需，不然关闭不了，后面再分析此参数作用# 我们正式进入run_command()函数，分析# I.设置mode=class,创建SPARK_PID_DIR，上面的pid文件是否存在，# II.SPARK_MASTER不为空，同步删除某些文件# III.回滚log日志# IV.进入case，command=org.apache.spark.deploy.master.Master，最终执行# nohup nice -n \"$SPARK_NICENESS\" \"$&#123;SPARK_HOME&#125;\"/bin/spark-class $command \"$@\" &gt;&gt; \"$log\" 2&gt;&amp;1 &lt; /dev/null &amp;# newpid=\"$!\"# echo \"$newpid\" &gt; \"$pid\"# 重点转向bin/spark-class org.apache.spark.deploy.master.Masterrun_command() &#123; mode=\"$1\" shift mkdir -p \"$SPARK_PID_DIR\" if [ -f \"$pid\" ]; then TARGET_ID=\"$(cat \"$pid\")\" if [[ $(ps -p \"$TARGET_ID\" -o comm=) =~ \"java\" ]]; then echo \"$command running as process $TARGET_ID. Stop it first.\" exit 1 fi fi if [ \"$SPARK_MASTER\" != \"\" ]; then echo rsync from \"$SPARK_MASTER\" rsync -a -e ssh --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' \"$SPARK_MASTER/\" \"$&#123;SPARK_HOME&#125;\" fi spark_rotate_log \"$log\" echo \"starting $command, logging to $log\" case \"$mode\" in (class) execute_command nice -n \"$SPARK_NICENESS\" \"$&#123;SPARK_HOME&#125;\"/bin/spark-class \"$command\" \"$@\" ;; (submit) execute_command nice -n \"$SPARK_NICENESS\" bash \"$&#123;SPARK_HOME&#125;\"/bin/spark-submit --class \"$command\" \"$@\" ;; (*) echo \"unknown mode: $mode\" exit 1 ;; esac&#125;case $option in (submit) run_command submit \"$@\" ;; (start) run_command class \"$@\" ;; (stop) if [ -f $pid ]; then TARGET_ID=\"$(cat \"$pid\")\" if [[ $(ps -p \"$TARGET_ID\" -o comm=) =~ \"java\" ]]; then echo \"stopping $command\" kill \"$TARGET_ID\" &amp;&amp; rm -f \"$pid\" else echo \"no $command to stop\" fi else echo \"no $command to stop\" fi ;; (status) if [ -f $pid ]; then TARGET_ID=\"$(cat \"$pid\")\" if [[ $(ps -p \"$TARGET_ID\" -o comm=) =~ \"java\" ]]; then echo $command is running. exit 0 else echo $pid file is present but $command not running exit 1 fi else echo $command not running. exit 2 fi ;; (*) echo $usage exit 1 ;;esac 1.7 spark-class1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then source \"$(dirname \"$0\")\"/find-spark-homefi#2.执行$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh，见上述分析. \"$&#123;SPARK_HOME&#125;\"/bin/load-spark-env.sh# Find the java binary#3.判断JAVA_HOME是否为NULL，不是则设置RUNNER=\"$&#123;JAVA_HOME&#125;/bin/java\"，否则找系统自带，在没有则报未设置，并退出if [ -n \"$&#123;JAVA_HOME&#125;\" ]; then RUNNER=\"$&#123;JAVA_HOME&#125;/bin/java\"else if [ \"$(command -v java)\" ]; then RUNNER=\"java\" else echo \"JAVA_HOME is not set\" &gt;&amp;2 exit 1 fifi# Find Spark jars.#4.查找SPARK_JARS_DIR，若$&#123;SPARK_HOME&#125;/RELEASE文件存在，则SPARK_JARS_DIR=\"$&#123;SPARK_HOME&#125;/jars\"，否则#SPARK_JARS_DIR=\"$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars\"if [ -d \"$&#123;SPARK_HOME&#125;/jars\" ]; then SPARK_JARS_DIR=\"$&#123;SPARK_HOME&#125;/jars\"else SPARK_JARS_DIR=\"$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars\"fi#5.若SPARK_JARS_DIR不存在且$SPARK_TESTING$SPARK_SQL_TESTING有值[注：一般我们不设置这两变量]，报错退出，否则LAUNCH_CLASSPATH=\"$SPARK_JARS_DIR/*\"if [ ! -d \"$SPARK_JARS_DIR\" ] &amp;&amp; [ -z \"$SPARK_TESTING$SPARK_SQL_TESTING\" ]; then echo \"Failed to find Spark jars directory ($SPARK_JARS_DIR).\" 1&gt;&amp;2 echo \"You need to build Spark with the target \\\"package\\\" before running this program.\" 1&gt;&amp;2 exit 1else LAUNCH_CLASSPATH=\"$SPARK_JARS_DIR/*\"fi# Add the launcher build dir to the classpath if requested.#6.SPARK_PREPEND_CLASSES不是NULL，则LAUNCH_CLASSPATH=\"$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH\"，#添加编译相关至LAUNCH_CLASSPATHif [ -n \"$SPARK_PREPEND_CLASSES\" ]; then LAUNCH_CLASSPATH=\"$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH\"fi# For tests#7.SPARK_TESTING不是NULL，则unset YARN_CONF_DIR和unset HADOOP_CONF_DIR，暂且当做是为了某种测试if [[ -n \"$SPARK_TESTING\" ]]; then unset YARN_CONF_DIR unset HADOOP_CONF_DIRfi#8.build_command函数，略过build_command() &#123; \"$RUNNER\" -Xmx128m -cp \"$LAUNCH_CLASSPATH\" org.apache.spark.launcher.Main \"$@\" printf \"%d\\0\" $?&#125;# Turn off posix mode since it does not allow process substitutionset +o posixCMD=()while IFS= read -d '' -r ARG; do CMD+=(\"$ARG\") #9.最终调用\"$RUNNER\" -Xmx128m -cp \"$LAUNCH_CLASSPATH\" org.apache.spark.launcher.Main \"$@\"， #直译：java -Xmx128m -cp \"$LAUNCH_CLASSPATH\" org.apache.spark.launcher.Main \"$@\" #转向java类org.apache.spark.launcher.Main，这就是java入口类done &lt; &lt;(build_command \"$@\")COUNT=$&#123;#CMD[@]&#125;LAST=$((COUNT - 1))LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;# Certain JVM failures result in errors being printed to stdout (instead of stderr), which causes# the code that parses the output of the launcher to get confused. In those cases, check if the# exit code is an integer, and if it's not, handle it as a special error case.if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then echo \"$&#123;CMD[@]&#125;\" | head -n-1 1&gt;&amp;2 exit 1fiif [ $LAUNCHER_EXIT_CODE != 0 ]; then exit $LAUNCHER_EXIT_CODEfiCMD=(\"$&#123;CMD[@]:0:$LAST&#125;\")exec \"$&#123;CMD[@]&#125;\" 1.8 start-slaves.sh12345678910111213141516171819202122232425262728293031323334#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi#2.执行$&#123;SPARK_HOME&#125;/sbin/spark-config.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"#3.执行$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh\"# Find the port number for the master#4.SPARK_MASTER_PORT为空则设置为7077if [ \"$SPARK_MASTER_PORT\" = \"\" ]; then SPARK_MASTER_PORT=7077fi#5.SPARK_MASTER_HOST为空则设置为`hostname`if [ \"$SPARK_MASTER_HOST\" = \"\" ]; then case `uname` in (SunOS) SPARK_MASTER_HOST=\"`/usr/sbin/check-hostname | awk '&#123;print $NF&#125;'`\" ;; (*) SPARK_MASTER_HOST=\"`hostname -f`\" ;; esacfi# Launch the slaves#6.启动slaves，# \"$&#123;SPARK_HOME&#125;/sbin/slaves.sh\" cd \"$&#123;SPARK_HOME&#125;\" \\; \"$&#123;SPARK_HOME&#125;/sbin/start-slave.sh\" \"spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT\"# 遍历conf/slaves中主机，其中有设置SPARK_SSH_OPTS，ssh每一台机器执行\"$&#123;SPARK_HOME&#125;/sbin/start-slave.sh\" \"spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT\"\"$&#123;SPARK_HOME&#125;/sbin/slaves.sh\" cd \"$&#123;SPARK_HOME&#125;\" \\; \"$&#123;SPARK_HOME&#125;/sbin/start-slave.sh\" \"spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT\" 1.9 转向start-slave.sh1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi#2.设置CLASS=\"org.apache.spark.deploy.worker.Worker\"CLASS=\"org.apache.spark.deploy.worker.Worker\"#3.如果参数结尾包含--help或者-h则打印帮助信息，并退出if [[ $# -lt 1 ]] || [[ \"$@\" = *--help ]] || [[ \"$@\" = *-h ]]; then echo \"Usage: ./sbin/start-slave.sh [options] &lt;master&gt;\" pattern=\"Usage:\" pattern+=\"\\|Using Spark's default log4j profile:\" pattern+=\"\\|Registered signal handlers for\" \"$&#123;SPARK_HOME&#125;\"/bin/spark-class $CLASS --help 2&gt;&amp;1 | grep -v \"$pattern\" 1&gt;&amp;2 exit 1fi#4.执行$&#123;SPARK_HOME&#125;/sbin/spark-config.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"#5.执行$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh\"#6.MASTER=$1,这里MASTER=spark://hostname:7077，然后shift，也就是说单独启动单个slave使用start-slave.sh spark://hostname:7077MASTER=$1shift#7.SPARK_WORKER_WEBUI_PORT为空则设置为8081if [ \"$SPARK_WORKER_WEBUI_PORT\" = \"\" ]; then SPARK_WORKER_WEBUI_PORT=8081fi#8.函数start_instance，略过function start_instance &#123;#设置WORKER_NUM=$1 WORKER_NUM=$1 shift if [ \"$SPARK_WORKER_PORT\" = \"\" ]; then PORT_FLAG= PORT_NUM= else PORT_FLAG=\"--port\" PORT_NUM=$(( $SPARK_WORKER_PORT + $WORKER_NUM - 1 )) fi WEBUI_PORT=$(( $SPARK_WORKER_WEBUI_PORT + $WORKER_NUM - 1 )) #直译：spark-daemon.sh start org.apache.spark.deploy.worker.Worker 1 --webui-port 7077 spark://hostname:7077 #代码再次转向spark-daemon.sh，见上诉分析 \"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start $CLASS $WORKER_NUM \\ --webui-port \"$WEBUI_PORT\" $PORT_FLAG $PORT_NUM $MASTER \"$@\"&#125;#9.判断SPARK_WORKER_INSTANCES(可以认为是单节点Worker进程数)是否为空# 为空，则start_instance 1 \"$@\"# 不为空，则循环# for ((i=0; i&lt;$SPARK_WORKER_INSTANCES; i++)); do# start_instance $(( 1 + $i )) \"$@\"# doneif [ \"$SPARK_WORKER_INSTANCES\" = \"\" ]; then start_instance 1 \"$@\"else for ((i=0; i&lt;$SPARK_WORKER_INSTANCES; i++)); do #10.转向start_instance函数 start_instance $(( 1 + $i )) \"$@\" donefi 二、其他脚本2.1 start-history-server.sh1234567891011#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi#2.执行$&#123;SPARK_HOME&#125;/sbin/spark-config.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"#3.执行$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh\"#4.exec \"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start org.apache.spark.deploy.history.HistoryServer 1 $@ ，见上诉分析exec \"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start org.apache.spark.deploy.history.HistoryServer 1 \"$@\" 2.2 start-shuffle-service.sh1234567891011#1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录if [ -z \"$&#123;SPARK_HOME&#125;\" ]; then export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"fi#2.执行$&#123;SPARK_HOME&#125;/sbin/spark-config.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/sbin/spark-config.sh\"#3.执行$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh，见上述分析. \"$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh\"#4.exec \"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start org.apache.spark.deploy.ExternalShuffleService 1 ，见上诉分析exec \"$&#123;SPARK_HOME&#125;/sbin\"/spark-daemon.sh start org.apache.spark.deploy.ExternalShuffleService 1","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器","slug":"2019-06-14-Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器","date":"2019-06-14T02:30:04.000Z","updated":"2019-09-17T02:15:20.277Z","comments":true,"path":"2019-06-14-Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-14-Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器.html","excerpt":"** Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器：** &lt;Excerpt in index | 首页摘要&gt; ​ 垃圾收集 Garbage Collection 通常被称为“GC”，它诞生于1960年 MIT 的 Lisp 语言，经过半个多世纪，目前已经十分成熟了。 jvm 中，程序计数器、虚拟机栈、本地方法栈都是随线程而生随线程而灭，栈帧随着方法的进入和退出做入栈和出栈操作，实现了自动的内存清理，因此，我们的内存垃圾回收主要集中于 java 堆和方法区中，在程序运行期间，这部分内存的分配和使用都是动态的。","text":"** Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器：** &lt;Excerpt in index | 首页摘要&gt; ​ 垃圾收集 Garbage Collection 通常被称为“GC”，它诞生于1960年 MIT 的 Lisp 语言，经过半个多世纪，目前已经十分成熟了。 jvm 中，程序计数器、虚拟机栈、本地方法栈都是随线程而生随线程而灭，栈帧随着方法的进入和退出做入栈和出栈操作，实现了自动的内存清理，因此，我们的内存垃圾回收主要集中于 java 堆和方法区中，在程序运行期间，这部分内存的分配和使用都是动态的。 &lt;The rest of contents | 余下全文&gt; 一、垃圾收集器(garbage collector (GC)) 是什么？GC其实是一种自动的内存管理工具，其行为主要包括2步 在Java堆中，为新创建的对象分配空间 在Java堆中，回收没用的对象占用的空间 二、为什么需要GC？**释放开发人员的生产力 三、为什么需要多种GC？**首先，Java平台被部署在各种各样的硬件资源上，其次，在Java平台上部署和运行着各种各样的应用，并且用户对不同的应用的 性能指标 (吞吐率和延迟) 预期也不同，为了满足不同应用的对内存管理的不同需求，JVM提供了多种GC以供选择 性能指标最大停顿时长：垃圾回收导致的应用停顿时间的最大值吞吐率：垃圾回收停顿时长和应用运行总时长的比例 不同的GC能满足不同应用不同的性能需求，现有的GC包括： 序列化GC(serial garbage collector)：适合占用内存少的应用 并行GC 或 吞吐率GC(parallel or throughput garbage collector)：适合占用内存较多，多CPU，追求高吞吐率的应用 并发GC：适合占用内存较多，多CPU的应用，对延迟有要求的应用 四、对象存活的判断判断对象是否存活一般有两种方式： 引用计数：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，缺点是无法解决对象相互循环引用的问题。 可达性分析（Reachability Analysis）：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。不可达对象。 在Java语言中，GC Roots包括： 虚拟机栈中引用的对象。 方法区中类静态属性实体引用的对象。 方法区中常量引用的对象。 本地方法栈中JNI引用的对象。 由于循环引用的问题，一般采用跟踪（可达性分析）方法 五、垃圾回收算法5.1 标记 -清除算法“标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。 它的主要缺点有两个：一个是效率问题，标记和清除过程的效率都不高；另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 5.2 复制算法“复制”（Copying）的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，持续复制长生存期的对象则导致效率降低。 5.3 标记-整理算法复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。 根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存 5.4 分代收集算法GC分代的基本假设：绝大部分对象的生命周期都非常短暂，存活时间短。 “分代收集”（Generational Collection）算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或“标记-整理”算法来进行回收。 六、垃圾收集器如果说收集算法是内存回收的方法论，垃圾收集器就是内存回收的具体实现，不同厂商、不同版本的虚拟机实现差别很大，HotSpot中包含的收集器如下： 6.1 Serial收集器串行收集器是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。新生代、老年代使用串行回收；新生代复制算法、老年代标记-压缩；垃圾收集的过程中会Stop The World（服务暂停） 参数控制：-XX:+UseSerialGC 串行收集器 6.2 ParNew收集器ParNew收集器其实就是Serial收集器的多线程版本。新生代并行，老年代串行；新生代复制算法、老年代标记-压缩 参数控制：-XX:+UseParNewGC ParNew收集器 -XX:ParallelGCThreads 限制线程数量 6.3 Parallel收集器Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。可以通过参数来打开自适应调节策略，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或最大的吞吐量；也可以通过参数控制GC的时间不大于多少毫秒或者比例；新生代复制算法、老年代标记-压缩 参数控制：-XX:+UseParallelGC 使用Parallel收集器+ 老年代串行 6.4 CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用都集中在互联网站或B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。 从名字（包含“Mark Sweep”）上就可以看出CMS收集器是基于“标记-清除”算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为4个步骤，包括： 初始标记（CMS initial mark） 并发标记（CMS concurrent mark） 重新标记（CMS remark） 并发清除（CMS concurrent sweep） 其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。 由于整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，所以总体上来说，CMS收集器的内存回收过程是与用户线程一起并发地执行。老年代收集器（新生代使用ParNew） 优点:并发收集、低停顿 缺点：产生大量空间碎片、并发阶段会降低吞吐量 参数控制：-XX:+UseConcMarkSweepGC 使用CMS收集器 ​ -XX:+ UseCMSCompactAtFullCollection Full GC后，进行一次碎片整理；整理过程是独占的，会引起停顿时间变长 ​ -XX:+CMSFullGCsBeforeCompaction 设置进行几次Full GC后，进行一次碎片整理 ​ -XX:ParallelCMSThreads 设定CMS的线程数量（一般情况约等于可用CPU数量） 6.5 G1收集器G1是目前技术发展的最前沿成果之一，HotSpot开发团队赋予它的使命是未来可以替换掉JDK1.5中发布的CMS收集器。与CMS收集器相比G1收集器有以下特点： 空间整合，G1收集器采用标记整理算法，不会产生内存空间碎片。分配大对象时不会因为无法找到连续空间而提前触发下一次GC。 可预测停顿，这是G1的另一大优势，降低停顿时间是G1和CMS的共同关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为N毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了。 上面提到的垃圾收集器，收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔阂了，它们都是一部分（可以不连续）Region的集合。 G1对Heap的划分 G1的新生代收集跟ParNew类似，当新生代占用达到一定比例的时候，开始出发收集。和CMS类似，G1收集器收集老年代对象会有短暂停顿。 收集步骤1、标记阶段，首先初始标记(Initial-Mark),这个阶段是停顿的(Stop the World Event)，并且会触发一次普通Mintor GC。对应GC log:GC pause (young) (inital-mark) 2、Root Region Scanning，程序运行过程中会回收survivor区(存活到老年代)，这一过程必须在young GC之前完成。 3、Concurrent Marking，在整个堆中进行并发标记(和应用程序并发执行)，此过程可能被young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那个这个区域会被立即回收(图中打X)。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。 4、Remark, 再标记，会有短暂停顿(STW)。再标记阶段是用来收集 并发标记阶段 产生新的垃圾(并发阶段和应用程序一同运行)；G1中采用了比CMS更快的初始快照算法:snapshot-at-the-beginning (SATB)。 5、Copy/Clean up，多线程清除失活对象，会有STW。G1将回收区域的存活对象拷贝到新区域，清除Remember Sets，并发清空回收区域并把它返回到空闲区域链表中。 6、复制/清除过程后。回收区域的活性对象已经被集中回收到深蓝色和深绿色区域。 八、常用的收集器组合 新生代GC策略 年老代GC策略 说明 组合1 Serial Serial Old Serial和Serial Old都是单线程进行GC，特点就是GC时暂停所有应用线程。 组合2 Serial CMS+Serial Old CMS（Concurrent Mark Sweep）是并发GC，实现GC线程和应用线程并发工作，不需要暂停所有应用线程。另外，当CMS进行GC失败时，会自动使用Serial Old策略进行GC。 组合3 ParNew CMS 使用-XX:+UseParNewGC选项来开启。ParNew是Serial的并行版本，可以指定GC线程数，默认GC线程数为CPU的数量。可以使用-XX:ParallelGCThreads选项指定GC的线程数。如果指定了选项-XX:+UseConcMarkSweepGC选项，则新生代默认使用ParNew GC策略。 组合4 ParNew Serial Old 使用-XX:+UseParNewGC选项来开启。新生代使用ParNew GC策略，年老代默认使用Serial Old GC策略。 组合5 Parallel Scavenge Serial Old Parallel Scavenge策略主要是关注一个可控的吞吐量：应用程序运行时间 / (应用程序运行时间 + GC时间)，可见这会使得CPU的利用率尽可能的高，适用于后台持久运行的应用程序，而不适用于交互较多的应用程序。 组合6 Parallel Scavenge Parallel Old Parallel Old是Serial Old的并行版本 组合7 G1GC G1GC -XX:+UnlockExperimentalVMOptions -XX:+UseG1GC #开启 -XX:MaxGCPauseMillis =50 #暂停时间目标 -XX:GCPauseIntervalMillis =200 #暂停间隔目标 -XX:+G1YoungGenSize=512m #年轻代大小 -XX:SurvivorRatio=6 #幸存区比例","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构","slug":"2019-06-13-Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构","date":"2019-06-13T02:30:04.000Z","updated":"2019-09-17T01:42:22.631Z","comments":true,"path":"2019-06-13-Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-13-Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构.html","excerpt":"** Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt; 一、JVM的结构图1.1 Java内存结构 JVM内存结构主要有三大块：堆内存、方法区和栈。 堆内存是JVM中最大的一块由年轻代和老年代组成，而年轻代内存又被分成三部分，Eden空间、From Survivor空间、To Survivor空间,默认情况下年轻代按照8:1:1的比例来分配； 方法区存储类信息、常量、静态变量等数据，是线程共享的区域，为与Java堆区分，方法区还有一个别名Non-Heap(非堆)； 栈又分为java虚拟机栈和本地方法栈主要用于方法的执行。 1.2 如何通过参数来控制各区域的内存大小 1.3 控制参数-Xms设置堆的最小空间大小。 -Xmx设置堆的最大空间大小。 -XX:NewSize设置新生代最小空间大小。 -XX:MaxNewSize设置新生代最大空间大小。 -XX:PermSize设置永久代最小空间大小。 -XX:MaxPermSize设置永久代最大空间大小。 -Xss设置每个线程的堆栈大小。 没有直接设置老年代的参数，但是可以设置堆空间大小和新生代空间大小两个参数来间接控制。 老年代空间大小=堆空间大小-年轻代大空间大小 1.4 JVM和系统调用之间的关系 方法区和堆是所有线程共享的内存区域；而java栈、本地方法栈和程序员计数器是运行是线程私有的内存区域。 二、JVM各区域的作用2.1 Java堆（Heap）​ 对于大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 ​ Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”。如果从内存回收的角度看，由于现在收集器基本都是采用的分代收集算法，所以Java堆中还可以细分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。 根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx和-Xms控制）。 如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 2.2 方法区（Method Area） 方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。 对于习惯在HotSpot虚拟机上开发和部署程序的开发者来说，很多人愿意把方法区称为“永久代”（Permanent Generation），本质上两者并不等价，仅仅是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已。 Java虚拟机规范对这个区域的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是有必要的。 根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 2.3 程序计数器（Program Counter Register）程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie方法，这个计数器值则为空（Undefined）。 此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 2.4 JVM栈（JVM Stacks）与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，根据不同的虚拟机实现，它可能是一个指向对象起始地址的引用指针，也可能指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。 其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。 2.5 本地方法栈（Native Method Stacks）本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十二）SparkCore的调优之资源调优","slug":"2019-06-12-Spark学习之路 （十二）SparkCore的调优之资源调优","date":"2019-06-12T02:30:04.000Z","updated":"2019-09-17T01:35:58.907Z","comments":true,"path":"2019-06-12-Spark学习之路 （十二）SparkCore的调优之资源调优.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-12-Spark学习之路 （十二）SparkCore的调优之资源调优.html","excerpt":"** Spark学习之路 （十二）SparkCore的调优之资源调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。","text":"** Spark学习之路 （十二）SparkCore的调优之资源调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。 &lt;The rest of contents | 余下全文&gt; 一、Spark作业基本运行原理 详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。 在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。 Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。 当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。 因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。 task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。 以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。 二、资源参数调优了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。 2.1 num-executors 参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。 参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。 2.2 executor-memory 参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。 参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。 2.3 executor-cores 参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。 参数调优建议：Executor的CPU core数量设置为24个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/31/2左右比较合适，也是避免影响其他同学的作业运行。最好的应该就是一个cpu core对应两到三个task 2.4 driver-memory 参数说明：该参数用于设置Driver进程的内存。 参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。 2.5 spark.default.parallelism 参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。一个分区对应一个task，也就是这个参数其实就是设置task的数量 参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。 2.6 spark.storage.memoryFraction 参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。 参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 2.7 spark.shuffle.memoryFraction 参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。 参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十一）SparkCore的调优之Spark内存模型","slug":"2019-06-11-Spark学习之路 （十一）SparkCore的调优之Spark内存模型","date":"2019-06-11T02:30:04.000Z","updated":"2019-09-17T01:21:32.711Z","comments":true,"path":"2019-06-11-Spark学习之路 （十一）SparkCore的调优之Spark内存模型.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-11-Spark学习之路 （十一）SparkCore的调优之Spark内存模型.html","excerpt":"** Spark学习之路 （十一）SparkCore的调优之Spark内存模型：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark 作为一个基于内存的分布式计算引擎，其内存管理模块在整个系统中扮演着非常重要的角色。理解 Spark 内存管理的基本原理，有助于更好地开发 Spark 应用程序和进行性能调优。本文旨在梳理出 Spark 内存管理的脉络，抛砖引玉，引出读者对这个话题的深入探讨。本文中阐述的原理基于 Spark 2.1 版本，阅读本文需要读者有一定的 Spark 和 Java 基础，了解 RDD、Shuffle、JVM 等相关概念。 在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能[1]。由于 Driver 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。","text":"** Spark学习之路 （十一）SparkCore的调优之Spark内存模型：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark 作为一个基于内存的分布式计算引擎，其内存管理模块在整个系统中扮演着非常重要的角色。理解 Spark 内存管理的基本原理，有助于更好地开发 Spark 应用程序和进行性能调优。本文旨在梳理出 Spark 内存管理的脉络，抛砖引玉，引出读者对这个话题的深入探讨。本文中阐述的原理基于 Spark 2.1 版本，阅读本文需要读者有一定的 Spark 和 Java 基础，了解 RDD、Shuffle、JVM 等相关概念。 在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能[1]。由于 Driver 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。 &lt;The rest of contents | 余下全文&gt; 1. 堆内和堆外内存规划​ 作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内（On-heap）空间进行了更为详细的分配，以充分利用内存。同时，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。 图 1 . 堆内和堆外内存示意图 1.1 堆内内存堆内内存的大小，由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置。Executor 内运行的并发任务共享 JVM 堆内内存，这些任务在缓存 RDD 数据和广播（Broadcast）数据时占用的内存被规划为存储（Storage）内存，而这些任务在执行 Shuffle 时占用的内存被规划为执行（Execution）内存，剩余的部分不做特殊规划，那些 Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间。不同的管理模式下，这三部分占用的空间大小各不相同（下面第 2 小节会进行介绍）。 Spark 对堆内内存的管理是一种逻辑上的”规划式”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前记录这些内存，我们来看其具体流程： 申请内存： Spark 在代码中 new 一个对象实例 JVM 从堆内内存分配空间，创建对象并返回对象引用 Spark 保存该对象的引用，记录该对象占用的内存 释放内存： Spark 记录该对象释放的内存，删除该对象的引用 等待 JVM 的垃圾回收机制释放该对象占用的堆内内存 我们知道，JVM 的对象可以以序列化的方式存储，序列化的过程是将对象转换为二进制字节流，本质上可以理解为将非连续空间的链式存储转化为连续空间或块存储，在访问时则需要进行序列化的逆过程——反序列化，将字节流转化为对象，序列化的方式可以节省存储空间，但增加了存储和读取时候的计算开销。 对于 Spark 中序列化的对象，由于是字节流的形式，其占用的内存大小可直接计算，而对于非序列化的对象，其占用的内存是通过周期性地采样近似估算而得，即并不是每次新增的数据项都会计算一次占用的内存大小，这种方法降低了时间开销但是有可能误差较大，导致某一时刻的实际内存有可能远远超出预期[2]。此外，在被 Spark 标记为释放的对象实例，很有可能在实际上并没有被 JVM 回收，导致实际可用的内存小于 Spark 记录的可用内存。所以 Spark 并不能准确记录实际可用的堆内内存，从而也就无法完全避免内存溢出（OOM, Out of Memory）的异常。 虽然不能精准控制堆内内存的申请和释放，但 Spark 通过对存储内存和执行内存各自独立的规划管理，可以决定是否要在存储内存里缓存新的 RDD，以及是否为新的任务分配执行内存，在一定程度上可以提升内存的利用率，减少异常的出现。 1.2 堆外内存为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。利用 JDK Unsafe API（从 Spark 2.0 开始，在管理堆外的存储内存时不再基于 Tachyon，而是与堆外的执行内存一样，基于 JDK Unsafe API 实现[3]），Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。堆外内存可以被精确地申请和释放，而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。 在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存。 1.3 内存管理接口Spark 为存储内存和执行内存的管理提供了统一的接口——MemoryManager，同一个 Executor 内的任务都调用这个接口的方法来申请或释放内存: 清单 1 . 内存管理接口的主要方法123456789101112//申请存储内存def acquireStorageMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean//申请展开内存def acquireUnrollMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean//申请执行内存def acquireExecutionMemory(numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long//释放存储内存def releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit//释放执行内存def releaseExecutionMemory(numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Unit//释放展开内存def releaseUnrollMemory(numBytes: Long, memoryMode: MemoryMode): Unit 我们看到，在调用这些方法时都需要指定其内存模式（MemoryMode），这个参数决定了是在堆内还是堆外完成这次操作。 MemoryManager 的具体实现上，Spark 1.6 之后默认为统一管理（Unified Memory Manager）方式，1.6 之前采用的静态管理（Static Memory Manager）方式仍被保留，可通过配置 spark.memory.useLegacyMode 参数启用。两种方式的区别在于对空间分配的方式，下面的第 2 小节会分别对这两种方式进行介绍。 2 . 内存空间分配2.1 静态内存管理在 Spark 最初采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置，堆内内存的分配如图 2 所示： 图 2 . 静态内存管理图示——堆内 可以看到，可用的堆内内存的大小需要按照下面的方式计算： 清单 2 . 可用堆内内存空间12可用的存储内存 = systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction可用的执行内存 = systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction 其中 systemMaxMemory 取决于当前 JVM 堆内内存的大小，最后可用的执行内存或者存储内存要在此基础上与各自的 memoryFraction 参数和 safetyFraction 参数相乘得出。上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 1-safetyFraction 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险（上文提到，对于非序列化对象的内存采样估算会产生误差）。值得注意的是，这个预留的保险区域仅仅是一种逻辑上的规划，在具体使用时 Spark 并没有区别对待，和”其它内存”一样交给了 JVM 去管理。 堆外的空间分配较为简单，只有存储内存和执行内存，如图 3 所示。可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域。 图 3 . 静态内存管理图示——堆外 静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成”一半海水，一半火焰”的局面，即存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。 2.2 统一内存管理Spark 1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域，如图 4 和图 5 所示 图 4 . 统一内存管理图示——堆内 图 5 . 统一内存管理图示——堆外 其中最重要的优化在于动态占用机制，其规则如下： 设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围 双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block） 执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间 存储内存的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂[4] 图 6 . 动态占用机制图示 凭借统一内存管理机制，Spark 在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护 Spark 内存的难度，但并不意味着开发者可以高枕无忧。譬如，所以如果存储内存的空间太大或者说缓存的数据过多，反而会导致频繁的全量垃圾回收，降低任务执行时的性能，因为缓存的 RDD 数据通常都是长期驻留内存的 [5] 。所以要想充分发挥 Spark 的性能，需要开发者进一步了解存储内存和执行内存各自的管理方式和实现原理。 3. 存储内存管理3.1 RDD 的持久化机制弹性分布式数据集（RDD）作为 Spark 最根本的数据抽象，是只读的分区记录（Partition）的集合，只能基于在稳定物理存储中的数据集上创建，或者在其他已有的 RDD 上执行转换（Transformation）操作产生一个新的 RDD。转换后的 RDD 与原始的 RDD 之间产生的依赖关系，构成了血统（Lineage）。凭借血统，Spark 保证了每一个 RDD 都可以被重新恢复。但 RDD 的所有转换都是惰性的，即只有当一个返回结果给 Driver 的行动（Action）发生时，Spark 才会创建任务读取 RDD，然后真正触发转换的执行。Task 在启动之初读取一个分区时，会先判断这个分区是否已经被持久化，如果没有则需要检查 Checkpoint 或按照血统重新计算。所以如果一个 RDD 上要执行多次行动，可以在第一次行动中使用 persist 或 cache 方法，在内存或磁盘中持久化或缓存这个 RDD，从而在后面的行动时提升计算速度。事实上，cache 方法是使用默认的 MEMORY_ONLY 的存储级别将 RDD 持久化到内存，故缓存是一种特殊的持久化。 堆内和堆外存储内存的设计，便可以对缓存 RDD 时使用的内存做统一的规划和管 理 （存储内存的其他应用场景，如缓存 broadcast 数据，暂时不在本文的讨论范围之内）。 RDD 的持久化由 Spark 的 Storage 模块 [7] 负责，实现了 RDD 与物理存储的解耦合。Storage 模块负责管理 Spark 在计算过程中产生的数据，将那些在内存或磁盘、在本地或远程存取数据的功能封装了起来。在具体实现时 Driver 端和 Executor 端的 Storage 模块构成了主从式的架构，即 Driver 端的 BlockManager 为 Master，Executor 端的 BlockManager 为 Slave。Storage 模块在逻辑上以 Block 为基本存储单位，RDD 的每个 Partition 经过处理后唯一对应一个 Block（BlockId 的格式为 rdd_RDD-ID_PARTITION-ID ）。Master 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 Slave 需要将 Block 的更新等状态上报到 Master，同时接收 Master 的命令，例如新增或删除一个 RDD。 图 7 . Storage 模块示意图 在对 RDD 持久化时，Spark 规定了 MEMORY_ONLY、MEMORY_AND_DISK 等12 种不同的 存储级别 ，而存储级别是以下 5 个变量的组合： 清单 3 . 存储级别1234567class StorageLevel private( private var _useDisk: Boolean, //磁盘 private var _useMemory: Boolean, //这里其实是指堆内内存 private var _useOffHeap: Boolean, //堆外内存 private var _deserialized: Boolean, //是否为非序列化 private var _replication: Int = 1 //副本个数) 通过对数据结构的分析，可以看出存储级别从三个维度定义了 RDD 的 Partition（同时也就是 Block）的存储方式： 存储位置：磁盘／堆内内存／堆外内存。如 MEMORY_AND_DISK 是同时在磁盘和堆内内存上存储，实现了冗余备份。OFF_HEAP 则是只在堆外内存存储，目前选择堆外内存时不能同时存储到其他位置。 存储形式：Block 缓存到存储内存后，是否为非序列化的形式。如 MEMORY_ONLY 是非序列化方式存储，OFF_HEAP 是序列化方式存储。 副本数量：大于 1 时需要远程冗余备份到其他节点。如 DISK_ONLY_2 需要远程备份 1 个副本。 3.2 RDD 缓存的过程RDD 在缓存到存储内存之前，Partition 中的数据一般以迭代器（Iterator）的数据结构来访问，这是 Scala 语言中一种遍历数据集合的方法。通过 Iterator 可以获取分区中每一条序列化或者非序列化的数据项(Record)，这些 Record 的对象实例在逻辑上占用了 JVM 堆内内存的 other 部分的空间，同一 Partition 的不同 Record 的空间并不连续。 RDD 在缓存到存储内存之后，Partition 被转换成 Block，Record 在堆内或堆外存储内存中占用一块连续的空间。将Partition由不连续的存储空间转换为连续存储空间的过程，Spark称之为”展开”（Unroll）。Block 有序列化和非序列化两种存储格式，具体以哪种方式取决于该 RDD 的存储级别。非序列化的 Block 以一种 DeserializedMemoryEntry 的数据结构定义，用一个数组存储所有的对象实例，序列化的 Block 则以 SerializedMemoryEntry的数据结构定义，用字节缓冲区（ByteBuffer）来存储二进制数据。每个 Executor 的 Storage 模块用一个链式 Map 结构（LinkedHashMap）来管理堆内和堆外存储内存中所有的 Block 对象的实例[6]，对这个 LinkedHashMap 新增和删除间接记录了内存的申请和释放。 因为不能保证存储空间可以一次容纳 Iterator 中的所有数据，当前的计算任务在 Unroll 时要向 MemoryManager 申请足够的 Unroll 空间来临时占位，空间不足则 Unroll 失败，空间足够时可以继续进行。对于序列化的 Partition，其所需的 Unroll 空间可以直接累加计算，一次申请。而非序列化的 Partition 则要在遍历 Record 的过程中依次申请，即每读取一条 Record，采样估算其所需的 Unroll 空间并进行申请，空间不足时可以中断，释放已占用的 Unroll 空间。如果最终 Unroll 成功，当前 Partition 所占用的 Unroll 空间被转换为正常的缓存 RDD 的存储空间，如下图 8 所示。 图 8. Spark Unroll 示意图 在图 3 和图 5 中可以看到，在静态内存管理时，Spark 在存储内存中专门划分了一块 Unroll 空间，其大小是固定的，统一内存管理时则没有对 Unroll 空间进行特别区分，当存储空间不足时会根据动态占用机制进行处理。 3.3 淘汰和落盘由于同一个 Executor 的所有的计算任务共享有限的存储内存空间，当有新的 Block 需要缓存但是剩余空间不足且无法动态占用时，就要对 LinkedHashMap 中的旧 Block 进行淘汰（Eviction），而被淘汰的 Block 如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘（Drop），否则直接删除该 Block。 存储内存的淘汰规则为： 被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存 新旧 Block 不能属于同一个 RDD，避免循环淘汰 旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题 遍历 LinkedHashMap 中 Block，按照最近最少使用（LRU）的顺序淘汰，直到满足新 Block 所需的空间。其中 LRU 是 LinkedHashMap 的特性。 落盘的流程则比较简单，如果其存储级别符合_useDisk 为 true 的条件，再根据其_deserialized 判断是否是非序列化的形式，若是则对其进行序列化，最后将数据存储到磁盘，在 Storage 模块中更新其信息。 4. 执行内存管理4.1 多任务间内存分配Executor 内运行的任务同样共享执行内存，Spark 用一个 HashMap 结构保存了任务到内存耗费的映射。每个任务可占用的执行内存大小的范围为 1/2N ~ 1/N，其中 N 为当前 Executor 内正在运行的任务的个数。每个任务在启动之时，要向 MemoryManager 请求申请最少为 1/2N 的执行内存，如果不能被满足要求则该任务被阻塞，直到有其他任务释放了足够的执行内存，该任务才可以被唤醒。 4.2 Shuffle 的内存占用执行内存主要用来存储任务在执行 Shuffle 时占用的内存，Shuffle 是按照一定规则对 RDD 数据重新分区的过程，我们来看 Shuffle 的 Write 和 Read 两阶段对执行内存的使用： Shuffle Write 若在 map 端选择普通的排序方式，会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间。 若在 map 端选择 Tungsten 的排序方式，则采用 ShuffleExternalSorter 直接对以序列化形式存储的数据排序，在内存中存储数据时可以占用堆外或堆内执行空间，取决于用户是否开启了堆外内存以及堆外执行内存是否足够。 Shuffle Read 在对 reduce 端的数据进行聚合时，要将数据交给 Aggregator 处理，在内存中存储数据时占用堆内执行空间。 如果需要进行最终结果排序，则要将再次将数据交给 ExternalSorter 处理，占用堆内执行空间。 在 ExternalSorter 和 Aggregator 中，Spark 会使用一种叫 AppendOnlyMap 的哈希表在堆内执行内存中存储数据，但在 Shuffle 过程中所有数据并不能都保存到该哈希表中，当这个哈希表占用的内存会进行周期性地采样估算，当其大到一定程度，无法再从 MemoryManager 申请到新的执行内存时，Spark 就会将其全部内容存储到磁盘文件中，这个过程被称为溢存(Spill)，溢存到磁盘的文件最后会被归并(Merge)。 Shuffle Write 阶段中用到的 Tungsten 是 Databricks 公司提出的对 Spark 优化内存和 CPU 使用的计划[9]，解决了一些 JVM 在性能上的限制和弊端。Spark 会根据 Shuffle 的情况来自动选择是否采用 Tungsten 排序。Tungsten 采用的页式内存管理机制建立在 MemoryManager 之上，即 Tungsten 对执行内存的使用进行了一步的抽象，这样在 Shuffle 过程中无需关心数据具体存储在堆内还是堆外。每个内存页用一个 MemoryBlock 来定义，并用 Object obj 和 long offset 这两个变量统一标识一个内存页在系统内存中的地址。堆内的 MemoryBlock 是以 long 型数组的形式分配的内存，其 obj 的值为是这个数组的对象引用，offset 是 long 型数组的在 JVM 中的初始偏移地址，两者配合使用可以定位这个数组在堆内的绝对地址；堆外的 MemoryBlock 是直接申请到的内存块，其 obj 为 null，offset 是这个内存块在系统内存中的 64 位绝对地址。Spark 用 MemoryBlock 巧妙地将堆内和堆外内存页统一抽象封装，并用页表(pageTable)管理每个 Task 申请到的内存页。 Tungsten 页式管理下的所有内存用 64 位的逻辑地址表示，由页号和页内偏移量组成： 页号：占 13 位，唯一标识一个内存页，Spark 在申请内存页之前要先申请空闲页号。 页内偏移量：占 51 位，是在使用内存页存储数据时，数据在页内的偏移地址。 有了统一的寻址方式，Spark 可以用 64 位逻辑地址的指针定位到堆内或堆外的内存，整个 Shuffle Write 排序的过程只需要对指针进行排序，并且无需反序列化，整个过程非常高效，对于内存访问效率和 CPU 使用效率带来了明显的提升[10]。 Spark 的存储内存和执行内存有着截然不同的管理方式：对于存储内存来说，Spark 用一个 LinkedHashMap 来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成；而对于执行内存，Spark 用 AppendOnlyMap 来存储 Shuffle 过程中的数据，在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制。 转自：https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （十）SparkCore的调优之Shuffle调优","slug":"2019-06-10-Spark学习之路 （十）SparkCore的调优之Shuffle调优","date":"2019-06-10T02:30:04.000Z","updated":"2019-09-17T00:42:16.880Z","comments":true,"path":"2019-06-10-Spark学习之路 （十）SparkCore的调优之Shuffle调优.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-10-Spark学习之路 （十）SparkCore的调优之Shuffle调优.html","excerpt":"** Spark学习之路 （十）SparkCore的调优之Shuffle调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。","text":"** Spark学习之路 （十）SparkCore的调优之Shuffle调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。 &lt;The rest of contents | 余下全文&gt; 一、shuffle的定义Spark的运行主要分为2部分： 一部分是驱动程序，其核心是SparkContext； 另一部分是Worker节点上Task,它是运行实际任务的。程序运行的时候，Driver和Executor进程相互交互：运行什么任务，即Driver会分配Task到Executor，Driver 跟 Executor 进行网络传输; 任务数据从哪儿获取，即Task要从 Driver 抓取其他上游的 Task 的数据结果，所以有这个过程中就不断的产生网络结果。其中，下一个 Stage 向上一个 Stage 要数据这个过程，我们就称之为 Shuffle。 二、ShuffleManager发展概述​ 在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。 在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。 因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。 下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。 三、HashShuffleManager的运行原理在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。 在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。 因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。 下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。 3.1 未经优化的HashShuffleManager图解说明 文字说明上图说明了未经优化的HashShuffleManager的原理。这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。 我们先从shuffle write开始说起。shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。 那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢？很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。 接着我们来说说shuffle read。shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。 shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。 3.2 优化后的HashShuffleManager图解说明 文字说明上图说明了优化后的HashShuffleManager的原理。这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。 开启consolidate机制之后，在shuffle write过程中，task就不是为下游stage的每个task创建一个磁盘文件了。此时会出现shuffleFileGroup的概念，每个shuffleFileGroup会对应一批磁盘文件，磁盘文件的数量与下游stage的task数量是相同的。一个Executor上有多少个CPU core，就可以并行执行多少个task。而第一批并行执行的每个task都会创建一个shuffleFileGroup，并将数据写入对应的磁盘文件内。 当Executor的CPU core执行完一批task，接着执行下一批task时，下一批task就会复用之前已有的shuffleFileGroup，包括其中的磁盘文件。也就是说，此时task会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。 假设第二个stage有100个task，第一个stage有50个task，总共还是有10个Executor，每个Executor执行5个task。那么原本使用未经优化的HashShuffleManager时，每个Executor会产生500个磁盘文件，所有Executor会产生5000个磁盘文件的。但是此时经过优化之后，每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。也就是说，每个Executor此时只会创建100个磁盘文件，所有Executor只会创建1000个磁盘文件。 四、SortShuffleManager运行原理SortShuffleManager的运行机制主要分成两种，一种是普通运行机制，另一种是bypass运行机制。当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。 4.1 普通运行机制图解说明 文字说明上图说明了普通的SortShuffleManager的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。 在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。 一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。 SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于每个task最终只有一个磁盘文件，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。 4.2 bypass运行机制图解说明 文字说明上图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下： shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。 不是聚合类的shuffle算子（比如reduceByKey）。 此时task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。 该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。 而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。 五、shuffle相关参数调优以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。 Spark各个版本的参数默认值可能会有不同，具体使用请参考官方网站的说明： （1）先选择对应的Spark版本：http://spark.apache.org/documentation.html （2）再查看对应的文档说明 spark.shuffle.file.buffer 默认值：32k 参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 spark.reducer.maxSizeInFlight 默认值：48m 参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 spark.shuffle.io.maxRetries 默认值：3 参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。 调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。 spark.shuffle.io.retryWait 默认值：5s 参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。 调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。 spark.shuffle.memoryFraction（已经弃用） 默认值：0.2 参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。 调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。 spark.shuffle.manager（已经弃用） 默认值：sort 参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。 调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。 spark.shuffle.sort.bypassMergeThreshold 默认值：200 参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。 调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。 spark.shuffle.consolidateFiles（已经弃用） 默认值：false 参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。 调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （九）SparkCore的调优之数据倾斜调优","slug":"2019-06-09-Spark学习之路 （九）SparkCore的调优之数据倾斜调优","date":"2019-06-09T02:30:04.000Z","updated":"2019-09-17T00:27:40.324Z","comments":true,"path":"2019-06-09-Spark学习之路 （九）SparkCore的调优之数据倾斜调优.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-09-Spark学习之路 （九）SparkCore的调优之数据倾斜调优.html","excerpt":"** Spark学习之路 （九）SparkCore的调优之数据倾斜调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。","text":"** Spark学习之路 （九）SparkCore的调优之数据倾斜调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。 &lt;The rest of contents | 余下全文&gt; 数据倾斜发生时的现象 绝大多数task执行得都非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。 原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。 数据倾斜发生的原理数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。 因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。 下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。 如何定位导致数据倾斜的代码数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。 某个task执行特别慢的情况首先要看的，就是数据倾斜发生在第几个stage中。 如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。 比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。 知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。 这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。 stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。 stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。 123456789val conf = new SparkConf()val sc = new SparkContext(conf)val lines = sc.textFile(\"hdfs://...\")val words = lines.flatMap(_.split(\" \"))val pairs = words.map((_, 1))val wordCounts = pairs.reduceByKey(_ + _)wordCounts.collect().foreach(println(_)) ​ 通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由reduceByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。 某个task莫名其妙内存溢出的情况​ 这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。 但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。 查看导致数据倾斜的key的数据分布情况​ 知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。 此时根据你执行操作的情况不同，可以有很多种查看key分布的方式： 如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。 如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。 举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。 123val sampledPairs = pairs.sample(false, 0.1)val sampledWordCounts = sampledPairs.countByKey()sampledWordCounts.foreach(println(_)) 数据倾斜的解决方案解决方案一：使用Hive ETL预处理数据方案适用场景：导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。 方案实现思路：此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。 方案实现原理：这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。 方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。 方案缺点：治标不治本，Hive ETL中还是会发生数据倾斜。 方案实践经验：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。 项目实践经验：在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。 解决方案二：过滤少数导致倾斜的key方案适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。 方案实现思路：如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。 方案实现原理：将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。 方案优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。 方案缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。 方案实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。 解决方案三：提高shuffle操作的并行度方案适用场景：如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。 方案实现思路：在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。 方案实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。 方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。 方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。 方案实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。 解决方案四：两阶段聚合（局部聚合+全局聚合）方案适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。 方案实现思路：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。 方案实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。 方案优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。 方案缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 第一步，给RDD中的每个key都打上一个随机前缀。JavaPairRDD&lt;String, Long&gt; randomPrefixRdd = rdd.mapToPair( new PairFunction&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(10); return new Tuple2&lt;String, Long&gt;(prefix + \"_\" + tuple._1, tuple._2); &#125; &#125;);// 第二步，对打上随机前缀的key进行局部聚合。JavaPairRDD&lt;String, Long&gt; localAggrRdd = randomPrefixRdd.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;);// 第三步，去除RDD中每个key的随机前缀。JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair( new PairFunction&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;String, Long&gt; tuple) throws Exception &#123; long originalKey = Long.valueOf(tuple._1.split(\"_\")[1]); return new Tuple2&lt;Long, Long&gt;(originalKey, tuple._2); &#125; &#125;);// 第四步，对去除了随机前缀的RDD进行全局聚合。JavaPairRDD&lt;Long, Long&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;); 解决方案五：将reduce join转为map join方案适用场景：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。 方案实现思路：不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。 方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。 方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。 方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。 123456789101112131415161718192021222324252627282930313233// 首先将数据量比较小的RDD的数据，collect到Driver中来。List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。// 可以尽可能节省内存空间，并且减少网络传输性能开销。final Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);// 对另外一个RDD执行map类操作，而不再是join类操作。JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; // 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。 List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value(); // 可以将rdd1的数据转换为一个Map，便于后面进行join操作。 Map&lt;Long, Row&gt; rdd1DataMap = new HashMap&lt;Long, Row&gt;(); for(Tuple2&lt;Long, Row&gt; data : rdd1Data) &#123; rdd1DataMap.put(data._1, data._2); &#125; // 获取当前RDD数据的key以及value。 String key = tuple._1; String value = tuple._2; // 从rdd1数据Map中，根据key获取到可以join到的数据。 Row rdd1Value = rdd1DataMap.get(key); return new Tuple2&lt;String, String&gt;(key, new Tuple2&lt;String, Row&gt;(value, rdd1Value)); &#125; &#125;);// 这里得提示一下。// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。// rdd2中每条数据都可能会返回多条join后的数据。 解决方案六：采样倾斜key并分拆join操作方案适用场景：两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。 方案实现思路： 对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。 然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。 接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。 再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。 而另外两个普通的RDD就照常join即可。 最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。 方案实现原理：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。 方案优点：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。 方案缺点：如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。JavaPairRDD&lt;Long, String&gt; sampledRDD = rdd1.sample(false, 0.1);// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。JavaPairRDD&lt;Long, Long&gt; mappedSampledRDD = sampledRDD.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return new Tuple2&lt;Long, Long&gt;(tuple._1, 1L); &#125; &#125;);JavaPairRDD&lt;Long, Long&gt; countedSampledRDD = mappedSampledRDD.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;);JavaPairRDD&lt;Long, Long&gt; reversedSampledRDD = countedSampledRDD.mapToPair( new PairFunction&lt;Tuple2&lt;Long,Long&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple) throws Exception &#123; return new Tuple2&lt;Long, Long&gt;(tuple._2, tuple._1); &#125; &#125;);final Long skewedUserid = reversedSampledRDD.sortByKey(false).take(1).get(0)._2;// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。JavaPairRDD&lt;Long, String&gt; skewedRDD = rdd1.filter( new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return tuple._1.equals(skewedUserid); &#125; &#125;);// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。JavaPairRDD&lt;Long, String&gt; commonRDD = rdd1.filter( new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return !tuple._1.equals(skewedUserid); &#125; &#125;);// rdd2，就是那个所有key的分布相对较为均匀的rdd。// 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。// 对扩容的每条数据，都打上0～100的前缀。JavaPairRDD&lt;String, Row&gt; skewedRdd2 = rdd2.filter( new Function&lt;Tuple2&lt;Long,Row&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; return tuple._1.equals(skewedUserid); &#125; &#125;).flatMapToPair(new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call( Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; Random random = new Random(); List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;(); for(int i = 0; i &lt; 100; i++) &#123; list.add(new Tuple2&lt;String, Row&gt;(i + \"_\" + tuple._1, tuple._2)); &#125; return list; &#125; &#125;);// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD1 = skewedRDD.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(100); return new Tuple2&lt;String, String&gt;(prefix + \"_\" + tuple._1, tuple._2); &#125; &#125;) .join(skewedUserid2infoRDD) .mapToPair(new PairFunction&lt;Tuple2&lt;String,Tuple2&lt;String,Row&gt;&gt;, Long, Tuple2&lt;String, Row&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt; call( Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; tuple) throws Exception &#123; long key = Long.valueOf(tuple._1.split(\"_\")[1]); return new Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt;(key, tuple._2); &#125; &#125;);// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);// 将倾斜key join后的结果与普通key join后的结果，uinon起来。// 就是最终的join结果。JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2); 解决方案七：使用随机前缀和扩容RDD进行join方案适用场景：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。 方案实现思路： 该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。 然后将该RDD的每条数据都打上一个n以内的随机前缀。 同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。 最后将两个处理后的RDD进行join即可。 方案实现原理：将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。 方案优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。 方案缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。 方案实践经验：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。 123456789101112131415161718192021222324252627282930// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。JavaPairRDD&lt;String, Row&gt; expandedRDD = rdd1.flatMapToPair( new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;(); for(int i = 0; i &lt; 100; i++) &#123; list.add(new Tuple2&lt;String, Row&gt;(0 + \"_\" + tuple._1, tuple._2)); &#125; return list; &#125; &#125;);// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。JavaPairRDD&lt;String, String&gt; mappedRDD = rdd2.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(100); return new Tuple2&lt;String, String&gt;(prefix + \"_\" + tuple._1, tuple._2); &#125; &#125;);// 将两个处理后的RDD进行join即可。JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD); 解决方案八：多种方案组合使用​ 在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，预处理一部分数据，并过滤一部分数据来缓解；其次可以对某些shuffle操作提升并行度，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"cache和persist的区别","slug":"2019-06-08-cache和persist的区别","date":"2019-06-08T03:30:04.000Z","updated":"2019-09-16T17:31:38.817Z","comments":true,"path":"2019-06-08-cache和persist的区别.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-08-cache和persist的区别.html","excerpt":"** cache和persist的区别：** &lt;Excerpt in index | 首页摘要&gt; cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间。","text":"** cache和persist的区别：** &lt;Excerpt in index | 首页摘要&gt; cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间。 &lt;The rest of contents | 余下全文&gt; cache和persist的区别基于Spark 1.6.1 的源码，可以看到 12/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */def cache(): this.type = persist() 说明是cache()调用了persist(), 想要知道二者的不同还需要看一下persist函数： 12/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) 可以看到persist()内部调用了persist(StorageLevel.MEMORY_ONLY)，继续深入： 1234567891011121314151617/** * Set this RDD's storage level to persist its values across operations after the first time * it is computed. This can only be used to assign a new storage level if the RDD does not * have a storage level set yet.. */def persist(newLevel: StorageLevel): this.type = &#123; // TODO: Handle changes of StorageLevel if (storageLevel != StorageLevel.NONE &amp;&amp; newLevel != storageLevel) &#123; throw new UnsupportedOperationException( \"Cannot change storage level of an RDD after it was already assigned a level\") &#125; sc.persistRDD(this) // Register the RDD with the ContextCleaner for automatic GC-based cleanup sc.cleaner.foreach(_.registerRDDForCleanup(this)) storageLevel = newLevel this&#125; 可以看出来persist有一个 StorageLevel 类型的参数，这个表示的是RDD的缓存级别。 至此便可得出cache和persist的区别了：cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。 RDD的缓存级别顺便看一下RDD都有哪些缓存级别，查看 StorageLevel 类的源码： 123456789101112131415object StorageLevel &#123; val NONE = new StorageLevel(false, false, false, false) val DISK_ONLY = new StorageLevel(true, false, false, false) val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) val MEMORY_ONLY = new StorageLevel(false, true, false, true) val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) val OFF_HEAP = new StorageLevel(false, false, true, false) ......&#125; 可以看到这里列出了12种缓存级别，但这些有什么区别呢？可以看到每个缓存级别后面都跟了一个StorageLevel的构造函数，里面包含了4个或5个参数，如下 1val MEMORY_ONLY = new StorageLevel(false, true, false, true) 查看其构造函数 123456789101112131415class StorageLevel private( private var _useDisk: Boolean, private var _useMemory: Boolean, private var _useOffHeap: Boolean, private var _deserialized: Boolean, private var _replication: Int = 1) extends Externalizable &#123; ...... def useDisk: Boolean = _useDisk def useMemory: Boolean = _useMemory def useOffHeap: Boolean = _useOffHeap def deserialized: Boolean = _deserialized def replication: Int = _replication ......&#125; 可以看到StorageLevel类的主构造器包含了5个参数： useDisk：使用硬盘（外存） useMemory：使用内存 useOffHeap：使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。 deserialized：反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象 replication：备份数（在多个节点上备份） 理解了这5个参数，StorageLevel 的12种缓存级别就不难理解了。 val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) 就表示使用这种缓存级别的RDD将存储在硬盘以及内存中，使用序列化（在硬盘中），并且在多个节点上备份2份（正常的RDD只有一份） 另外还注意到有一种特殊的缓存级别 1val OFF_HEAP = new StorageLevel(false, false, true, false) 使用了堆外内存，StorageLevel 类的源码中有一段代码可以看出这个的特殊性，它不能和其它几个参数共存。 123456if (useOffHeap) &#123; require(!useDisk, \"Off-heap storage level does not support using disk\") require(!useMemory, \"Off-heap storage level does not support using heap memory\") require(!deserialized, \"Off-heap storage level does not support deserialized storage\") require(replication == 1, \"Off-heap storage level does not support multiple replication\")&#125;","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （八）SparkCore的调优之开发调优","slug":"2019-06-08-Spark学习之路 （八）SparkCore的调优之开发调优","date":"2019-06-08T02:30:04.000Z","updated":"2019-09-16T15:05:55.685Z","comments":true,"path":"2019-06-08-Spark学习之路 （八）SparkCore的调优之开发调优.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-08-Spark学习之路 （八）SparkCore的调优之开发调优.html","excerpt":"** Spark学习之路 （八）SparkCore的调优之开发调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。 ​ 然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。","text":"** Spark学习之路 （八）SparkCore的调优之开发调优：** &lt;Excerpt in index | 首页摘要&gt; ​ 在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。 ​ 然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。 &lt;The rest of contents | 余下全文&gt; 前言​ Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。 ​ 笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。 本文作为Spark性能优化指南的基础篇，主要讲解开发调优以及资源调优。 调优概述​ Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。 原则一：避免创建重复的RDD​ 通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。 我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。 一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。 一个简单的例子1234567891011121314151617// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。val rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\")rdd1.map(...)val rdd2 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\")rdd2.reduce(...)// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。val rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\")rdd1.map(...)rdd1.reduce(...) 原则二：尽可能复用同一个RDD​ 除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。 一个简单的例子1234567891011121314151617181920212223242526// 错误的做法。// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。JavaPairRDD&lt;Long, String&gt; rdd1 = ...JavaRDD&lt;String&gt; rdd2 = rdd1.map(...)// 分别对rdd1和rdd2执行了不同的算子操作。rdd1.reduceByKey(...)rdd2.map(...)// 正确的做法。// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。// 其实在这种情况下完全可以复用同一个RDD。// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。JavaPairRDD&lt;Long, String&gt; rdd1 = ...rdd1.reduceByKey(...)rdd1.map(tuple._2...)// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。 原则三：对多次使用的RDD进行持久化​ 当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。 ​ Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。 ​ 因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。 对多次使用的RDD进行持久化的代码示例1234567891011121314151617// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。// 正确的做法。// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。val rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\").cache()rdd1.map(...)rdd1.reduce(...)// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。val rdd1 = sc.textFile(\"hdfs://192.168.0.1:9000/hello.txt\").persist(StorageLevel.MEMORY_AND_DISK_SER)rdd1.map(...)rdd1.reduce(...) 对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。 Spark的持久化级别 持久化级别 含义解释 MEMORY_ONLY 使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。 MEMORY_AND_DISK 使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。 MEMORY_ONLY_SER 基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 MEMORY_AND_DISK_SER 基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 DISK_ONLY 使用未序列化的Java对象格式，将数据全部写入磁盘文件中。 MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等. 对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。 如何选择一种最合适的持久化策略 默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。 如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。 如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。 通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。 原则四：尽量避免使用shuffle类算子​ 如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。 shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。 因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。 Broadcast与map进行join代码示例12345678910111213141516// 传统的join操作会导致shuffle操作。// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。val rdd3 = rdd1.join(rdd2)// Broadcast+map的join操作，不会导致shuffle操作。// 使用Broadcast将一个数据量较小的RDD作为广播变量。val rdd2Data = rdd2.collect()val rdd2DataBroadcast = sc.broadcast(rdd2Data)// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。val rdd3 = rdd1.map(rdd2DataBroadcast...)// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。 原则五：使用map-side预聚合的shuffle操作如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。 ​ 所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。 比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。 原则六：使用高性能的算子除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。 使用reduceByKey/aggregateByKey替代groupByKey详情见“原则五：使用map-side预聚合的shuffle操作”。 使用mapPartitions替代普通map​ mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！ 使用foreachPartitions替代foreach原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。 使用filter之后进行coalesce操作通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。 使用repartitionAndSortWithinPartitions替代repartition与sort类操作repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。 原则七：广播大变量有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。 在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。 因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。 广播大变量的代码示例123456789101112// 以下代码在算子函数中，使用了外部的变量。// 此时没有做任何特殊操作，每个task都会有一份list1的副本。val list1 = ...rdd1.map(list1...)// 以下代码将list1封装成了Broadcast类型的广播变量。// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。// 每个Executor内存中，就只会驻留一份广播变量副本。val list1 = ...val list1Broadcast = sc.broadcast(list1)rdd1.map(list1Broadcast...) 原则八：使用Kryo优化序列化性能在Spark中，主要有三个地方涉及到了序列化： 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。 对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。 以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）： 123456// 创建SparkConf对象。val conf = new SparkConf().setMaster(...).setAppName(...)// 设置序列化器为KryoSerializer。conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")// 注册要序列化的自定义类型。conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2])) 原则九：优化数据结构Java中，有三种类型比较耗费内存： 对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。 字符串，每个字符串内部都有一个字符数组以及长度等额外信息。 集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。 因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。 但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。 原则十：Data Locality本地化级别PROCESS_LOCAL：进程本地化，代码和数据在同一个进程中，也就是在同一个executor中；计算数据的task由executor执行，数据在executor的BlockManager中；性能最好 NODE_LOCAL：节点本地化，代码和数据在同一个节点中；比如说，数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行；或者是，数据和task在一个节点上的不同executor中；数据需要在进程间进行传输NO_PREF：对于task来说，数据从哪里获取都一样，没有好坏之分RACK_LOCAL：机架本地化，数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输ANY：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差 spark.locality.wait，默认是3s Spark在Driver上，对Application的每一个stage的task，进行分配之前，都会计算出每个task要计算的是哪个分片数据，RDD的某个partition；Spark的task分配算法，优先，会希望每个task正好分配到它要计算的数据所在的节点，这样的话，就不用在网络间传输数据； 但是可能task没有机会分配到它的数据所在的节点，因为可能那个节点的计算资源和计算能力都满了；所以呢，这种时候，通常来说，Spark会等待一段时间，默认情况下是3s钟（不是绝对的，还有很多种情况，对不同的本地化级别，都会去等待），到最后，实在是等待不了了，就会选择一个比较差的本地化级别，比如说，将task分配到靠它要计算的数据所在节点，比较近的一个节点，然后进行计算。 但是对于第二种情况，通常来说，肯定是要发生数据传输，task会通过其所在节点的BlockManager来获取数据，BlockManager发现自己本地没有数据，会通过一个getRemote()方法，通过TransferService（网络数据传输组件）从数据所在节点的BlockManager中，获取数据，通过网络传输回task所在节点。 对于我们来说，当然不希望是类似于第二种情况的了。最好的，当然是task和数据在一个节点上，直接从本地executor的BlockManager中获取数据，纯内存，或者带一点磁盘IO；如果要通过网络传输数据的话，那么实在是，性能肯定会下降的，大量网络传输，以及磁盘IO，都是性能的杀手。 什么时候要调节这个参数？ 观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。日志里面会显示，starting task。。。，PROCESS LOCAL、NODE LOCAL，观察大部分task的数据本地化级别。 如果大多都是PROCESS_LOCAL，那就不用调节了如果是发现，好多的级别都是NODE_LOCAL、ANY，那么最好就去调节一下数据本地化的等待时长调节完，应该是要反复调节，每次调节完以后，再来运行，观察日志看看大部分的task的本地化级别有没有提升；看看，整个spark作业的运行时间有没有缩短 但是注意别本末倒置，本地化级别倒是提升了，但是因为大量的等待时长，spark作业的运行时间反而增加了，那就还是不要调节了。 spark.locality.wait，默认是3s；可以改成6s，10s 默认情况下，下面3个的等待时长，都是跟上面那个是一样的，都是3s 123spark.locality.wait.process//建议60sspark.locality.wait.node//建议30sspark.locality.wait.rack//建议20s","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （七）Spark 运行流程","slug":"2019-06-07-Spark学习之路 （七）Spark 运行流程","date":"2019-06-07T02:30:04.000Z","updated":"2019-09-16T15:05:27.815Z","comments":true,"path":"2019-06-07-Spark学习之路 （七）Spark 运行流程.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-07-Spark学习之路 （七）Spark 运行流程.html","excerpt":"** Spark学习之路 （七）Spark 运行流程：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （七）Spark 运行流程","text":"** Spark学习之路 （七）Spark 运行流程：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （七）Spark 运行流程 &lt;The rest of contents | 余下全文&gt; 一、Spark中的基本概念（1）Application：表示你的应用程序 （2）Driver：表示main()函数，创建SparkContext。由SparkContext负责与ClusterManager通信，进行资源的申请，任务的分配和监控等。程序执行完毕后关闭SparkContext （3）Executor：某个Application运行在Worker节点上的一个进程，该进程负责运行某些task，并且负责将数据存在内存或者磁盘上。在Spark on Yarn模式下，其进程名称为 CoarseGrainedExecutor Backend，一个CoarseGrainedExecutor Backend进程有且仅有一个executor对象，它负责将Task包装成taskRunner，并从线程池中抽取出一个空闲线程运行Task，这样，每个CoarseGrainedExecutorBackend能并行运行Task的数据就取决于分配给它的CPU的个数。 （4）Worker：集群中可以运行Application代码的节点。在Standalone模式中指的是通过slave文件配置的worker节点，在Spark on Yarn模式中指的就是NodeManager节点。 （5）Task：在Executor进程中执行任务的工作单元，多个Task组成一个Stage （6）Job：包含多个Task组成的并行计算，是由Action行为触发的 （7）Stage：每个Job会被拆分很多组Task，作为一个TaskSet，其名称为Stage （8）DAGScheduler：根据Job构建基于Stage的DAG，并提交Stage给TaskScheduler，其划分Stage的依据是RDD之间的依赖关系 （9）TaskScheduler：将TaskSet提交给Worker（集群）运行，每个Executor运行什么Task就是在此处分配的。 二、Spark的运行流程2.1 Spark的基本运行流程1、说明 (1)构建Spark Application的运行环境（启动SparkContext），SparkContext向资源管理器（可以是Standalone、Mesos或YARN）注册并申请运行Executor资源； (2)资源管理器分配Executor资源并启动StandaloneExecutorBackend，Executor运行情况将随着心跳发送到资源管理器上； (3)SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler。Executor向SparkContext申请Task (4)Task Scheduler将Task发放给Executor运行同时SparkContext将应用程序代码发放给Executor。 (5)Task在Executor上运行，运行完毕释放所有资源。 2、图解 3、Spark运行架构特点 （1）每个Application获取专属的executor进程，该进程在Application期间一直驻留，并以多线程方式运行tasks。这种Application隔离机制有其优势的，无论是从调度角度看（每个Driver调度它自己的任务），还是从运行角度看（来自不同Application的Task运行在不同的JVM中）。当然，这也意味着Spark Application不能跨应用程序共享数据，除非将数据写入到外部存储系统。 （2）Spark与资源管理器无关，只要能够获取executor进程，并能保持相互通信就可以了。 （3）提交SparkContext的Client应该靠近Worker节点（运行Executor的节点)，最好是在同一个Rack里，因为Spark Application运行过程中SparkContext和Executor之间有大量的信息交换；如果想在远程集群中运行，最好使用RPC将SparkContext提交给集群，不要远离Worker运行SparkContext。 （4）Task采用了数据本地性和推测执行的优化机制。 4、DAGSchedulerJob=多个stage，Stage=多个同种task, Task分为ShuffleMapTask和ResultTask，Dependency分为ShuffleDependency和NarrowDependency 面向stage的切分，切分依据为宽依赖 维护waiting jobs和active jobs，维护waiting stages、active stages和failed stages，以及与jobs的映射关系 主要职能： 1、接收提交Job的主入口，submitJob(rdd, ...)或runJob(rdd, ...)。在SparkContext里会调用这两个方法。 生成一个Stage并提交，接着判断Stage是否有父Stage未完成，若有，提交并等待父Stage，以此类推。结果是：DAGScheduler里增加了一些waiting stage和一个running stage。 running stage提交后，分析stage里Task的类型，生成一个Task描述，即TaskSet。 调用TaskScheduler.submitTask(taskSet, ...)方法，把Task描述提交给TaskScheduler。TaskScheduler依据资源量和触发分配条件，会为这个TaskSet分配资源并触发执行。 DAGScheduler提交job后，异步返回JobWaiter对象，能够返回job运行状态，能够cancel job，执行成功后会处理并返回结果 2、处理TaskCompletionEvent 如果task执行成功，对应的stage里减去这个task，做一些计数工作： 如果task是ResultTask，计数器Accumulator加一，在job里为该task置true，job finish总数加一。加完后如果finish数目与partition数目相等，说明这个stage完成了，标记stage完成，从running stages里减去这个stage，做一些stage移除的清理工作 如果task是ShuffleMapTask，计数器Accumulator加一，在stage里加上一个output location，里面是一个MapStatus类。MapStatus是ShuffleMapTask执行完成的返回，包含location信息和block size(可以选择压缩或未压缩)。同时检查该stage完成，向MapOutputTracker注册本stage里的shuffleId和location信息。然后检查stage的output location里是否存在空，若存在空，说明一些task失败了，整个stage重新提交；否则，继续从waiting stages里提交下一个需要做的stage 如果task是重提交，对应的stage里增加这个task 如果task是fetch失败，马上标记对应的stage完成，从running stages里减去。如果不允许retry，abort整个stage；否则，重新提交整个stage。另外，把这个fetch相关的location和map任务信息，从stage里剔除，从MapOutputTracker注销掉。最后，如果这次fetch的blockManagerId对象不为空，做一次ExecutorLost处理，下次shuffle会换在另一个executor上去执行。 其他task状态会由TaskScheduler处理，如Exception, TaskResultLost, commitDenied等。 3、其他与job相关的操作还包括：cancel job， cancel stage, resubmit failed stage等 其他职能： cacheLocations 和 preferLocation 5、TaskScheduler维护task和executor对应关系，executor和物理资源对应关系，在排队的task和正在跑的task。 内部维护一个任务队列，根据FIFO或Fair策略，调度任务。 TaskScheduler本身是个接口，spark里只实现了一个TaskSchedulerImpl，理论上任务调度可以定制。 主要功能： 1、submitTasks(taskSet)，接收DAGScheduler提交来的tasks 为tasks创建一个TaskSetManager，添加到任务队列里。TaskSetManager跟踪每个task的执行状况，维护了task的许多具体信息。 触发一次资源的索要。 首先，TaskScheduler对照手头的可用资源和Task队列，进行executor分配(考虑优先级、本地化等策略)，符合条件的executor会被分配给TaskSetManager。 然后，得到的Task描述交给SchedulerBackend，调用launchTask(tasks)，触发executor上task的执行。task描述被序列化后发给executor，executor提取task信息，调用task的run()方法执行计算。 2、cancelTasks(stageId)，取消一个stage的tasks 调用SchedulerBackend的killTask(taskId, executorId, ...)方法。taskId和executorId在TaskScheduler里一直维护着。 3、resourceOffer(offers: Seq[Workers])，这是非常重要的一个方法，调用者是SchedulerBacnend，用途是底层资源SchedulerBackend把空余的workers资源交给TaskScheduler，让其根据调度策略为排队的任务分配合理的cpu和内存资源，然后把任务描述列表传回给SchedulerBackend 从worker offers里，搜集executor和host的对应关系、active executors、机架信息等等 worker offers资源列表进行随机洗牌，任务队列里的任务列表依据调度策略进行一次排序 遍历每个taskSet，按照进程本地化、worker本地化、机器本地化、机架本地化的优先级顺序，为每个taskSet提供可用的cpu核数，看是否满足 默认一个task需要一个cpu，设置参数为&quot;spark.task.cpus=1&quot; 为taskSet分配资源，校验是否满足的逻辑，最终在TaskSetManager的resourceOffer(execId, host, maxLocality)方法里 满足的话，会生成最终的任务描述，并且调用DAGScheduler的taskStarted(task, info)方法，通知DAGScheduler，这时候每次会触发DAGScheduler做一次submitMissingStage的尝试，即stage的tasks都分配到了资源的话，马上会被提交执行 4、statusUpdate(taskId, taskState, data),另一个非常重要的方法，调用者是SchedulerBacnend，用途是SchedulerBacnend会将task执行的状态汇报给TaskScheduler做一些决定 若TaskLost，找到该task对应的executor，从active executor里移除，避免这个executor被分配到其他task继续失败下去。 task finish包括四种状态：finished, killed, failed, lost。只有finished是成功执行完成了。其他三种是失败。 task成功执行完，调用TaskResultGetter.enqueueSuccessfulTask(taskSet, tid, data)，否则调用TaskResultGetter.enqueueFailedTask(taskSet, tid, state, data)。TaskResultGetter内部维护了一个线程池，负责异步fetch task执行结果并反序列化。默认开四个线程做这件事，可配参数&quot;spark.resultGetter.threads&quot;=4。 TaskResultGetter取task result的逻辑 1、对于success task，如果taskResult里的数据是直接结果数据，直接把data反序列出来得到结果；如果不是，会调用blockManager.getRemoteBytes(blockId)从远程获取。如果远程取回的数据是空的，那么会调用TaskScheduler.handleFailedTask，告诉它这个任务是完成了的但是数据是丢失的。否则，取到数据之后会通知BlockManagerMaster移除这个block信息，调用TaskScheduler.handleSuccessfulTask，告诉它这个任务是执行成功的，并且把result data传回去。 2、对于failed task，从data里解析出fail的理由，调用TaskScheduler.handleFailedTask，告诉它这个任务失败了，理由是什么。 6、SchedulerBackend在TaskScheduler下层，用于对接不同的资源管理系统，SchedulerBackend是个接口，需要实现的主要方法如下： 12345def start(): Unitdef stop(): Unitdef reviveOffers(): Unit // 重要方法：SchedulerBackend把自己手头上的可用资源交给TaskScheduler，TaskScheduler根据调度策略分配给排队的任务吗，返回一批可执行的任务描述，SchedulerBackend负责launchTask，即最终把task塞到了executor模型上，executor里的线程池会执行task的run()def killTask(taskId: Long, executorId: String, interruptThread: Boolean): Unit = throw new UnsupportedOperationException 粗粒度：进程常驻的模式，典型代表是standalone模式，mesos粗粒度模式，yarn 细粒度：mesos细粒度模式 这里讨论粗粒度模式，更好理解：CoarseGrainedSchedulerBackend。 维护executor相关信息(包括executor的地址、通信端口、host、总核数，剩余核数)，手头上executor有多少被注册使用了，有多少剩余，总共还有多少核是空的等等。 主要职能 1、Driver端主要通过actor监听和处理下面这些事件： RegisterExecutor(executorId, hostPort, cores, logUrls)。这是executor添加的来源，通常worker拉起、重启会触发executor的注册。CoarseGrainedSchedulerBackend把这些executor维护起来，更新内部的资源信息，比如总核数增加。最后调用一次makeOffer()，即把手头资源丢给TaskScheduler去分配一次，返回任务描述回来，把任务launch起来。这个makeOffer()的调用会出现在任何与资源变化相关的事件中，下面会看到。 StatusUpdate(executorId, taskId, state, data)。task的状态回调。首先，调用TaskScheduler.statusUpdate上报上去。然后，判断这个task是否执行结束了，结束了的话把executor上的freeCore加回去，调用一次makeOffer()。 ReviveOffers。这个事件就是别人直接向SchedulerBackend请求资源，直接调用makeOffer()。 KillTask(taskId, executorId, interruptThread)。这个killTask的事件，会被发送给executor的actor，executor会处理KillTask这个事件。 StopExecutors。通知每一个executor，处理StopExecutor事件。 RemoveExecutor(executorId, reason)。从维护信息中，那这堆executor涉及的资源数减掉，然后调用TaskScheduler.executorLost()方法，通知上层我这边有一批资源不能用了，你处理下吧。TaskScheduler会继续把executorLost的事件上报给DAGScheduler，原因是DAGScheduler关心shuffle任务的output location。DAGScheduler会告诉BlockManager这个executor不可用了，移走它，然后把所有的stage的shuffleOutput信息都遍历一遍，移走这个executor，并且把更新后的shuffleOutput信息注册到MapOutputTracker上，最后清理下本地的CachedLocationsMap。 2、reviveOffers()方法的实现。直接调用了makeOffers()方法，得到一批可执行的任务描述，调用launchTasks。 3、launchTasks(tasks: Seq[Seq[TaskDescription]])方法。 遍历每个task描述，序列化成二进制，然后发送给每个对应的executor这个任务信息 如果这个二进制信息太大，超过了9.2M(默认的akkaFrameSize 10M 减去 默认 为akka留空的200K)，会出错，abort整个taskSet，并打印提醒增大akka frame size 如果二进制数据大小可接受，发送给executor的actor，处理LaunchTask(serializedTask)事件。 7、ExecutorExecutor是spark里的进程模型，可以套用到不同的资源管理系统上，与SchedulerBackend配合使用。 内部有个线程池，有个running tasks map，有个actor，接收上面提到的由SchedulerBackend发来的事件。 事件处理 launchTask。根据task描述，生成一个TaskRunner线程，丢尽running tasks map里，用线程池执行这个TaskRunner killTask。从running tasks map里拿出线程对象，调它的kill方法。 三、Spark在不同集群中的运行架构Spark注重建立良好的生态系统，它不仅支持多种外部文件存储系统，提供了多种多样的集群运行模式。部署在单台机器上时，既可以用本地（Local）模式运行，也可以使用伪分布式模式来运行；当以分布式集群部署的时候，可以根据自己集群的实际情况选择Standalone模式（Spark自带的模式）、YARN-Client模式或者YARN-Cluster模式。Spark的各种运行模式虽然在启动方式、运行位置、调度策略上各有不同，但它们的目的基本都是一致的，就是在合适的位置安全可靠的根据用户的配置和Job的需要运行和管理Task。 3.1 Spark on Standalone运行过程Standalone模式是Spark实现的资源调度框架，其主要的节点有Client节点、Master节点和Worker节点。其中Driver既可以运行在Master节点上中，也可以运行在本地Client端。当用spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行；当使用spark-submit工具提交Job或者在Eclips、IDEA等开发平台上使用”new SparkConf().setMaster(“spark://master:7077”)”方式运行Spark任务时，Driver是运行在本地Client端上的。 运行过程文字说明 1、我们提交一个任务，任务就叫Application2、初始化程序的入口SparkContext， 2.1 初始化DAG Scheduler 2.2 初始化Task Scheduler3、Task Scheduler向master去进行注册并申请资源（CPU Core和Memory）4、Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上获取资源，然后启动StandaloneExecutorBackend；顺便初 始化好了一个线程池5、StandaloneExecutorBackend向Driver(SparkContext)注册,这样Driver就知道哪些Executor为他进行服务了。 到这个时候其实我们的初始化过程基本完成了，我们开始执行transformation的代码，但是代码并不会真正的运行，直到我们遇到一个action操作。生产一个job任务，进行stage的划分6、SparkContext将Applicaiton代码发送给StandaloneExecutorBackend；并且SparkContext解析Applicaiton代码，构建DAG图，并提交给DAG Scheduler分解成Stage（当碰到Action操作 时，就会催生Job；每个Job中含有1个或多个Stage，Stage一般在获取外部数据和shuffle之前产生）。7、将Stage（或者称为TaskSet）提交给Task Scheduler。Task Scheduler负责将Task分配到相应的Worker，最后提交给StandaloneExecutorBackend执行；8、对task进行序列化，并根据task的分配算法，分配task9、对接收过来的task进行反序列化，把task封装成一个线程10、开始执行Task，并向SparkContext报告，直至Task完成。11、资源注销 运行过程图形说明 3.2 Spark on YARN运行过程YARN是一种统一资源管理机制，在其上面可以运行多套计算框架。目前的大数据技术世界，大多数公司除了使用Spark来进行数据计算，由于历史原因或者单方面业务处理的性能考虑而使用着其他的计算框架，比如MapReduce、Storm等计算框架。Spark基于此种情况开发了Spark on YARN的运行模式，由于借助了YARN良好的弹性资源管理机制，不仅部署Application更加方便，而且用户在YARN集群中运行的服务和Application的资源也完全隔离，更具实践应用价值的是YARN可以通过队列的方式，管理同时运行在集群中的多个服务。 Spark on YARN模式根据Driver在集群中的位置分为两种模式：一种是YARN-Client模式，另一种是YARN-Cluster（或称为YARN-Standalone模式）。 3.2.1 YARN框架流程任何框架与YARN的结合，都必须遵循YARN的开发模式。在分析Spark on YARN的实现细节之前，有必要先分析一下YARN框架的一些基本原理。 参考：http://www.cnblogs.com/qingyunzong/p/8615096.html 3.2.2 YARN-ClientYarn-Client模式中，Driver在客户端本地运行，这种模式可以使得Spark Application和客户端进行交互，因为Driver在客户端，所以可以通过webUI访问Driver的状态，默认是http://hadoop1:4040访问，而YARN通过http:// hadoop1:8088访问。 YARN-client的工作流程分为以下几个步骤： 文字说明 1.Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend； 2.ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派； 3.Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）； 4.一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task； 5.Client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务； 6.应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。 图片说明 3.2.3 YARN-Cluster在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动；第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成。 YARN-cluster的工作流程分为以下几个步骤： 文字说明 Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等； ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化； ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束； 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等； ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务； 应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己。 图片说明 3.2.4 YARN-Client 与 YARN-Cluster 区别理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别。 1、YARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业； 2、YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"map与flatMap的区别","slug":"2019-06-06-map与flatMap的区别","date":"2019-06-06T04:30:04.000Z","updated":"2019-09-16T09:39:14.905Z","comments":true,"path":"2019-06-06-map与flatMap的区别.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-06-map与flatMap的区别.html","excerpt":"** map与flatMap的区别：** &lt;Excerpt in index | 首页摘要&gt; ​ map与flatMap的区别","text":"** map与flatMap的区别：** &lt;Excerpt in index | 首页摘要&gt; ​ map与flatMap的区别 &lt;The rest of contents | 余下全文&gt; spark的转换算子中map和flatMap都十分常见，要了解清楚它们的区别，我们必须弄懂每执行一次的数据结构是什么。 we are superman torrow is good color green red 总结：map操作结果：Array[Array[String]] = Array(Array(we, are, superman), Array(torrow, is, good), Array(color, green, red)) flatmap操作结果：Array[String] = Array(we, are, superman, torrow, is, good, color, green, red) spark中map函数会对每一条输入进行指定操作，然后为每一条输入返回一个对象； 而flatmap函数则是两个操作的集合，最后将所有对象合并为一个对象。需要特别说明一下，flatmap适用于统计文件单词类的。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （五）Spark伪分布式安装","slug":"2019-06-05-Spark学习之路 （五）Spark伪分布式安装","date":"2019-06-05T02:30:04.000Z","updated":"2019-09-16T08:27:02.534Z","comments":true,"path":"2019-06-05-Spark学习之路 （五）Spark伪分布式安装.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-05-Spark学习之路 （五）Spark伪分布式安装.html","excerpt":"** Spark学习之路 （五）Spark伪分布式安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （五）Spark伪分布式安装","text":"** Spark学习之路 （五）Spark伪分布式安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （五）Spark伪分布式安装 &lt;The rest of contents | 余下全文&gt; Hadoop部分建议参考hadoop伪分布部署 一、JDK的安装​ LINUX系统安装jdk（最好是1.8版本） 1.1 上传安装包并解压1[root@hadoop1 soft]# tar -zxvf jdk-8u73-linux-x64.tar.gz -C /usr/local/ 1.2 配置环境变量12345[root@hadoop1 soft]# vi /etc/profile#JAVAexport JAVA_HOME=/usr/local/jdk1.8.0_73export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH:$HOME/bin 1.3 验证Java版本1[root@hadoop1 soft]# java -version 二、配置免密登陆2.1 检测正常情况下，本机通过ssh连接自己也是需要输入密码的 2.2 生成私钥和公钥秘钥对1[hadoop@hadoop1 ~]$ ssh-keygen -t rsa 2.3 将公钥添加到authorized_keys1[hadoop@hadoop1 ~]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 2.4 赋予authorized_keys文件600的权限1[hadoop@hadoop1 ~]$ chmod 600 ~/.ssh/authorized_keys 2.5 修改Linux映射文件(root用户)1[root@hadoop1 ~]$ vi /etc/hosts 2.6 验证1[hadoop@hadoop1 ~]$ ssh hadoop1 此时不需要输入密码，免密登录设置成功。 三、安装Hadoop-2.7.53.1 上传解压缩1[hadoop@hadoop1 ~]$ tar -zxvf hadoop-2.7.5-centos-6.7.tar.gz -C apps/ 3.2 创建安装包对应的软连接为解压的hadoop包创建软连接 12345[hadoop@hadoop1 ~]$ cd apps/[hadoop@hadoop1 apps]$ ll总用量 4drwxr-xr-x. 9 hadoop hadoop 4096 12月 24 13:43 hadoop-2.7.5[hadoop@hadoop1 apps]$ ln -s hadoop-2.7.5/ hadoop 3.3 修改配置文件进入/home/hadoop/apps/hadoop/etc/hadoop/目录下修改配置文件 （1）修改hadoop-env.sh12[hadoop@hadoop1 hadoop]$ vi hadoop-env.sh export JAVA_HOME=/usr/local/jdk1.8.0_73 （2）修改core-site.xml1[hadoop@hadoop1 hadoop]$ vi core-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/hadoopdata&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; （3）修改hdfs-site.xml1[hadoop@hadoop1 hadoop]$ vi hdfs-site.xml dfs的备份数目，单机用1份就行 1234567891011121314151617&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/hadoopdata/name&lt;/value&gt; &lt;description&gt;为了保证元数据的安全一般配置多个不同目录&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/hadoopdata/data&lt;/value&gt; &lt;description&gt;datanode 的数据存储目录&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;description&gt;HDFS 的数据块的副本存储个数, 默认是3&lt;/description&gt;&lt;/property&gt; （4）修改mapred-site.xml12[hadoop@hadoop1 hadoop]$ cp mapred-site.xml.template mapred-site.xml[hadoop@hadoop1 hadoop]$ vi mapred-site.xml mapreduce.framework.name：指定mr框架为yarn方式,Hadoop二代MP也基于资源管理系统Yarn来运行 。 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; （5）修改yarn-site.xml1[hadoop@hadoop1 hadoop]$ vi yarn-site.xml 123456&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;description&gt;YARN 集群为 MapReduce 程序提供的 shuffle 服务&lt;/description&gt;&lt;/property&gt; 3.4 配置环境变量千万注意： 1、如果你使用root用户进行安装。 vi /etc/profile 即可 系统变量 2、如果你使用普通用户进行安装。 vi ~/.bashrc 用户变量 123[hadoop@hadoop1 ~]$ vi .bashrc#HADOOP_HOMEexport HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin: 使环境变量生效 1[hadoop@hadoop1 bin]$ source ~/.bashrc 3.5 查看hadoop版本1[hadoop@hadoop1 ~]$ hadoop version 3.6 创建文件夹文件夹的路径参考配置文件hdfs-site.xml里面的路径 12[hadoop@hadoop1 ~]$ mkdir -p /home/hadoop/data/hadoopdata/name[hadoop@hadoop1 ~]$ mkdir -p /home/hadoop/data/hadoopdata/data 3.7 Hadoop的初始化1[hadoop@hadoop1 ~]$ hadoop namenode -format 3.8 启动HDFS和YARN1[hadoop@hadoop1 ~]$ start-dfs.sh[hadoop@hadoop1 ~]$ start-yarn.sh 3.9 检查WebUI浏览器打开端口50070：http://hadoop1:50070 其他端口说明：port 8088: cluster and all applicationsport 50070: Hadoop NameNodeport 50090: Secondary NameNodeport 50075: DataNode 四、Scala的安装（可选）使用root安装 4.1 下载Scala下载地址http://www.scala-lang.org/download/all.html 选择对应的版本，此处在Linux上安装，选择的版本是scala-2.11.8.tgz 4.2 上传解压缩1[root@hadoop1 hadoop]# tar -zxvf scala-2.11.8.tgz -C /usr/local/ 4.3 配置环境变量1234[root@hadoop1 hadoop]# vi /etc/profile#Scalaexport SCALA_HOME=/usr/local/scala-2.11.8export PATH=$SCALA_HOME/bin:$PATH 保存并使其立即生效 1[root@hadoop1 scala-2.11.8]# source /etc/profile 4.4 验证是否安装成功1[root@hadoop1 ~]# scala -version 五、Spark的安装5.1 下载安装包下载地址： http://spark.apache.org/downloads.html 5.2 上传解压缩1[hadoop@hadoop1 ~]$ tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz -C apps/ 5.3 为解压包创建一个软连接1234[hadoop@hadoop1 ~]$ cd apps/[hadoop@hadoop1 apps]$ lshadoop hadoop-2.7.5 spark-2.3.0-bin-hadoop2.7[hadoop@hadoop1 apps]$ ln -s spark-2.3.0-bin-hadoop2.7/ spark 5.4 进入spark/conf修改配置文件1[hadoop@hadoop1 apps]$ cd spark/conf/ 复制spark-env.sh.template并重命名为spark-env.sh，并在文件最后添加配置内容 12[hadoop@hadoop1 conf]$ cp spark-env.sh.template spark-env.sh[hadoop@hadoop1 conf]$ vi spark-env.sh 123456export JAVA_HOME=/usr/local/jdk1.8.0_73export SCALA_HOME=/usr/share/scala-2.11.8export HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5export HADOOP_CONF_DIR=/home/hadoop/apps/hadoop-2.7.5/etc/hadoopexport SPARK_MASTER_IP=hadoop1export SPARK_MASTER_PORT=7077 5.5 配置环境变量1234[hadoop@hadoop1 conf]$ vi ~/.bashrc #SPARK_HOMEexport SPARK_HOME=/home/hadoop/apps/sparkexport PATH=$PATH:$SPARK_HOME/bin 保存使其立即生效 1[hadoop@hadoop1 conf]$ source ~/.bashrc 5.6 启动Spark1[hadoop@hadoop1 ~]$ ~/apps/spark/sbin/start-all.sh 5.7 查看进程 5.8 查看web界面http://hadoop1:8080/","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （四）Spark的广播变量和累加器","slug":"2019-06-04-Spark学习之路 （四）Spark的广播变量和累加器","date":"2019-06-04T03:30:04.000Z","updated":"2019-09-16T08:06:46.996Z","comments":true,"path":"2019-06-04-Spark学习之路 （四）Spark的广播变量和累加器.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-04-Spark学习之路 （四）Spark的广播变量和累加器.html","excerpt":"** Spark学习之路 （四）Spark的广播变量和累加器：** &lt;Excerpt in index | 首页摘要&gt; ​ 在spark程序中，当一个传递给Spark操作(例如map和reduce)的函数在远程节点上面运行时，Spark操作实际上操作的是这个函数所用变量的一个独立副本。这些变量会被复制到每台机器上，并且这些变量在远程机器上的所有更新都不会传递回驱动程序。通常跨任务的读写变量是低效的，但是，Spark还是为两种常见的使用模式提供了两种有限的共享变量：广播变（broadcast variable）和累加器（accumulator）","text":"** Spark学习之路 （四）Spark的广播变量和累加器：** &lt;Excerpt in index | 首页摘要&gt; ​ 在spark程序中，当一个传递给Spark操作(例如map和reduce)的函数在远程节点上面运行时，Spark操作实际上操作的是这个函数所用变量的一个独立副本。这些变量会被复制到每台机器上，并且这些变量在远程机器上的所有更新都不会传递回驱动程序。通常跨任务的读写变量是低效的，但是，Spark还是为两种常见的使用模式提供了两种有限的共享变量：广播变（broadcast variable）和累加器（accumulator） &lt;The rest of contents | 余下全文&gt; 一、广播变量broadcast variable​ 广播变量允许程序员在每台机器上保留一个只读变量，而不是随副本一起发送它的副本。例如，它们可用于以有效的方式为每个节点提供大输入数据集的副本。Spark还尝试使用有效的广播算法来分发广播变量，以降低通信成本。 ​ Spark动作通过一组阶段执行，由分布式“shuffle”操作分隔。Spark自动广播每个阶段中任务所需的公共数据。以这种方式广播的数据以序列化形式缓存并在运行每个任务之前反序列化。这意味着显式创建广播变量仅在跨多个阶段的任务需要相同数据或以反序列化形式缓存数据很重要时才有用。 ​ 广播变量是v通过调用从变量创建的SparkContext.broadcast(v)。广播变量是一个包装器v，可以通过调用该value 方法来访问它的值。下面的代码显示了这个： 1.1 为什么要将变量定义成广播变量？如果我们要在分布式计算里面分发大对象，例如：字典，集合，黑白名单等，这个都会由Driver端进行分发，一般来讲，如果这个变量不是广播变量，那么每个task就会分发一份，这在task数目十分多的情况下Driver的带宽会成为系统的瓶颈，而且会大量消耗task服务器上的资源，如果将这个变量声明为广播变量，那么知识每个executor拥有一份，这个executor启动的task会共享这个变量，节省了通信的成本和服务器的资源。 1.2 广播变量图解错误的，不使用广播变量 正确的，使用广播变量的情况 2.3 如何定义一个广播变量？12val a = 3val broadcast = sc.broadcast(a) 2.4 如何还原一个广播变量？1val c = broadcast.value 2.5 定义广播变量需要的注意点？变量一旦被定义为一个广播变量，那么这个变量只能读，不能修改 2.6 注意事项1、能不能将一个RDD使用广播变量广播出去？ ​ 不能，因为RDD是不存储数据的。可以将RDD的结果广播出去。 2、 广播变量只能在Driver端定义，不能在Executor端定义。 3、 在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值。 4、如果executor端用到了Driver的变量，如果不使用广播变量在Executor有多少task就有多少Driver端的变量副本。 5、如果Executor端用到了Driver的变量，如果使用广播变量在每个Executor中只有一份Driver端的变量副本。 二、累加器​ 累加器是仅通过关联和交换操作“添加”的变量，因此可以并行有效地支持。它们可用于实现计数器（如MapReduce）或总和。Spark本身支持数值类型的累加器，程序员可以添加对新类型的支持。 作为用户，您可以创建命名或未命名的累加器。如下图所示，命名累加器（在此实例中counter）将显示在Web UI中，用于修改该累加器的阶段。Spark显示“任务”表中任务修改的每个累加器的值。 跟踪UI中的累加器对于理解运行阶段的进度非常有用（注意：Python中尚不支持）。 2.1 为什么要将一个变量定义为一个累加器？​ 在spark应用程序中，我们经常会有这样的需求，如异常监控，调试，记录符合某特性的数据的数目，这种需求都需要用到计数器，如果一个变量不被声明为一个累加器，那么它将在被改变时不会再driver端进行全局汇总，即在分布式运行时每个task运行的只是原始变量的一个副本，并不能改变原始变量的值，但是当这个变量被声明为累加器后，该变量就会有分布式计数的功能。 2.2 图解累加器错误的图解 正确的图解 2.3 如何定义一个累加器？1val a = sc.accumulator(0) 2.4 如何还原一个累加器？1val b = a.value 2.5 注意事项1、 累加器在Driver端定义赋初始值，累加器只能在Driver端读取最后的值，在Excutor端更新。 2、累加器不是一个调优的操作，因为如果不这样做，结果是错的","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （三）Spark之RDD","slug":"2019-06-03-Spark学习之路 （三）Spark之RDD","date":"2019-06-03T02:30:04.000Z","updated":"2019-09-16T04:12:59.674Z","comments":true,"path":"2019-06-03-Spark学习之路 （三）Spark之RDD.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-03-Spark学习之路 （三）Spark之RDD.html","excerpt":"** Spark学习之路 （三）Spark之RDD：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （三）Spark之RDD","text":"** Spark学习之路 （三）Spark之RDD：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （三）Spark之RDD &lt;The rest of contents | 余下全文&gt; 一、RDD的概述1.1 什么是RDD​ RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。 1.2 RDD的属性https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala A list of partitions A function for computing each split A list of dependencies on other RDDs Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned) Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file) （1）一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。 （2）一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。 （3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 （4）一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。 （5）一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 其中hello.txt 二、RDD的创建方式2.1 通过读取文件生成的由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等 1scala&gt; val file = sc.textFile(&quot;/spark/hello.txt&quot;) 2.2 通过并行化的方式创建RDD由一个已经存在的Scala集合创建。 1234567scala&gt; val array = Array(1,2,3,4,5)array: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt; val rdd = sc.parallelize(array)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[27] at parallelize at &lt;console&gt;:26scala&gt; 2.3 其他方式读取数据库等等其他的操作。也可以生成RDD。RDD转换为ParallelCollectionRDD。 三、RDD编程APISpark支持两个类型（算子）操作：Transformation和Action 3.1 Transformation​ 主要做的是就是将一个已有的RDD生成另外一个RDD。Transformation具有lazy**特性(延迟加载)**。Transformation算子的代码不会真正被执行。只有当我们的程序里面遇到一个action算子的时候，代码才会真正的被执行。这种设计让Spark更加有效率地运行。 http://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds 常用的Transformation： 转换 含义 map(func) 返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成 filter(func) 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成 flatMap(func) 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素） mapPartitions(func) 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U] mapPartitionsWithIndex(func) 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U] sample(withReplacement, fraction, seed) 根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子 union(otherDataset) 对源RDD和参数RDD求并集后返回一个新的RDD intersection(otherDataset) 对源RDD和参数RDD求交集后返回一个新的RDD distinct([numTasks])) 对源RDD进行去重后返回一个新的RDD groupByKey([numTasks]) 在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD reduceByKey(func, [numTasks]) 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置 aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) 先按分区聚合 再总的聚合 每次要跟初始值交流 例如：aggregateByKey(0)(+,+) 对k/y的RDD进行操作 sortByKey([ascending], [numTasks]) 在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD sortBy(func,[ascending], [numTasks]) 与sortByKey类似，但是更灵活 第一个参数是根据什么排序 第二个是怎么排序 false倒序 第三个排序后分区数 默认与原RDD一样 join(otherDataset, [numTasks]) 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD 相当于内连接（求交集） cogroup(otherDataset, [numTasks]) 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable,Iterable))类型的RDD cartesian(otherDataset) 两个RDD的笛卡尔积 的成很多个K/V pipe(command, [envVars]) 调用外部程序 coalesce(numPartitions) 重新分区 第一个参数是要分多少区，第二个参数是否shuffle 默认false 少分区变多分区 true 多分区变少分区 false repartition(numPartitions) 重新分区 必须shuffle 参数是要分多少区 少变多 repartitionAndSortWithinPartitions(partitioner) 重新分区+排序 比先分区再排序效率高 对K/V的RDD进行操作 foldByKey(zeroValue)(seqOp) 该函数用于K/V做折叠，合并处理 ，与aggregate类似 第一个括号的参数应用于每个V值 第二括号函数是聚合例如：+ combineByKey 合并相同的key的值 rdd1.combineByKey(x =&gt; x, (a: Int, b: Int) =&gt; a + b, (m: Int, n: Int) =&gt; m + n) partitionBy**（partitioner）** 对RDD进行分区 partitioner是分区器 例如new HashPartition(2 cache RDD缓存，可以避免重复计算从而减少时间，区别：cache内部调用了persist算子，cache默认就一个缓存级别MEMORY-ONLY ，而persist则可以选择缓存级别 persist Subtract**（rdd）** 返回前rdd元素不在后rdd的rdd leftOuterJoin leftOuterJoin类似于SQL中的左外关联left outer join，返回结果以前面的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可。 rightOuterJoin rightOuterJoin类似于SQL中的有外关联right outer join，返回结果以参数中的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可 subtractByKey substractByKey和基本转换操作中的subtract类似只不过这里是针对K的，返回在主RDD中出现，并且不在otherRDD中出现的元素 3.2 Action触发代码的运行，我们一段spark代码里面至少需要有一个action操作。 常用的Action: 动作 含义 reduce(func) 通过func函数聚集RDD中的所有元素，这个功能必须是课交换且可并联的 collect() 在驱动程序中，以数组的形式返回数据集的所有元素 count() 返回RDD的元素个数 first() 返回RDD的第一个元素（类似于take(1)） take(n) 返回一个由数据集的前n个元素组成的数组 takeSample(withReplacement,num, [seed]) 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子 takeOrdered(n, [ordering]) saveAsTextFile(path) 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本 saveAsSequenceFile(path) 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。 saveAsObjectFile(path) countByKey() 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。 foreach(func) 在数据集的每一个元素上，运行函数func进行更新。 aggregate 先对分区进行操作，在总体操作 reduceByKeyLocally lookup top fold foreachPartition 3.3 Spark WordCount代码编写使用maven进行项目构建 （1）使用scala进行编写查看官方网站，需要导入2个依赖包 详细代码 SparkWordCountWithScala.scala 12345678910111213141516171819202122232425262728293031import org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object SparkWordCountWithScala &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() /** * 如果这个参数不设置，默认认为你运行的是集群模式 * 如果设置成local代表运行的是local模式 */ conf.setMaster(\"local\") //设置任务名 conf.setAppName(\"WordCount\") //创建SparkCore的程序入口 val sc = new SparkContext(conf) //读取文件 生成RDD val file: RDD[String] = sc.textFile(\"E:\\\\hello.txt\") //把每一行数据按照，分割 val word: RDD[String] = file.flatMap(_.split(\",\")) //让每一个单词都出现一次 val wordOne: RDD[(String, Int)] = word.map((_,1)) //单词计数 val wordCount: RDD[(String, Int)] = wordOne.reduceByKey(_+_) //按照单词出现的次数 降序排序 val sortRdd: RDD[(String, Int)] = wordCount.sortBy(tuple =&gt; tuple._2,false) //将最终的结果进行保存 sortRdd.saveAsTextFile(\"E:\\\\result\") sc.stop() &#125; 运行结果 （2）使用java jdk7进行编写SparkWordCountWithJava7.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import scala.Tuple2;import java.util.Arrays;import java.util.Iterator;public class SparkWordCountWithJava7 &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf(); conf.setMaster(\"local\"); conf.setAppName(\"WordCount\"); JavaSparkContext sc = new JavaSparkContext(conf); JavaRDD&lt;String&gt; fileRdd = sc.textFile(\"E:\\\\hello.txt\"); JavaRDD&lt;String&gt; wordRDD = fileRdd.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterator&lt;String&gt; call(String line) throws Exception &#123; return Arrays.asList(line.split(\",\")).iterator(); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; wordOneRDD = wordRDD.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123; return new Tuple2&lt;&gt;(word, 1); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; wordCountRDD = wordOneRDD.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer i1, Integer i2) throws Exception &#123; return i1 + i2; &#125; &#125;); JavaPairRDD&lt;Integer, String&gt; count2WordRDD = wordCountRDD.mapToPair(new PairFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, String&gt;() &#123; @Override public Tuple2&lt;Integer, String&gt; call(Tuple2&lt;String, Integer&gt; tuple) throws Exception &#123; return new Tuple2&lt;&gt;(tuple._2, tuple._1); &#125; &#125;); JavaPairRDD&lt;Integer, String&gt; sortRDD = count2WordRDD.sortByKey(false); JavaPairRDD&lt;String, Integer&gt; resultRDD = sortRDD.mapToPair(new PairFunction&lt;Tuple2&lt;Integer, String&gt;, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(Tuple2&lt;Integer, String&gt; tuple) throws Exception &#123; return new Tuple2&lt;&gt;(tuple._2, tuple._1); &#125; &#125;); resultRDD.saveAsTextFile(\"E:\\\\result7\"); &#125;&#125; （3）使用java jdk8进行编写lambda表达式 SparkWordCountWithJava8.java 12345678910111213141516171819202122232425import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import scala.Tuple2;import java.util.Arrays;public class SparkWordCountWithJava8 &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf(); conf.setAppName(\"WortCount\"); conf.setMaster(\"local\"); JavaSparkContext sc = new JavaSparkContext(conf); JavaRDD&lt;String&gt; fileRDD = sc.textFile(\"E:\\\\hello.txt\"); JavaRDD&lt;String&gt; wordRdd = fileRDD.flatMap(line -&gt; Arrays.asList(line.split(\",\")).iterator()); JavaPairRDD&lt;String, Integer&gt; wordOneRDD = wordRdd.mapToPair(word -&gt; new Tuple2&lt;&gt;(word, 1)); JavaPairRDD&lt;String, Integer&gt; wordCountRDD = wordOneRDD.reduceByKey((x, y) -&gt; x + y); JavaPairRDD&lt;Integer, String&gt; count2WordRDD = wordCountRDD.mapToPair(tuple -&gt; new Tuple2&lt;&gt;(tuple._2, tuple._1)); JavaPairRDD&lt;Integer, String&gt; sortRDD = count2WordRDD.sortByKey(false); JavaPairRDD&lt;String, Integer&gt; resultRDD = sortRDD.mapToPair(tuple -&gt; new Tuple2&lt;&gt;(tuple._2, tuple._1)); resultRDD.saveAsTextFile(\"E:\\\\result8\"); &#125; 四、RDD的宽依赖和窄依赖4.1 RDD依赖关系的本质内幕由于RDD是粗粒度的操作数据集，每个Transformation操作都会生成一个新的RDD，所以RDD之间就会形成类似流水线的前后依赖关系；RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。如图所示显示了RDD之间的依赖关系。 从图中可知： 窄依赖：是指每个父RDD的一个Partition最多被子RDD的一个Partition所使用，例如map、filter、union等操作都会产生窄依赖；（独生子女） 宽依赖：是指一个父RDD的Partition会被多个子RDD的Partition所使用，例如groupByKey、reduceByKey、sortByKey等操作都会产生宽依赖；（超生） 需要特别说明的是对join操作有两种情况： （1）图中左半部分join：如果两个RDD在进行join操作时，一个RDD的partition仅仅和另一个RDD中已知个数的Partition进行join，那么这种类型的join操作就是窄依赖，例如图1中左半部分的join操作(join with inputs co-partitioned)； （2）图中右半部分join：其它情况的join操作就是宽依赖,例如图1中右半部分的join操作(join with inputs not co-partitioned)，由于是需要父RDD的所有partition进行join的转换，这就涉及到了shuffle，因此这种类型的join操作也是宽依赖。 总结： 在这里我们是从父RDD的partition被使用的个数来定义窄依赖和宽依赖，因此可以用一句话概括下：如果父RDD的一个Partition被子RDD的一个Partition所使用就是窄依赖，否则的话就是宽依赖。因为是确定的partition数量的依赖关系，所以RDD之间的依赖关系就是窄依赖；由此我们可以得出一个推论：即窄依赖不仅包含一对一的窄依赖，还包含一对固定个数的窄依赖。 一对固定个数的窄依赖的理解：即子RDD的partition对父RDD依赖的Partition的数量不会随着RDD数据规模的改变而改变；换句话说，无论是有100T的数据量还是1P的数据量，在窄依赖中，子RDD所依赖的父RDD的partition的个数是确定的，而宽依赖是shuffle级别的，数据量越大，那么子RDD所依赖的父RDD的个数就越多，从而子RDD所依赖的父RDD的partition的个数也会变得越来越多。 4.2 依赖关系下的数据流视图 在spark中，会根据RDD之间的依赖关系将DAG图（有向无环图）划分为不同的阶段，对于窄依赖，由于partition依赖关系的确定性，partition的转换处理就可以在同一个线程里完成，窄依赖就被spark划分到同一个stage中，而对于宽依赖，只能等父RDD shuffle处理完成后，下一个stage才能开始接下来的计算。 因此spark划分stage的整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。因此在图2中RDD C,RDD D,RDD E,RDDF被构建在一个stage中,RDD A被构建在一个单独的Stage中,而RDD B和RDD G又被构建在同一个stage中。 在spark中，Task的类型分为2种：ShuffleMapTask和ResultTask； 简单来说，DAG的最后一个阶段会为每个结果的partition生成一个ResultTask，即每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！而其余所有阶段都会生成ShuffleMapTask；之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中；也就是说上图中的stage1和stage2相当于mapreduce中的Mapper,而ResultTask所代表的stage3就相当于mapreduce中的reducer。 在之前动手操作了一个wordcount程序，因此可知，Hadoop中MapReduce操作中的Mapper和Reducer在spark中的基本等量算子是map和reduceByKey;不过区别在于：Hadoop中的MapReduce天生就是排序的；而reduceByKey只是根据Key进行reduce，但spark除了这两个算子还有其他的算子；因此从这个意义上来说，Spark比Hadoop的计算算子更为丰富。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler","slug":"2019-06-02-Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler","date":"2019-06-02T05:30:04.000Z","updated":"2019-09-16T02:59:42.540Z","comments":true,"path":"2019-06-02-Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler.html","excerpt":"** Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler","text":"** Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler：** &lt;Excerpt in index | 首页摘要&gt; ​ Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler &lt;The rest of contents | 余下全文&gt; 目前Hadoop有三种比较流行的资源调度器：FIFO 、Capacity Scheduler、Fair Scheduler。目前hadoop2.7默认使用的是Capacity Scheduler容量调度器。 一、FIFO（先入先出调度器）hadoop1.x使用的默认调度器就是FIFO。FIFO采用队列方式将一个一个job任务按照时间先后顺序进行服务。比如排在最前面的job需要若干maptask和若干reducetask，当发现有空闲的服务器节点就分配给这个job，直到job执行完毕。 二、Capacity Scheduler（容量调度器）hadoop2.x使用的默认调度器是Capacity Scheduler。 1、支持多个队列，每个队列可配置一定量的资源，每个采用FIFO的方式调度。 2、为了防止同一个用户的job任务独占队列中的资源，调度器会对同一用户提交的job任务所占资源进行限制。 3、分配新的job任务时，首先计算每个队列中正在运行task个数与其队列应该分配的资源量做比值，然后选择比值最小的队列。比如如图队列A15个task，20%资源量，那么就是15%0.2=70，队列B是25%0.5=50 ，队列C是25%0.3=80.33 。所以选择最小值队列B。 4、其次，按照job任务的优先级和时间顺序，同时要考虑到用户的资源量和内存的限制，对队列中的job任务进行排序执行。 5、多个队列同时按照任务队列内的先后顺序一次执行。例如下图中job11、job21、job31分别在各自队列中顺序比较靠前，三个任务就同时执行。 三、Fair Scheduler（公平调度器）1、支持多个队列，每个队列可以配置一定的资源，每个队列中的job任务公平共享其所在队列的所有资源。 2、队列中的job任务都是按照优先级分配资源，优先级越高分配的资源越多，但是为了确保公平每个job任务都会分配到资源。优先级是根据每个job任务的理想获取资源量减去实际获取资源量的差值决定的，差值越大优先级越高。 原文链接：https://blog.csdn.net/xiaomage510/article/details/82500067","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/tags/Hadoop/"}],"keywords":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://zhangfuxin.cn/categories/Hadoop/"}]},{"title":"Spark调度模式-FIFO和FAIR","slug":"2019-06-02-Spark调度模式-FIFO和FAIR","date":"2019-06-02T05:20:04.000Z","updated":"2019-09-16T03:25:31.638Z","comments":true,"path":"2019-06-02-Spark调度模式-FIFO和FAIR.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-Spark调度模式-FIFO和FAIR.html","excerpt":"** Spark调度模式-FIFO和FAIR：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark调度模式-FIFO和FAIR","text":"** Spark调度模式-FIFO和FAIR：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark调度模式-FIFO和FAIR &lt;The rest of contents | 余下全文&gt; Spark中的调度模式主要有两种：FIFO和FAIR。默认情况下Spark的调度模式是FIFO（先进先出），谁先提交谁先执行，后面的任务需要等待前面的任务执行。而FAIR（公平调度）模式支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。对这两种调度模式的具体实现，接下来会根据spark-1.6.0的源码来进行详细的分析。使用哪种调度器由参数spark.scheduler.mode来设置，可选的参数有FAIR和FIFO，默认是FIFO。 一、源码入口 在Scheduler模块中，当Stage划分好，然后提交Task的过程中，会进入TaskSchedulerImpl#submitTasks方法。 schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties) //目前支持FIFO和FAIR两种调度策略。 在上面代码中有一个schedulableBuilder对象，这个对象在TaskSchedulerImpl类中的定义及实现可以参考下面这段源代码： 12345678910111213141516var schedulableBuilder: SchedulableBuilder = null... def initialize(backend: SchedulerBackend) &#123; this.backend = backend // temporarily set rootPool name to empty rootPool = new Pool(\"\", schedulingMode, 0, 0) schedulableBuilder = &#123; schedulingMode match &#123; case SchedulingMode.FIFO =&gt; new FIFOSchedulableBuilder(rootPool) //rootPool包含了一组TaskSetManager case SchedulingMode.FAIR =&gt; new FairSchedulableBuilder(rootPool, conf) //rootPool包含了一组Pool树，这棵树的叶子节点都是TaskSetManager &#125; &#125; schedulableBuilder.buildPools() //在FIFO中的实现是空 &#125; 根据用户配置的SchedulingMode决定是生成FIFOSchedulableBuilder还是生成FairSchedulableBuilder类型的schedulableBuilder对象。 在生成schedulableBuilder后，调用其buildPools方法生成调度池。 调度模式由配置参数spark.scheduler.mode（默认值为FIFO）来确定。 两种模式的调度逻辑图如下： 二、FIFOSchedulableBuilder FIFO的rootPool包含一组TaskSetManager。从上面的类继承图中看出在FIFOSchedulableBuilder中有两个方法： 1、buildPools实现为空: 123override def buildPools() &#123; // nothing &#125; 所以，对于FIFO模式，获取到schedulableBuilder对象后，在调用buildPools方法后，不做任何操作。 2、addTaskSetManager 该方法将TaskSetManager装载到rootPool中。直接调用的方法是Pool#addSchedulable()。 123override def addTaskSetManager(manager: Schedulable, properties: Properties) &#123; rootPool.addSchedulable(manager)&#125; Pool#addSchedulable()方法： 12345678val schedulableQueue = new ConcurrentLinkedQueue[Schedulable]... override def addSchedulable(schedulable: Schedulable) &#123; require(schedulable != null) schedulableQueue.add(schedulable) schedulableNameToSchedulable.put(schedulable.name, schedulable) schedulable.parent = this &#125; 将该TaskSetManager加入到调度队列schedulableQueue中。 三、FairSchedulableBuilder FAIR的rootPool中包含一组Pool，在Pool中包含了TaskSetManager。 1、buildPools 在该方法中，会读取配置文件，按照配置文件中的配置参数调用buildFairSchedulerPool生成配置的调度池，以及调用buildDefaultPool生成默认调度池。 默认情况下FAIR模式的配置文件是位于SPARK_HOME/conf/fairscheduler.xml文件，也可以通过参数spark.scheduler.allocation.file设置用户自定义配置文件。spark中提供的fairscheduler.xml模板如下所示： 123456789101112&lt;allocations&gt; &lt;pool name=\"production\"&gt; &lt;schedulingMode&gt;FAIR&lt;/schedulingMode&gt; &lt;weight&gt;1&lt;/weight&gt; &lt;minShare&gt;2&lt;/minShare&gt; &lt;/pool&gt; &lt;pool name=\"test\"&gt; &lt;schedulingMode&gt;FIFO&lt;/schedulingMode&gt; &lt;weight&gt;2&lt;/weight&gt; &lt;minShare&gt;3&lt;/minShare&gt; &lt;/pool&gt;&lt;/allocations&gt; 参数含义：（1）name: 该调度池的名称，可根据该参数使用指定pool，入sc.setLocalProperty(“spark.scheduler.pool”, “test”)（2）weight: 该调度池的权重，各调度池根据该参数分配系统资源。每个调度池得到的资源数为weight / sum(weight)，weight为2的分配到的资源为weight为1的两倍。（3）minShare: 该调度池需要的最小资源数（CPU核数）。fair调度器首先会尝试为每个调度池分配最少minShare资源，然后剩余资源才会按照weight大小继续分配。（4）schedulingMode: 该调度池内的调度模式。 2、buildFairSchedulerPool 从上面的配置文件可以看到，每一个调度池有一个name属性指定名字，然后在该pool中可以设置其schedulingMode(可为空，默认为FIFO), weight(可为空，默认值是1), 以及minShare(可为空，默认值是0)参数。然后使用这些参数生成一个Pool对象，把该pool对象放入rootPool中。入下所示： 12val pool = new Pool(poolName, schedulingMode, minShare, weight)rootPool.addSchedulable(pool) 3、buildDefaultPool 如果如果配置文件中没有设置一个name为default的pool，系统才会自动生成一个使用默认参数生成的pool对象。各项参数的默认值在buildFairSchedulerPool中有提到。 4、addTaskSetManager 这一段逻辑中是把配置文件中的pool，或者default pool放入rootPool中，然后把TaskSetManager存入rootPool对应的子pool。 12345678910111213141516171819override def addTaskSetManager(manager: Schedulable, properties: Properties) &#123; var poolName = DEFAULT_POOL_NAME var parentPool = rootPool.getSchedulableByName(poolName) if (properties != null) &#123; poolName = properties.getProperty(FAIR_SCHEDULER_PROPERTIES, DEFAULT_POOL_NAME) parentPool = rootPool.getSchedulableByName(poolName) if (parentPool == null) &#123; // we will create a new pool that user has configured in app // instead of being defined in xml file parentPool = new Pool(poolName, DEFAULT_SCHEDULING_MODE, DEFAULT_MINIMUM_SHARE, DEFAULT_WEIGHT) rootPool.addSchedulable(parentPool) logInfo(\"Created pool %s, schedulingMode: %s, minShare: %d, weight: %d\".format( poolName, DEFAULT_SCHEDULING_MODE, DEFAULT_MINIMUM_SHARE, DEFAULT_WEIGHT)) &#125; &#125; parentPool.addSchedulable(manager) logInfo(\"Added task set \" + manager.name + \" tasks to pool \" + poolName) &#125; 5、FAIR调度池使用方法 在Spark-1.6.1官方文档中写道： 如果不加设置，jobs会提交到default调度池中。由于调度池的使用是Thread级别的，只能通过具体的SparkContext来设置local属性（即无法在配置文件中通过参数spark.scheduler.pool来设置，因为配置文件中的参数会被加载到SparkConf对象中）。所以需要使用指定调度池的话，需要在具体代码中通过SparkContext对象sc来按照如下方法进行设置：sc.setLocalProperty(“spark.scheduler.pool”, “test”)设置该参数后，在该thread中提交的所有job都会提交到test Pool中。如果接下来不再需要使用到该test调度池，sc.setLocalProperty(“spark.scheduler.pool”, null) 四、FIFO和FAIR的调度顺序这里必须提到的一个类是上面提到的Pool，在这个类中实现了不同调度模式的调度算法。 12345678var taskSetSchedulingAlgorithm: SchedulingAlgorithm = &#123; schedulingMode match &#123; case SchedulingMode.FAIR =&gt; new FairSchedulingAlgorithm() case SchedulingMode.FIFO =&gt; new FIFOSchedulingAlgorithm() &#125;&#125; FIFO模式的算法类是FIFOSchedulingAlgorithm，FAIR模式的算法实现类是FairSchedulingAlgorithm。 接下来的两节中comparator方法传入参数Schedulable类型是一个trait，具体实现主要有两个：1，Pool；2，TaskSetManager。与最前面那个调度模式的逻辑图相对应。 1、FIFO模式的调度算法FIFOSchedulingAlgorithm在这个类里面，主要逻辑是一个comparator方法。 123456789101112131415override def comparator(s1: Schedulable, s2: Schedulable): Boolean = &#123; val priority1 = s1.priority //实际上是Job ID val priority2 = s2.priority var res = math.signum(priority1 - priority2) if (res == 0) &#123; //如果Job ID相同，就比较Stage ID val stageId1 = s1.stageId val stageId2 = s2.stageId res = math.signum(stageId1 - stageId2) &#125; if (res &lt; 0) &#123; true &#125; else &#123; false &#125;&#125; 如果有两个调度任务s1和s2，首先获得两个任务的priority，在FIFO中该优先级实际上是Job ID。首先比较两个任务的Job ID，如果priority1比priority2小，那么返回true，表示s1的优先级比s2的高。我们知道Job ID是顺序生成的，先生成的Job ID比较小，所以先提交的job肯定比后提交的job先执行。但是如果是同一个job的不同任务，接下来就比较各自的Stage ID，类似于比较Job ID，Stage ID小的优先级高。 2、FAIR模式的调度算法FairSchedulingAlgorithm 这个类中的comparator方法源代码如下： 123456789101112131415161718192021222324252627282930override def comparator(s1: Schedulable, s2: Schedulable): Boolean = &#123; val minShare1 = s1.minShare //在这里share理解成份额，即每个调度池要求的最少cpu核数 val minShare2 = s2.minShare val runningTasks1 = s1.runningTasks // 该Pool或者TaskSetManager中正在运行的任务数 val runningTasks2 = s2.runningTasks val s1Needy = runningTasks1 &lt; minShare1 // 如果正在运行任务数比该调度池最小cpu核数要小 val s2Needy = runningTasks2 &lt; minShare2 val minShareRatio1 = runningTasks1.toDouble / math.max(minShare1, 1.0).toDouble val minShareRatio2 = runningTasks2.toDouble / math.max(minShare2, 1.0).toDouble val taskToWeightRatio1 = runningTasks1.toDouble / s1.weight.toDouble val taskToWeightRatio2 = runningTasks2.toDouble / s2.weight.toDouble var compare: Int = 0 if (s1Needy &amp;&amp; !s2Needy) &#123; return true &#125; else if (!s1Needy &amp;&amp; s2Needy) &#123; return false &#125; else if (s1Needy &amp;&amp; s2Needy) &#123; compare = minShareRatio1.compareTo(minShareRatio2) &#125; else &#123; compare = taskToWeightRatio1.compareTo(taskToWeightRatio2) &#125; if (compare &lt; 0) &#123; true &#125; else if (compare &gt; 0) &#123; false &#125; else &#123; s1.name &lt; s2.name &#125; &#125; minShare对应fairscheduler.xml配置文件中的minShare属性。（1）如果s1所在Pool或者TaskSetManager中运行状态的task数量比minShare小，s2所在Pool或者TaskSetManager中运行状态的task数量比minShare大，那么s1会优先调度。反之，s2优先调度。（2）如果s1和s2所在Pool或者TaskSetManager中运行状态的task数量都比各自minShare小，那么minShareRatio小的优先被调度。minShareRatio是运行状态task数与minShare的比值，即相对来说minShare使用较少的先被调度。（3）如果minShareRatio相同，那么最后比较各自Pool的名字。 原文链接：https://blog.csdn.net/dabokele/article/details/51526048","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark中parallelize函数和makeRDD函数的区别","slug":"2019-06-02-Spark中parallelize函数和makeRDD函数的区别","date":"2019-06-02T03:30:04.000Z","updated":"2019-09-16T01:39:52.469Z","comments":true,"path":"2019-06-02-Spark中parallelize函数和makeRDD函数的区别.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-Spark中parallelize函数和makeRDD函数的区别.html","excerpt":"** Spark中parallelize函数和makeRDD函数的区别：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark中parallelize函数和makeRDD函数的区别","text":"** Spark中parallelize函数和makeRDD函数的区别：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark中parallelize函数和makeRDD函数的区别 &lt;The rest of contents | 余下全文&gt; 我们知道，在Spark中创建RDD的创建方式大概可以分为三种： （1）、从集合中创建RDD； （2）、从外部存储创建RDD； （3）、从其他RDD创建。 而从集合中创建RDD，Spark主要提供了两中函数：parallelize和makeRDD。我们可以先看看这两个函数的声明： 123456789def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] 我们可以从上面看出makeRDD有两种实现，而且第一个makeRDD函数接收的参数和parallelize完全一致。其实第一种makeRDD函数实现是依赖了parallelize函数的实现，来看看Spark中是怎么实现这个makeRDD函数的： 12345def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; parallelize(seq, numSlices)&#125; ​ 我们可以看出，这个makeRDD函数完全和parallelize函数一致。但是我们得看看第二种makeRDD函数函数实现了，它接收的参数类型是Seq[(T, Seq[String])]，Spark文档的说明是 Distribute a local Scala collection to form an RDD, with one or more location preferences (hostnames of Spark nodes) for each object. Create a new partition for each collection item. 分发本地scala集合以形成RDD，每个对象具有一个或多个位置首选项（spark节点的主机名）。为每个集合项创建一个新分区。 原来，这个函数还为数据提供了位置信息，来看看我们怎么使用： 12345678910111213141516171819202122scala&gt; val iteblog1 = sc.parallelize(List(1,2,3))iteblog1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:21 scala&gt; val iteblog2 = sc.makeRDD(List(1,2,3))iteblog2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at makeRDD at &lt;console&gt;:21 scala&gt; val seq = List((1, List(\"iteblog.com\", \"sparkhost1.com\", \"sparkhost2.com\")), | (2, List(\"iteblog.com\", \"sparkhost2.com\")))seq: List[(Int, List[String])] = List((1,List(iteblog.com, sparkhost1.com, sparkhost2.com)), (2,List(iteblog.com, sparkhost2.com))) scala&gt; val iteblog3 = sc.makeRDD(seq)iteblog3: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at makeRDD at &lt;console&gt;:23 scala&gt; iteblog3.preferredLocations(iteblog3.partitions(1))res26: Seq[String] = List(iteblog.com, sparkhost2.com) scala&gt; iteblog3.preferredLocations(iteblog3.partitions(0))res27: Seq[String] = List(iteblog.com, sparkhost1.com, sparkhost2.com) scala&gt; iteblog1.preferredLocations(iteblog1.partitions(0))res28: Seq[String] = List() 我们可以看到，makeRDD函数有两种实现，第一种实现其实完全和parallelize一致；而第二种实现可以为数据提供位置信息，而除此之外的实现和parallelize函数也是一致的，如下： 123456789101112def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())&#125; def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope &#123; assertNotStopped() val indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap new ParallelCollectionRDD[T](this, seq.map(_._1), seq.size, indexToPrefs)&#125; 都是返回ParallelCollectionRDD，而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。 转载自过往记忆（https://www.iteblog.com/）","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （二）Spark2.3 HA集群的分布式安装","slug":"2019-06-02-Spark学习之路 （二）Spark2.3 HA集群的分布式安装","date":"2019-06-02T02:31:04.000Z","updated":"2019-09-16T02:02:45.545Z","comments":true,"path":"2019-06-02-Spark学习之路 （二）Spark2.3 HA集群的分布式安装.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-Spark学习之路 （二）Spark2.3 HA集群的分布式安装.html","excerpt":"** Spark学习之路 （二）Spark2.3 HA集群的分布式安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （二）Spark2.3 HA集群的分布式安装","text":"** Spark学习之路 （二）Spark2.3 HA集群的分布式安装：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （二）Spark2.3 HA集群的分布式安装 &lt;The rest of contents | 余下全文&gt; 一、下载Spark安装包1、从官网下载http://spark.apache.org/downloads.html 二、安装基础1、Java8安装成功 2、Zookeeper安装成功 3、hadoop2.7.5 HA安装成功 4、Scala安装成功（不安装进程也可以启动） 三、Spark安装过程1、上传并解压缩12345[hadoop@hadoop1 ~]$ lsapps data exam inithive.conf movie spark-2.3.0-bin-hadoop2.7.tgz udf.jarcookies data.txt executions json.txt projects student zookeeper.outcourse emp hive.sql log sougou temp[hadoop@hadoop1 ~]$ tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz -C apps/ 2、为安装包创建一个软连接12345678910111213[hadoop@hadoop1 ~]$ cd apps/[hadoop@hadoop1 apps]$ lshadoop-2.7.5 hbase-1.2.6 spark-2.3.0-bin-hadoop2.7 zookeeper-3.4.10 zookeeper.out[hadoop@hadoop1 apps]$ ln -s spark-2.3.0-bin-hadoop2.7/ spark[hadoop@hadoop1 apps]$ ll总用量 36drwxr-xr-x. 10 hadoop hadoop 4096 3月 23 20:29 hadoop-2.7.5drwxrwxr-x. 7 hadoop hadoop 4096 3月 29 13:15 hbase-1.2.6lrwxrwxrwx. 1 hadoop hadoop 26 4月 20 13:48 spark -&gt; spark-2.3.0-bin-hadoop2.7/drwxr-xr-x. 13 hadoop hadoop 4096 2月 23 03:42 spark-2.3.0-bin-hadoop2.7drwxr-xr-x. 10 hadoop hadoop 4096 3月 23 2017 zookeeper-3.4.10-rw-rw-r--. 1 hadoop hadoop 17559 3月 29 13:37 zookeeper.out[hadoop@hadoop1 apps]$ 3、进入spark/conf修改配置文件（1）进入配置文件所在目录 1234567891011[hadoop@hadoop1 ~]$ cd apps/spark/conf/[hadoop@hadoop1 conf]$ ll总用量 36-rw-r--r--. 1 hadoop hadoop 996 2月 23 03:42 docker.properties.template-rw-r--r--. 1 hadoop hadoop 1105 2月 23 03:42 fairscheduler.xml.template-rw-r--r--. 1 hadoop hadoop 2025 2月 23 03:42 log4j.properties.template-rw-r--r--. 1 hadoop hadoop 7801 2月 23 03:42 metrics.properties.template-rw-r--r--. 1 hadoop hadoop 865 2月 23 03:42 slaves.template-rw-r--r--. 1 hadoop hadoop 1292 2月 23 03:42 spark-defaults.conf.template-rwxr-xr-x. 1 hadoop hadoop 4221 2月 23 03:42 spark-env.sh.template[hadoop@hadoop1 conf]$ （2）复制spark-env.sh.template并重命名为spark-env.sh，并在文件最后添加配置内容 12[hadoop@hadoop1 conf]$ cp spark-env.sh.template spark-env.sh[hadoop@hadoop1 conf]$ vi spark-env.sh 1234567export JAVA_HOME=/usr/local/jdk1.8.0_73#export SCALA_HOME=/usr/share/scalaexport HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5export HADOOP_CONF_DIR=/home/hadoop/apps/hadoop-2.7.5/etc/hadoopexport SPARK_WORKER_MEMORY=500mexport SPARK_WORKER_CORES=1export SPARK_DAEMON_JAVA_OPTS=\"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop1:2181,hadoop2:2181,hadoop3:2181,hadoop4:2181 -Dspark.deploy.zookeeper.dir=/spark\" 注： #export SPARK_MASTER_IP=hadoop1 这个配置要注释掉。集群搭建时配置的spark参数可能和现在的不一样，主要是考虑个人电脑配置问题，如果memory配置太大，机器运行很慢。说明：-Dspark.deploy.recoveryMode=ZOOKEEPER #说明整个集群状态是通过zookeeper来维护的，整个集群状态的恢复也是通过zookeeper来维护的。就是说用zookeeper做了spark的HA配置，Master(Active)挂掉的话，Master(standby)要想变成Master（Active）的话，Master(Standby)就要像zookeeper读取整个集群状态信息，然后进行恢复所有Worker和Driver的状态信息，和所有的Application状态信息；-Dspark.deploy.zookeeper.url=hadoop1:2181,hadoop2:2181,hadoop3:2181,hadoop4:2181#将所有配置了zookeeper，并且在这台机器上有可能做master(Active)的机器都配置进来；（我用了4台，就配置了4台） -Dspark.deploy.zookeeper.dir=/spark这里的dir和zookeeper配置文件zoo.cfg中的dataDir的区别？？？-Dspark.deploy.zookeeper.dir是保存spark的元数据，保存了spark的作业运行状态；zookeeper会保存spark集群的所有的状态信息，包括所有的Workers信息，所有的Applactions信息，所有的Driver信息,如果集群 （3）复制slaves.template成slaves 12[hadoop@hadoop1 conf]$ cp slaves.template slaves[hadoop@hadoop1 conf]$ vi slaves 添加如下内容 1234hadoop1hadoop2hadoop3hadoop4 （4）将安装包分发给其他节点 1234[hadoop@hadoop1 ~]$ cd apps/[hadoop@hadoop1 apps]$ scp -r spark-2.3.0-bin-hadoop2.7/ hadoop2:$PWD[hadoop@hadoop1 apps]$ scp -r spark-2.3.0-bin-hadoop2.7/ hadoop3:$PWD[hadoop@hadoop1 apps]$ scp -r spark-2.3.0-bin-hadoop2.7/ hadoop4:$PWD 创建软连接 123456789101112[hadoop@hadoop2 ~]$ cd apps/[hadoop@hadoop2 apps]$ lshadoop-2.7.5 hbase-1.2.6 spark-2.3.0-bin-hadoop2.7 zookeeper-3.4.10[hadoop@hadoop2 apps]$ ln -s spark-2.3.0-bin-hadoop2.7/ spark[hadoop@hadoop2 apps]$ ll总用量 16drwxr-xr-x 10 hadoop hadoop 4096 3月 23 20:29 hadoop-2.7.5drwxrwxr-x 7 hadoop hadoop 4096 3月 29 13:15 hbase-1.2.6lrwxrwxrwx 1 hadoop hadoop 26 4月 20 19:26 spark -&gt; spark-2.3.0-bin-hadoop2.7/drwxr-xr-x 13 hadoop hadoop 4096 4月 20 19:24 spark-2.3.0-bin-hadoop2.7drwxr-xr-x 10 hadoop hadoop 4096 3月 21 19:31 zookeeper-3.4.10[hadoop@hadoop2 apps]$ 4、配置环境变量所有节点均要配置 1234[hadoop@hadoop1 spark]$ vi ~/.bashrc #Sparkexport SPARK_HOME=/home/hadoop/apps/sparkexport PATH=$PATH:$SPARK_HOME/bin 保存并使其立即生效 1[hadoop@hadoop1 spark]$ source ~/.bashrc 四、启动1、先启动zookeeper集群所有节点均要执行 123456789[hadoop@hadoop1 ~]$ zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /home/hadoop/apps/zookeeper-3.4.10/bin/../conf/zoo.cfgStarting zookeeper ... STARTED[hadoop@hadoop1 ~]$ zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /home/hadoop/apps/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower[hadoop@hadoop1 ~]$ 2、在启动HDFS集群任意一个节点执行即可 1[hadoop@hadoop1 ~]$ start-dfs.sh 3、在启动Spark集群在一个节点上执行 12[hadoop@hadoop1 ~]$ cd apps/spark/sbin/[hadoop@hadoop1 sbin]$ start-all.sh 4、查看进程 5、问题查看进程发现spark集群只有hadoop1成功启动了Master进程，其他3个节点均没有启动成功，需要手动启动，进入到/home/hadoop/apps/spark/sbin目录下执行以下命令，3个节点都要执行 12[hadoop@hadoop2 ~]$ cd ~/apps/spark/sbin/[hadoop@hadoop2 sbin]$ start-master.sh 6、执行之后再次查看进程Master进程和Worker进程都以启动成功 五、验证1、查看Web界面Master状态hadoop1是ALIVE状态，hadoop2、hadoop3和hadoop4均是STANDBY状态 hadoop1节点 hadoop2节点 hadoop3 hadoop4 2、验证HA的高可用手动干掉hadoop1上面的Master进程，观察是否会自动进行切换 干掉hadoop1上的Master进程之后，再次查看web界面 hadoo1节点，由于Master进程被干掉，所以界面无法访问 hadoop2节点，Master被干掉之后，hadoop2节点上的Master成功篡位成功，成为ALIVE状态 hadoop3节点 hadoop4节点 1、执行第一个Spark程序1234567[hadoop@hadoop3 ~]$ /home/hadoop/apps/spark/bin/spark-submit \\&gt; --class org.apache.spark.examples.SparkPi \\&gt; --master spark://hadoop1:7077 \\&gt; --executor-memory 500m \\&gt; --total-executor-cores 1 \\&gt; /home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar \\&gt; 100 其中的spark://hadoop1:7077是下图中的地址 运行结果 2、启动spark shell1234[hadoop@hadoop1 ~]$ /home/hadoop/apps/spark/bin/spark-shell \\&gt; --master spark://hadoop1:7077 \\&gt; --executor-memory 500m \\&gt; --total-executor-cores 1 参数说明： –master spark://hadoop1:7077 指定Master的地址 –executor-memory 500m:指定每个worker可用内存为500m –total-executor-cores 1: 指定整个集群使用的cup核数为1个 注意： 如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。 Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可 Spark Shell中已经默认将SparkSQl类初始化为对象spark。用户代码如果需要用到，则直接应用spark即可 3、 在spark shell中编写WordCount程序（1）编写一个hello.txt文件并上传到HDFS上的spark目录下 123[hadoop@hadoop1 ~]$ vi hello.txt[hadoop@hadoop1 ~]$ hadoop fs -mkdir -p /spark[hadoop@hadoop1 ~]$ hadoop fs -put hello.txt /spark hello.txt的内容如下 12345you,jumpi,jumpyou,jumpi,jumpjump （2）在spark shell中用scala语言编写spark程序 1scala&gt; sc.textFile(&quot;/spark/hello.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;/spark/out&quot;) 说明： sc是SparkContext对象，该对象是提交spark程序的入口 textFile(“/spark/hello.txt”)是hdfs中读取数据 flatMap(_.split(“ “))先map再压平 map((_,1))将单词和1构成元组 reduceByKey(+)按照key进行reduce，并将value累加 saveAsTextFile(“/spark/out”)将结果写入到hdfs中 （3）使用hdfs命令查看结果 12345[hadoop@hadoop2 ~]$ hadoop fs -cat /spark/out/p*(jump,5)(you,2)(i,2)[hadoop@hadoop2 ~]$ 七、 执行Spark程序on YARN1、前提成功启动zookeeper集群、HDFS集群、YARN集群 2、启动Spark on YARN1[hadoop@hadoop1 bin]$ spark-shell --master yarn --deploy-mode client 报错如下： 报错原因：内存资源给的过小，yarn直接kill掉进程，则报rpc连接失败、ClosedChannelException等错误。 解决方法： 先停止YARN服务，然后修改yarn-site.xml，增加如下内容 12345678910&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;Whether virtual memory limits will be enforced for containers&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt; &lt;value&gt;4&lt;/value&gt; &lt;description&gt;Ratio between virtual memory to physical memory when setting memory limits for containers&lt;/description&gt;&lt;/property&gt; 将新的yarn-site.xml文件分发到其他Hadoop节点对应的目录下，最后在重新启动YARN。 重新执行以下命令启动spark on yarn 1[hadoop@hadoop1 hadoop]$ spark-shell --master yarn --deploy-mode client 启动成功 3、打开YARN的web界面打开YARN WEB页面：http://hadoop4:8088可以看到Spark shell应用程序正在运行 单击ID号链接，可以看到该应用程序的详细信息 单击“ApplicationMaster”链接 4、运行程序12345678910scala&gt; val array = Array(1,2,3,4,5)array: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt; val rdd = sc.makeRDD(array)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at makeRDD at &lt;console&gt;:26scala&gt; rdd.countres0: Long = 5 scala&gt; 再次查看YARN的web界面 查看executors 5、执行Spark自带的示例程序PI 12345678[hadoop@hadoop1 ~]$ spark-submit --class org.apache.spark.examples.SparkPi \\&gt; --master yarn \\&gt; --deploy-mode cluster \\&gt; --driver-memory 500m \\&gt; --executor-memory 500m \\&gt; --executor-cores 1 \\&gt; /home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar &gt; 10 执行过程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687[hadoop@hadoop1 ~]$ spark-submit --class org.apache.spark.examples.SparkPi \\&gt; --master yarn \\&gt; --deploy-mode cluster \\&gt; --driver-memory 500m \\&gt; --executor-memory 500m \\&gt; --executor-cores 1 \\&gt; /home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar \\&gt; 102018-04-21 17:57:32 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable2018-04-21 17:57:34 INFO ConfiguredRMFailoverProxyProvider:100 - Failing over to rm22018-04-21 17:57:34 INFO Client:54 - Requesting a new application from cluster with 4 NodeManagers2018-04-21 17:57:34 INFO Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)2018-04-21 17:57:34 INFO Client:54 - Will allocate AM container, with 884 MB memory including 384 MB overhead2018-04-21 17:57:34 INFO Client:54 - Setting up container launch context for our AM2018-04-21 17:57:34 INFO Client:54 - Setting up the launch environment for our AM container2018-04-21 17:57:34 INFO Client:54 - Preparing resources for our AM container2018-04-21 17:57:36 WARN Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.2018-04-21 17:57:39 INFO Client:54 - Uploading resource file:/tmp/spark-93bd68c9-85de-482e-bbd7-cd2cee60e720/__spark_libs__8262081479435245591.zip -&gt; hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005/__spark_libs__8262081479435245591.zip2018-04-21 17:57:44 INFO Client:54 - Uploading resource file:/home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar -&gt; hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005/spark-examples_2.11-2.3.0.jar2018-04-21 17:57:44 INFO Client:54 - Uploading resource file:/tmp/spark-93bd68c9-85de-482e-bbd7-cd2cee60e720/__spark_conf__2498510663663992254.zip -&gt; hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005/__spark_conf__.zip2018-04-21 17:57:44 INFO SecurityManager:54 - Changing view acls to: hadoop2018-04-21 17:57:44 INFO SecurityManager:54 - Changing modify acls to: hadoop2018-04-21 17:57:44 INFO SecurityManager:54 - Changing view acls groups to: 2018-04-21 17:57:44 INFO SecurityManager:54 - Changing modify acls groups to: 2018-04-21 17:57:44 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set()2018-04-21 17:57:44 INFO Client:54 - Submitting application application_1524303370510_0005 to ResourceManager2018-04-21 17:57:44 INFO YarnClientImpl:273 - Submitted application application_1524303370510_00052018-04-21 17:57:45 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:45 INFO Client:54 - client token: N/A diagnostics: N/A ApplicationMaster host: N/A ApplicationMaster RPC port: -1 queue: default start time: 1524304664749 final status: UNDEFINED tracking URL: http://hadoop4:8088/proxy/application_1524303370510_0005/ user: hadoop2018-04-21 17:57:46 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:47 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:48 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:49 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:50 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:51 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:52 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:53 INFO Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)2018-04-21 17:57:54 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:57:54 INFO Client:54 - client token: N/A diagnostics: N/A ApplicationMaster host: 192.168.123.104 ApplicationMaster RPC port: 0 queue: default start time: 1524304664749 final status: UNDEFINED tracking URL: http://hadoop4:8088/proxy/application_1524303370510_0005/ user: hadoop2018-04-21 17:57:55 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:57:56 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:57:57 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:57:58 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:57:59 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:00 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:01 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:02 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:03 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:04 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:05 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:06 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:07 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:08 INFO Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)2018-04-21 17:58:09 INFO Client:54 - Application report for application_1524303370510_0005 (state: FINISHED)2018-04-21 17:58:09 INFO Client:54 - client token: N/A diagnostics: N/A ApplicationMaster host: 192.168.123.104 ApplicationMaster RPC port: 0 queue: default start time: 1524304664749 final status: SUCCEEDED tracking URL: http://hadoop4:8088/proxy/application_1524303370510_0005/ user: hadoop2018-04-21 17:58:09 INFO Client:54 - Deleted staging directory hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_00052018-04-21 17:58:09 INFO ShutdownHookManager:54 - Shutdown hook called2018-04-21 17:58:09 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-93bd68c9-85de-482e-bbd7-cd2cee60e7202018-04-21 17:58:09 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-06de6905-8067-4f1e-a0a0-bc8a51daf535[hadoop@hadoop1 ~]$","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"模板","slug":"2019-06-02-模板 - 副本 (8)","date":"2019-06-01T02:30:04.000Z","updated":"2019-09-16T01:53:35.632Z","comments":true,"path":"2019-06-02-模板 - 副本 (8).html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-模板 - 副本 (8).html","excerpt":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt; 1.1官网官网地址：http://spark.apache.org 1、什么是Spark","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"模板","slug":"2019-06-02-模板 - 副本 (11)","date":"2019-06-01T02:30:04.000Z","updated":"2019-09-16T01:53:35.632Z","comments":true,"path":"2019-06-02-模板 - 副本 (11).html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-模板 - 副本 (11).html","excerpt":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt; 1.1官网官网地址：http://spark.apache.org 1、什么是Spark","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"模板","slug":"2019-06-02-模板 - 副本 (6)","date":"2019-06-01T02:30:04.000Z","updated":"2019-09-16T01:53:35.632Z","comments":true,"path":"2019-06-02-模板 - 副本 (6).html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-模板 - 副本 (6).html","excerpt":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt; 1.1官网官网地址：http://spark.apache.org 1、什么是Spark","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"模板","slug":"2019-06-02-模板 - 副本 (9)","date":"2019-06-01T02:30:04.000Z","updated":"2019-09-16T01:53:35.632Z","comments":true,"path":"2019-06-02-模板 - 副本 (9).html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-模板 - 副本 (9).html","excerpt":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt; 1.1官网官网地址：http://spark.apache.org 1、什么是Spark","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"模板","slug":"2019-06-02-模板 - 副本 (10)","date":"2019-06-01T02:30:04.000Z","updated":"2019-09-16T01:53:35.632Z","comments":true,"path":"2019-06-02-模板 - 副本 (10).html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-模板 - 副本 (10).html","excerpt":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt; 1.1官网官网地址：http://spark.apache.org 1、什么是Spark","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"Spark学习之路 （一）Spark初识","slug":"2019-06-01-Spark学习之路 （一）Spark初识","date":"2019-06-01T02:30:04.000Z","updated":"2019-09-16T02:07:51.317Z","comments":true,"path":"2019-06-01-Spark学习之路 （一）Spark初识.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-01-Spark学习之路 （一）Spark初识.html","excerpt":"** Spark学习之路 （一）Spark初识：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** Spark学习之路 （一）Spark初识：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt; 1.1官网官网地址：http://spark.apache.org 1、什么是Spark Apache Spark™是用于大规模数据处理的统一分析引擎。 spark是一个实现快速通用的集群计算平台。它是由加州大学伯克利分校AMP实验室 开发的通用内存并行计算框架，用来构建大型的、低延迟的数据分析应用程序。它扩展了广泛使用的MapReduce计算 模型。高效的支撑更多计算模式，包括交互式查询和流处理。spark的一个主要特点是能够在内存中进行计算，及时依赖磁盘进行复杂的运算，Spark依然比MapReduce更加高效。 2、为什么要学Spark中间结果输出：基于MapReduce的计算引擎通常会将中间结果输出到磁盘上，进行存储和容错。出于任务管道承接的，考虑，当一些查询翻译到MapReduce任务时，往往会产生多个Stage，而这些串联的Stage又依赖于底层文件系统（如HDFS）来存储每一个Stage的输出结果。 Spark是MapReduce的替代方案，而且兼容HDFS、Hive，可融入Hadoop的生态系统，以弥补MapReduce的不足。 二、Spark的四大特性1、高效性运行速度提高100倍。 ​ Apache Spark使用最先进的DAG调度程序，查询优化程序和物理执行引擎，实现批量和流式数据的高性能。 2、易用性​ Spark提供80多个高级算法，可以轻松构建并行应用程序。您可以 从Scala，Python，R和SQL shell中以交互方式使用它。 3、通用性​ Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。 Spark组成(BDAS)：全称伯克利数据分析栈，通过大规模集成算法、机器、人之间展现大数据应用的一个平台。也是处理大数据、云计算、通信的技术解决方案。 它的主要组件有： SparkCore：将分布式数据抽象为弹性分布式数据集（RDD），实现了应用任务调度、RPC、序列化和压缩，并为运行在其上的上层组件提供API。 SparkSQL：Spark Sql 是Spark来操作结构化数据的程序包，可以让我使用SQL语句的方式来查询数据，Spark支持 多种数据源，包含Hive表，parquest以及JSON等内容。 SparkStreaming： 是Spark提供的实时数据进行流式计算的组件。 MLlib：提供常用机器学习算法的实现库。 GraphX：提供一个分布式图计算框架，能高效进行图计算。 4、兼容性​ Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。 Mesos：Spark可以运行在Mesos里面（Mesos 类似于yarn的一个资源调度框架） standalone：Spark自己可以给自己分配资源（master，worker） YARN：Spark可以运行在yarn上面 Kubernetes：Spark接收 Kubernetes的资源调度 三、应用场景Yahoo将Spark用在Audience Expansion中的应用，进行点击预测和即席查询等 淘宝技术团队使用了Spark来解决多次迭代的机器学习算法、高计算复杂度的算法等。应用于内容推荐、社区发现等腾讯大数据精准推荐借助Spark快速迭代的优势，实现了在“数据实时采集、算法实时训练、系统实时预测”的全流程实时并行高维算法，最终成功应用于广点通pCTR投放系统上。优酷土豆将Spark应用于视频推荐(图计算)、广告业务，主要实现机器学习、图计算等迭代计算。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"模板","slug":"2019-06-02-模板 - 副本 (12)","date":"2019-06-01T02:30:04.000Z","updated":"2019-09-16T01:53:35.632Z","comments":true,"path":"2019-06-02-模板 - 副本 (12).html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-模板 - 副本 (12).html","excerpt":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt; 1.1官网官网地址：http://spark.apache.org 1、什么是Spark","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"模板","slug":"2019-06-02-模板","date":"2019-06-01T02:30:04.000Z","updated":"2019-09-16T01:53:35.632Z","comments":true,"path":"2019-06-02-模板.html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-模板.html","excerpt":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt; 1.1官网官网地址：http://spark.apache.org 1、什么是Spark","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"模板","slug":"2019-06-02-模板 - 副本 (5)","date":"2019-06-01T02:30:04.000Z","updated":"2019-09-16T01:53:35.632Z","comments":true,"path":"2019-06-02-模板 - 副本 (5).html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-模板 - 副本 (5).html","excerpt":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt; 1.1官网官网地址：http://spark.apache.org 1、什么是Spark","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"模板","slug":"2019-06-02-模板 - 副本 (7)","date":"2019-06-01T02:30:04.000Z","updated":"2019-09-16T01:53:35.632Z","comments":true,"path":"2019-06-02-模板 - 副本 (7).html","link":"","permalink":"http://zhangfuxin.cn/2019-06-02-模板 - 副本 (7).html","excerpt":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识","text":"** 模板：** &lt;Excerpt in index | 首页摘要&gt; ​ Spark学习之路 （一）Spark初识 &lt;The rest of contents | 余下全文&gt; 1.1官网官网地址：http://spark.apache.org 1、什么是Spark","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"一文读懂 Spark","slug":"2018-10-30-spark-一文读懂spark","date":"2018-10-30T04:30:04.000Z","updated":"2019-09-15T23:44:33.871Z","comments":true,"path":"2018-10-30-spark-一文读懂spark.html","link":"","permalink":"http://zhangfuxin.cn/2018-10-30-spark-一文读懂spark.html","excerpt":"** 一文读懂 Spark：** &lt;Excerpt in index | 首页摘要&gt; ​ Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。","text":"** 一文读懂 Spark：** &lt;Excerpt in index | 首页摘要&gt; ​ Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。 &lt;The rest of contents | 余下全文&gt; 1.1 前言​ Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。 ​ Apache Spark 诞生于大名鼎鼎的 AMPLab（这里还诞生过 Mesos 和 Alluxio），从创立之初就带有浓厚的学术气质，其设计目标是为各种大数据处理需求提供一个统一的技术栈。如今 Spark 背后的商业公司 Databricks 创始人也是来自 AMPLab 的博士毕业生。 ​ Spark 本身使用 Scala 语言编写，Scala 是一门融合了面向对象与函数式的“双范式”语言，运行在 JVM 之上。Spark 大量使用了它的函数式、即时代码生成等特性。Spark 目前提供了 Java、Scala、Python、R 四种语言的 API，前两者因为同样运行在 JVM 上可以达到更原生的支持。 MapReduce 的问题所在 ​ Hadoop 是大数据处理领域的开创者。严格来说，Hadoop 不只是一个软件，而是一整套生态系统，例如 MapReduce 负责进行分布式计算，而 HDFS 负责存储大量文件。 ​ MapReduce 模型的诞生是大数据处理从无到有的飞跃。但随着技术的进步，对大数据处理的需求也变得越来越复杂，MapReduce 的问题也日渐凸显。通常，我们将 MapReduce 的输入和输出数据保留在 HDFS 上，很多时候，复杂的 ETL、数据清洗等工作无法用一次 MapReduce 完成，所以需要将多个 MapReduce 过程连接起来： ▲ 上图中只有两个 MapReduce 串联，实际上可能有几十个甚至更多，依赖关系也更复杂。 这种方式下，每次中间结果都要写入 HDFS 落盘保存，代价很大（别忘了，HDFS 的每份数据都需要冗余若干份拷贝）。另外，由于本质上是多次 MapReduce 任务，调度也比较麻烦，实时性无从谈起。 Spark 与 RDD 模型针对上面的问题，如果能把中间结果保存在内存里，岂不是快的多？之所以不能这么做，最大的障碍是：分布式系统必须能容忍一定的故障，所谓 fault-tolerance。如果只是放在内存中，一旦某个计算节点宕机，其他节点无法恢复出丢失的数据，只能重启整个计算任务，这对于动辄成百上千节点的集群来说是不可接受的。 一般来说，想做到 fault-tolerance 只有两个方案：要么存储到外部（例如 HDFS），要么拷贝到多个副本。Spark 大胆地提出了第三种——重算一遍。但是之所以能做到这一点，是依赖于一个额外的假设：所有计算过程都是确定性的（deterministic）。Spark 借鉴了函数式编程思想，提出了 RDD（Resilient Distributed Datasets），译作“弹性分布式数据集”。 RDD 是一个只读的、分区的（partitioned）数据集合。RDD 要么来源于不可变的外部文件（例如 HDFS 上的文件），要么由确定的算子由其他 RDD 计算得到。RDD 通过算子连接构成有向无环图（DAG），上图演示了一个简单的例子，其中节点对应 RDD，边对应算子。 回到刚刚的问题，RDD 如何做到 fault-tolerance？很简单，RDD 中的每个分区都能被确定性的计算出来，所以一旦某个分区丢失了，另一个计算节点可以从它的前继节点出发、用同样的计算过程重算一次，即可得到完全一样的 RDD 分区。这个过程可以递归的进行下去。 ▲ 上图演示了 RDD 分区的恢复。为了简洁并没有画出分区，实际上恢复是以分区为单位的。 Spark 的编程接口和 Java 8 的 Stream 很相似：RDD 作为数据，在多种算子间变换，构成对执行计划 DAG 的描述。最后，一旦遇到类似 collect()这样的输出命令，执行计划会被发往 Spark 集群、开始计算。不难发现，算子分成两类： map()、filter()、join() 等算子称为 Transformation，它们输入一个或多个 RDD，输出一个 RDD。 collect()、count()、save() 等算子称为 Action，它们通常是将数据收集起来返回； ▲ 上图的例子用来收集包含“HDFS”关键字的错误日志时间戳。当执行到 collect() 时，右边的执行计划开始运行。 像之前提到的，RDD 的数据由多个分区（partition）构成，这些分区可以分布在集群的各个机器上，这也就是 RDD 中 “distributed” 的含义。熟悉 DBMS 的同学可以把 RDD 理解为逻辑执行计划，partition 理解为物理执行计划。 此外，RDD 还包含它的每个分区的依赖分区（dependency），以及一个函数指出如何计算出本分区的数据。Spark 的设计者发现，依赖关系依据执行方式的不同可以很自然地分成两种：窄依赖（Narrow Dependency）和宽依赖（Wide Dependency），举例来说： map()、filter() 等算子构成窄依赖：生产的每个分区只依赖父 RDD 中的一个分区。 groupByKey() 等算子构成宽依赖：生成的每个分区依赖父 RDD 中的多个分区（往往是全部分区）。 ▲ 左图展示了宽依赖和窄依赖，其中 Join 算子因为 Join key 分区情况不同二者皆有；右图展示了执行过程，由于宽依赖的存在，执行计划被分成 3 个阶段。 在执行时，窄依赖可以很容易的按流水线（pipeline）的方式计算：对于每个分区从前到后依次代入各个算子即可。然而，宽依赖需要等待前继 RDD 中所有分区计算完成；换句话说，宽依赖就像一个栅栏（barrier）会阻塞到之前的所有计算完成。整个计算过程被宽依赖分割成多个阶段（stage），如上右图所示。 了解 MapReduce 的同学可能已经发现，宽依赖本质上就是一个 MapReduce 过程。但是相比 MapReduce 自己写 Map 和 Reduce 函数的编程接口，Spark 的接口要容易的多；并且在 Spark 中，多个阶段的 MapReduce 只需要构造一个 DAG 即可。 声明式接口：Spark SQL命令式编程中，你需要编写一个程序。下面给出了一种伪代码实现： employees = db.getAllEmployees() countByDept = dict() // 统计各部门女生人数 (dept_id -&gt; count) for employee in employees: if (employee.gender == ‘female’) countByDept[employee.dept_id] += 1 results = list() // 加上 dept.name 列 depts = db.getAllDepartments() for dept in depts: if (countByDept containsKey dept.id) results.add(row(dept.id, dept.name, countByDept[dept.id])) return results; 声明式编程中，你只要用关系代数的运算表达出结果： employees.join(dept, employees.deptId == dept.id) .where(employees.gender == ‘female’) .groupBy(dept.id, dept.name) .agg() 等价地，如果你更熟悉 SQL，也可以写成这样： SELECTdept.id,dept.name,COUNT(*)FROMemployees JOINdept ONemployees.dept_id ==dept.idWHEREemployees.gender =’female’GROUPBYdept.id,dept.name 显然，声明式的要简洁的多！但声明式编程依赖于执行者产生真正的程序代码，所以除了上面这段程序，还需要把数据模型（即 schema）一并告知执行者。声明式编程最广为人知的形式就是 SQL。 Spark SQL 就是这样一个基于 SQL 的声明式编程接口。你可以将它看作在 Spark 之上的一层封装，在 RDD 计算模型的基础上，提供了 DataFrame API 以及一个内置的 SQL 执行计划优化器 Catalyst。 ▲ 上图黄色部分是 Spark SQL 中新增的部分。 DataFrame 就像数据库中的表，除了数据之外它还保存了数据的 schema 信息。计算中，schema 信息也会经过算子进行相应的变换。DataFrame 的数据是行（row）对象组成的 RDD，对 DataFrame 的操作最终会变成对底层 RDD 的操作。 Catalyst 是一个内置的 SQL 优化器，负责把用户输入的 SQL 转化成执行计划。Catelyst 强大之处是它利用了 Scala 提供的代码生成（codegen）机制，物理执行计划经过编译，产出的执行代码效率很高，和直接操作 RDD 的命令式代码几乎没有分别。 ▲ 上图是 Catalyst 的工作流程，与大多数 SQL 优化器一样是一个 Cost-Based Optimizer (CBO)，但最后使用代码生成（codegen）转化成直接对 RDD 的操作。 流计算框架：Spark Streaming以往，批处理和流计算被看作大数据系统的两个方面。我们常常能看到这样的架构——以 Kafka、Storm 为代表的流计算框架用于实时计算，而 Spark 或 MapReduce 则负责每天、每小时的数据批处理。在 ETL 等场合，这样的设计常常导致同样的计算逻辑被实现两次，耗费人力不说，保证一致性也是个问题。 Spark Streaming 正是诞生于此类需求。传统的流计算框架大多注重于低延迟，采用了持续的（continuous）算子模型；而 Spark Streaming 基于 Spark，另辟蹊径提出了 D-Stream（Discretized Streams）方案：将流数据切成很小的批（micro-batch），用一系列的短暂、无状态、确定性的批处理实现流处理。 Spark Streaming 的做法在流计算框架中很有创新性，它虽然牺牲了低延迟（一般流计算能做到 100ms 级别，Spark Streaming 延迟一般为 1s 左右），但是带来了三个诱人的优势： 更高的吞吐量（大约是 Storm 的 2-5 倍） 更快速的失败恢复（通常只要 1-2s），因此对于 straggler（性能拖后腿的节点）直接杀掉即可 开发者只需要维护一套 ETL 逻辑即可同时用于批处理和流计算 ▲ 上左图中，为了在持续算子模型的流计算系统中保证一致性，不得不在主备机之间使用同步机制，导致性能损失，Spark Streaming 完全没有这个问题；右图是 D-Stream 的原理示意图。 你可能会困惑，流计算中的状态一直是个难题。但我们刚刚提到 D-Stream 方案是无状态的，那诸如 word count 之类的问题，怎么做到保持 count 算子的状态呢？ 答案是通过 RDD：将前一个时间步的 RDD 作为当前时间步的 RDD 的前继节点，就能造成状态不断更替的效果。实际上，新的状态 RDD 总是不断生成，而旧的 RDD 并不会被“替代”，而是作为新 RDD 的前继依赖。对于底层的 Spark 框架来说，并没有时间步的概念，有的只是不断扩张的 DAG 图和新的 RDD 节点。 ▲ 上图是流式计算 word count 的例子，count 结果在不同时间步中不断累积。 那么另一个问题也随之而来：随着时间的推进，上图中的状态 RDD counts会越来越多，他的祖先（lineage）变得越来越长，极端情况下，恢复过程可能溯源到很久之前。这是不可接受的！因此，Spark Streming 会定期地对状态 RDD 做 checkpoint，将其持久化到 HDFS 等存储中，这被称为 lineage cut，在它之前更早的 RDD 就可以没有顾虑地清理掉了。 关于流行的几个开源流计算框架的对比，可以参考文章 Comparison of Apache Stream Processing Frameworks。 流计算与 SQL：Spark Structured Streaming 出人意料的是，Spark Structured Streaming 的流式计算引擎并没有复用 Spark Streaming，而是在 Spark SQL 上设计了新的一套引擎。因此，从 Spark SQL 迁移到 Spark Structured Streaming 十分容易，但从 Spark Streaming 迁移过来就要困难得多。 很自然的，基于这样的模型，Spark SQL 中的大部分接口、实现都得以在 Spark Structured Streaming 中直接复用。将用户的 SQL 执行计划转化成流计算执行计划的过程被称为增量化（incrementalize），这一步是由 Spark 框架自动完成的。对于用户来说只要知道：每次计算的输入是某一小段时间的流数据，而输出是对应数据产生的计算结果。 ▲ 左图是 Spark Structured Streaming 模型示意图；右图展示了同一个任务的批处理、流计算版本，可以看到，除了输入输出不同，内部计算过程完全相同。 与 Spark SQL 相比，流式 SQL 计算还有两个额外的特性，分别是窗口（window）和水位（watermark）。 窗口（window）是对过去某段时间的定义。批处理中，查询通常是全量的（例如：总用户量是多少）；而流计算中，我们通常关心近期一段时间的数据（例如：最近24小时新增的用户量是多少）。用户通过选用合适的窗口来获得自己所需的计算结果，常见的窗口有滑动窗口（Sliding Window）、滚动窗口（Tumbling Window）等。 水位（watermark）用来丢弃过早的数据。在流计算中，上游的输入事件可能存在不确定的延迟，而流计算系统的内存是有限的、只能保存有限的状态，一定时间之后必须丢弃历史数据。以双流 A JOIN B 为例，假设窗口为 1 小时，那么 A 中比当前时间减 1 小时更早的数据（行）会被丢弃；如果 B 中出现 1 小时前的事件，因为无法处理只能忽略。 ▲ 上图为水位的示意图，“迟到”太久的数据（行）由于已经低于当前水位无法处理，将被忽略。 水位和窗口的概念都是因时间而来。在其他流计算系统中，也存在相同或类似的概念。 关于 SQL 的流计算模型，常常被拿来对比的还有另一个流计算框架 Apache Flink。与 Spark 相比，它们的实现思路有很大不同，但在模型上是很相似的。 系统架构 驱动程序（Driver）即用户编写的程序，对应一个 SparkContext，负责任务的构造、调度、故障恢复等。驱动程序可以直接运行在客户端，例如用户的应用程序中；也可以托管在 Master 上，这被称为集群模式（cluster mode），通常用于流计算等长期任务。 Cluster Manager顾名思义负责集群的资源分配，Spark 自带的 Spark Master 支持任务的资源分配，并包含一个 Web UI 用来监控任务运行状况。多个 Master 可以构成一主多备，通过 ZooKeeper 进行协调和故障恢复。通常 Spark 集群使用 Spark Master 即可，但如果用户的集群中不仅有 Spark 框架、还要承担其他任务，官方推荐使用 Mesos 作为集群调度器。 Worker节点负责执行计算任务，上面保存了 RDD 等数据。 总结Spark 是一个同时支持批处理和流计算的分布式计算系统。Spark 的所有计算均构建于 RDD 之上，RDD 通过算子连接形成 DAG 的执行计划，RDD 的确定性及不可变性是 Spark 实现故障恢复的基础。Spark Streaming 的 D-Stream 本质上也是将输入数据分成一个个 micro-batch 的 RDD。 Spark SQL 是在 RDD 之上的一层封装，相比原始 RDD，DataFrame API 支持数据表的 schema 信息，从而可以执行 SQL 关系型查询，大幅降低了开发成本。Spark Structured Streaming 是 Spark SQL 的流计算版本，它将输入的数据流看作不断追加的数据行。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/tags/Spark/"}],"keywords":[{"name":"Spark","slug":"Spark","permalink":"http://zhangfuxin.cn/categories/Spark/"}]},{"title":"分布式锁的几种实现方式","slug":"dslock","date":"2018-03-02T14:18:29.000Z","updated":"2019-08-29T03:01:15.917Z","comments":true,"path":"dslock.html","link":"","permalink":"http://zhangfuxin.cn/dslock.html","excerpt":"** 分布式锁的几种实现方式：** &lt;Excerpt in index | 首页摘要&gt;在分布式架构中，由于多线程和多台服务器，何难保证顺序性。如果需要对某一个资源进行限制，比如票务，比如请求幂等性控制等，这个时候分布式锁就排上用处。","text":"** 分布式锁的几种实现方式：** &lt;Excerpt in index | 首页摘要&gt;在分布式架构中，由于多线程和多台服务器，何难保证顺序性。如果需要对某一个资源进行限制，比如票务，比如请求幂等性控制等，这个时候分布式锁就排上用处。 &lt;The rest of contents | 余下全文&gt; 什么是分布式锁分布式锁是控制分布式系统或不同系统之间共同访问共享资源的一种锁实现，如果不同的系统或同一个系统的不同主机之间共享了某个资源时，往往需要互斥来防止彼此干扰来保证一致性。 分布式锁需要解决的问题 互斥性：任意时刻，只能有一个客户端获取锁，不能同时有两个客户端获取到锁。 安全性：锁只能被持有该锁的客户端删除，不能由其它客户端删除。 死锁：获取锁的客户端因为某些原因（如down机等）而未能释放锁，其它客户端再也无法获取到该锁。 容错：当部分节点（redis节点等）down机时，客户端仍然能够获取锁和释放锁。 分布式锁的实现方式 数据库实现 缓存实现，比如redis zookeeper实现","categories":[{"name":"分布式架构","slug":"分布式架构","permalink":"http://zhangfuxin.cn/categories/分布式架构/"}],"tags":[{"name":"java","slug":"java","permalink":"http://zhangfuxin.cn/tags/java/"}],"keywords":[{"name":"分布式架构","slug":"分布式架构","permalink":"http://zhangfuxin.cn/categories/分布式架构/"}]},{"title":"分布式系统理论基础","slug":"dsbasic","date":"2018-02-26T14:31:40.000Z","updated":"2019-08-29T03:00:59.583Z","comments":true,"path":"dsbasic.html","link":"","permalink":"http://zhangfuxin.cn/dsbasic.html","excerpt":"** 分布式系统理论基础：** &lt;Excerpt in index | 首页摘要&gt;分布式系统不是万能，不能解决所有痛点。在高可用，一致性，分区容错性必须有所权衡。","text":"** 分布式系统理论基础：** &lt;Excerpt in index | 首页摘要&gt;分布式系统不是万能，不能解决所有痛点。在高可用，一致性，分区容错性必须有所权衡。 &lt;The rest of contents | 余下全文&gt; CAP理论定理：任何分布式架构都只能同时满足两点，无法三者兼顾。 Consistency（一致性），数据一致更新，所有的数据变动都是同步的。 Availability（可用性），好的响应性能。 Partition tolerance（分区容忍性）可靠性，机器宕机是否影响使用。 关系数据库的ACID模型拥有 高一致性 + 可用性 很难进行分区： Atomicity原子性：一个事务中所有操作都必须全部完成，要么全部不完成。 Consistency一致性. 在事务开始或结束时，数据库应该在一致状态。 Isolation隔离性. 事务将假定只有它自己在操作数据库，彼此不知晓。 Durability持久性 一旦事务完成，就不能返回。跨数据库两段提交事务：2PC (two-phase commit)， 2PC is the anti-scalability pattern (Pat Helland)是反可伸缩模式的，JavaEE中的JTA事务可以支持2PC。因为2PC是反模式，尽量不要使用2PC，使用BASE来回避。 BASE理论 Basically Available 基本可用，支持分区失败 Soft state 软状态，允许状态某个时间短不同步，或者异步 Eventually consistent 最终一致性，要求数据最终结果一致，而不是时刻高度一致。 paxos协议Paxos算法的目的是为了解决分布式环境下一致性的问题。多个节点并发操纵数据，如何保证在读写过程中数据的一致性，并且解决方案要能适应分布式环境下的不可靠性（系统如何就一个值达到统一）。 Paxos的两个组件: Proposer：提议发起者，处理客户端请求，将客户端的请求发送到集群中，以便决定这个值是否可以被批准。 Acceptor:提议批准者，负责处理接收到的提议，他们的回复就是一次投票。会存储一些状态来决定是否接收一个值 Paxos有两个原则 安全原则—保证不能做错的事 a） 针对某个实例的表决只能有一个值被批准，不能出现一个被批准的值被另一个值覆盖的情况；(假设有一个值被多数Acceptor批准了，那么这个值就只能被学习) b） 每个节点只能学习到已经被批准的值，不能学习没有被批准的值。 存活原则—只要有多数服务器存活并且彼此间可以通信，最终都要做到的下列事情： a）最终会批准某个被提议的值； b）一个值被批准了，其他服务器最终会学习到这个值。 zab协议(ZooKeeper Atomic broadcast protocol)ZAB协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。 Phase 0: Leader election（选举阶段）节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。只有到达 Phase 3 准 leader 才会成为真正的 leader。这一阶段的目的是就是为了选出一个准 leader，然后进入下一个阶段。 Phase 1: Discovery（发现阶段）在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。这个一阶段的主要目的是发现当前大多数节点接收的最新提议，并且准 leader 生成新的 epoch，让 followers 接受，更新它们的 acceptedEpoch。一个 follower 只会连接一个 leader，如果有一个节点 f 认为另一个 follower p 是 leader，f 在尝试连接 p 时会被拒绝，f 被拒绝之后，就会进入 Phase 0。 Phase 2: Synchronization（同步阶段）同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。只有当 quorum 都同步完成，准 leader 才会成为真正的 leader。follower 只会接收 zxid 比自己的 lastZxid 大的提议。 Phase 3: Broadcast（广播阶段）到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。 raft协议在Raft中，每个结点会处于下面三种状态中的一种： follower所有结点都以follower的状态开始。如果没收到leader消息则会变成candidate状态。 candidate会向其他结点“拉选票”，如果得到大部分的票则成为leader。这个过程就叫做Leader选举(Leader Election) leader所有对系统的修改都会先经过leader。每个修改都会写一条日志(log entry)。leader收到修改请求后的过程如下，这个过程叫做日志复制(Log Replication)： 1. 复制日志到所有follower结点(replicate entry) 2. 大部分结点响应时才提交日志 3. 通知所有follower结点日志已提交 4. 所有follower也提交日志 5. 现在整个系统处于一致的状态","categories":[{"name":"分布式架构","slug":"分布式架构","permalink":"http://zhangfuxin.cn/categories/分布式架构/"}],"tags":[{"name":"protocol","slug":"protocol","permalink":"http://zhangfuxin.cn/tags/protocol/"}],"keywords":[{"name":"分布式架构","slug":"分布式架构","permalink":"http://zhangfuxin.cn/categories/分布式架构/"}]},{"title":"为什么使用zookeeper？","slug":"zookeeper","date":"2018-02-18T14:15:44.000Z","updated":"2019-08-29T02:22:11.829Z","comments":true,"path":"zookeeper.html","link":"","permalink":"http://zhangfuxin.cn/zookeeper.html","excerpt":"** 为什么使用zookeeper？：** &lt;Excerpt in index | 首页摘要&gt;随着大型互联网的发展，分布式系统应用越来越来越广泛，zookeeper成了分布式系统的标配。集群容错，动态负载均衡，动态扩容，异地多活等架构都依赖于zookeeper而搭建。","text":"** 为什么使用zookeeper？：** &lt;Excerpt in index | 首页摘要&gt;随着大型互联网的发展，分布式系统应用越来越来越广泛，zookeeper成了分布式系统的标配。集群容错，动态负载均衡，动态扩容，异地多活等架构都依赖于zookeeper而搭建。 &lt;The rest of contents | 余下全文&gt; zookeeper是什么？zookeeper是由雅虎创建的，基于google chubby,一个开源的分布式协调服务，是分布式数据一致性的解决方案。 zookeeper的特性 顺序一致性，从同一个客户端发起的事务请求，最终会严格按照顺序被应用到zookeeper中。 原子性，事务请求在所有集群是一致的，要么都成功，要么都失败。 可靠性，一旦服务器成功应用某个事务，那么所有集群中一定同步并保留。 实时性，一个事务被应用，客户端能立即从服务端读取到状态变化。 zookeeper的原理？基于分布式协议pasxo，而实现了自己的zab协议。保证数据的一致性。 zookeeper的数据模型 持久化节点，节点创建后一直存在，直到主动删除。 持久化有序节点，每个节点都会为它的一级子节点维护一个顺序。 临时节点，临时节点的生命周期和客户端会话保持一直。客户端会话失效，节点自动清理。 临时有序节点，临时节点基础上多一个顺序性特性。 zookeeper使用场景有哪些？ 订阅发布 watcher机制 统一配置管理(disconf) 分布式锁（redis也可以） 分布式队列 负载均衡(dubbo) ID生成器 master选举(kafka,hadoop,hbase) 集群角色有哪些？leader 事务请求的唯一调度者和处理者，保证集群事务的处理顺序 集群内部服务的调度者 follower 处理非事务请求，以及转发事务请求到leader 参与事务请求提议的投票 参与leader选举的投票 observer 观察集群中最新状态的变化并同步到observer服务器上 增加observer不影响集群事务处理能力，还能提升非事务请求的处理能力 zookeeper集群为什么是奇数？zookeeper事务请求提议需要超过半数的机器，假如是2(n+1)台,需要n+2台机器同意，由于在增删改操作中需要半数以上服务器通过，来分析以下情况。2台服务器，至少2台正常运行才行（2的半数为1，半数以上最少为2），正常运行1台服务器都不允许挂掉3台服务器，至少2台正常运行才行（3的半数为1.5，半数以上最少为2），正常运行可以允许1台服务器挂掉4台服务器，至少3台正常运行才行（4的半数为2，半数以上最少为3），正常运行可以允许1台服务器挂掉5台服务器，至少3台正常运行才行（5的半数为2.5，半数以上最少为3），正常运行可以允许2台服务器挂掉6台服务器，至少3台正常运行才行（6的半数为3，半数以上最少为4），正常运行可以允许2台服务器挂掉 zookeeper日志管理？leader选举的原理未完待续","categories":[{"name":"分布式架构","slug":"分布式架构","permalink":"http://zhangfuxin.cn/categories/分布式架构/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://zhangfuxin.cn/tags/zookeeper/"}],"keywords":[{"name":"分布式架构","slug":"分布式架构","permalink":"http://zhangfuxin.cn/categories/分布式架构/"}]},{"title":"查找链表倒数第n个元素","slug":"descNode","date":"2018-02-16T13:45:45.000Z","updated":"2019-08-29T03:00:24.260Z","comments":true,"path":"descNode.html","link":"","permalink":"http://zhangfuxin.cn/descNode.html","excerpt":"** 查找链表倒数第n个元素：** &lt;Excerpt in index | 首页摘要&gt;链表应用很广泛，有单向链表，双向链表。单向链表如何查找倒数第n个元素呢？本文以java代码实现链表反向查找。","text":"** 查找链表倒数第n个元素：** &lt;Excerpt in index | 首页摘要&gt;链表应用很广泛，有单向链表，双向链表。单向链表如何查找倒数第n个元素呢？本文以java代码实现链表反向查找。 &lt;The rest of contents | 余下全文&gt; 单向链表的定义单向链表，主要有数据存储，下一个节点的引用这两个元素组成。 12345678public class Node &#123; int value; Node next; Node(int value) &#123; this.value = value; &#125;&#125; 遍历倒数第n个元素在查找过程中，设置两个指针，让其中一个指针比另一个指针先前移k-1步，然后两个指针同时往前移动。循环直到先行的指针指为NULL时，另一个指针所指的位置就是所要找的位置算法复杂度为o（n） 123456789101112131415161718192021222324public Node findDescEle(Node head, int k) &#123; if (k &lt; 1 || head == null) &#123; return null; &#125; Node p1 = head; Node p2 = head; //前移k-1步 int step = 0; for (int i = 0; i &lt; k; i++) &#123; step++; if (p1.next != null) &#123; p1 = p1.next; &#125; else &#123; return null; &#125; &#125; while (p1 != null) &#123; step++; p1 = p1.next; p2 = p2.next; &#125; System.out.println(&quot;o(n)==&quot; + step); return p2;&#125; 总结查找链表倒数第n个元素，复杂度为o(n),使用两个指针即可简单实现。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"sprigmvc项目转为springboot","slug":"sprigmvc2boot","date":"2018-02-06T14:12:55.000Z","updated":"2019-08-29T02:30:40.437Z","comments":true,"path":"sprigmvc2boot.html","link":"","permalink":"http://zhangfuxin.cn/sprigmvc2boot.html","excerpt":"** sprigmvc项目转为springboot：** &lt;Excerpt in index | 首页摘要&gt;是否有老掉牙的springmvc项目，想转成springboot项目，看这个文章就对了。","text":"** sprigmvc项目转为springboot：** &lt;Excerpt in index | 首页摘要&gt;是否有老掉牙的springmvc项目，想转成springboot项目，看这个文章就对了。 &lt;The rest of contents | 余下全文&gt; 说明 如果你的项目连maven项目都不是，请自行转为maven项目，在按照本教程进行。 本教程适用于spring+springmvc+mybatis+shiro的maven项目。 1.修改pom文件依赖 删除之前的spring依赖，添加springboot依赖 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;!-- 这个是剔除掉自带的 tomcat部署的--&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- tomcat容器部署 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;!--&lt;scope&gt;compile&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;!-- 支持 @ConfigurationProperties 注解 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 添加springboot构建插件 12345678910111213141516171819202122&lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt;&lt;/plugins&gt; 2.添加application启动文件注意，如果Application在controller，service，dao的上一层包里，无需配置@ComponentScan,否则，需要指明要扫描的包。 12345678910111213@SpringBootApplication//@ComponentScan(&#123;\"com.cms.controller\",\"com.cms.service\",\"com.cms.dao\"&#125;)public class Application extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) &#123; return application.sources(Application.class); &#125; public static void main(String[] args) throws Exception &#123; SpringApplication.run(Application.class, args); &#125;&#125; 3.添加springboot配置文件 在resources下面添加application.properties文件 添加基本配置1234567891011121314151617#默认前缀server.contextPath=/# 指定环境spring.profiles.active=local# jsp配置spring.mvc.view.prefix=/WEB-INF/jsp/spring.mvc.view.suffix=.jsp#log配置文件logging.config=classpath:logback-cms.xml#log路径logging.path=/Users/mac/work-tommy/cms-springboot/logs/#数据源spring.datasource.name=adminDataSourcespring.datasource.driverClassName = com.mysql.jdbc.Driverspring.datasource.url = jdbc:mysql://localhost:3306/mycms?useUnicode=true&amp;autoReconnect=true&amp;characterEncoding=utf-8spring.datasource.username = rootspring.datasource.password = 123456 4.使用@Configuration注入配置 注入mybatis配置,分页插件请自主选择 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Configuration@MapperScan(basePackages = \"com.kuwo.dao\",sqlSessionTemplateRef = \"adminSqlSessionTemplate\")public class AdminDataSourceConfig &#123; @Bean(name = \"adminDataSource\") @ConfigurationProperties(prefix = \"spring.datasource\") @Primary public DataSource adminDataSource() &#123; return DataSourceBuilder.create().build(); &#125; @Bean(name = \"adminSqlSessionFactory\") @Primary public SqlSessionFactory adminSqlSessionFactory(@Qualifier(\"adminDataSource\") DataSource dataSource) throws Exception &#123; SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dataSource); //分页插件// PageHelper pageHelper = new PageHelper(); PagePlugin pagePlugin = new PagePlugin();// Properties props = new Properties();// props.setProperty(\"reasonable\", \"true\");// props.setProperty(\"supportMethodsArguments\", \"true\");// props.setProperty(\"returnPageInfo\", \"check\");// props.setProperty(\"params\", \"count=countSql\");// pageHelper.setProperties(props); //添加插件 bean.setPlugins(new Interceptor[]&#123;pagePlugin&#125;); // 添加mybatis配置文件 bean.setConfigLocation(new DefaultResourceLoader().getResource(\"classpath:mybatis/mybatis-config.xml\")); // 添加mybatis映射文件 bean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(\"classpath:mybatis/system/*.xml\")); return bean.getObject(); &#125; @Bean(name = \"adminTransactionManager\") @Primary public DataSourceTransactionManager adminTransactionManager(@Qualifier(\"adminDataSource\") DataSource dataSource) &#123; return new DataSourceTransactionManager(dataSource); &#125; @Bean(name = \"adminSqlSessionTemplate\") @Primary public SqlSessionTemplate adminSqlSessionTemplate(@Qualifier(\"adminSqlSessionFactory\") SqlSessionFactory sqlSessionFactory) throws Exception &#123; return new SqlSessionTemplate(sqlSessionFactory); &#125;&#125; 添加Interceptor配置,注意addInterceptor的顺序，不要搞乱了 1234567@Configurationpublic class InterceptorConfiguration extends WebMvcConfigurerAdapter&#123; @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(new LoginHandlerInterceptor()); &#125;&#125; 添加shiro配置文件 注意：本来使用redis做session缓存，但是和shiro集成发现一个问题，user对象存储以后，从shiro中获取后，无法进行类型转换，所以暂时放弃了redis做session缓存。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140@Configurationpublic class ShiroConfiguration &#123; @Value(\"$&#123;spring.redis.host&#125;\") private String host; @Value(\"$&#123;spring.redis.port&#125;\") private int port; @Value(\"$&#123;spring.redis.timeout&#125;\") private int timeout; @Bean public static LifecycleBeanPostProcessor getLifecycleBeanPostProcessor() &#123; return new LifecycleBeanPostProcessor(); &#125; /** * ShiroFilterFactoryBean 处理拦截资源文件问题。 * 注意：单独一个ShiroFilterFactoryBean配置是或报错的，因为在 * 初始化ShiroFilterFactoryBean的时候需要注入：SecurityManager * Filter Chain定义说明 1、一个URL可以配置多个Filter，使用逗号分隔 2、当设置多个过滤器时，全部验证通过，才视为通过 3、部分过滤器可指定参数，如perms，roles * */ @Bean public ShiroFilterFactoryBean shiroFilter(SecurityManager securityManager)&#123; System.out.println(\"ShiroConfiguration.shirFilter()\"); ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean(); // 必须设置 SecurityManager shiroFilterFactoryBean.setSecurityManager(securityManager); // 如果不设置默认会自动寻找Web工程根目录下的\"/login.jsp\"页面 shiroFilterFactoryBean.setLoginUrl(\"/login_toLogin\"); // 登录成功后要跳转的链接 shiroFilterFactoryBean.setSuccessUrl(\"/usersPage\"); //未授权界面; shiroFilterFactoryBean.setUnauthorizedUrl(\"/403\"); //拦截器. Map&lt;String,String&gt; filterChainDefinitionMap = new LinkedHashMap&lt;&gt;(); //配置退出 过滤器,其中的具体的退出代码Shiro已经替我们实现了 filterChainDefinitionMap.put(\"/logout\", \"logout\"); filterChainDefinitionMap.put(\"/login_toLogin\", \"anon\"); filterChainDefinitionMap.put(\"/login_login\", \"anon\"); filterChainDefinitionMap.put(\"/static/login/**\",\"anon\"); filterChainDefinitionMap.put(\"/static/js/**\",\"anon\"); filterChainDefinitionMap.put(\"/uploadFiles/uploadImgs/**\",\"anon\"); filterChainDefinitionMap.put(\"/code.do\",\"anon\"); filterChainDefinitionMap.put(\"/font-awesome/**\",\"anon\"); //&lt;!-- 过滤链定义，从上向下顺序执行，一般将 /**放在最为下边 --&gt;:这是一个坑呢，一不小心代码就不好使了; //&lt;!-- authc:所有url都必须认证通过才可以访问; anon:所有url都都可以匿名访问--&gt; filterChainDefinitionMap.put(\"/**\", \"authc\"); shiroFilterFactoryBean.setFilterChainDefinitionMap(filterChainDefinitionMap); return shiroFilterFactoryBean; &#125; @Bean public SecurityManager securityManager()&#123; DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); //设置realm. securityManager.setRealm(myShiroRealm()); // 自定义缓存实现 使用redis //securityManager.setCacheManager(cacheManager()); // 自定义session管理 使用redis securityManager.setSessionManager(sessionManager()); return securityManager; &#125; @Bean public ShiroRealm myShiroRealm()&#123; ShiroRealm myShiroRealm = new ShiroRealm();// myShiroRealm.setCredentialsMatcher(hashedCredentialsMatcher()); return myShiroRealm; &#125;&#125; /** * 开启shiro aop注解支持. * 使用代理方式;所以需要开启代码支持; * @param securityManager * @return */ @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor(SecurityManager securityManager)&#123; AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor = new AuthorizationAttributeSourceAdvisor(); authorizationAttributeSourceAdvisor.setSecurityManager(securityManager); return authorizationAttributeSourceAdvisor; &#125; /** * 配置shiro redisManager * 使用的是shiro-redis开源插件 * @return */ public RedisManager redisManager() &#123; RedisManager redisManager = new RedisManager(); redisManager.setHost(host); redisManager.setPort(port); redisManager.setExpire(1800); redisManager.setTimeout(timeout); // redisManager.setPassword(password); return redisManager; &#125; /** * cacheManager 缓存 redis实现 * 使用的是shiro-redis开源插件 * @return */ public RedisCacheManager cacheManager() &#123; RedisCacheManager redisCacheManager = new RedisCacheManager(); redisCacheManager.setRedisManager(redisManager()); return redisCacheManager; &#125; /** * RedisSessionDAO shiro sessionDao层的实现 通过redis * 使用的是shiro-redis开源插件 */ @Bean public RedisSessionDAO redisSessionDAO() &#123; RedisSessionDAO redisSessionDAO = new RedisSessionDAO(); redisSessionDAO.setRedisManager(redisManager()); return redisSessionDAO; &#125; @Bean public DefaultWebSessionManager sessionManager() &#123; DefaultWebSessionManager sessionManager = new DefaultWebSessionManager();// sessionManager.setSessionDAO(redisSessionDAO()); return sessionManager; &#125;&#125; 总结搞了一天时间把项目转成springboot，查阅各种资料，希望这篇文章能够为你带来帮助。","categories":[{"name":"项目实战","slug":"项目实战","permalink":"http://zhangfuxin.cn/categories/项目实战/"}],"tags":[{"name":"java","slug":"java","permalink":"http://zhangfuxin.cn/tags/java/"}],"keywords":[{"name":"项目实战","slug":"项目实战","permalink":"http://zhangfuxin.cn/categories/项目实战/"}]},{"title":"mybatis-generator","slug":"mybatis-generator","date":"2018-01-28T09:30:19.000Z","updated":"2019-08-29T02:33:08.554Z","comments":true,"path":"mybatis-generator.html","link":"","permalink":"http://zhangfuxin.cn/mybatis-generator.html","excerpt":"** mybatis-generator：** &lt;Excerpt in index | 首页摘要&gt;mybatis反向生成器，根据数据库表，自动创建pojo，mapper以及mybatis配置文件，能极大的提高开发效率。","text":"** mybatis-generator：** &lt;Excerpt in index | 首页摘要&gt;mybatis反向生成器，根据数据库表，自动创建pojo，mapper以及mybatis配置文件，能极大的提高开发效率。 &lt;The rest of contents | 余下全文&gt; 插件介绍本插件fork自mybatis-generator-gui,在此基础上加了批量生成表。 插件特性 保存数据库配置 根据表生成pojo，mapper以及mybatis配置文件 批量生成 其它功能（待开发） 插件使用要求本工具由于使用了Java 8的众多特性，所以要求JDK 1.8.0.60以上版本，对于JDK版本还没有升级的童鞋表示歉意。 启动本软件 方法一: 自助构建 12345git clone https://github.com/maochunguang/mybatis-generator-guicd mybatis-generator-guimvn jfx:jarcd target/jfx/app/java -jar mybatis-generator-gui.jar 方法二: IDE中运行Eclipse or IntelliJ IDEA中启动, 找到com.zzg.mybatis.generator.MainUI类并运行就可以了 文档更多详细文档请参考本库的Wiki Usage 截图参考","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://zhangfuxin.cn/tags/mysql/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"突破算法第11天-红黑树","slug":"suanfa-11","date":"2017-10-30T14:35:37.000Z","updated":"2019-08-29T02:27:53.213Z","comments":true,"path":"suanfa-11.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-11.html","excerpt":"** 突破算法第11天-红黑树：** &lt;Excerpt in index | 首页摘要&gt;红黑树","text":"** 突破算法第11天-红黑树：** &lt;Excerpt in index | 首页摘要&gt;红黑树 &lt;The rest of contents | 余下全文&gt; 红黑树本文的主要内容： 红黑树的基本概念以及最重要的 5 点规则。 红黑树的左旋转、右旋转、重新着色的原理与 Java 实现； 红黑树的增加结点、删除结点过程解析； 红黑树的基本概念与数据结构表示首先红黑树来个定义： 红黑树定义：红黑树又称红 - 黑二叉树，它首先是一颗二叉树，它具体二叉树所有的特性。同时红黑树更是一颗自平衡的排序二叉树 (平衡二叉树的一种实现方式)。 我们知道一颗基本的二叉排序树他们都需要满足一个基本性质：即树中的任何节点的值大于它的左子节点，且小于它的右子节点。 按照这个基本性质使得树的检索效率大大提高。我们知道在生成二叉排序树的过程是非常容易失衡的，最坏的情况就是一边倒（只有右 / 左子树），这样势必会导致二叉树的检索效率大大降低（O(n)），所以为了维持二叉排序树的平衡，大牛们提出了各种平衡二叉树的实现算法，如：AVL，SBT，伸展树，TREAP ，红黑树等等。 平衡二叉树必须具备如下特性：它是一棵空树或它的左右两个子树的高度差的绝对值不超过 1，并且左右两个子树都是一棵平衡二叉树。也就是说该二叉树的任何一个子节点，其左右子树的高度都相近。下面给出平衡二叉树的几个示意图： 红黑树顾名思义就是结点是红色或者是黑色的平衡二叉树，它通过颜色的约束来维持着二叉树的平衡。对于一棵有效的红黑树而言我们必须增加如下规则，这也是红黑树最重要的 5 点规则： 每个结点都只能是红色或者黑色中的一种。 根结点是黑色的。 每个叶结点（NIL 节点，空节点）是黑色的。 如果一个结点是红的，则它两个子节点都是黑的。也就是说在一条路径上不能出现相邻的两个红色结点。 从任一结点到其每个叶子的所有路径都包含相同数目的黑色结点。 这些约束强制了红黑树的关键性质: 从根到叶子最长的可能路径不多于最短的可能路径的两倍长。结果是这棵树大致上是平衡的。因为操作比如插入、删除和查找某个值的最坏情况时间都要求与树的高度成比例，这个在高度上的理论上限允许红黑树在最坏情况下都是高效的，而不同于普通的二叉查找树。所以红黑树它是复杂而高效的，其检索效率 O(lg n)。下图为一颗典型的红黑二叉树： 上面关于红黑树的概念基本已经说得很清楚了，下面给出红黑树的结点用 Java 表示数据结构： 123456789101112131415161718192021222324252627282930313233343536373839404142434445private static final boolean RED = true;private static final boolean BLACK = false;private Node root;//二叉查找树的根节点//结点数据结构private class Node&#123; private Key key;//键 private Value value;//值 private Node left, right;//指向子树的链接:左子树和右子树. private int N;//以该节点为根的子树中的结点总数 boolean color;//由其父结点指向它的链接的颜色也就是结点颜色. public Node(Key key, Value value, int N, boolean color) &#123; this.key = key; this.value = value; this.N = N; this.color = color; &#125;&#125;/** * 获取整个二叉查找树的大小 * @return */public int size()&#123; return size(root);&#125;/** * 获取某一个结点为根结点的二叉查找树的大小 * @param x * @return */private int size(Node x)&#123; if(x == null)&#123; return 0; &#125; else &#123; return x.N; &#125;&#125;private boolean isRed(Node x)&#123; if(x == null)&#123; return false; &#125; return x.color == RED;&#125; 红黑树的三个基本操作红黑树在插入，删除过程中可能会破坏原本的平衡条件导致不满足红黑树的性质，这时候一般情况下要通过左旋、右旋和重新着色这个三个操作来使红黑树重新满足平衡化条件。 旋转旋转分为左旋和右旋。在我们实现某些操作中可能会出现红色右链接或则两个连续的红链接，这时候就要通过旋转修复。 通常左旋操作用于将一个向右倾斜的红色链接 (这个红色链接链连接的两个结点均是红色结点) 旋转为向左链接。对比操作前后，可以看出，该操作实际上是将红线链接的两个结点中的一个较大的结点移动到根结点上。 左旋的示意图如下： 左旋的 Java 实现如下： 12345678910111213141516171819/** * 左旋转 * @param h * @return */private Node rotateLeft(Node h)&#123; Node x = h.right; //把x的左结点赋值给h的右结点 h.right = x.left; //把h赋值给x的左结点 x.left = h; // x.color = h.color; h.color = RED; x.N = h.N; h.N = 1+ size(h.left) + size(h.right); return x;&#125; 左旋的动画效果如下： 右旋其实就是左旋的逆操作：右旋的代码如下： 12345678910111213141516/** * 右旋转 * @param h * @return */private Node rotateRight(Node h)&#123; Node x = h.left; h.left = x.right; x.right = h; x.color = h.color; h.color = RED; x.N = h.N; h.N = 1+ size(h.left) + size(h.right); return x;&#125; 右旋的动态示意图： 颜色反转当出现一个临时的 4-node 的时候，即一个节点的两个子节点均为红色，如下图：我们需要将 E 提升至父节点，操作方法很简单，就是把 E 对子节点的连线设置为黑色，自己的颜色设置为红色。颜色反转之后颜色如下：实现代码如下： 123456789/** * 颜色转换 * @param h */private void flipColors(Node h)&#123; h.color = RED;//父结点颜色变红 h.left.color = BLACK;//子结点颜色变黑 h.right.color = BLACK;//子结点颜色变黑&#125; 注意：以上的旋转和颜色反转操作都是针对单一结点的，反转或则颜色反转操作之后可能引起其父结点又不满足平衡性质。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第10天-二叉树","slug":"suanfa-10","date":"2017-10-29T13:17:09.000Z","updated":"2019-08-29T02:28:07.188Z","comments":true,"path":"suanfa-10.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-10.html","excerpt":"** 突破算法第10天-二叉树：** &lt;Excerpt in index | 首页摘要&gt;用java实现算法求出二叉树的高度","text":"** 突破算法第10天-二叉树：** &lt;Excerpt in index | 首页摘要&gt;用java实现算法求出二叉树的高度 &lt;The rest of contents | 余下全文&gt; 树 先序遍历：先访问根结点，然后左节点，最后右节点 中序遍历：先访问左结点，然后根节点，最后右节点 后续遍历：先访问左结点，然后右节点，最后根节点 java实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class TreeNode &#123; TreeNode left; TreeNode right; int val; TreeNode(int val) &#123; this.val = val; &#125; public static void main(String[] args) &#123; TreeNode root = new TreeNode(1); TreeNode left1 = new TreeNode(2); TreeNode left2 = new TreeNode(3); TreeNode right1 = new TreeNode(4); //创建一棵树 root.left = left1; left1.right = left2; root.right = right1; scanNodes(root); System.out.println(\"树的深度是：\" + getDepth(root)); System.out.println(\"非递归深度：\" + findDeep2(root)); &#125; // 递归返回二叉树的深度 static int getDepth(TreeNode root) &#123; if (root == null) &#123; return 0; &#125; int left = getDepth(root.left); int right = getDepth(root.right); return left &gt; right ? left + 1 : right + 1; &#125; static void scanNodes(TreeNode root) &#123; if (root == null) &#123; return; &#125;// System.out.println(root.val); //先序遍历 scanNodes(root.left);// System.out.println(root.val); //中序遍历 scanNodes(root.right); System.out.println(root.val); // 后序遍历 &#125; // 非递归求深度 public static int findDeep2(TreeNode root) &#123; if (root == null) return 0; TreeNode current = null; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); int cur, next; int level = 0; while (!queue.isEmpty()) &#123; cur = 0; //当遍历完当前层以后，队列里元素全是下一层的元素，队列的长度是这一层的节点的个数 next = queue.size(); while (cur &lt; next) &#123; current = queue.poll(); cur++; //把当前节点的左右节点入队（如果存在的话） if (current.left != null) &#123; queue.offer(current.left); &#125; if (current.right != null) &#123; queue.offer(current.right); &#125; &#125; level++; &#125; return level; &#125;&#125; 树的变种二叉查找树，平衡二叉查找树，红黑树，b树红黑树和平衡二叉树（AVL树）类似，都是在进行插入和删除操作时通过特定操作保持二叉查找树的平衡，从而获得较高的查找性能。红黑树和AVL树的区别在于它使用颜色来标识结点的高度，它所追求的是局部平衡而不是AVL树中的非常严格的平衡。由于二叉树的效率和深度息息相关，于是出现了多路的B树，B+树等等。b树是叶子为n的平衡树。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第九天-排序算法比较","slug":"suanfa-9","date":"2017-10-28T15:10:58.000Z","updated":"2019-08-29T02:28:19.550Z","comments":true,"path":"suanfa-9.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-9.html","excerpt":"** 突破算法第九天-排序算法比较：** &lt;Excerpt in index | 首页摘要&gt;排序算法个有千秋，有的性能高，有的性能很低。这就要求我们对常用的排序算法要全面了解，不要用错了算法，导致性能问题。","text":"** 突破算法第九天-排序算法比较：** &lt;Excerpt in index | 首页摘要&gt;排序算法个有千秋，有的性能高，有的性能很低。这就要求我们对常用的排序算法要全面了解，不要用错了算法，导致性能问题。 &lt;The rest of contents | 余下全文&gt; 排序算法性能比较·借一张网路上的比较图。特别直观。 \b\b排序算法总结个人看法： 一般的情况还是以快速排序为主， 对于多个有序的数组合并的情况使用归并排序 性能要求快，空间足够，待排序的元素都要在一定的范围内使用桶排序","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第八天-桶排序","slug":"suanfa-8","date":"2017-10-27T14:51:06.000Z","updated":"2019-08-29T02:28:31.888Z","comments":true,"path":"suanfa-8.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-8.html","excerpt":"** 突破算法第八天-桶排序：** &lt;Excerpt in index | 首页摘要&gt;桶排序是个神奇的排序，在某些情况下可以达到O(N)的复杂度，快的离谱。但是桶排序是利用空间换时间，在空间充足的情况下，可以用桶排序进行高效的排序。","text":"** 突破算法第八天-桶排序：** &lt;Excerpt in index | 首页摘要&gt;桶排序是个神奇的排序，在某些情况下可以达到O(N)的复杂度，快的离谱。但是桶排序是利用空间换时间，在空间充足的情况下，可以用桶排序进行高效的排序。 &lt;The rest of contents | 余下全文&gt; 桶排序的基本原理将阵列分到有限数量的桶子里。每个桶子再个别排序（有可能再使用别的排序算法或是以递回方式继续使用桶排序进行排序）。当要被排序的阵列内的数值是均匀分配的时候，桶排序使用线性时间（Θ（n））。但桶排序并不是 比较排序，他不受到 O(nlogn) 下限的影响， 简单来说，就是把数据分组，放在一个个的桶中，然后对每个桶里面的在进行排序 桶排序的java实现12345678910111213141516171819202122232425262728293031323334353637383940414243public static void bucketSort1(int[] arr)&#123; //分桶，这里采用映射函数f(x)=x/10。 int bucketCount =10; Integer[][] bucket = new Integer[bucketCount][arr.length]; for (int i=0; i&lt;arr.length; i++)&#123; int quotient = arr[i]/10; for (int j=0; j&lt;arr.length; j++)&#123; if (bucket[quotient][j]==null)&#123; bucket[quotient][j]=arr[i]; break; &#125; &#125; &#125; //小桶排序 for (int i=0; i&lt;bucket.length; i++)&#123; //insertion sort for (int j=1; j&lt;bucket[i].length; ++j)&#123; if(bucket[i][j]==null)&#123; break; &#125; int value = bucket[i][j]; int position=j; while (position&gt;0 &amp;&amp; bucket[i][position-1]&gt;value)&#123; bucket[i][position] = bucket[i][position-1]; position--; &#125; bucket[i][position] = value; &#125; &#125; //输出 for (int i=0, index=0; i&lt;bucket.length; i++)&#123; for (int j=0; j&lt;bucket[i].length; j++)&#123; if (bucket[i][j]!=null)&#123; arr[index] = bucket[i][j]; index++; &#125; else&#123; break; &#125; &#125; &#125; &#125; 算法复杂度前面说的几大排序算法 ，大部分时间复杂度都是O（n2），也有部分排序算法时间复杂度是O(nlogn)。而桶式排序却能实现O（n）的时间复杂度。但桶排序的缺点是： 首先是空间复杂度比较高，需要的额外开销大。排序有两个数组的空间开销，一个存放待排序数组，一个就是所谓的桶，比如待排序值是从0到m-1，那就需要m个桶，这个桶数组就要至少m个空间。 其次待排序的元素都要在一定的范围内，限制较多。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第七天-堆排序","slug":"suanfa-7","date":"2017-10-26T14:50:57.000Z","updated":"2019-08-29T02:28:44.226Z","comments":true,"path":"suanfa-7.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-7.html","excerpt":"** 突破算法第七天-堆排序：** &lt;Excerpt in index | 首页摘要&gt;堆排序是利用二叉树的原理实现的一种排序，难点在于要构建堆,构建堆一般可以采用下沉或者上浮的算法进行。","text":"** 突破算法第七天-堆排序：** &lt;Excerpt in index | 首页摘要&gt;堆排序是利用二叉树的原理实现的一种排序，难点在于要构建堆,构建堆一般可以采用下沉或者上浮的算法进行。 &lt;The rest of contents | 余下全文&gt; 堆排序的基本原理初始时把要排序的n个数的序列看作是一棵顺序存储的二叉树（一维数组存储二叉树），调整它们的存储序，使之成为一个堆，将堆顶元素输出，得到n 个元素中最小(或最大)的元素，这时堆的根节点的数最小（或者最大）。然后对前面(n-1)个元素重新调整使之成为堆，输出堆顶元素，得到n 个元素中次小(或次大)的元素。依此类推，直到只有两个节点的堆，并对它们作交换，最后得到有n个节点的有序序列。称这个过程为堆排序。因此，实现堆排序需解决两个问题： 如何将n 个待排序的数建成堆； 输出堆顶元素后，怎样调整剩余n-1 个元素，使其成为一个新堆。 堆排序java实现1234567891011121314151617181920212223242526272829303132public static void sort(int[] a) &#123; int n = a.length; for (int k = n / 2; k &gt;= 1; k--) sink(a, k, n); while (n &gt; 1) &#123; swap(a, 1, n--); sink(a, 1, n); &#125; &#125;private static void sink(int[] a, int k, int n) &#123; while (2 * k &lt;= n) &#123; int j = 2 * k; if (j &lt; n &amp;&amp; a[j - 1] &lt; a[j + 1 - 1]) j++; if (a[k - 1] &gt;= a[j - 1]) break; swap(a, k, j); k = j; &#125;&#125;private static void swap(int[] a, int i, int j) &#123; int swap = a[i - 1]; a[i - 1] = a[j - 1]; a[j - 1] = swap;&#125;public static void main(String[] args) &#123; int[] arr = &#123;49, 38, 65, 97, 76, 13, 27, 4, 78, 34, 12, 64, 1, 8&#125;; sort(arr); System.out.println(\"排序之后：\"); System.out.println(Arrays.toString(arr));&#125; 算法复杂度堆排序的平均时间复杂度为Ο(nlogn)","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第六天-冒泡排序","slug":"suanfa-6","date":"2017-10-25T14:06:06.000Z","updated":"2019-08-29T02:28:56.527Z","comments":true,"path":"suanfa-6.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-6.html","excerpt":"** 突破算法第六天-冒泡排序：** &lt;Excerpt in index | 首页摘要&gt;冒泡排序也非常简单，效率比较低。了解即可。","text":"** 突破算法第六天-冒泡排序：** &lt;Excerpt in index | 首页摘要&gt;冒泡排序也非常简单，效率比较低。了解即可。 &lt;The rest of contents | 余下全文&gt; 冒泡排序的原理在要排序的一组数中，对当前还未排好序的范围内的全部数，自上而下对相邻的两个数依次进行比较和调整，让较大的数往下沉，较小的往上冒。即：每当两相邻的数比较后发现它们的排序与排序要求相反时，就将它们互换。 冒泡排序的java实现1234567891011private static void bubbleSort(int a[], int n) &#123; for (int i = 0; i &lt; n - 1; ++i) &#123; for (int j = 0; j &lt; n - i - 1; ++j) &#123; if (a[j] &gt; a[j + 1]) &#123; int tmp = a[j]; a[j] = a[j + 1]; a[j + 1] = tmp; &#125; &#125; &#125;&#125; 算法复杂度冒泡排序的复杂度为O(n^2)","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第五天-选择排序","slug":"suanfa-5","date":"2017-10-24T13:46:33.000Z","updated":"2019-08-29T02:29:09.780Z","comments":true,"path":"suanfa-5.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-5.html","excerpt":"** 突破算法第五天-选择排序：** &lt;Excerpt in index | 首页摘要&gt;选择排序很简单，属于交换排序算法。通过比较找到最大值或最小值，然后进行交换。","text":"** 突破算法第五天-选择排序：** &lt;Excerpt in index | 首页摘要&gt;选择排序很简单，属于交换排序算法。通过比较找到最大值或最小值，然后进行交换。 &lt;The rest of contents | 余下全文&gt; 选择排序的原理首先找到数组中最小的元素，与数组第一个元素交换，然后在剩下的元素中选择最小的，与第二个元素交换，以此类推，直到排序完成。 选择排序的java实现1234567891011121314private static void sort(int[] a) &#123; int len = a.length; for (int i = 1; i &lt; len; i++) &#123; for (int j = i; j &gt; 0 &amp;&amp; (a[j] &lt; a[j - 1]); j--) &#123; swap(a, j, j - 1); &#125; &#125;&#125;private static void swap(int[] a, int i, int j) &#123; int swap = a[i]; a[i] = a[j]; a[j] = swap;&#125; 算法复杂度选择排序的算法复杂度是O(n^2) 改进 每次选择的时候把最大值和最小值都比较出来，双向进行交换排序","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第一天-归并排序","slug":"suanfa-4","date":"2017-10-23T15:56:27.000Z","updated":"2019-08-29T02:29:22.206Z","comments":true,"path":"suanfa-4.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-4.html","excerpt":"** 突破算法第一天-归并排序：** &lt;Excerpt in index | 首页摘要&gt;归并排序是利用分治思想进行排序的典型应用，特别是对几个基本有序的子序列合并时，效率最高。在实际应用中，分布式应用，分布式查询排序会比较多应用到归并排序。","text":"** 突破算法第一天-归并排序：** &lt;Excerpt in index | 首页摘要&gt;归并排序是利用分治思想进行排序的典型应用，特别是对几个基本有序的子序列合并时，效率最高。在实际应用中，分布式应用，分布式查询排序会比较多应用到归并排序。 &lt;The rest of contents | 余下全文&gt; 归并排序的原理归并（Merge）排序法是将两个（或两个以上）有序表合并成一个新的有序表，即把待排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。归并排序分为两种种，第一种是自底向上的归并。 第二种是自顶向下的归并。 自底向上的归并排序java实现1234567891011121314151617181920212223242526272829303132333435public class MergeSortBU &#123; private static void merge(int[] a, int[] aux, int lo, int mid, int hi) &#123; // 复制到aux[] for (int k = lo; k &lt;= hi; k++) &#123; aux[k] = a[k]; &#125; // 合并回 a[] int i = lo, j = mid + 1; for (int k = lo; k &lt;= hi; k++) &#123; if (i &gt; mid) a[k] = aux[j++]; else if (j &gt; hi) a[k] = aux[i++]; else if (aux[j] &lt; aux[i]) a[k] = aux[j++]; else a[k] = aux[i++]; &#125; &#125; public static void mergeSort(int[] a) &#123; int n = a.length; int[] aux = new int[n]; for (int len = 1; len &lt; n; len *= 2) &#123; for (int lo = 0; lo &lt; n - len; lo += len + len) &#123; int mid = lo + len - 1; int hi = Math.min(lo + len + len - 1, n - 1); merge(a, aux, lo, mid, hi); &#125; &#125; &#125; public static void main(String[] args) &#123; int[] arr = &#123;49, 38, 65, 97, 76, 13, 27, 4, 78, 34, 12, 64, 1, 8&#125;; mergeSort(arr); System.out.println(\"排序之后：\"); System.out.println(Arrays.toString(arr)); &#125;&#125; 自顶向下的归并排序java实现1234567891011121314151617181920212223private static void sort(int[] a, int low, int high) &#123; if (high &lt;= low) return; int mid = low + (high - low) / 2; sort(a, low, mid); sort(a, mid + 1, high); merge(a, low, mid, high); &#125; private static void merge(int[] a, int lo, int mid, int hi) &#123; // 复制到aux[] int[] aux = new int[a.length]; for (int k = lo; k &lt;= hi; k++) &#123; aux[k] = a[k]; &#125; // 合并回 a[] int i = lo, j = mid + 1; for (int k = lo; k &lt;= hi; k++) &#123; if (i &gt; mid) a[k] = aux[j++]; else if (j &gt; hi) a[k] = aux[i++]; else if (aux[j] &lt; aux[i]) a[k] = aux[j++]; else a[k] = aux[i++]; &#125; &#125; 算法复杂度归并排序的算法复杂度是nlgn 应用场景 几个基本有序的数组进行排序 部分有序的数组","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第三天-希尔排序","slug":"suanfa-3","date":"2017-10-22T13:51:03.000Z","updated":"2019-08-29T02:29:34.152Z","comments":true,"path":"suanfa-3.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-3.html","excerpt":"** 突破算法第三天-希尔排序：** &lt;Excerpt in index | 首页摘要&gt;希尔排序平常用的比较少，主要是基于插入排序的改进。但是希尔排序的性能很高，数组越大，性能优势越明显。","text":"** 突破算法第三天-希尔排序：** &lt;Excerpt in index | 首页摘要&gt;希尔排序平常用的比较少，主要是基于插入排序的改进。但是希尔排序的性能很高，数组越大，性能优势越明显。 &lt;The rest of contents | 余下全文&gt; 希尔排序的基本原理基本思想：先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录“基本有序”时，再对全体记录进行依次直接插入排序。操作方法： 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1； 按增量序列个数k，对序列进行k 趟排序； 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 希尔排序java实现12345678910111213141516171819public static void shellSort(int[] a) &#123; int n = a.length; int h = 1; while (h &lt; n/3) h = 3*h + 1; while (h &gt;= 1) &#123; // h-sort the array for (int i = h; i &lt; n; i++) &#123; for (int j = i; j &gt;= h &amp;&amp; (a[j]&lt; a[j-h]); j -= h) &#123; swap(a, j, j-h); &#125; &#125; h /= 3; &#125;&#125;private static void swap(int[] a, int i, int j) &#123; int swap = a[i]; a[i] = a[j]; a[j] = swap;&#125; 算法复杂度希尔排序时效分析很难，关键码的比较次数与记录移动次数依赖于增量因子序列d的选取，是一个不稳定排序算法 适用场景","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第二天-插入排序","slug":"suanfa-2","date":"2017-10-21T01:41:27.000Z","updated":"2019-08-29T02:29:46.170Z","comments":true,"path":"suanfa-2.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-2.html","excerpt":"** 突破算法第二天-插入排序：** &lt;Excerpt in index | 首页摘要&gt;今天是突破算法第二天，插入排序，比较简单。效率比较低，但是思想很广泛，应用很广，是很多高级排序算法的一个子过程。","text":"** 突破算法第二天-插入排序：** &lt;Excerpt in index | 首页摘要&gt;今天是突破算法第二天，插入排序，比较简单。效率比较低，但是思想很广泛，应用很广，是很多高级排序算法的一个子过程。 &lt;The rest of contents | 余下全文&gt; 插入排序的原理 将一个记录插入到已排序好的有序表中，从而得到一个新，记录数增1的有序表。即：先将序列的第1个记录 看成是一个有序的子序列，然后从第2个记录逐个进行插入，直至整个序列有序为止。 要点：设立哨兵，作为临时存储和判断数组边界之用 插入排序java实现123456789101112public static void insertSort(int[] a, int n) &#123; for (int i = 0; i &lt; n; i++) &#123; for (int j = i; j &gt; 0 &amp;&amp; (a[j]&lt;a[j-1]); j--) &#123; swap(a, j, j-1); &#125; &#125; &#125; private static void swap(int[] a, int i, int j) &#123; int swap = a[i]; a[i] = a[j]; a[j] = swap; &#125; 算法复杂度插入排序的复杂度为O（n^2） 改进方法希尔排序，其他的插入排序有二分插入排序，2-路插入排序。 适用场景插入排序比较适合部分有序的数组（以下四种数组） 数组中每个元素距离它的最终位置都不远 一个有序的大数组接一个小数组 数组中只有几个位置不正确 数组比较小","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"突破算法第一天-快速排序","slug":"suanfa-1","date":"2017-10-20T15:46:59.000Z","updated":"2019-08-29T02:29:59.957Z","comments":true,"path":"suanfa-1.html","link":"","permalink":"http://zhangfuxin.cn/suanfa-1.html","excerpt":"** 突破算法第一天-快速排序：** &lt;Excerpt in index | 首页摘要&gt;30天突破算法是我给自己定的一个学习计划，希望在这30天，每天都能完成计划。第一天学习最重要的快速排序。","text":"** 突破算法第一天-快速排序：** &lt;Excerpt in index | 首页摘要&gt;30天突破算法是我给自己定的一个学习计划，希望在这30天，每天都能完成计划。第一天学习最重要的快速排序。 &lt;The rest of contents | 余下全文&gt; 30天突破算法算法种类不计其数，说30天突破只是给自己定的学习计划。目的是通过30天的记录熟悉常见的算法，提高自己的算法能力。对以后的工作来说也是打下夯实的基础。 快速排序的原理快速排序也是分治法思想的一种实现，他的思路是使数组中的每个元素与基准值（Pivot，通常是数组的首个值，A[0]）比较，数组中比基准值小的放在基准值的左边，形成左部；大的放在右边，形成右部；接下来将左部和右部分别递归地执行上面的过程：选基准值，小的放在左边，大的放在右边。重复此过程，直到排序结束。步骤如下： 1.找基准值，设Pivot = a[0] 2.分区（Partition）：比基准值小的放左边，大的放右边，基准值(Pivot)放左部与右部的之间。 3.进行左部（a[0] - a[pivot-1]）的递归，以及右部（a[pivot+1] - a[n-1]）的递归，重复上述步骤。 快速排序java实现（递归版）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class QuickSort &#123; public static void main(String[] args) &#123; int[] a=&#123;49,38,65,97,76,13,27,49,78,34,12,64,1,8&#125;; System.out.println(\"排序之前：\"); for (int i = 0; i &lt; a.length; i++) &#123; System.out.print(a[i]+\" \"); &#125; //快速排序 quick(a); System.out.println(); System.out.println(\"排序之后：\"); for (int i = 0; i &lt; a.length; i++) &#123; System.out.print(a[i]+\" \"); &#125; &#125; private static void quick(int[] a) &#123; if(a.length&gt;0)&#123; quickSort(a,0,a.length-1); &#125; &#125; private static void quickSort(int[] a, int low, int high) &#123; if(low&lt;high)&#123; //如果不加这个判断递归会无法退出导致堆栈溢出异常 int middle = getMiddle(a,low,high); quickSort(a, 0, middle-1); quickSort(a, middle+1, high); &#125; &#125; private static int getMiddle(int[] a, int low, int high) &#123; int temp = a[low];//基准元素 while(low&lt;high)&#123; //找到比基准元素小的元素位置 while(low&lt;high &amp;&amp; a[high]&gt;=temp)&#123; high--; &#125; a[low] = a[high]; while(low&lt;high &amp;&amp; a[low]&lt;=temp)&#123; low++; &#125; a[high] = a[low]; &#125; a[low] = temp; return low; &#125;&#125; 快速排序三向切分法（改进的实现）12345678910111213141516171819private static void quick3Sort(int[] a, int low, int high) &#123; if (low &gt;= high) return; int lt = low, gt = high; int temp = a[low]; int i = low; while (i &lt;= gt) &#123; if (a[i] &lt; temp) swap(a, lt++, i++); else if (a[i] &gt; temp) swap(a, i, gt--); else i++; &#125; quick3Sort(a, low, lt - 1); quick3Sort(a, gt + 1, high);&#125;private static void swap(int[] a, int i, int j) &#123; int swap = a[i]; a[i] = a[j]; a[j] = swap;&#125; 快速排序的复杂度时间复杂度 nlogn,排序方法中平均性能最好的。但若初始序列按关键码有序或基本有序时，快排序反而蜕化为冒泡排序。快速排序是一个不稳定的排序方法。 改进方法 当数组比较小的时候，快速排序比插入排序慢，这个时候用插入排序替换比较好。 通常以“三者取中法”来选取基准记录，即将排序区间的两个端点与中点三个记录关键码居中的调整为支点记录 适用场景 普通的无序集合排序，使用快速排序。 包含很多重复元素的集合排序，使用三向切分的快速排序。 基本有序的集合使用归并排序。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"学习计划-30天突破算法","slug":"study-plan","date":"2017-10-18T11:47:55.000Z","updated":"2019-08-29T02:30:13.270Z","comments":true,"path":"study-plan.html","link":"","permalink":"http://zhangfuxin.cn/study-plan.html","excerpt":"** 学习计划-30天突破算法：** &lt;Excerpt in index | 首页摘要&gt;作为一个非专业出身的程序员，一直对算法的学习赶紧断断续续，终于下定决心对算法做一次详细总结。30天时间把程序员常用算法逐一突破。这次计划更是对自己的一次挑战，希望自己能坚持到最后！","text":"** 学习计划-30天突破算法：** &lt;Excerpt in index | 首页摘要&gt;作为一个非专业出身的程序员，一直对算法的学习赶紧断断续续，终于下定决心对算法做一次详细总结。30天时间把程序员常用算法逐一突破。这次计划更是对自己的一次挑战，希望自己能坚持到最后！ &lt;The rest of contents | 余下全文&gt; 学习排序算法的意义 学会比较算法的性能的方法 相关的排序能解决类似的问题 排序算法很多时候是解决问题的第一步 排序算法 快速排序 插入排序 希尔排序 归并排序 选择排序 冒泡排序 堆排序 桶排序 排序算法比较 树 二叉树高度和二叉树的遍历 红黑树 b树 查找算法 二分查找 二叉查找树 平衡查找树 散列表 算法思想 递归（普通递归，尾递归） 动态规划 贪婪算法 分治法 图的算法 深度优先 广度优先 最小生成树 最短路径 字符串算法 字符串查找 单词查找树 子字符串查找 典型算法分析 拓扑排序 关键路径排序 遗传算法 RSA算法 英语技术文档阅读突破 熟悉常用技术词汇 阅读常见的技术文档（官网文档看一遍） 记住常用的词汇 阅读英文技术书籍","categories":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/categories/算法/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/tags/algorithm/"}],"keywords":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/categories/算法/"}]},{"title":"用java将GBK工程转为uft8","slug":"trandsferProject","date":"2017-08-11T23:43:59.000Z","updated":"2019-08-29T02:27:34.042Z","comments":true,"path":"trandsferProject.html","link":"","permalink":"http://zhangfuxin.cn/trandsferProject.html","excerpt":"** 用java将GBK工程转为uft8：** &lt;Excerpt in index | 首页摘要&gt;windows下的默认编码为GBK还有gb2312，如何把gbk的java工程转为utf8的呢，如果直接修改工程编码，其实里面的java文件中中文是会乱码的，写了个批量转换java工程的程序，消遣一下。","text":"** 用java将GBK工程转为uft8：** &lt;Excerpt in index | 首页摘要&gt;windows下的默认编码为GBK还有gb2312，如何把gbk的java工程转为utf8的呢，如果直接修改工程编码，其实里面的java文件中中文是会乱码的，写了个批量转换java工程的程序，消遣一下。 &lt;The rest of contents | 余下全文&gt; 为什么要转码？有些老的项目，或者朋友的项目之前没注意在windows上不是utf8，而你有需要看注释或者什么，总不能一个文件一个文件的去改编码属性吧。 本程序试用范围gbk的代码，或者gb2312的工程均可以转换 编码转换的思路本来想做成一个通用的会自动检测编码，自动转换的程序。但是由于判断编码类型不准，所以做成了针对GBK的转换。 制定gbk编码把文件流读进来，加载到内存，转为String类型的内容 将String内容转为utf8的String 将String内容写入文件 核心代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class TransferProject &#123; public static void transferFile(String pathName, int depth) throws Exception &#123; File dirFile = new File(pathName); if (!isValidFile(dirFile)) return; //获取此目录下的所有文件名与目录名 String[] fileList = dirFile.list(); int currentDepth = depth + 1; for (int i = 0; i &lt; fileList.length; i++) &#123; String string = fileList[i]; File file = new File(dirFile.getPath(), string); String name = file.getName(); //如果是一个目录，搜索深度depth++，输出目录名后，进行递归 if (file.isDirectory()) &#123; //递归 transferFile(file.getCanonicalPath(), currentDepth); &#125; else &#123; if (name.contains(\".java\") || name.contains(\".properties\") || name.contains(\".xml\")) &#123; readAndWrite(file); System.out.println(name + \" has converted to utf8 \"); &#125; &#125; &#125; &#125; private static boolean isValidFile(File dirFile) throws IOException &#123; if (dirFile.exists()) &#123; System.out.println(\"file exist\"); return true; &#125; if (dirFile.isDirectory()) &#123; if (dirFile.isFile()) &#123; System.out.println(dirFile.getCanonicalFile()); &#125; return true; &#125; return false; &#125; private static void readAndWrite(File file) throws Exception &#123; String content = FileUtils.readFileByEncode(file.getPath(), \"GBK\"); FileUtils.writeByBufferedReader(file.getPath(), new String(content.getBytes(\"UTF-8\"), \"UTF-8\")); &#125; public static void main(String[] args) throws Exception &#123; //程序入口，制定src的path String path = \"/Users/mac/Downloads/unit06_jdbc/src\"; transferFile(path, 1); &#125;&#125; 123456789101112131415161718192021222324252627282930313233public class FileUtils &#123; public static void writeByBufferedReader(String path, String content) &#123; try &#123; File file = new File(path); file.delete(); if (!file.exists()) &#123; file.createNewFile(); &#125; FileWriter fw = new FileWriter(file, false); BufferedWriter bw = new BufferedWriter(fw); bw.write(content); bw.flush(); bw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static String readFileByEncode(String path, String chatSet) throws Exception &#123; InputStream input = new FileInputStream(path); InputStreamReader in = new InputStreamReader(input, chatSet); BufferedReader reader = new BufferedReader(in); StringBuffer sb = new StringBuffer(); String line = reader.readLine(); while (line != null) &#123; sb.append(line); sb.append(\"\\r\\n\"); line = reader.readLine(); &#125; return sb.toString(); &#125;&#125; 总结遇到类似的问题，都可以试着用代码来进行实现，给自己的编码带来一些新的乐趣，也增加自己的信心。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}],"tags":[{"name":"java","slug":"java","permalink":"http://zhangfuxin.cn/tags/java/"}],"keywords":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}]},{"title":"阿拉伯数字转汉字写法","slug":"num2Chinese","date":"2017-07-29T14:25:27.000Z","updated":"2019-08-29T02:31:56.019Z","comments":true,"path":"num2Chinese.html","link":"","permalink":"http://zhangfuxin.cn/num2Chinese.html","excerpt":"** 阿拉伯数字转汉字写法：** &lt;Excerpt in index | 首页摘要&gt;找工作时看到“某团”的题目，把一个int的数字转为汉字的读法，比如123，转成一百二十三，限时20分钟。如果二十分钟做不出来，简历就不要投了。说实话，20分钟能调通的人真的不多，感觉某团还是装逼成分太多！","text":"** 阿拉伯数字转汉字写法：** &lt;Excerpt in index | 首页摘要&gt;找工作时看到“某团”的题目，把一个int的数字转为汉字的读法，比如123，转成一百二十三，限时20分钟。如果二十分钟做不出来，简历就不要投了。说实话，20分钟能调通的人真的不多，感觉某团还是装逼成分太多！ &lt;The rest of contents | 余下全文&gt; 题目要求用java实现，把int的数字转为汉字读音，比如123，转成一百二十三，10020转为一万零二十 思路分析中文计数的特点，以万为小节，万以内的都是以“十百千”为权位单独计数，比如一千百，一千千都是非法的。而“十百千”这样的权位可以与“万”，“亿”进行搭配，二十亿，五千万等等。 中文数字的零中文的零的使用总结起来有三个规则， 以10000为小节，结尾是0，不使用零，比如1020 以10000为小节，小节内两个非0数字之间需要零 小节的千位是0，若小节前无其他数字，不用零，否者用零 完整代码（参考算法的乐趣第四章）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class NumberTransfer &#123; public final String[] chnNumChar = new String[]&#123;\"零\", \"一\", \"二\", \"三\", \"四\", \"五\", \"六\", \"七\", \"八\", \"九\"&#125;; public final String[] chnUnitSection = new String[]&#123;\"\", \"万\", \"亿\", \"万亿\"&#125;; public final String[] chnUnitChar = new String[]&#123;\"\", \"十\", \"百\", \"千\"&#125;; @Test public void testNumberToChinese() &#123; int[] nums = new int[]&#123;304, 4006, 4000, 10003, 10030, 21010011, 101101101&#125;; for (int i = 0; i &lt; nums.length; i++) &#123; System.out.println(numberToChinese(nums[i])); &#125; &#125; public String numberToChinese(int num) &#123; String strIns; String chnStr = \"\"; int unitPos = 0; boolean needZero = false; if (num == 0) return \"零\"; while (num &gt; 0) &#123; strIns = \"\"; int section = num % 10000; if (needZero) &#123; chnStr = chnNumChar[0] + chnStr; &#125; // 添加节权（万，亿） strIns += (section != 0) ? chnUnitSection[unitPos] : chnUnitSection[0]; chnStr = strIns + chnStr; // 以万为单位，求万以内的权位 chnStr = sectionToChinese(section, chnStr); needZero = (section &lt; 1000) &amp;&amp; (section &gt; 0); num = num / 10000; unitPos++; &#125; return chnStr; &#125; private String sectionToChinese(int section, String chnStr) &#123; String strIns; int unitPos = 0; boolean zero = true; while (section &gt; 0) &#123; int v = section % 10; if (v == 0) &#123; if (section == 0 || !zero) &#123; zero = true;// zero确保不会出现多个零 chnStr = chnNumChar[v] + chnStr; &#125; &#125; else &#123; zero = false; strIns = chnNumChar[v]; // 此位置对应等中文数字 strIns += chnUnitChar[unitPos];// 此位置对应的权位 chnStr = strIns + chnStr; &#125; unitPos++; section = section / 10; &#125; return chnStr; &#125;&#125;","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"java面试大全自制版","slug":"java-interview","date":"2017-07-24T13:07:00.000Z","updated":"2019-08-29T02:59:50.120Z","comments":true,"path":"java-interview.html","link":"","permalink":"http://zhangfuxin.cn/java-interview.html","excerpt":"** java面试大全自制版：** &lt;Excerpt in index | 首页摘要&gt;java语言知识点多而杂，面试时很多人找不到重点。这份java面试大全，有部分网络上资源，大多数是从好的文章和书籍里总结出来的知识点。","text":"** java面试大全自制版：** &lt;Excerpt in index | 首页摘要&gt;java语言知识点多而杂，面试时很多人找不到重点。这份java面试大全，有部分网络上资源，大多数是从好的文章和书籍里总结出来的知识点。 &lt;The rest of contents | 余下全文&gt; 本书的目的每个java程序员在面试前都不知该准备什么？或者是随便看几个文章就去面试，这样的结果很容易失败！希望本书能给java程序员一个好的指引，让java程序员没有难找的工作！ 目录gitbook地址","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://zhangfuxin.cn/categories/学习笔记/"}],"tags":[{"name":"java","slug":"java","permalink":"http://zhangfuxin.cn/tags/java/"}],"keywords":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://zhangfuxin.cn/categories/学习笔记/"}]},{"title":"kobo aura one导出笔记高级配置","slug":"kobo-config","date":"2017-06-23T02:22:57.000Z","updated":"2019-08-21T15:13:24.129Z","comments":true,"path":"kobo-config.html","link":"","permalink":"http://zhangfuxin.cn/kobo-config.html","excerpt":"** kobo aura one导出笔记高级配置：** &lt;Excerpt in index | 首页摘要&gt;kobo电子书折腾记，导出笔记，从激活到设置，打补丁实现自定义配置，还是自己折腾起来有意思啊。","text":"** kobo aura one导出笔记高级配置：** &lt;Excerpt in index | 首页摘要&gt;kobo电子书折腾记，导出笔记，从激活到设置，打补丁实现自定义配置，还是自己折腾起来有意思啊。 &lt;The rest of contents | 余下全文&gt; 建议买电子书是为了阅读和学习，不是天天折腾电子书，一天刷一次机，如果只是看书，做笔记，学个英文什么的原生系统是最好的。如果看pdf为主，不建议买这电子书，看pdf首选电脑，平板，sony dsp系列，用普通的电子书阅读器，体验太差。 kobo原生系统的功能（推荐原生系统，打上补丁） 格式支持epub，mobi，cbz漫画，txt，kobo epub格式 高亮，笔记，导出笔记（需要配置一下） 字典（英文，中文，法文等多国字典，可以自己修改） 阅读pocket文章（可以把网页保存到pocket，实用pocket同步到阅读器） 自动亮度（最大的优点） koreader的功能 格式支持epub，mobi，cbz漫画，txt，kobo epub格式 扫描版pdf支持重拍，切边（最大特色） 笔记导出到印象笔记 字典（强大的字典扩展） 激活说明：wifi激活需要翻墙，可以实用笔记连接vpn，然后共享wifi给kobo wifi激活, kobo setup desktop激活，去kobo官网下载软件，然后电脑需要翻墙，电子书连接上电脑，用软件登录激活。这个软件很不好用，bug也多，建议使用wifi激活。 更新固件，打补丁kobo的更新固件，更新补丁都是一个模式，把固件或者补丁放到.kobo文件夹，弹出设备就会自动重启 字体电脑连接kobo，在根目录建立一个fonts文件夹，把需要的字体放进去即可 词典下载网上改好的字典，直接放到.kobo文件夹下的dict目录下，然后重启就可以了 自定义配置 刷新页数（打补丁） 上下页宽（打补丁） 全屏模式（修改配置文件） 字体高级设置（修改配置文件） 导出笔记和高亮（修改配置文件） kobo高级配置文件详解用电脑连接kobo电子书，打开Kobo找到eReader.conf文件，最好用notepad++修改，或者其他文本编辑器。 12345678910111213141516171819202122232425262728[FeatureSettings]#导出笔记ExportHighlightsEnabled=true#显示全书的页码，而不是章节的页码FullBookPageNumbers=true#用在线等维基百科代替词典查询OnlineWikipedia=true#全屏阅读FullScreenReading=true#图片缩放ImageZoom=true#浏览器全屏FullScreenBrowser=true#关机键截图，但是关机键就无法关机了，不要设置这个鸡肋的功能Screenshots=true[Reading]#翻页刷新的页数，20页全刷一次numPartialUpdatePageTurns=20#左边距readingLeftMargin=0#右边距readingRightMargin=0#行高readingLineHeight=1.4[PowerOptions]#自动关机时间AutoOffMinutes=60 博客搬家，请访问新博客地址吧! 我的博客","categories":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/categories/others/"}],"tags":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/tags/开发工具/"}],"keywords":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/categories/others/"}]},{"title":"Illegal mix of collations","slug":"mysql-collation","date":"2017-06-12T03:00:14.000Z","updated":"2019-08-29T02:32:44.417Z","comments":true,"path":"mysql-collation.html","link":"","permalink":"http://zhangfuxin.cn/mysql-collation.html","excerpt":"** mysql排序字符集问题：** &lt;Excerpt in index | 首页摘要&gt;mysql表的每个字段都可以设置单独的排序字符集和文本字符集，如果你创建表的时候不注意，很可能会遇到Illegal mix of collations这个问题。","text":"** mysql排序字符集问题：** &lt;Excerpt in index | 首页摘要&gt;mysql表的每个字段都可以设置单独的排序字符集和文本字符集，如果你创建表的时候不注意，很可能会遇到Illegal mix of collations这个问题。 &lt;The rest of contents | 余下全文&gt; 问题描述用mysql进行两个表的联合查询的时候，出现下面的错误。 1Illegal mix of collations (utf8_unicode_ci,IMPLICIT) and (utf8_general_ci,IMPLICIT) for operation &apos;=&apos; 排查过程 通过google搜索找到原因，这个错误是mysql的排序字符集不一致导致的。 把联合查询的表使用navicat查看字段的设置，发现了有一个关联字段排序字符集的问题，如图： 这两个表中openid的排序规则不一致，导致出现问题。 解决方法将user表中的字符集和排序规则设置为默认，保持一致即可。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://zhangfuxin.cn/tags/mysql/"}],"keywords":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}]},{"title":"mongodb从入门到精通","slug":"mongodb-study","date":"2017-05-26T15:21:23.000Z","updated":"2019-08-21T15:13:24.131Z","comments":true,"path":"mongodb-study.html","link":"","permalink":"http://zhangfuxin.cn/mongodb-study.html","excerpt":"** mongodb从入门到精通** &lt;Excerpt in index | 首页摘要&gt; mongodb日常使用的一些知识，增删改查，索引，分片。","text":"** mongodb从入门到精通** &lt;Excerpt in index | 首页摘要&gt; mongodb日常使用的一些知识，增删改查，索引，分片。 &lt;The rest of contents | 余下全文&gt; mongodb学习1.mongodb特性1）mongo是一个面向文档的数据库，它集合了nosql和sql数据库两方面的特性。 2）所有实体都是在首次使用时创建。 3）没有严格的事务特性，但是它保证任何一次数据变更都是原子性的。 4）也没有固定的数据模型 5）mongo以javascript作为命令行执行引擎，所以利用shell进行复杂的计算和查询时会相当的慢。 6）mongo本身支持集群和数据分片 7）mongo是c++实现的，支持windows mac linux等主流操作系统 8）性能优越，速度快2.mongo常用操作增删操作123456db.user.insert(&#123;name:'aaaa',age:30&#125;);db.user.save(&#123;name:'aaaa',age:30&#125;);db.collection.insertOne(&#123;&#125;);//(3.2新特性)db.collection.deleteOne(&#123;&#125;,&#123;&#125;);//(3.2新特性)db.collection.remove(&#123;name:'aaa'&#125;);db.collection.remove();//(删除全部) 更新操作12345db.users.update(&#123;\"name\": \"joe\"&#125;, joe );//upsert模式db.users.update(&#123;\"name\": \"joe\"&#125;, joe, true );//MULTI模式db.users.update(&#123;\"name\": \"joe\"&#125;, joe, true ，true); update是对文档替换，而不是局部修改默认情况update更新匹配的第一条文档，multi模式更新所有匹配的 查询操作普通查询 123db.user.find();db.user.find(&#123;name:'aaa'&#125;);db.user.findOne(&#123;name:'aaa'&#125;); 模糊查询 12db.UserInfo.find(&#123;userName :'/A/'&#125;) //（名称%A%）db.UserInfo.find(&#123;userName :'/^A/'&#125;) //(名称A%) 操作符 $lt, $lte,$gt, $gte(&lt;, &lt;=, &gt;, &gt;= ) $all 数组中的元素是否完全匹配 db.things.find( { a: { $all: [ 2, 3 ] } } ); $exists 可选：true，false db.things.find( { a : { $exists : true } } ); $mod 取模：a % 10 == 1 db.things.find( { a : { $mod : [ 10 , 1 ] } } ); $ne 取反：即not equals db.things.find( { x : { $ne : 3 } } ); $in 类似于SQL的IN操作 db.things.find({j:{$in: [2,4,6]}}); $nin $in的反操作，即SQL的 NOT IN db.things.find({j:{$nin: [2,4,6]}}); $nor $or的反操作，即不匹配(a或b) db.things.find( { name : “bob”, $nor : [ { a : 1 },{ b : 2 }]}) $or Or子句，注意$or不能嵌套使用 db.things.find( { name : “bob” , $or : [ { a : 1 },{ b : 2 }]}) $size 匹配数组长度 db.things.find( { a : { $size: 1 } } ); $type 匹配子键的数据类型，详情请看 db.things.find( { a : { $type : 2 } } ); 数组查询$size 用来匹配数组长度（即最大下标）// 返回comments包含5个元素的文档db.posts.find({}, {comments:{‘$size’: 5}});// 使用冗余字段来实现db.posts.find({}, {‘commentCount’: { ‘$gt’: 5 }});$slice 操作符类似于子键筛选，只不过它筛选的是数组中的项// 仅返回数组中的前5项db.posts.find({}, {comments:{‘$slice’: 5}});// 仅返回数组中的最后5项db.posts.find({}, {comments:{‘$slice’: -5}});// 跳过数组中的前20项，返回接下来的10项db.posts.find({}, {comments:{‘$slice’: [20, 10]}});// 跳过数组中的最后20项，返回接下来的10项db.posts.find({}, {comments:{‘$slice’: [-20, 10]}});MongoDB 允许在查询中指定数组的下标，以实现更加精确的匹配// 返回comments中第1项的by子键为Abe的所有文档db.posts.find( { “comments.0.by” : “Abe” } ); 3.索引的使用创建索引12345678db.things.ensureIndex(&#123;'j': 1&#125;);//创建子文档 索引db.things.ensureIndex(&#123;'user.Name' : - 1&#125;);//创建 复合 索引db.things.ensureIndex(&#123;'j' : 1 , // 升序'x' : - 1 // 降序&#125;); 如果 您的 find 操作只用到了一个键，那么索引方向是无关紧要的 当创建复合索引的时候，一定要谨慎斟酌每个键的排序方向 修改索引修改索引，只需要重新 运行索引 命令即可如果索引已经存在则会 重建， 不存在的索引会被 添加 1234567891011db.things.ensureIndex (&#123; //原来的索引会 重建 'user.Name ' : - 1 , //新增一个升序 索引 'user.Name ' : 1 , //为 Age 新建降序 索引 'user.Age ' : - 1 //打开后台执行&#125;,&#123; 'background' : true&#125;);//重建索引db.things.reIndex(); 删除索引1234567891011121314//删除集合中的所有 索引db.things.dropIndexes (); //删除指定键的索引 db.things.dropIndex (&#123; x : 1 , y : - 1&#125;); //使用 command 删除指定键的 索引db.runCommand (&#123; dropIndexes : 'foo ' , index:&#123; y : 1 &#125;&#125;); //使用 command 删除所有 索引db.runCommand (&#123;dropIndexes : 'foo ',index: '*'&#125;) 如果是删除集合中所有的文档（remove）则不会影响索引，当有新文档插入时，索引就会重建。 唯一索引创建唯一索引，同时这也是一个符合唯一索引 12345678910db.things.ensureIndex (&#123; 'firstName ' : 1 , 'lastName ' : 1&#125;, &#123;//指定为唯一索引'unique': true ,//删除重复 记录'dropDups': true&#125;); 强制使用索引12345678910111213//强制使用索引 a 和 bdb.collection.find(&#123; 'a' : 4 , 'b' : 5 , 'c' : 6&#125;).hint(&#123; 'a' : 1 , 'b' : 1&#125;);//强制不使用任何 索引db.collection.find().hint(&#123; '$natural' : 1&#125;); 索引总结: 索引可以加速查询； 单个索引无需在意其索引方向； 多键索引需要慎重考虑每个索引的方向； 做海量数据更新时应当先卸载所有索引，待数据更新完成后再重建索引； 不要试图为每个键都创建索引，应考虑实际需要，并不是索引越多越好； 唯一索引可以用来消除重复记录； 地理空间索引是没有单位的，其内部实现是基本的勾股定理算法 4.mongo数据库管理安全与认证 默认为无认证，启动用登录 shell ； 添加账号； 关闭 shell .关闭 MongoDB ； 为 MongoDB 增加 — auth 参数； 重 启 MongoDB ； 登录 shell ，此时就需要认证了 冷备份 关闭MongoDB引擎 拷贝数据库文件夹及文件 恢复时反向操作即可 优点：可以完全保证数据完整性； 缺点：需要数据库引擎离线 热备份 保持MongoDB为运行状态 使用mongodump备份数据 使用mongorestore恢复数据 优点：数据库引擎无须离线 缺点：不能保证数据完整性，操作时会降低MongoDB性能 主从复制备份 创建主从复制机制 配置完成后数据会自动同步 恢复途径很多 优点：可以保持MongoDB处于联机状态，不影响性能 缺点：在数据写入密集的情况下可能无法保证数据完整性 修复db.repairDatabase(); 修复数据库还可以起到压缩数据的作用； 修复数据库的操作相当耗时，万不得已请不要使用； 建议经常做数据备份；5.mongo复制(集群) 主从复制选项 说明 –only 作用是限定仅复制指定的某个数据库–slavedelay 为复制设置操作延迟，单位为秒–fastsync 以主节点的数据快照为基础启动从节点。–autoresync 当主从节点数据不一致时，是否自动重新同步–oplogSize 设定主节点中的oplog的容量，单位是MB 副本集与普通主从复制集群相比，具有自动检测机制需要使用—replSet 选项指定副本同伴任何时候，副本集当中最多只允许有1个活跃节点 读写分离将密集的读取操作分流到从节点上，降低主节点的负载默认情况下，从节点是不允许处理客户端请求的，需要使用—slaveOkay打开不适用于实时性要求非常高的应用 工作原理—— OPLOGoplog保存在local数据库中，oplog就在其中的oplog.$main集合内保存。该集合的每个文档都记录了主节点上执行的一个操作，其键定义如下： ts：操作时间戳，占用4字节 op：操作类型，占用1字节 ns：操作对象的命名空间（或理解为集合全名） o：进一步指定所执行的操作，例如插入 工作原理—— 同步 从节点首次启动时，做完整同步 主节点数据发生变化时，做增量同步 从节点与主节点数据严重不一致时，做完整同步 复制管理—— 诊断db.printReplicationInfo()在主节点上使用 返回信息是oplog的大小以及各种操作的耗时. 空间占用等数据在从节点上使用db.printSlaveReplicationInfo() 返回信息是从节点的数据源列表. 同步延迟时间等 复制管理—— 变更OPLOG 容量在主节点上使用 设定—oplogSize参数 重启MongoDB 复制管理—— 复制认证主从节点皆须配置 存储在local.system.users 优先尝试repl用户 主从节点的用户配置必须保持一致 6.MONGODB分片分片与自动分片分片是指将数据拆分，分散到不同的实例上进行负载分流的做法。我们常说的“分表”、“分库”、“分区”等概念都属于分片的实际体现。传统分片做法是手工分表、分库。自动分片技术是根据指定的“片键”自动拆分数据并维护数据请求路由的过程。 递增片键–连续 不均匀 写入集中 分流较差 随机片键–不连续 均匀 写入分散 分流较好 三个组成部分 片,保存子集数据的容器 mongos,MongoDB的路由器进程 配置服务器,分·片集群的配置信息创建分片 –启动配置服务器,可以创建一个或多个 –添加片,每个片都应该是副本集 –物理服务器,性能、安全和稳定性管理分片12345678910//查询分片db.shards.find();//数据库db.databases.find();//块db.chunks.find();//分片状态db.printShardingStatus();//删除片db.runCommand(&#123; removeshard : 'ip:port' &#125;);","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}],"tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://zhangfuxin.cn/tags/mongodb/"}],"keywords":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}]},{"title":"hexo自用黑色主题","slug":"hexo-theme","date":"2017-05-23T03:32:59.000Z","updated":"2019-08-21T15:13:24.127Z","comments":true,"path":"hexo-theme.html","link":"","permalink":"http://zhangfuxin.cn/hexo-theme.html","excerpt":"** hexo和coding打造静态博客 ：** &lt;Excerpt in index | 首页摘要&gt;使用hexo一年有余，对所有主题都感觉有所缺陷，便修改了一个自用黑色主题，本主题以黑色和蓝色为主，色彩鲜明，主题明确。","text":"** hexo和coding打造静态博客 ：** &lt;Excerpt in index | 首页摘要&gt;使用hexo一年有余，对所有主题都感觉有所缺陷，便修改了一个自用黑色主题，本主题以黑色和蓝色为主，色彩鲜明，主题明确。 &lt;The rest of contents | 余下全文&gt; 主题图片 black-blue主题来源本主题修改自spfk主题，但之前spfk主题有很多问题，本主题改进如下： 压缩js，css提高性能 代码段样式显示更完美 增加本地搜索 设置更合适的字体大小 颜色以黑色和蓝色为主，色彩鲜明 seo适当优化 删除多说，有言，增加畅言评论 删除stylus，全部改用css方便修改 主题地址black-blue 注意：大家使用主题的时候，把主题配置文件_config.yml以下几项必须修改，项目里实用的是我博客的正式代码，请大家修改成自己的！ 12345678910google_analytics: xxxbaidu_analytics: xxxxxxxdisqus: on: false shortname: xxxx# 畅言评论changyan: on: true appid: xxxx conf: xxxxx black-blue主题配置切换主题复制主题到themes目录下cd themes &amp;&amp; git clone https://github.com/maochunguang/black-blue，修改_config.yml theme: black-blue 安装常用插件，建议全部安装123456789## rss插件npm install hexo-generator-feed --save## 站点sitemap生成插件npm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save## 百度url提交npm install hexo-baidu-url-submit --save## 本地搜索插件集成npm install hexo-generator-search --save 博客全局配置，修改根目录下_config.yml插件配置 1234Plugins:- hexo-generator-feed- hexo-generator-sitemap- hexo-generator-baidu-sitemap rss设置 1234feed: type: atom path: atom.xml limit: 20 本地搜索配置 123search: path: search.json field: post 站点地图，seo搜索引擎需要 1234sitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 主题配置菜单配置 123456789## 添加单独的页面:hexo new page about，about是页面的路径，也是名称## Tags Cloud Page: `hexo new page tags`menu: # 主页: /archives/ 所有文章: /archives/ 玩转开发工具: /categories/开发工具/ 玩转数码: /categories/digital 认知提升: /categories/cognition 关于我: /about/ 评论配置 123456789# 是否开启畅言评论，changyan: on: true appid: xxxx conf: xxxxxxxxxxxx# 是否开启disqus，disqus: on: false shortname: mmmmmm 其他配置，详细的配置请下载主题，都有注释1234567# 数学公式支持mathjax: false# Socail Share | 是否开启分享baidushare: true# 谷歌分析，百度分析，seo分析很有用google_analytics: xxxxxxbaidu_analytics: xxcxcxcsdsf 自定义配置（对前端技术有了解即可）显示更多和折叠文章你的md文件格式需要按下面的来： 12345678910title: 突破算法第11天-红黑树date: 2017-10-30 22:35:37tags: 算法categories: algorithm---** &#123;&#123; title &#125;&#125;：** &lt;Excerpt in index | 首页摘要&gt;红黑树&lt;!-- more --&gt;&lt;The rest of contents | 余下全文&gt;正文…… 头像配置在themes/black-blue/source/img/avatar.png,替换此头像即可实现自定义头像 背景图片配置在themes/black-blue/source/background/,替换为自己喜欢的图片，图片名称不能改 添加评论插件比如把畅言替换为有言 先修改themes/black-blue/_config.yml文件 123changyan: on: true uid: xxxxxxx 修改themes/black-blue/layout/_partial/comments/changyan.ejs 12345&lt;section class=\"changyan\" id=\"comments\"&gt;&lt;div id=\"uyan_frame\"&gt;&lt;/div&gt;&lt;script type=\"text/javascript\" src=\"http://v2.uyan.cc/code/uyan.js?uid=&lt;%= uid%&gt;\"&gt;&lt;/script&gt;&lt;/section&gt; 修改themes/black-blue/layout/_partial/article.ejs 123&lt;%- partial('comments/changyan', &#123; uid: theme.changyan.uid&#125;) %&gt; 重新生成页面hexo g","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://zhangfuxin.cn/tags/hexo/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"数码产品选购","slug":"digital-info","date":"2017-05-18T02:35:22.000Z","updated":"2019-08-29T03:00:47.925Z","comments":true,"path":"digital-info.html","link":"","permalink":"http://zhangfuxin.cn/digital-info.html","excerpt":"** 数码产品选购：** &lt;Excerpt in index | 首页摘要&gt;作为一个数码产品控，一出新的的电子产品，我都欣喜若狂。看参数，看评价，感觉合适，就会买。","text":"** 数码产品选购：** &lt;Excerpt in index | 首页摘要&gt;作为一个数码产品控，一出新的的电子产品，我都欣喜若狂。看参数，看评价，感觉合适，就会买。 &lt;The rest of contents | 余下全文&gt; 我喜欢的电子产品 电脑（笔记本，台式机，游戏主机，工作站） 手机（苹果，安卓，其它智能手机） 平板（安卓平板，ios平板） 电子书阅读器 电子手表 选购的原则 产品生态，买电子产品虽然不是随大流，但是用户群体一定程度决定了生态。用的人多，相应的资源会比较丰富，遇到问题很快找到解决方案。 产品价格，性价比在中国，乃至全世界都是很具有吸引力的。物美价廉的都不买的要么是脑残，要么是钱多没地方花。 产品硬件参数，买电子产品不看参数，肯定是买不到物美价廉的产品。 产品外观，现在是看脸的时代，新时代的数码产品对外观要求更高，更时尚。 功能，买电子产品，首要的就是功能，如果功能都不齐全，再漂亮，再便宜都没用。 买电子产品的目的，没有任何需求就是瞎买。 电子产品的使用我见过很多人买电子产品，比如买电子书阅读器，买一个kobo电子书折腾来折腾去，今天刷这个系统，明天改那个设置，书还没读几本，系统刷了几十次，天天刷固件。这真的是得不偿失，捡了芝麻丢了西瓜。第一，买电子产品是为了用的，买回来之后配置好之后，就不要来回折腾系统和配置了，把时间放到核心功能上。第二，买电子产品不要攀比，就跟买苹果手机一样，如果只是为了装B买，真没必要，结果自己还用不习惯。第三，了解自己的需求，需要什么买什么，","categories":[{"name":"digital","slug":"digital","permalink":"http://zhangfuxin.cn/categories/digital/"}],"tags":[{"name":"数码产品","slug":"数码产品","permalink":"http://zhangfuxin.cn/tags/数码产品/"}],"keywords":[{"name":"digital","slug":"digital","permalink":"http://zhangfuxin.cn/categories/digital/"}]},{"title":"如何写一篇好博客？","slug":"bestblog","date":"2017-05-15T15:04:48.000Z","updated":"2019-08-29T03:01:51.712Z","comments":true,"path":"bestblog.html","link":"","permalink":"http://zhangfuxin.cn/bestblog.html","excerpt":"** 提高自己博客的质量：** &lt;Excerpt in index | 首页摘要&gt;写博客陆陆续续也有一年了，但是一直没有多少访问量，仔细看了很多大神的博客，总结了几点，分享一下。","text":"** 提高自己博客的质量：** &lt;Excerpt in index | 首页摘要&gt;写博客陆陆续续也有一年了，但是一直没有多少访问量，仔细看了很多大神的博客，总结了几点，分享一下。 &lt;The rest of contents | 余下全文&gt; 好博客，好文章是什么样的？ 文章名称鲜明，一看名称就知道关于什么的内容 整体结构清晰，把事件或者原理的始末按照‘什么样（what？）’，‘为什么（why）’，‘怎么做（how）’说明 简明扼要。太啰嗦，没人看。 难易适中，太高深也没人看 图文搭配，有句话说的好，一图胜千文，好的图片胜过千言万语 怎么写出好博客？ 定主题和文章名称。如果想写一个关于redis后台启动的文章，名称要准确，就叫redis后台启动，不要起啰嗦的名字，比如redis如何后台启动 准备资料阶段，熟悉redis配置相关资料，做好功课 定文章的结构和提纲。还拿这个redis后台启动为例，你得说明什么是后台启动？为什么要后台启动？如何做到后台启动？ 语言表单，简单直白，不用凑字数 深入主题，比如挖掘更多redis的配置，把参数简要说明 找一个好图片，如果找不到，自己制作一个最契合自己主题的图片 把文章发给好友阅读，提出宝贵的意见 改进博客 坚持写博客","categories":[{"name":"个人随笔","slug":"个人随笔","permalink":"http://zhangfuxin.cn/categories/个人随笔/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"个人随笔","slug":"个人随笔","permalink":"http://zhangfuxin.cn/categories/个人随笔/"}]},{"title":"redis后台启动详细配置","slug":"redis-config","date":"2017-05-15T14:58:07.000Z","updated":"2019-08-29T02:31:44.684Z","comments":true,"path":"redis-config.html","link":"","permalink":"http://zhangfuxin.cn/redis-config.html","excerpt":"** redis后台启动详细配置：** &lt;Excerpt in index | 首页摘要&gt; redis启动的时候有多种模式，后台启动，集群启动等等。","text":"** redis后台启动详细配置：** &lt;Excerpt in index | 首页摘要&gt; redis启动的时候有多种模式，后台启动，集群启动等等。 &lt;The rest of contents | 余下全文&gt; 说明在开发中一般都是在命令行中直接运行redis-server,但是这样命令行关闭，服务就停止了。如果要在后台运行redis服务，需要制定配置文件。这里以ubuntu14为例子 准备配置文件查看‘/etc/redis/redis.conf’,没有可以创建一个，或者下载一个，配置文件位置没有要求 修改配置文件把daemonize设置为yes，然后redis-server /etc/redis/redis.conf启动服务， 查看服务ps -ef|grep redis-server查看是否有redis进程存在 更多配置，在conf文件有说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 是否以后台daemon方式运行，默认是 no，一般我们会改为 yesdaemonize nopidfile /var/run/redis.pid# 只允许本机访问bind 127.0.0.1# 端口设置port 6379tcp-backlog 511timeout 0tcp-keepalive 0loglevel notice# 日志文件logfile &quot;&quot;# 开启数据库的数量，Redis 是有数据库概念的，默认是 16 个，数字从 0 ~ 15databases 16save 900 1save 300 10save 60 10000stop-writes-on-bgsave-error yesrdbcompression yesrdbchecksum yesdbfilename dump.rdbdir ./slave-serve-stale-data yesslave-read-only yesrepl-diskless-sync norepl-diskless-sync-delay 5repl-disable-tcp-nodelay no# 密码设置，需要设置密码打开requirepass 123455slave-priority 100appendonly noappendfilename &quot;appendonly.aof&quot;appendfsync everysecno-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mbaof-load-truncated yeslua-time-limit 5000slowlog-log-slower-than 10000slowlog-max-len 128latency-monitor-threshold 0notify-keyspace-events &quot;&quot;hash-max-ziplist-entries 512hash-max-ziplist-value 64list-max-ziplist-entries 512list-max-ziplist-value 64set-max-intset-entries 512zset-max-ziplist-entries 128zset-max-ziplist-value 64hll-sparse-max-bytes 3000activerehashing yesclient-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60hz 10aof-rewrite-incremental-fsync yes","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://zhangfuxin.cn/tags/redis/"}],"keywords":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}]},{"title":"用koa2.x写下载漫画的爬虫","slug":"spider-koa2","date":"2017-05-13T23:15:38.000Z","updated":"2019-08-29T02:30:56.460Z","comments":true,"path":"spider-koa2.html","link":"","permalink":"http://zhangfuxin.cn/spider-koa2.html","excerpt":"** 用koa2.x写下载漫画的爬虫：** &lt;Excerpt in index | 首页摘要&gt;使用koa2.x的async ，await解决异步问题，写一个下载漫画的爬虫，代码里有惊喜和福利哦！","text":"** 用koa2.x写下载漫画的爬虫：** &lt;Excerpt in index | 首页摘要&gt;使用koa2.x的async ，await解决异步问题，写一个下载漫画的爬虫，代码里有惊喜和福利哦！ &lt;The rest of contents | 余下全文&gt; 项目搭建 安装nodejs&gt;7.6,安装koa-generator 直接koa2 spider,生成项目 安装request,request-promise,cheerio,mkdirp npm install安装依赖 思路图片或者漫画爬虫的思路很简单，首先观察url的规律，把url按规律加入到下载任务，其实就是请求获得html内容，然后对html进行解析，找到下载的图片url（一般都是img标签的src属性值），把url放到数组保存，使用async await控制所有的任务，直到把所有的图片下载完。 难点但是nodejs本身上异步的，如果你直接在for循环里去下载，肯定是不行的，必须控制好异步的执行上关键。爬虫简单，处理好异步难。这里我使用的es7中async，await配合promise解决异步问题，还可以使用async模块，eventproxy，等等异步控制模块来解决。 核心代码,spider.js1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950const fs = require('fs');const request = require(\"request-promise\");const cheerio = require(\"cheerio\");const mkdirp = require('mkdirp');const config = require('../config');exports.download = async function(ctx, next) &#123; const dir = 'images'; // 图片链接地址 let links = []; // 创建目录 mkdirp(dir); var urls = []; let tasks = []; let downloadTask = []; let url = config.url; for (var i = 1; i &lt;= config.size; i++) &#123; let link = url + '_' + i + '.html'; if (i == 1) &#123; link = url + '.html'; &#125; tasks.push(getResLink(i, link)) &#125; links = await Promise.all(tasks) console.log('links==========', links.length); for (var i = 0; i &lt; links.length; i++) &#123; let item = links[i]; let index = item.split('___')[0]; let src = item.split('___')[1]; downloadTask.push(downloadImg(src, dir, index + links[i].substr(-4, 4))); &#125; await Promise.all(downloadTask);&#125;async function downloadImg(url, dir, filename) &#123; console.log('download begin---', url); request.get(url).pipe(fs.createWriteStream(dir + \"/\" + filename)).on('close', function() &#123; console.log('download success', url); &#125;);&#125;async function getResLink(index, url) &#123; const body = await request(url); let urls = []; var $ = cheerio.load(body); $(config.rule).each(function() &#123; var src = $(this).attr('src'); urls.push(src); &#125;); return index + '___' + urls[0];&#125; 基础配置由于爬虫的复杂性基于不同的网站，不同的任务很不一样，这里只是把几个常用的变量抽取到了config.js。 1234567module.exports = &#123; //初始url url: 'http://www.xieet.com/meinv/230', size: 10, // 选中图片img标签的选择器 rule: '.imgbox a img'&#125;; 运行代码 下载我上传的代码koa-spider npm install,npm start即可运行 总结其实无论是写爬虫还是些其他程序，使用nodejs很大一部分都是要处理异步，要学好nodejs必须学好异步处理。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://zhangfuxin.cn/categories/学习笔记/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://zhangfuxin.cn/tags/nodejs/"}],"keywords":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://zhangfuxin.cn/categories/学习笔记/"}]},{"title":"微信公众号开发","slug":"wechat-dev","date":"2017-04-28T04:55:33.000Z","updated":"2019-08-29T02:26:56.748Z","comments":true,"path":"wechat-dev.html","link":"","permalink":"http://zhangfuxin.cn/wechat-dev.html","excerpt":"** 微信公众号开发：** &lt;Excerpt in index | 首页摘要&gt;微信公众号开发的一些注意事项","text":"** 微信公众号开发：** &lt;Excerpt in index | 首页摘要&gt;微信公众号开发的一些注意事项 &lt;The rest of contents | 余下全文&gt; 开发环境搭建 微信公众号开发者配置，url，token， 本地调试，使用内网穿透工具，花生壳，或者netapp，买一个可以自定义域名的，内网映射到制定端口， 项目搭建，express或koa搭建项目，npm有微信的现成包，直接配置 回复 回复和发消息并没有什么特别注意的地方，这里不多说 菜单 微信菜单有自定义菜单，有个性化菜单，但是个性化菜单优先级高于个性化菜单 个性化菜单可以根据用户的tag，sex，group等属性进行区分菜单 注意，我在使用时发现个性化菜单经常会失效，不起作用，偶尔会起作用，如果线上打算使用个性化菜单，请慎重并仔细测试 授权授权有网页授权，js sdk授权，网页授权也有两种，一个上静默授权，一个是点击授权，贴一下js sdk调用前认证的代码，要使用sha1加密 123456789101112131415async getSignConfig(originUrl) &#123; let data = &#123;&#125; const sha1 = crypto.createHash('sha1') const appId = this.app.config.weixin.appID const jsapi_ticket = await this.ctx.service.token.getJSApiTicket() const noncestr = this.app.config.jsapi.noncestr const url = this.app.config.domain + originUrl const timestamp = parseInt(new Date().getTime() / 1000) // sha1加密 const str = `jsapi_ticket=$&#123;jsapi_ticket&#125;&amp;noncestr=$&#123;noncestr&#125;&amp;timestamp=$&#123;timestamp&#125;&amp;url=$&#123;url&#125;` sha1.update(str) const signature = sha1.digest('hex') data = &#123; jsapi_ticket, noncestr, timestamp, url, signature, appId &#125; return data &#125; 调用js sdk页面上代码 12345678910111213wx.config(&#123; debug: false, // 开启调试模式, appId: appId, // 必填，公众号的唯一标识 timestamp: timestamp, // 必填，生成签名的时间戳 nonceStr: nonceStr, // 必填，生成签名的随机串 signature: signature,// 必填，签名，见附录1 jsApiList: ['closeWindow'] // 必填，需要使用的JS接口列表，所有JS接口列表见附录2&#125;);wx.ready(function()&#123; setTimeout(function()&#123; wx.closeWindow(); &#125;,2000);&#125;); 实用的常识 tag不能重复创建，但是给用户可以重复打同一个tag 更改菜单一般五分钟生效，或者重新关注公众号，立马能看到 如果调用js sdk，务必使用https，防止因为安全问题，导致ios下js下载失败。如果你的服务是https，而引用了https的微信js，在ios下肯定会下载失败，这是ios的安全机制导致的。 微信关闭窗口的js接口，不管jsconfig验证是否通过，窗口都可以关闭 微信的token过期时间上2h，但是很多时候30分钟不到可能已经失效，建议把token过期时间设置为10分钟之内 常见报错 创建菜单的时候，菜单长度不合法，仔细检查自己传的json菜单，一般都是json格式问题，而不是长度 redirect_uri不合法，是创建授权菜单的redirect_uri和网页授权域名配置不一样 关注公众号，服务端设置的欢迎消息发不过去，如果自己代码无异常，一般是因为token过期 以后遇到其他问题继续补充","categories":[{"name":"javacript","slug":"javacript","permalink":"http://zhangfuxin.cn/categories/javacript/"}],"tags":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/tags/编程语言/"}],"keywords":[{"name":"javacript","slug":"javacript","permalink":"http://zhangfuxin.cn/categories/javacript/"}]},{"title":"那些年读的书","slug":"mybooks","date":"2017-04-15T06:49:46.000Z","updated":"2019-08-29T02:32:56.582Z","comments":true,"path":"mybooks.html","link":"","permalink":"http://zhangfuxin.cn/mybooks.html","excerpt":"** 那些年读的书：** &lt;Excerpt in index | 首页摘要&gt;人生漫漫，不知不觉读了好多书，此贴只记录自己读过哪些书，不做多余的分析和总结。","text":"** 那些年读的书：** &lt;Excerpt in index | 首页摘要&gt;人生漫漫，不知不觉读了好多书，此贴只记录自己读过哪些书，不做多余的分析和总结。 &lt;The rest of contents | 余下全文&gt; 读过哪些种类的 编程专业类 小说类 励志类 小说 平凡的世界 白鹿原 穆斯林的葬礼 金庸武侠系列 古龙武侠小说 梁羽生武侠小说 余华作品集 雷米小说全集（侦探类） 网络小说： 诛仙， 盗墓笔记， 泡沫之夏， 芈月传， 编程类 java编程思想 effective java java并发编程的艺术 代码整洁之道 黑客与画家 深入浅出nodejs nodejs实战 js高级程序设计 survivejs redux和react中文手册 你不知道的javascript 算法javascript实现 mysql权威指南 mongodb权威指南 mongodb实战第二版 redis入门 经管励志 时间管理 一分钟系列 番茄工作法图解","categories":[{"name":"book","slug":"book","permalink":"http://zhangfuxin.cn/categories/book/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://zhangfuxin.cn/tags/学习笔记/"}],"keywords":[{"name":"book","slug":"book","permalink":"http://zhangfuxin.cn/categories/book/"}]},{"title":"免费的开源书籍","slug":"free-books","date":"2016-11-28T16:15:52.000Z","updated":"2019-08-29T03:01:31.623Z","comments":true,"path":"free-books.html","link":"","permalink":"http://zhangfuxin.cn/free-books.html","excerpt":"** 免费的开源书籍：** &lt;Excerpt in index | 首页摘要&gt;国外程序员在 stackoverflow 推荐的程序员必读书籍，中文版。","text":"** 免费的开源书籍：** &lt;Excerpt in index | 首页摘要&gt;国外程序员在 stackoverflow 推荐的程序员必读书籍，中文版。 &lt;The rest of contents | 余下全文&gt; 目录 语言无关 IDE MySQL NoSQL PostgreSQL Web WEB服务器 其它 函数式概念 分布式系统 在线教育 大数据 操作系统 数据库 智能系统 正则表达式 版本控制 程序员杂谈 管理和监控 编程艺术 编译原理 编辑器 计算机图形学 设计模式 软件开发方法 项目相关 语言相关 Android AWK C C# C++ CoffeeScript Dart Elasticsearch Elixir Erlang Fortran Golang Haskell HTML / CSS HTTP iOS Java JavaScript LaTeX LISP Lua Markdown Node.js Perl PHP Python R reStructuredText Ruby Rust Scala Scheme Shell Swift Vim Visual Prolog 语言无关IDE IntelliJ IDEA 简体中文专题教程 MySQL 21分钟MySQL入门教程 MySQL索引背后的数据结构及算法原理 NoSQL Disque 使用教程 Neo4j .rb 中文資源 Neo4j 简体中文手册 v1.8 Redis 命令参考 Redis 设计与实现 The Little MongoDB Book The Little Redis Book 带有详细注释的 Redis 2.6 代码 带有详细注释的 Redis 3.0 代码 PostgreSQL PostgreSQL 8.2.3 中文文档 PostgreSQL 9.3.1 中文文档 Web 3 Web Designs in 3 Weeks Chrome 开发者工具中文手册 Chrome扩展开发文档 Growth: 全栈增长工程师指南 Grunt中文文档 Gulp 入门指南 gulp中文文档 HTTP 接口设计指北 HTTP/2.0 中文翻译 http2讲解 JSON风格指南 Wireshark用户手册 一站式学习Wireshark 关于浏览器和网络的 20 项须知 前端代码规范 及 最佳实践 前端开发体系建设日记 前端资源分享（一） 前端资源分享（二） 正则表达式30分钟入门教程 浏览器开发工具的秘密 移动Web前端知识库 移动前端开发收藏夹 WEB服务器 Apache 中文手册 Nginx开发从入门到精通 (淘宝团队出品) Nginx教程从入门到精通 (PDF版本，运维生存时间出品) 其它 OpenWrt智能、自动、透明翻墙路由器教程 SAN 管理入门系列 Sketch 中文手册 深入理解并行编程 函数式概念 傻瓜函数编程 分布式系统 走向分布式 (PDF) 在线教育 51CTO学院 Codecademy CodeSchool Coursera Learn X in Y minutes (数十种语言快速入门教程) shiyanlou TeamTreeHouse Udacity xuetangX 慕课网 (丰富的移动端开发、php开发、web前端、html5教程以及css3视频教程等课程资源) 极客学院 计蒜客 大数据 Spark 编程指南简体中文版 大型集群上的快速和通用数据处理架构 大数据/数据挖掘/推荐系统/机器学习相关资源 数据挖掘中经典的算法实现和详细的注释 面向程序员的数据挖掘指南 操作系统 Debian 参考手册 Docker —— 从入门到实践 Docker中文指南 Docker入门实战 FreeBSD 使用手册 FreeRADIUS新手入门 Linux Documentation (中文版) Linux Guide for Complete Beginners Linux 构建指南 Linux 系统高级编程 Linux工具快速教程 Mac 开发配置手册 Operating Systems: Three Easy Pieces The Linux Command Line (中英文版) Ubuntu 参考手册 uCore Lab: Operating System Course in Tsinghua University UNIX TOOLBOX 命令行的艺术 嵌入式 Linux 知识库 (eLinux.org 中文版) 开源世界旅行手册 深入分析Linux内核源码 理解Linux进程 鸟哥的 Linux 私房菜 基础学习篇 鸟哥的 Linux 私房菜 服务器架设篇 数据库 Redis 设计与实现 The Little MongoDB Book 中文版 智能系统 一步步搭建物联网系统 正则表达式 正则表达式30分钟入门教程 版本控制 Git - 简易指南 Git-Cheat-Sheet （感谢 @flyhigher139 翻译了中文版） Git Community Book 中文版 git-flow 备忘清单 Git magic Git Magic Git 参考手册 Github帮助文档 GitHub秘籍 Git教程 （本文由 @廖雪峰 创作，如果觉得本教程对您有帮助，可以去 iTunes 购买） Got GitHub GotGitHub HgInit (中文版) Mercurial 使用教程 Pro Git Pro Git 中文版 (整理在gitbook上) svn 手册 学习 Git 分支 (点击右下角按钮可切换至简体及正体中文) 沉浸式学 Git 猴子都能懂的GIT入门 程序员杂谈 程序员的自我修养 管理和监控 ElasticSearch 权威指南 Elasticsearch 权威指南（中文版） ELKstack 中文指南 Logstash 最佳实践 Mastering Elasticsearch(中文版) Puppet 2.7 Cookbook 中文版 编程艺术 取悦的工序：如何理解游戏 (豆瓣阅读，免费书籍) 每个程序员都应该了解的内存知识(译)【第一部分】 程序员编程艺术 编程入门指南 编译原理 《计算机程序的结构和解释》公开课 翻译项目 编辑器 exvim–vim 改良成IDE项目 Vim中文文档 所需即所获：像 IDE 一样使用 vim 笨方法学Vimscript 中译本 计算机图形学 OpenGL 教程 设计模式 史上最全设计模式导学目录 图说设计模式 软件开发方法 傻瓜函数编程 (《Functional Programming For The Rest of Us》中文版) 硝烟中的 Scrum 和 XP 项目相关 GNU make 指南 Gradle 2 用户指南 Gradle 中文使用文档 Joel谈软件 selenium 中文文档 开源软件架构 持续集成（第二版） (译言网) 約耳談軟體(Joel on Software) 编码规范 让开发自动化系列专栏 追求代码质量 语言相关Android Android Design(中文版) Android Note(开发过程中积累的知识点) Android6.0新特性详解 Android学习之路 Android开发技术前线(android-tech-frontier) Google Android官方培训课程中文版 Google Material Design 正體中文版 (译本一 译本二) Material Design 中文版 Point-of-Android Android 一些重要知识点解析整理 AWK awk中文指南 awk程序设计语言 C C 语言常见问题集 C/C++ 学习教程 Linux C 编程一站式学习 新概念 C 语言教程 C Sharp 精通C#(第6版) C++ 100个gcc小技巧 100个gdb小技巧 C 语言编程透视 C/C++ Primer - @andycai C++ FAQ LITE(中文版) C++ Primer 5th Answers C++ Template 进阶指南 C++ 基础教程 C++ 并发编程(基于C++11) C++ 并发编程指南 CGDB中文手册 Cmake 实践 (PDF版) GNU make 指南 Google C++ 风格指南 QT 教程 ZMQ 指南 像计算机科学家一样思考（C++版) (《How To Think Like a Computer Scientist: C++ Version》中文版) 简单易懂的C魔法 跟我一起写Makefile(PDF) (PDF) CoffeeScript CoffeeScript 中文 CoffeeScript 编程风格指南 Dart Dart 语言导览 Elasticsearch Elasticsearch 权威指南 （《Elasticsearch the definitive guide》中文版） ELKstack 中文指南 Mastering Elasticsearch(中文版) Elixir Elixir Getting Started 中文翻译 Elixir 编程语言教程 (Elixir School) Elixir元编程与DSL 中文翻译 Phoenix 框架中文文档 Erlang Erlang 并发编程 (《Concurrent Programming in Erlang (Part I)》中文版) Fortran Fortran77和90/95编程入门 Golang Effective Go Go Web 编程 Go 入门指南 (《The Way to Go》中文版) Go 官方文档翻译 Go 指南 (《A Tour of Go》中文版) Go 简易教程 (《The Little Go Book》中文版) Go 编程基础 Go 语言标准库 Go命令教程 Go实战开发 Go语言博客实践 Java程序员的Golang入门指南 Network programming with Go 中文翻译版本 Revel 框架手册 学习Go语言 Groovy 实战 Groovy 系列 Haskell Haskell 趣学指南 Real World Haskell 中文版 HTML / CSS CSS3 Tutorial 《CSS3 教程》 CSS参考手册 Emmet 文档 HTML5 教程 HTML和CSS编码规范 Sass Guidelines 中文 前端代码规范 (腾讯 AlloyTeam 团队) 学习CSS布局 通用 CSS 笔记、建议与指导 iOS Apple Watch开发初探 Google Objective-C Style Guide 中文版 iOS7人机界面指南 iOS开发60分钟入门 iPhone 6 屏幕揭秘 网易斯坦福大学公开课：iOS 7应用开发字幕文件 Java Activiti 5.x 用户指南 Apache MINA 2 用户指南 Apache Shiro 用户指南 Google Java编程风格指南 H2 Database 教程 Java Servlet 3.1 规范 Java 编码规范 Jersey 2.x 用户指南 JSSE 参考指南 MyBatis中文文档 Netty 4.x 用户指南 Netty 实战(精髓) REST 实战 Spring Boot参考指南 (翻译中) Spring Framework 4.x参考文档 用jersey构建REST服务 Javascript Airbnb JavaScript 规范 AngularJS AngularJS中译本 AngularJS入门教程 AngularJS最佳实践和风格指南 在Windows环境下用Yeoman构建AngularJS项目 构建自己的AngularJS backbone.js backbone.js中文文档 backbone.js入门教程 (PDF) Backbone.js入门教程第二版 Developing Backbone.js Applications(中文版) Chrome扩展及应用开发 CoffeeScript CoffeeScript 编码风格指南 D3.js D3.js 入门系列 (还有进阶、高级等系列) 官方API文档 张天旭的D3教程 楚狂人的D3教程 ECMAScript 6 入门 (作者：阮一峰) ExtJS Ext4.1.0 中文文档 Google JavaScript 代码风格指南 Google JSON 风格指南 impress.js impress.js的中文教程 JavaScript Promise迷你书 Javascript 原理 JavaScript 标准参考教程（alpha） 《JavaScript 模式》 “JavaScript patterns”中译本 javascript 的 12 个怪癖 JavaScript 秘密花园 JavaScript核心概念及实践 (PDF) (此书已由人民邮电出版社出版发行，但作者依然免费提供PDF版本，希望开发者们去购买，支持作者) Javascript编程指南 (源码) jQuery How to write jQuery plugin 简单易懂的JQuery魔法 Meteor Discover Meteor Node.js express.js 中文文档 Express框架 koa 中文文档 Learn You The Node.js For Much Win! (中文版) Node debug 三法三例 Node.js Fullstack《從零到一的進撃》 Node.js 包教不包会 Nodejs Wiki Book (繁体中文) nodejs中文文档 Node入门 七天学会NodeJS 使用 Express + MongoDB 搭建多人博客 React.js Learn React &amp; Webpack by building the Hacker News front page React Native 中文文档(含最新Android内容) React webpack-cookbook React 入门教程 React.js 中文文档 underscore.js Underscore.js中文文档 You-Dont-Know-JS (深入JavaScript语言核心机制的系列图书) Zepto.js Zepto.js 中文文档 命名函数表达式探秘 (注:原文由为之漫笔 翻译，原始地址无法打开，所以此处地址为我博客上的备份) 学用 JavaScript 设计模式 (开源中国) 深入理解JavaScript系列 LaTeX LaTeX 笔记 一份不太简短的 LaTeX2ε 介绍 大家來學 LaTeX (PDF) LISP ANSI Common Lisp 中文翻译版 Common Lisp 高级编程技术 (《On Lisp》中文版) Lua Lua 5.3 参考手册 Markdown Markdown 快速入门 Markdown 简明教程 Markdown 语法说明 献给写作者的 Markdown 新手指南 Node.js Node 入门 The NodeJS 中文文档（社区翻译） 七天学会NodeJS 阿里出品，很好的入门资料 Perl Master Perl Today 《Modern Perl》中文版 Perl 5 教程 Perl 教程 PHP PHP 之道 PHP5中文手册 PHP扩展开发及内核应用 Symfony2 实例教程 深入理解 PHP 内核 Python Django book 2.0 Python 3 文档(简体中文) 3.2.2 documentation Python 中文学习大本营 深入 Python 3 笨办法学 Python R 153分钟学会 R (PDF) 《R for beginners》中文版 (PDF) R 导论 (《An Introduction to R》中文版) (PDF) 用 R 构建 Shiny 应用程序 (《Building ‘Shiny’ Applications with R》中文版) 统计学与 R 读书笔记 (PDF) reStructuredText reStructuredText 入门 reStructuredText 简明教程 Ruby Rails 风格指南 Ruby on Rails Tutorial 原书第 2 版 Ruby on Rails 实战圣经 Ruby 风格指南 笨方法学 Ruby Rust Rust 官方教程 Rust 语言学习笔记 RustPrimer 通过例子学习 Rust Scala Effective Scala Scala 初学者指南 (The Neophyte’s Guide to Scala) Scala 课堂 (Twitter的Scala中文教程) Scheme Scheme 入门教程 (《Yet Another Scheme Tutorial》中文版) Shell Shell 编程基础 Shell 脚本编程30分钟入门 The Linux Command Line 中文版 Swift 《The Swift Programming Language》中文版 Vim Vim Manual(中文版) 大家來學 VIM Visual Prolog Visual Prolog 7初学指南 Visual Prolog 7边练边学","categories":[{"name":"资源分享","slug":"资源分享","permalink":"http://zhangfuxin.cn/categories/资源分享/"}],"tags":[{"name":"book","slug":"book","permalink":"http://zhangfuxin.cn/tags/book/"}],"keywords":[{"name":"资源分享","slug":"资源分享","permalink":"http://zhangfuxin.cn/categories/资源分享/"}]},{"title":"hexo配置和优化记录","slug":"hexo-config","date":"2016-11-28T15:07:12.000Z","updated":"2019-08-29T03:00:02.297Z","comments":true,"path":"hexo-config.html","link":"","permalink":"http://zhangfuxin.cn/hexo-config.html","excerpt":"** hexo配置和优化高级篇：** &lt;Excerpt in index | 首页摘要&gt;本文章不讲解hexo的基础配置，只针对hexo的高级配置，性能优化，seo配置进行讲解。","text":"** hexo配置和优化高级篇：** &lt;Excerpt in index | 首页摘要&gt;本文章不讲解hexo的基础配置，只针对hexo的高级配置，性能优化，seo配置进行讲解。 &lt;The rest of contents | 余下全文&gt; 前言仔细想想，使用hexo搭建博客也有半年多了，但是发现访问量一直几乎没有，特别是经历几次迁移之后，之前从github到coding，现在迁移到了云服务器，研究了一下如何进行seo和网站性能优化，便有了这篇文章。 实用的功能 站内搜索（百度的） 本地搜索（本地插件） 网站统计 留言功能 rss订阅功能 性能优化 html压缩 css压缩 js压缩· img压缩 nginx代理，开启gzip压缩 cdn代理css和图·片 删除主题无用的js和css seo优化 sitemap 对于没有价值的外链a标签添加rel=&quot;external nofollow&quot; 使用meta标签 使用robots文件 主动提交sitemap到搜索引擎 添加外链和内链","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://zhangfuxin.cn/tags/hexo/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"ubuntu服务器详细配置","slug":"server-config","date":"2016-11-28T12:36:03.000Z","updated":"2019-08-29T02:31:18.936Z","comments":true,"path":"server-config.html","link":"","permalink":"http://zhangfuxin.cn/server-config.html","excerpt":"** ubuntu服务器私人定制：** &lt;Excerpt in index | 首页摘要&gt;把ubuntu服务器打造成自己的个性服务器，装逼必备！！！","text":"** ubuntu服务器私人定制：** &lt;Excerpt in index | 首页摘要&gt;把ubuntu服务器打造成自己的个性服务器，装逼必备！！！ &lt;The rest of contents | 余下全文&gt; ## 说明此教程针对Ubuntu14,其他版本仅作参考 ## 用户密码管理sudo passwd root 添加一个用户组并指定id为1002sudo groupadd －g 1002 www 添加一个用户到www组并指定id为1003sudo useradd wyx -g 1002 -u 1003 -m 修改用户的密码sudo passwd wyx 删除一个用户sudo userdel wyx 为该用户添加sudo权限 12sudo usermod -a -G adm wyxsudo usermod -a -G sudo wyx 查看所有用户和用户组：12cat /etc/passwdcat /etc/group 安装nodejs 安装nvmcurl -o- https://raw.githubusercontent.com/creationix/nvm/v0.32.1/install.sh | bash 安装nodenvm install v4.4.4,安装nvm install v6.9.1 设置默认的node版本nvm alias default v4.4.4 安装npm3 npm install -g npm@3 设置淘宝的cnpm源 npm install -g cnpm --registry=https://registry.npm.taobao.org 验证安装node -v,npm -v,cnpm -v安装node常用包 安装pm2cnpm install -g pm2 安装hexo博客cnpm install -g hexo-cli 安装同步插件rsynccnpm install -g rsync 安装docker apt安装 1234sudo apt-get updatesudo apt-get install -y docker.iosudo ln -sf /usr/bin/docker.io /usr/local/bin/dockersudo sed -i '$acomplete -F _docker docker' /etc/bash_completion.d/docker.io 源码安装最新版本 12345sudo apt-get install apt-transport-httpssudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9sudo bash -c \"echo deb https://get.docker.io/ubuntu docker main &gt; /etc/apt/sources.list.d/docker.list\"sudo apt-get updatesudo apt-get install lxc-docker 验证安装版本docker -v 安装nginxsudo apt-get install nginx启动和配置nginx 安装redissudo apt-get install redis-server启动和配置文件: 安装mongodb 安装3.0 1234apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10echo \"deb http://repo.mongodb.org/apt/debian wheezy/mongodb-org/3.0 main\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.0.listapt-get update apt-get install mongodb-org 安装3.2最新版 1234sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927echo \"deb http://repo.mongodb.org/apt/ubuntu \"$(lsb_release -sc)\"/mongodb-org/3.2 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb.listsudo apt-get updatesudo apt-get install mongodb-org 制定版本apt-get install mongodb-org=3.2.0 mongodb-org-server=3.2.0 mongodb-org-shell=3.2.0 mongodb-org-mongos=3.2.0 mongodb-org-tools=3.2.0 启动服务 12sudo service mongod startsudo service mongod stop 验证安装mongod --version 配置 安装jdk安装jdk1.7sudo apt-get install openjdk-7-jdk源码安装 1234567891011sudo mkdir /usr/lib/jvmsudo tar zxvf jdk-7u21-linux-i586.tar.gz -C /usr/lib/jvmcd /usr/lib/jvmsudo mv jdk1.7.0_21 javasudo vim ~/.bashrcexport JAVA_HOME=/usr/lib/jvm/javaexport JRE_HOME=$&#123;JAVA_HOME&#125;/jre export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 安装mysql实用ubuntu自带的工具下载sudo apt-get install mysql-server 环境变量常见的方法有两种。 在用户主目录下有一个 .bashrc 文件，可以在此文件中加入 PATH 的设置如下：export PATH=”$PATH:/your path1/:/your path2/…..” 在 /etc/profile中增加 12PATH=\"$PATH:/home/zhengb66/bin\" export PATH 开机自启动 方法一，编辑rc.loacl脚本Ubuntu开机之后会执行/etc/rc.local文件中的脚本，所以我们可以直接在/etc/rc.local中添加启动脚本。当然要添加到语句：exit 0 前面才行。代码如下:sudo vi /etc/rc.local然后在 exit 0 前面添加好脚本代码。 方法二，添加一个Ubuntu的开机启动服务。如果要添加为开机启动执行的脚本文件，可先将脚本复制或者软连接到/etc/init.d/目录下，然后用：update-rc.d xxx defaults NN命令(NN为启动顺序)，将脚本添加到初始化执行的队列中去。注意如果脚本需要用到网络，则NN需设置一个比较大的数字，如99。1) 将你的启动脚本复制到 /etc/init.d目录下以下假设你的脚本文件名为 test。2) 设置脚本文件的权限 代码如下:sudo chmod 755 /etc/init.d/test3) 执行如下命令将脚本放到启动脚本中去：代码如下:cd /etc/init.d sudo update-rc.d test defaults 95 注：其中数字95是脚本启动的顺序号，按照自己的需要相应修改即可。在你有多个启动脚本，而它们之间又有先后启动的依赖关系时你就知道这个数字的具体作用了。该命令的输出信息参考如下：卸载启动脚本的方法：代码如下:cd /etc/init.dsudo update-rc.d -f test remove 定时任务在Ubuntu下，cron是被默认安装并启动的。通过查看/etc/crontab推荐使用crontab -e命令添加自定义的任务（编辑的是/var/spool/cron下对应用户的cron文件，在/var/spool/cron下的crontab文件 不可以直接创建或者直接修改，crontab文件是通过crontab命令得到的）。crontab -e 直接执行命令行每2分钟打印一个字符串“Hello World”，保存至文件/home/laigw/cron/HelloWorld.txt中，cron 格式如下：*/2 * * * * echo “Hello World.” &gt;&gt; /home/HelloWorld.txt shell 文件每3分钟调用一次 /home/laigw/cron/test.sh 文件，cron 格式如下：*/3 * * * * /home/laigw/cron/test.sh ftp和rsync配置 持续集成环境 jenkens配置 gitlab配置 git服务器","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://zhangfuxin.cn/tags/linux/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"mac开发环境配置","slug":"mac-dev","date":"2016-11-27T07:52:38.000Z","updated":"2019-08-29T02:58:55.017Z","comments":true,"path":"mac-dev.html","link":"","permalink":"http://zhangfuxin.cn/mac-dev.html","excerpt":"** mac开发环境配置：** &lt;Excerpt in index | 首页摘要&gt;工欲善其事，必先利其器，做好开发者，先搞好开发环境啊。针对mac开发者的开发配置，把mac打造成最具生产力工具！","text":"** mac开发环境配置：** &lt;Excerpt in index | 首页摘要&gt;工欲善其事，必先利其器，做好开发者，先搞好开发环境啊。针对mac开发者的开发配置，把mac打造成最具生产力工具！ &lt;The rest of contents | 余下全文&gt; 软件下载说明下面所提到的软件，有很多需要付费或者破解版，为了方便大家使用，会在网盘分享给大家，只需在评论的地方留下自己的百度云账号！！！ 软件分类说明 通用（开发者必备的软件） java类（java开发者必不可少） 前端类（偏前端和nodejs） python类 数据库类 其他（php，ruby等等） 通用软件 Alfred dash homebrew zsh（oh my zsh） sublime text3, vscode paste(剪切板工具) BetterSnapTool(分屏软件) cornerstone(svn) tower(git) alternote() paw chrome firefox pdf expert CheatSheet snippetslab java软件 jdk idea eclipse maven zookeeper,dubbo tomcat apache 前端必备 nvm(nodejs,npm,cnpm) webpack yo webstorm python必备 pycharm sublime text（插件） 数据库类 mysql mongodb sqllite navicate robomongo redis 其他软件 office keynote,pages,number photoshop 文章长期更新，请收藏","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"mac","slug":"mac","permalink":"http://zhangfuxin.cn/tags/mac/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"atom中最好的js代码补全","slug":"best-js-snippet","date":"2016-07-21T22:11:30.000Z","updated":"2019-08-29T02:21:45.014Z","comments":true,"path":"best-js-snippet.html","link":"","permalink":"http://zhangfuxin.cn/best-js-snippet.html","excerpt":"** atom中最好的js代码补全：** &lt;Excerpt in index | 首页摘要&gt; 这或许是atom中最好的js代码补全,包含了express,nodejs,es6,目前仍在继续更新","text":"** atom中最好的js代码补全：** &lt;Excerpt in index | 首页摘要&gt; 这或许是atom中最好的js代码补全,包含了express,nodejs,es6,目前仍在继续更新 &lt;The rest of contents | 余下全文&gt; best-js-snippets这个package的名字就叫 best-js-snippets ,用atom的可以下载使用一下,提出建议,我会尽快修改 特性 express补全 es6补全 js补全(string,dom操作) nodejs补全(fs,event,util,module,class,assert) 如何安装 atom编辑器中找到设置,搜索package,安装即可. 重启atom,享受吧!","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"常用排序算法学习","slug":"sort-study","date":"2016-07-08T14:39:24.000Z","updated":"2019-08-21T15:13:24.138Z","comments":true,"path":"sort-study.html","link":"","permalink":"http://zhangfuxin.cn/sort-study.html","excerpt":"** 常用排序算法学习：** &lt;Excerpt in index | 首页摘要&gt; 程序员各种排序算法，算法的实现和分析","text":"** 常用排序算法学习：** &lt;Excerpt in index | 首页摘要&gt; 程序员各种排序算法，算法的实现和分析 &lt;The rest of contents | 余下全文&gt; 排序算法的分类 排序分内排序和外排序。 内排序:指在排序期间数据对象全部存放在内存的排序。 外排序:指在排序期间全部对象个数太多,不能同时存放在内存,必须根据排序过程的要求,不断在内、外存之间移动的排序。 内排序的方法有许多种,按所用策略不同,可归纳为五类:插入排序、选择排序、交换排序、归并排序、分配排序和计数排序。 插入排序主要包括直接插入排序，折半插入排序和希尔排序两种; 选择排序主要包括直接选择排序和堆排序; 交换排序主要包括冒泡排序和快速排序; 归并排序主要包括二路归并(常用的归并排序)和自然归并。 分配排序主要包括箱排序和基数排序 冒泡排序 冒泡排序就是把小的元素往前调或者把大的元素往后调。比较是相邻的两个元素比较，交换也发生在这两个元素之间。所以，如果两个元素相等，是不用交换的；如果两个相等的元素没有相邻，那么即使通过前面的两两交换把两个相邻起来，这时候也不会交换，所以相同元素的前后顺序并没有改变，所以冒泡排序是一种稳定排序算法1234567891011121314151617// js代码function sort(arr) &#123;if (arr.length == 0) &#123; return [];&#125;var length = arr.length;for (var i = 0; i &lt; length; i++) &#123; for (var j = 0; j &lt; length - i - 1; j++) &#123; if (arr[j] &gt; arr[j + 1]) &#123; var temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; console.log(arr); &#125; &#125; &#125;&#125; 快速排序 快速排序是对冒泡排序的一种改进。它的基本思想是：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列. 时间复杂度：O（nlgn）最坏：O（n^2）空间复杂度：O（nlgn） 1234567891011121314151617181920212223// js递归实现function quickSort(arr) &#123; if (arr.length == 0) &#123; return []; &#125; var left = []; var right = []; var pivot = arr[0]; for (var i = 1; i &lt; arr.length; i++) &#123; if (arr[i] &lt; pivot) &#123; left.push(arr[i]); &#125; else &#123; right.push(arr[i]); &#125; &#125; return quickSort(left).concat(pivot, quickSort(right));&#125;var a = [];for (var i = 0; i &lt; 10; ++i) &#123; a[i] = Math.floor((Math.random() * 100) + 1);&#125;console.log(a);console.log(quickSort(a)); 直接插入排序 直接插入排序(straight insertion sort)的作法是：每次从无序表中取出第一个元素，把它插入到有序表的合适位置，使有序表仍然有序. 12345678910111213141516171819function insertionSort(arr) &#123; var temp, inner; for (var outer = 1; outer &lt;= arr.length - 1; ++outer) &#123; temp = arr[outer]; inner = outer; while (inner &gt; 0 &amp;&amp; (arr[inner - 1] &gt;= temp)) &#123; arr[inner] = arr[inner - 1]; --inner; &#125; arr[inner] = temp; &#125; return arr;&#125;var a = [];for (var i = 0; i &lt; 10; ++i) &#123; a[i] = Math.floor((Math.random() * 100) + 1);&#125;console.log(a);console.log(insertionSort(a)); 折半插入排序 折半插入排序算法的具体操作为：在将一个新元素插入已排好序的数组的过程中，寻找插入点时，将待插入区域的首元素设置为a[low],末元素设置为 a[high]，则轮比较时将待插入元素与a[m],其中m=(low+high)/2相比较,如果比参考元素小，则选择a[low]到a[m-1]为新 的插入区域(即high=m-1)，否则选择a[m+1]到a[high]为新的插入区域（即low=m+1），如此直至low&lt;=high不成 立，即将此位置之后所有元素后移一位，并将新元素插入a[high+1] 希尔排序 先取一个小于n的整数d1作为第一个增量，把文件的全部记录分成d1个组。所有距离为dl的倍数的记录放在同一个组中。先在各组内进行直接插入 排序；然后，取第二个增量d2&lt;d1重复上述的分组和排序，直至所取的增量dt=1(dt&lt;dt-l&lt;…&lt;d2&lt;d1)， 即所有记录放在同一组中进行直接插入排序为止。 该方法实质上是一种分组插入方法。插入排序（Insertion Sort）的一个重要的特点是，如果原始数据的大部分元素已经排序，那么插入排序的速度很快（因为需要移动的元素很少）。从这个事实我们可以想到，如果原 始数据只有很少元素，那么排序的速度也很快。－－希尔排序就是基于这两点对插入排序作出了改进。 直接选择排序 直接选择排序是给每个位置选择当前元素最小的，比如给第一个位置选择最小的，在剩余元素里面给第二个元素选择第二小的，依次类推，直到第n-1个元素，第n个 元素不用选择了，因为只剩下它一个最大的元素了。那么，在一趟选择，如果当前元素比一个元素小，而该小的元素又出现在一个和当前元素相等的元素后面，那么 交换后稳定性就被破坏了。比较拗口，举个例子，序列5 8 5 2 9，我们知道第一遍选择第1个元素5会和2交换，那么原序列中2个5的相对前后顺序就被破坏了，所以选择排序不是一个稳定的排序算法。时间复杂度是O(n^2) 堆排序 我们知道堆的结构是节点i的孩子为2i和2i+1节点，大顶堆要求父节点大于等于其2个子节点，小顶堆要求父节点小于等于其2个子节点。在一个长为n 的序列，堆排序的过程是从第n/2开始和其子节点共3个值选择最大(大顶堆)或者最小(小顶堆),这3个元素之间的选择当然不会破坏稳定性。但当为n /2-1, n/2-2, …1这些个父节点选择元素时，就会破坏稳定性。有可能第n/2个父节点交换把后面一个元素交换过去了，而第n/2-1个父节点把后面一个相同的元素没 有交换，那么这2个相同的元素之间的稳定性就被破坏了。所以，堆排序不是稳定的排序算法。 二路归并排序 归并排序是把序列递归地分成短序列，递归出口是短序列只有1个元素(认为直接有序)或者2个序列(1次比较和交换),然后把各个有序的段序列合并成一个有 序的长序列，不断合并直到原序列全部排好序。可以发现，在1个或2个元素时，1个元素不会交换，2个元素如果大小相等也没有人故意交换，这不会破坏稳定 性。那么，在短的有序序列合并的过程中，稳定是是否受到破坏？没有，合并过程中我们可以保证如果两个当前元素相等时，我们把处在前面的序列的元素保存在结 果序列的前面，这样就保证了稳定性。所以，归并排序也是稳定的排序算法。","categories":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://zhangfuxin.cn/tags/算法/"}],"keywords":[{"name":"algorithm","slug":"algorithm","permalink":"http://zhangfuxin.cn/categories/algorithm/"}]},{"title":"mysql优化的常用方法","slug":"mysql-optimize","date":"2016-06-10T23:25:13.000Z","updated":"2019-08-29T02:32:33.395Z","comments":true,"path":"mysql-optimize.html","link":"","permalink":"http://zhangfuxin.cn/mysql-optimize.html","excerpt":"** mysql优化：** &lt;Excerpt in index | 首页摘要&gt; mysql的优化措施，从sql优化做起","text":"** mysql优化：** &lt;Excerpt in index | 首页摘要&gt; mysql的优化措施，从sql优化做起 &lt;The rest of contents | 余下全文&gt; 优化sql的一般步骤 通过show status了解各种sql的执行频率 定位执行效率低的sql语句 通过explain分析效率低的sql 通过show profile分析sql 通过trace分析优化器如何选择执行计划 确定问题，采取措施优化 索引优化措施 mysql中使用索引的典型场景 匹配全值，条件所有列都在索引中而且是等值匹配 匹配值的范围查找，字段必须在索引中 匹配最左前缀，复合索引只会根据最左列进行查找 仅仅对索引进行查询，即查询的所有字段都在索引上 匹配列前缀，比如like ‘ABC%’,如果是like ‘%aaa’就不可以 如果列名是索引，使用column is null会使用索引 存在索引但不会使用索引的典型场景 以%开头的like查询不能使用b树索引 数据类型出现隐式转换不能使用索引 复合索引，查询条件不符合最左列原则 用or分割的条件，如果前面的条件有索引，而后面的条件没有索引 查看索引使用的情况 1show status like &apos;Handler_read%&apos;; 如果Handler_read_rnd_next的值比较高，说明索引不正确或者查询没有使用到索引 简单实用的优化方法 定期检查表和分析表分析表语法：1analyze table 表名； 检查表语法： 1check table 表名； 定期优化表 对于字节大小不固定的字段，数据更新和删除会造成磁盘空间不释放，这时候就行优化表，可以整理磁盘碎片，提高性能语法如下：1optimize table user(表名)；","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://zhangfuxin.cn/tags/mysql/"}],"keywords":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}]},{"title":"mac下mysql5.6字符集设置","slug":"mac-mysql-unicode","date":"2016-05-28T15:10:37.000Z","updated":"2019-08-29T02:33:30.478Z","comments":true,"path":"mac-mysql-unicode.html","link":"","permalink":"http://zhangfuxin.cn/mac-mysql-unicode.html","excerpt":"** mac下mysql5.6字符集设置：** &lt;Excerpt in index | 首页摘要&gt; 在mac下设置mysql5.6字符集时踩过的坑，百分百保证有效","text":"** mac下mysql5.6字符集设置：** &lt;Excerpt in index | 首页摘要&gt; 在mac下设置mysql5.6字符集时踩过的坑，百分百保证有效 &lt;The rest of contents | 余下全文&gt; 为什么要设置字符集 设置字符集主要是解决乱码问题，由于中文和英文编码不同导致，中文出现乱码，所以一般都设置为utf8格式 不同的字符集和编码占用的字节不同，选择适合的编码会提高数据库性能 mac下设置 在/etc/my.cnf文件进行设置，如果没有此文件可以从/usr/local/mysql/support-files/拷贝，命令如下12cd /usr/local/mysql/support-filessudo cp my.cnf /etc/my.cnf 查看文件的读写权限，如果为644（rw- r– r–）则改为(664) (rw- rw- r–)如果改为(666)(rw- rw- rw-)则修改以后配置文件不会生效 1sudo chmod 664 /etc/my.cnf my.cnf设置如下：12345678[client]default-character-set=utf8[mysqld]collation-server = utf8_unicode_ciinit-connect=&apos;SET NAMES utf8&apos;character-set-server = utf8[mysql]default-character-set=utf8 查看设置是否成功在命令行输入mysql，如果提示没有命令的话，在bash或者zsh的文件里修改，我用的是zsh，设置~/.zshrc, 12export MYSQL=&quot;/usr/local/mysql/bin/&quot;export PATH=&quot;/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:$MYSQL&quot; 在命令行输入mysql,进入mysql命令行后，输入status;或者show variables like &#39;%char%&#39;; 12345678| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/local/mysql-5.6.30-osx10.11-x86_64/share/charsets/","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://zhangfuxin.cn/tags/mysql/"}],"keywords":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}]},{"title":"mysql学习笔记","slug":"mysql-study","date":"2016-05-28T14:24:56.000Z","updated":"2019-08-21T15:13:24.133Z","comments":true,"path":"mysql-study.html","link":"","permalink":"http://zhangfuxin.cn/mysql-study.html","excerpt":"** mysql学习笔记：** &lt;Excerpt in index | 首页摘要&gt; mysql学习，基础的增删改查，数据库优化，索引，分片，集群搭建等等。","text":"** mysql学习笔记：** &lt;Excerpt in index | 首页摘要&gt; mysql学习，基础的增删改查，数据库优化，索引，分片，集群搭建等等。 &lt;The rest of contents | 余下全文&gt; mysql的特点 关系型数据库，免费使用， 插入式存储引擎， 性能高， 基础的增删改查 ddl语句，数据定义语句 123456789101112create database test1;drop database test1;use test1;create table emp(ename varchar(10),hiredate date,sal decimal(10,2),deptno int(2));drop table emp;alter table emp modify ename varchar(20);alter table emp add column age int(3);alter table emp drop column age;alter table emp change age age1 int(4);alter table emp add birth date after ename;alter table emp modify age int(3) first;alter table emp rename emp1; dml语句，数据操纵语句 123456789101112insert into emp(ename,hiredate,sal,deptno) values(&apos;zzx1&apos;,&apos;2000-10-11&apos;,2000,1);insert into emp values(&apos;lisa&apos;,&apos;2004-05-09&apos;,3000,2);insert into dept values(5,&apos;dept5&apos;),(6,&apos;dept6&apos;);update emp set sal=4000 where ename=&apos;lisa&apos;;update emp a,dept b set a.sal=a.sal*b.deptno,b.deptname=a.ename where a.deptno=b.deptno;delete from emp where ename=&apos;dony&apos;;delete a,b from emp a,dept b where a.deptno=b.deptno and a.deptno=3;select * from emp where ename=&apos;lisa&apos;;select distinct deptno from emp;select * from emp order by sal(desc);select * from emp order by sal limit 5;select * from emp order by sal limit 1,5;ss dcl语句，数据控制语句 sql优化 尽量使用 prepareStatement(java)，利用预处理功能。 在进行多条记录的增加、修改、删除时，建议使用批处理功能，批处理的次数以整个 SQL 语句不超过相应数据库的 SQL 语句大小的限制为准。 建议每条 SQL 语句中 in 中的元素个数在 200 以下，如果个数超过时，应拆分为多条 SQL 语句。禁止使用 xx in(‘’,’’….) or xx in(‘’,’’,’’)。 ★ 禁止使用 or 超过 200，如 xx =’123’ or xx=’456’。 ★ 尽量不使用外连接。 禁止使用 not in 语句，建议用 not exist。 ★ 禁止使用 Union, 如果有业务需要，请拆分为两个查询。 ★ 禁止在一条 SQL 语句中使用 3 层以上的嵌套查询，如果有，请考虑使用临时表或中间结果集。 尽量避免在一条 SQL 语句中从&gt;= 4 个表中同时取数， 对于仅是作为过滤条件关联，但不涉及取数的表，不参与表个数计算 查询条件里任何对列的操作都将导致表扫描，所以应尽量将数据库函数、计算表达式写在逻辑操作符右边。 在对 char 类型比较时,建议不要使用 rtrim()函数,应该在程序中将不足的长度补齐。 用多表连接代替 EXISTS 子句。 如果有多表连接时， 应该有主从之分， 并尽量从一个表取数， 如 select a.col1, a.col2from a join b on a.col3=b.col4 where b.col5 = ‘a’。 在使用 Like 时，建议 Like 的一边是字符串，表列在一边出现。 不允许将 where 子句的条件放到 having 中。 将更新操作放到事务的最后执行。如 一个事务需更新多个对象时，需保证更新的顺序一致以避免死锁的发生。如总是先更新子表再更新主表，根据存货档案批量更新现存量时，对传入的存货档案 PK 进行排序，再做更新处理等。 禁止随意使用临时表，在临时数据不超过 200 行的情况下禁止使用临时表。 禁止随意使用 distinct，避免造成不必要的排序。 索引优化 创建索引，删除索引 12create index cityname on city(city(10));drop index cityname on city; 搜索的索引列最好在where的字句或者连接子句 使用唯一索引 使用短索引，对于较长的字段，使用其前缀做索引 不要过度使用索引，索引引起额外的性能开销和维护 高级优化措施集群搭建","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://zhangfuxin.cn/tags/mysql/"}],"keywords":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}]},{"title":"nodejs开发规范","slug":"node-develop","date":"2016-05-23T06:18:02.000Z","updated":"2019-08-21T15:13:24.134Z","comments":true,"path":"node-develop.html","link":"","permalink":"http://zhangfuxin.cn/node-develop.html","excerpt":"** nodejs开发规范：** &lt;Excerpt in index | 首页摘要&gt; nodejs开发中应当遵循的规范，以及最佳实践","text":"** nodejs开发规范：** &lt;Excerpt in index | 首页摘要&gt; nodejs开发中应当遵循的规范，以及最佳实践 &lt;The rest of contents | 余下全文&gt; node开发需要编程规范吗？ js的灵活性非常大，如果开发人员每个人都按自己的习惯随意编写，js的代码会非常混乱不堪。js程序员需要更强的自律性和规范，才能写出易读性，易维护的代码。 随着前端mvc的崛起，前端的js代码会更加庞大难以管理，如果没有统一的规范，后期维护会比登天还难。 编码规范 缩进采用两个空格缩进，在编辑器中设置tab为两个空格 变量声明 用var声明变量var assert = require(‘assert’);var fork = require(‘child_process’).fork;var net = require(‘net’); 错误实例：var assert = require(‘assert’), fork = require(‘child_process’).fork, net = require(‘net’)； 用字面量声明方式var num = 123;var aaa = {};var arr = [];var isAdmin = true; 避免使用：var obj =new Object();var arr = new Array();var test =new String(“”);var size = new Number(); 不要在for循环等循环里声明var变量首先var是函数作用域，在循环声明以后只有等函数声明周期结束这些资源才会释放 空格在操作符前后需要加上空格,= 、% 、* 、- 、+ 前后都应该加一个空格比如：var foo = ‘bar’ + baz;错误实例：var foo=’bar’+baz; 单双引号的使用在node中尽量使用单引号，var html = ‘CNode‘;在json中使用双引号 分号给表达式结尾加分号，尽管js会自动在行尾加上分号，但是会产生一些误解 命名规范在编码中，命名是重头戏。好的命名可以使代码赏心悦目，具有良好的维护性。 变量命名变量名采用小驼峰命名，单词之间没有任何符号如：var adminUser = {};var callNum = 2134323; 方法命名也是采用小驼峰命名，与变量不同的是采用动词或判断行词汇，如：var getUser = function(){};var isAdmin = function(){};var findUser = function(){}; 类命名类名采用大驼峰，所有单词首字母大写，如：function User{} 常量命名作为常量，单词所有字母大写，用下划线分割，如：var PINK_COLOR = “PINK”; 文件命名命名文件时，尽量使用下划线分割单词，比如child_process.js和string_decode.js 包名在包名中尽量不要包含js和node的字样，应当适当短并且有意义 其它要点 作用域慎用with和eval（），容易引起作用域混乱 比较操作尽量使用===代替==,否则会遇到下面的情况，’0’==0;//true;‘’==0;//true;‘0’===’’//false; 严格模式在node后台中尽量全使用严格模式‘use strict’; 对象和数组遍历数组遍历使用普通for循环，避免使用for in对数组遍历，对象的遍历使用for in 项目中实践 sublime和webstorm都有JSLint,JSHint这样的代码质量工具，在配置文件中制定好模板规范即可 在版本控制工具中设置hook，在precommit的脚本中设置，如果代码不符合标准，就无法提交 参考文献 深入浅出nodejs js秘密花园 js高级编程","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}],"tags":[{"name":"node","slug":"node","permalink":"http://zhangfuxin.cn/tags/node/"}],"keywords":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}]},{"title":"redis学习笔记","slug":"redis-study","date":"2016-05-23T00:25:57.000Z","updated":"2019-08-21T15:13:24.136Z","comments":true,"path":"redis-study.html","link":"","permalink":"http://zhangfuxin.cn/redis-study.html","excerpt":"** redis学习笔记：** &lt;Excerpt in index | 首页摘要&gt; redis数据库的基本操作，增删改查","text":"** redis学习笔记：** &lt;Excerpt in index | 首页摘要&gt; redis数据库的基本操作，增删改查 &lt;The rest of contents | 余下全文&gt; keysredis本质上是一个key-value数据库 设置：set key value 获取：get key 判断存在：exists key 删除：del key del test:fan:age 重命名：rename oldkey newkey 数量：dbsize 返回数据 获取所有key（通配符）：Keys test:*:ageKeys test:?:age 清空：flushdb flushall 设置有效时间：expire test:fan:age 30 查询有效时间：ttl test:fan:age String类型 设置： set key value setnx ky value(nx是not exist) mset key1 value1 keyN valueN msetnx key1 value1 keyN valueN 获取： get 不存在返回nil getset 设置key的值，并返回key的旧值，不存在返回nil mget 自增减： incr key 对key的值进行++操作，返回新的值 decr key incrby key integer 对key加上一个数值 decrby key integer 截取： substr key indexStart indexEnd 下标从0开始 追加： append key value list类型redis的list其实就是一个每个元素都是string 的双向链表，所以push和pop的时间复杂度都是O（1） 添加 lpush key string 在头部添加 rpush key string 在尾部添加 修改 lset key index value 修改指定下标的key的值 删除 lpop key 从头部返回删除 rpop key 从尾部 lrem key count value 删除count个相同的value，count为0删除全部 blpop key …keyN timeout brpop 从尾部删除 获取 lrange key indexStart indexEnd 数量 llen key 返回key对应的list长度 截取 ltrim key start end 转移 rpoplpush key1 key2 从key1尾部移到key2头部 set集合redis的set就是String的无序集合，通过hashtable实现 添加 sadd key member 删除 srem key member 移除指定的元素 spop key 删除并返回一个随机的 获取 smembers key 返回所有 srandmember 随机取一个不删除 判断存在 sismember key member 数量 scard key 返回元素个数 转移 smove srckey dstkey member 取交集 sinter key1 key2 keyN sinterstore dstkey key1 keyN 将交集存在dstkey 取并集 sunion key1 key2 keyN sunionstore dstkey key1 keyN 将并集存在dstkey 取差集 sdiff key1 key2 keyN sdiffstore dstkey key1 keyN 将差集存在dstkey 有序set类型和set一样，不同的是每个元素关联一个double类型的score，根据score排序，sorted set的实现由skip list和hashtable 添加 zadd key score member 删除 zrem key member zremrangebyrank key min max zremrangebyscore key min max 删除集合score在给定区间的元素 获取 zrange key start end zrevrange key start end 按score的逆序 zrangebyscore key min max 判断存在 zrank key member 返回下标 zrerank key member 返回逆序的下标 数量 zcard key 总数 zcount key min max 区间的数量 修改 zincrby key incr member 增加member的score值并排序 hash类型redis的hash是一个string类型的field和value的映射表，hash特别适合存储对象， 设置： hset key field value hmset key field1 value1 field2 value2 获取： hget key field hmget key field1 field2 判断存在 hexists key field 删除 hdel key field 查找 hkeys key 返回所有 field hvals key 返回所有的value hgetall key 返回所有field和value 数量 hlen key 值加减 hincrby key field integer 将指定的hash field加上定值","categories":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://zhangfuxin.cn/tags/redis/"}],"keywords":[{"name":"数据库","slug":"数据库","permalink":"http://zhangfuxin.cn/categories/数据库/"}]},{"title":"git比svn的优势","slug":"git-svn","date":"2016-05-22T03:13:00.000Z","updated":"2019-08-21T15:13:24.126Z","comments":true,"path":"git-svn.html","link":"","permalink":"http://zhangfuxin.cn/git-svn.html","excerpt":"** git比svn的优势：** &lt;Excerpt in index | 首页摘要&gt; 主要介绍svn和git在使用的时候一些区别","text":"** git比svn的优势：** &lt;Excerpt in index | 首页摘要&gt; 主要介绍svn和git在使用的时候一些区别 &lt;The rest of contents | 余下全文&gt; 合并操作时对提交过程的保留 git:合并操作保留原有的提交过程 svn:多个提交合并为一个提交 不用因为合并操作而导致追踪的困难 修正提交 git：可以修正提交。使用功能分支工作流，在自己的分支可以方便修正提交而不会影响大家。 svn：一旦提交就到服务器上，实际使用中就是不能修改（svn可以在服务器上修改，因为过程复杂需要权限实际上从不会这样做） 本地分支 git可以方便的创建本地分支,创建时间极短,分支可以是本地的,不会存在svn中目录权限的问题 强大的合并能力 git：重命名（无论文件还有目录）提交 可以合并上 文件重命名前的这些文件的提交 svn：重命名（无论文件还有目录）提交后，你本地/或是分支上 有文件重命名前的这些文件的修改或提交，在做合并操作时,你会碰上传说中难搞的树冲突！ 这就导致在调整目录名称和类名调整的时候比较繁琐,需要告诉大家,我修改完以后你再修改 tag的支持 svn在模型上是没有分支和tag的。tag是通过目录权限限制（对开发只读）来保证不变。 git模型上一等公民支持tag，保证只读。 速度优势 git的提交是个本地提交,相对svn来说如闪电一般 git提供了暂存区,可以方便制定提交内容,而不是全部内容 日志查看 git：本地包含了完整的日志，闪电的速度（并且无需网络) svn：需要从服务拉取。 一旦用了git后，等待svn日志过程简直让我发狂","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"java和javascript日期详解","slug":"java-date","date":"2016-05-13T13:48:00.000Z","updated":"2019-08-21T15:13:24.128Z","comments":true,"path":"java-date.html","link":"","permalink":"http://zhangfuxin.cn/java-date.html","excerpt":"** java，js日期转换：** &lt;Excerpt in index | 首页摘要&gt; java的各种日期转换","text":"** java，js日期转换：** &lt;Excerpt in index | 首页摘要&gt; java的各种日期转换 &lt;The rest of contents | 余下全文&gt; 日期表示类型 获取long类型的日期格式 1234long time = System.currentTimeMillis();System.out.printf(time+\"\");Date date =new Date();System.out.println(date.getTime()); 获取制定格式的日期 123SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd hh:mm:ss\");Date date =new Date();System.out.println(sdf.format(date) ); 把制定格式的日期转为date或者毫秒值 123SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd hh:mm:ss\");Date date = sdf.parse(\"2016-05-22 10:15:21\");long mills = date.getTime(); 说明:System.currentTimeMillis()并不能精确到1ms的级别,它取决于运行的系统,你再windows,mac,linux精确的范围都有差异,对于有高精度时间的要求,不能使用这个 日期计算 最方便的方式是将时间转为毫秒值进行计算1234Date from =new Date();Thread.sleep(200);//线程休眠2msDate to =new Date();System.out.println(to.getTime()-from.getTime()); 高精度时间12long time1 =System.nanoTime();System.out.printf(time1+\"\"); 说明:System.nanoTime()提高了ns级别的精度,1ms=1000000ns, javascript日期 获取时间的毫秒值，获取月份，时间 1234567891011121314var myDate = new Date();myDate.getYear(); //获取当前年份(2位)myDate.getFullYear(); //获取完整的年份(4位,1970-????)myDate.getMonth(); //获取当前月份(0-11,0代表1月)myDate.getDate(); //获取当前日(1-31)myDate.getDay(); //获取当前星期X(0-6,0代表星期天)myDate.getTime(); //获取当前时间(从1970.1.1开始的毫秒数)myDate.getHours(); //获取当前小时数(0-23)myDate.getMinutes(); //获取当前分钟数(0-59)myDate.getSeconds(); //获取当前秒数(0-59)myDate.getMilliseconds(); //获取当前毫秒数(0-999)myDate.toLocaleDateString(); //获取当前日期var mytime=myDate.toLocaleTimeString(); //获取当前时间myDate.toLocaleString( ); //获取日期与时间 时间戳获取注意，java，php等生成的时间戳是秒，不是毫秒，所以需要签名时间戳的时候，需要转为秒时间戳 12var time = new Date();var timestamp = parseInt(time.getTime()/1000); 格式化时间 12345678910111213141516//获取当前时间，格式YYYY-MM-DDfunction getNowFormatDate() &#123; var date = new Date(); var seperator1 = \"-\"; var year = date.getFullYear(); var month = date.getMonth() + 1; var strDate = date.getDate(); if (month &gt;= 1 &amp;&amp; month &lt;= 9) &#123; month = \"0\" + month; &#125; if (strDate &gt;= 0 &amp;&amp; strDate &lt;= 9) &#123; strDate = \"0\" + strDate; &#125; var currentdate = year + seperator1 + month + seperator1 + strDate; return currentdate;&#125;","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}],"tags":[{"name":"java","slug":"java","permalink":"http://zhangfuxin.cn/tags/java/"}],"keywords":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}]},{"title":"制定学习目标和计划","slug":"study-goals","date":"2016-05-06T02:20:45.000Z","updated":"2019-08-21T15:13:24.140Z","comments":true,"path":"study-goals.html","link":"","permalink":"http://zhangfuxin.cn/study-goals.html","excerpt":"** 制定学习目标和计划：** &lt;Excerpt in index | 首页摘要&gt; 近期的学习目标和学习重点,提高自己的能力","text":"** 制定学习目标和计划：** &lt;Excerpt in index | 首页摘要&gt; 近期的学习目标和学习重点,提高自己的能力 &lt;The rest of contents | 余下全文&gt; 找到自己的兴趣 自己主动学习一定要基于自己的兴趣,不要看什么框架流行,什么语言火,就去学,学的不温不火,然后放弃. 一定看自己的兴趣,比如你对色彩,对布局,对特效比较痴迷,那你去css3,html5做出特酷的效果,肯定能让你肯定自己,收获知识和自信. 没有兴趣的时候,可以适当的多接触一些东西,在最短的时间多接触一些领域,让自己的心去做选择, 制定目标 为什么要制定目标? 制定目标是对自己学习能力的检验,同时也是提高学习效率的关键,而不是自己没有目的的瞎看, 如何制定目标? 结合自身的能力,定制比自己能力稍高的目标,这样自己通过一定程度的努力可以实现目标.这样自己的能力能一次一次提高. 及时反馈 古人说的好,吾日三省吾身,对待学习目标也是一样,要时不时的看自己的目标完成的如何,进度如何,是不是需要调整,不能闷着头蛮干,方向错了,再多的努力也是白搭了. 总结 我在刚开始学编程的时候,每天都给自己定制了目标,一天完成多少课时,完成多少练习,都是按量完成,在最初的几个月收到了立竿见影的效果,让我也在短短三个月的时间学会了java,所以,目标的制定对于结果的影响是非常大.","categories":[{"name":"个人随笔","slug":"个人随笔","permalink":"http://zhangfuxin.cn/categories/个人随笔/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"个人随笔","slug":"个人随笔","permalink":"http://zhangfuxin.cn/categories/个人随笔/"}]},{"title":"使用ghost搭建个人博客","slug":"ghost-blog","date":"2016-05-03T23:59:22.000Z","updated":"2019-08-21T15:13:24.125Z","comments":true,"path":"ghost-blog.html","link":"","permalink":"http://zhangfuxin.cn/ghost-blog.html","excerpt":"** 使用ghost搭建个人博客：** &lt;Excerpt in index | 首页摘要&gt; 使用ghost搭建个人博客","text":"** 使用ghost搭建个人博客：** &lt;Excerpt in index | 首页摘要&gt; 使用ghost搭建个人博客 &lt;The rest of contents | 余下全文&gt; ghost简介 ghost是轻量级的博客建站工具,使用起来简单,功能强大,适合个人搭建小型网站,个人博客,或者个人展示的网站 ghost基于nodejs,对于熟悉js的前端小伙伴来说,入手起来也是简单不少. 准备工作 安装nodejs 安转git 配置ssh 下载ghost 购买域名 搭建博客定制个人博客享受吧","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"git学习笔记","slug":"git-config-study","date":"2016-05-01T00:24:45.000Z","updated":"2019-08-21T15:13:24.125Z","comments":true,"path":"git-config-study.html","link":"","permalink":"http://zhangfuxin.cn/git-config-study.html","excerpt":"** git学习笔记：** &lt;Excerpt in index | 首页摘要&gt; git的常用操作，高级技巧都要哦","text":"** git学习笔记：** &lt;Excerpt in index | 首页摘要&gt; git的常用操作，高级技巧都要哦 &lt;The rest of contents | 余下全文&gt; 安装git 下载安装包 ￼下载地址￼ 安装git 进入命令行,输入git看看是否成功 配置git 配置全局用户名和密码 `git config –global user.name “John Doe” git config –global user.email johndoe@example.com ` 配置ssh公钥 cd ~/.ssh 然后ls 如果没有,直接生成,一路点击enter ``` ssh-keygen cat ~/.ssh/id_rsa.pub ``` 把公钥配置到github的个人设置 常用的命令 repository操作 检出（clone）仓库代码：git clone repository-url / git clone repository-url local-directoryname 例如，clone jquery 仓库到本地： git clone git://github.com/jquery/jquery.git clone jquery 仓库到本地，并且重命名为 my-jquery ：git clone git://github.com/jquery/jquery.git my-jquery 查看远程仓库：git remote -v 添加远程仓库：git remote add [name] [repository-url] 删除远程仓库：git remote rm [name] 修改远程仓库地址：git remote set-url origin new-repository-url 拉取远程仓库： git pull [remoteName] [localBranchName] 推送远程仓库： git push [remoteName] [localBranchName] 提交/拉取/合并/删除 添加文件到暂存区（staged）：git add filename / git stage filename 将所有修改文件添加到暂存区（staged）： git add --all / git add -A 提交修改到暂存区（staged）：git commit -m &#39;commit message&#39; / git commit -a -m &#39;commit message&#39; 注意理解 -a 参数的意义 从Git仓库中删除文件：git rm filename 从Git仓库中删除文件，但本地文件保留：git rm --cached filename 重命名某个文件：git mv filename newfilename 或者直接修改完毕文件名 ，进行git add -A &amp;&amp; git commit -m &#39;commit message&#39; Git会自动识别是重命名了文件 获取远程最新代码到本地：git pull (origin branchname) 可以指定分支名，也可以忽略。pull 命令自动 fetch 远程代码并且 merge，如果有冲突，会显示在状态栏，需要手动处理。更推荐使用：git fetch 之后 git merge --no-ff origin branchname 拉取最新的代码到本地仓库，并手动 merge 。 日志查看 查看日志：git log 查看日志，并查看每次的修改内容：git log -p 查看日志，并查看每次文件的简单修改状态：git log --stat 一行显示日志：git log --pretty=oneline / git log --pretty=&#39;format:&quot;%h - %an, %ar : %s&#39; 查看日志范围： 查看最近10条日志：git log -10 查看2周前：git log --until=2week 或者指定2周的明确日期，比如：git log --until=2015-08-12 查看最近2周内：git log --since=2week 或者指定2周明确日志，比如：git log --since=2015-08-12 只查看某个用户的提交：git log --committer=user.name / git log --author=user.name 取消操作 上次提交msg错误/有未提交的文件应该同上一次一起提交，需要重新提交备注：git commit --amend -m &#39;new msg&#39; 一次git add -A后，需要将某个文件撤回到工作区，即：某个文件不应该在本次commit中：git reset HEAD filename 撤销某些文件的修改内容：git checkout -- filename 注意：一旦执行，所有的改动都没有了，谨慎！谨慎！谨慎！ 将工作区内容回退到远端的某个版本：git reset --hard &lt;sha1-of-commit&gt; --hard：reset stage and working directory , 以来所有的变更全部丢弃，并将 HEAD 指向 --soft：nothing changed to stage and working directory ,仅仅将HEAD指向 ，所有变更显示在”changed to be committed”中 --mixed：default,reset stage ,nothing to working directory ，这也就是第二个例子的原因 比较差异 查看工作区（working directory）和暂存区（staged）之间差异：git diff 查看工作区（working directory）与当前仓库版本（repository）HEAD版本差异：git diff HEAD 查看暂存区（staged）与当前仓库版本（repository）差异：git diff --cached / git diff --staged 合并操作 解决冲突后/获取远程最新代码后合并代码：git merge branchname 保留该存在版本合并log：git merge --no-ff branchname 参数--no-ff防止 fast-forward 的提交","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://zhangfuxin.cn/categories/学习笔记/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://zhangfuxin.cn/categories/学习笔记/"}]},{"title":"ubuntu16服务器环境配置","slug":"ubuntu-dev-config","date":"2016-04-26T10:48:11.000Z","updated":"2019-08-21T15:13:24.147Z","comments":true,"path":"ubuntu-dev-config.html","link":"","permalink":"http://zhangfuxin.cn/ubuntu-dev-config.html","excerpt":"** ubuntu开发环境配置：** &lt;Excerpt in index | 首页摘要&gt; ubuntu16下node,java开发环境配置","text":"** ubuntu开发环境配置：** &lt;Excerpt in index | 首页摘要&gt; ubuntu16下node,java开发环境配置 &lt;The rest of contents | 余下全文&gt; ubuntu14升级到ubuntu16 终端下执行命令sudo apt-get update &amp;&amp; sudo apt-get dist-upgrade 重启系统以完成更新的安装sudo init 6 用命令安装更新管理器核心update-manager-core，如果服务器已安装则可以跳过sudo apt-get install update-manager-core 编辑/etc/update-manager/release-upgrades配置文件，设置Prompt=ltssudo vi /etc/update-manager/release-upgrades 启动升级进程sudo do-release-upgrade -d 安装系统软件 更新系统和软件 12sudo apt-get updatesudo apt-get upgade 谷歌浏览器，火狐浏览器，atom编辑器，sublime编辑器，webstome,idea,eclipse 安装搜狗输入法（官网），安装fcitx配置搜狗输入法 安装jdk 下载jdk并新建一个文件夹 1sudo mkdir /usr/lib/jvm 解压文件 1sudo tar zxvf jdk-7u71-linux-x64.tar.gz -C /usr/lib/jvm/jdk1.7 设置环境变量,设置~/.zshrc文件,或者编辑/etc/profile（全局）文件 1234export JAVA_HOME=/usr/lib/jvm/jdk1.7export JRE_HOME=$&#123;JAVA_HOME&#125;/jre export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 检查是否安装成功 打开shell, 1java --version 安装nodejs nodejs版本迭代较快，有时候需要检查在不同版本下的兼容性问题，用nvm来控制版本 安装nvm,source的时候根据自己的shell版本，~/.bashrc, ~/.profile, 或者 ~/.zshrc 1234curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.31.0/install.sh | bashexport NVM_DIR=&quot;$HOME/.nvm&quot;[ -s &quot;$NVM_DIR/nvm.sh&quot; ] &amp;&amp; . &quot;$NVM_DIR/nvm.sh&quot; # This loads nvmsource ~/.profile 安装不同版本的nodejs 12345nvm ls-remotenvm install v0.12.9nvm install 5.0nvm use 0.12.9nvm alias default 0.12.9 安装mongodb 配置公钥 12sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10echo \"deb http://repo.mongodb.org/apt/ubuntu \"$(lsb_release -sc)\"/mongodb-org/3.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.0.list 更新软件列表 12sudo apt-get updatesudo apt-get install -y mongodb-org 完成上面的安装步骤配置mongodb的数据库的位置 1sudo mongod --dbpath /data/db 启动mongod 123sudo service mongod startsudo service mongod stopsudo service mongod restart 安装redis 下载软件 1wget http://download.redis.io/releases/redis-2.8.11.tar.gz 解压安装 12tar xvfz redis-2.8.11.tar.gzcd redis-2.8.11 &amp;&amp; sudo make &amp;&amp; sudo make install 配置使用 下载配置文件和init启动脚本 12345wget https://github.com/ijonas/dotfiles/raw/master/etc/init.d/redis-serverwget https://github.com/ijonas/dotfiles/raw/master/etc/redis.confsudo mv redis-server /etc/init.d/redis-serversudo chmod +x /etc/init.d/redis-serversudo mv redis.conf /etc/redis.conf 初始化用户和日志路径 12345sudo useradd redissudo mkdir -p /var/lib/redissudo mkdir -p /var/log/redissudo chown redis.redis /var/lib/redissudo chown redis.redis /var/log/redis 设置开机自动启动，关机自动关闭 1sudo update-rc.d redis-server defaults 环境变量配置 认识环境变量相关的文件 /etc/profile —— 此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行.并从/etc/profile.d目录的配置文件中搜集shell的设置； /etc/environment —— 在登录时操作系统使用的第二个文件,系统在读取你自己的profile前,设置环境文件的环境变量； /etc/bashrc —— 为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取； ~/.profile —— 每个用户都可使用该文件输入专用于自己使用的shell信息，当用户登录时，该文件仅仅执行一次！默认情况下,它设置一些环境变量,执行用户的.bashrc文件； ~/.bashrc —— 该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该文件被读取； 配置环境变量 在Ubuntu14.04的~/.bashrc中添加的环境变量,在文件添加 1export PATH=$PATH:/home/qtcreator-2.6.1/bin 修改profile文件,vim编辑/etc/profile 12sudo vim /etc/profilesource /etc/profile 安装开发工具 zsh命令行工具 mysql客户端workbench，mongo客户端工具robomongo 安装git,svn版本控制工具12sudo apt-get install gitsudo apt-get install subversion","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://zhangfuxin.cn/tags/linux/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"我的梦想","slug":"dream","date":"2016-04-24T14:07:27.000Z","updated":"2019-08-21T15:13:24.122Z","comments":true,"path":"dream.html","link":"","permalink":"http://zhangfuxin.cn/dream.html","excerpt":"** 我的梦想：** &lt;Excerpt in index | 首页摘要&gt; 一个人如果活着没有梦想,那和咸鱼有什么区别?","text":"** 我的梦想：** &lt;Excerpt in index | 首页摘要&gt; 一个人如果活着没有梦想,那和咸鱼有什么区别? 请问你的梦想是什么? &lt;The rest of contents | 余下全文&gt; 我的梦想是什么? 刚开始接触编程的时候,感觉代码是个神器的世界,在这里你可以为所欲为,然后看到很多大神的框架,软件,在使用别人好的框架,好的软件,那一刻我感觉 “我的梦想就是用代码改变世界!” 感觉自己迷失了好久,找不到方向,曾经的激情不知道去了哪里? 开始追梦 有了梦想,我开始了疯狂的奋斗,每天休息4,5个小时,全身心去学习编程,努力还是很快得到了回报,我用了3个月就入门学好了java,然后找了java程序员的工作,就这样开始了我程序员的追梦之旅! 初级程序员 虽然入门了,但是刚开始的工作并不是一帆风顺的.我还记得第一份任务,老大让我写一个稍微复杂的接口,客户专用的接口,使用springmvc,还要提交到git上,对我而言,这一切都是新东西,经过我几天的努力,还是搞砸了,就这样第一个任务以失败告终! 虽然第一个任务失败了,但是工作还在继续,我还是继续努力的工作,我必须承认我不是编程的天才,可能别人一个小时完成的任务,我需要一个半小时,但是我必须做好,因为我有梦想! 中级程序员 在工作的时候就感觉时间飞逝,一天天很快过去.晚上睡觉的时候,我就会问自己,我今天到底做了什么功能?我收获了哪些技能?曾经有段时间每天都是该页面,我几乎烦的崩溃,感觉每天都在做无用的东西,后来发现,无论是前段后端,其实都是必不可少的技能,我的心态应该调整,让自己去喜欢前段,同时保持后端的热情. 一个成熟的程序员和菜鸟最大的区别应该是心态! 高级程序员 不再是代码搬运工，根据业务和需求自己随便造个轮子什么的。强大的代码能力，考虑事情应该全面，深刻 架构师 未完待续","categories":[{"name":"个人随笔","slug":"个人随笔","permalink":"http://zhangfuxin.cn/categories/个人随笔/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"个人随笔","slug":"个人随笔","permalink":"http://zhangfuxin.cn/categories/个人随笔/"}]},{"title":"程序员入门指南","slug":"coder-study","date":"2016-04-17T04:37:19.000Z","updated":"2019-08-21T15:13:24.120Z","comments":true,"path":"coder-study.html","link":"","permalink":"http://zhangfuxin.cn/coder-study.html","excerpt":"** 程序员入门指南 ：** &lt;Excerpt in index | 首页摘要&gt; 程序员入门必须了解的一些知识，个人经验，不喜勿喷！","text":"** 程序员入门指南 ：** &lt;Excerpt in index | 首页摘要&gt; 程序员入门必须了解的一些知识，个人经验，不喜勿喷！ &lt;The rest of contents | 余下全文&gt; 程序员的入门规划1.我该学习什么语言？ 这个问题困扰了几乎所有的程序员，比如java应用广好就业，比如php入门简单，ios待遇高， python是万能语言，HTML和js前端缺人才等等 个人见解：先学习难度小，大众化的编程语言，比如java，php，python，javascript,c/c++,这几个学哪一种其实差不多，入门以后看自己兴趣在进行其它语言的学习。 2.我该怎么学习编程？这个问题是所有的程序员都有的，我也经常会疑问，到底该怎么学习呢？ 个人见解： 先了解语言的特性，适用的范围场景，比如是适合web开发，还是适合客户端程序，有的适合并发多线程，有的适合异步，还有的比较稳定，适合构建大型项目，有的开发效率高，等等。 了解语言的语法和常用api的使用，比如变量的声明，循环的使用，io的读取，http服务的创建，把这些基本的语法搞清楚，在进行下一步的学习。 学习web开发之前的准备，数据库的学习，http协议的学习，html，css和javacript的常用知识了解 学习常用框架，比如java学习常用的ssh三大框架，node的学习express，一定要做2个项目练习，把自己的之前学习的知识都巩固一下， 总结一下自己学习的过程，明白编程的思想在哪里，思路在哪里，学习编程，首先应该培养的是编程的思维和思想，有个正确的思维后面都简单多了。 养成写博客或者学习笔记的习惯，推荐写博客， 熟悉项目管理工具，svn，git之类的必须要会，工作中这些都是必须的 准备面试，通过面试题进一步巩固自己的知识，夯实基础。 3.我应该去哪里学习编程？其实这个看个人，如果自学能力强，自控能力强，自学挺好的，下面我列举几个程序员常用的网站 网易云课堂，很多免费的视频课程，适合入门学习 慕课网，很多it入门教学视频，资源也不错 极客学院，和前两个网站差不多， 北风网，类似的教学网站，其它的就不说了 4.编程遇到问题怎么办？ 百度或者谷歌看看网上有没有类似的问题，一回生，二回熟，很快就明白了 去官网查看api文档查找原因 自己要学会debug代码，查找原因 去各大论坛逛逛，说不定早有人提问此类问题了 5.我想看编程的书籍去哪找呢？经典书籍还是买纸质的，买正版的，支持正版！ 新浪微盘，非常多的it书籍 脚本之家，非常多的pdf书籍，可惜大多数不是文字版pdf 英文原版书籍，都是高清文字版pdf，强烈推荐，都是英文原版的 计算机书控，都是免费的pdf文档，大多数不是文字版pdf 6.代码资源 最好的代码仓库 github csdn代码仓库 gist 代码片段之家 7.学习心态 不要老是折腾工具，ide工具和文本编辑器一样一个就够了 不要自满，编程的东西学一辈子也学不会，要谦虚好学 不要急躁，既然知识学不完，我们应该掌握学习方法，指定计划去学习 要持之以恒，学习是一辈子的事，如果你没有这个打算，还是不要做程序员的好 切忌眼高手低，必须要敲代码才能达到效果 8.编程进阶之路当有了一定的编程基础之后,最大的问题是确定自己的方向,这个时候最容易迷茫和困惑,学习什么技术?怎么去学,这些真的很难 个人建议如下:1.技术型方向:提高自己的编程能力和语言造诣,最有效的是”造轮子”,量变引起质变 写插件,写框架,写爬虫,写数据库,自制编程语言,等等.2.业务型方向:提高自己的业务能力,和客户的沟通能力,分析需求,解决客户的难题 多出去见客户,去现场,了解需求,分析需求,","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}],"tags":[{"name":"others","slug":"others","permalink":"http://zhangfuxin.cn/tags/others/"}],"keywords":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}]},{"title":"hexo和github打造个人博客","slug":"hexo-githup-blog","date":"2015-12-20T14:35:04.000Z","updated":"2019-08-21T15:13:24.127Z","comments":true,"path":"hexo-githup-blog.html","link":"","permalink":"http://zhangfuxin.cn/hexo-githup-blog.html","excerpt":"** hexo和github打造个人博客 ：** &lt;Excerpt in index | 首页摘要&gt; 使用hexo和github打造属于自己的静态博客，展示自己的作品，思想……","text":"** hexo和github打造个人博客 ：** &lt;Excerpt in index | 首页摘要&gt; 使用hexo和github打造属于自己的静态博客，展示自己的作品，思想…… &lt;The rest of contents | 余下全文&gt; ##说明 自己在使用hexo搭建静态博客的时候踩了许多坑,最终去官网看教程搞定了, 建议用hexo搭建个人博客的时候,最好看清教程的日期和使用的版本,这样就 不会因为版本的不同导致的问题了.建议先去hexo官网了解一下 hexo官网 1.准备工作 安装nodejs 去官网下载nodejs安装(推荐安装4.x),安装之后在命令行 node -v,如果成功说明node环境ok,不成功就去环境变量配置一下. 安装hexo 使用命令 npm install hexo -g,执行hexo -v 查看版本,本教程适合3.1.1以上版本 安装git 去官网下载git安装,不会自行百度 配置git 配置ssh私钥,上传到github上 2.github-pages的说明 github有两种主页,一种是github-page(个人主页),一种是项目主页,本教程针对个人主页 github-page需要将hexo博客发布到repository的master(主干)即可 github的个人主页要求repository的名称和username一致，加入username是tom，则repository的名称为tom.github.io 3.使用hexo写博客- 新建一个文件夹myblog, - 右键git bash here使用git的shell - 在shell中输入hexo init,回车执行 - 在shell中输入hexo g ,回车 - 在shell中hexo s,回车 - 去浏览器访问http://localhost:4000,访问到主页,然后在shell中ctrl c停止 - 在shell中hexo new &quot;first-blog&quot;,回车 - 在shell中hexo g ,回车 - 在shell中hexo s ,回车,在访问 - ok,在本地测试就没问题了4.发布到github打开项目根部录下的.config.yml,找到deploy,修改如下: 123deploy: - type: git repo: git@github.com:yourname/yourname.github.io.git,master 12345deploy: type: git repo: &lt;repository url&gt; branch: [branch] message: [message] 访问地址就是 http://tom.github.io/ 5.常用命令命令的简写为： 12345hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deployhexo clean 删除public文件夹6.常见问题 部署时出现git not found npm install hexo-deployer-git –save 安装依赖包 7.详细设置每个人对自己的博客都有不一样的要求，比如主题，分类，标签，评论插件的选择， 这些对程序员的你来说，都是小菜一碟，下面是官网教程： hexo官方文档 博客效果可以看我的个人博客 我的个人博客","categories":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://zhangfuxin.cn/tags/hexo/"}],"keywords":[{"name":"开发工具","slug":"开发工具","permalink":"http://zhangfuxin.cn/categories/开发工具/"}]},{"title":"node学习","slug":"node-study","date":"2015-12-19T10:58:56.000Z","updated":"2019-08-21T15:13:24.134Z","comments":true,"path":"node-study.html","link":"","permalink":"http://zhangfuxin.cn/node-study.html","excerpt":"** node学习： ** &lt;Excerpt in index | 首页摘要&gt; nodejs学习的方法，进阶路线","text":"** node学习： ** &lt;Excerpt in index | 首页摘要&gt; nodejs学习的方法，进阶路线 &lt;The rest of contents | 余下全文&gt; 一 学习内容 node的常用模块,buffer,fs,http,net等. node常用框架express,mongoose,koa,mocha,should 部署上线,pm2,grunt, 二 学习要点 了解node的特性和语法 编写扩展node模块 用异步的思想编程 常用框架的使用 回调的解决方案(promise) 三 入门实战 参照nodejs实战上的微博系统,使用express4.x+ mongoose实现 使用socket.io实现一个简单的即时聊天的系统 使用mongoose+express+node开发一个论坛系统 使用koa+mongoose做一个简单的cms或者权限系统 四 学习方法 建议有基础的直接开始入门实战,在练习中熟悉node的api,做完一个项目再去看书 不要一直看书,没什么效果的,实战永远是最有效的","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}],"tags":[{"name":"node","slug":"node","permalink":"http://zhangfuxin.cn/tags/node/"}],"keywords":[{"name":"编程语言","slug":"编程语言","permalink":"http://zhangfuxin.cn/categories/编程语言/"}]}]}