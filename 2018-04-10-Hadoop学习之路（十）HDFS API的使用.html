<!DOCTYPE HTML>
<html lang="zh-CN">

<head><meta name="generator" content="Hexo 3.9.0">
    <!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="福星">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <meta name="theme-version" content="1.2.3">
    <meta name="root" content="/">
    <link rel="dns-prefetch" href="http://zhangfuxin.cn">
    <!--SEO-->

<meta name="keywords" content="Hadoop">


<meta name="description" content="** Hadoop学习之路（十）HDFS API的使用：** &lt;Excerpt in index | 首页摘要&gt;
​        Hadoop学习之路（十）HDFS API的使用
...">


<meta name="robots" content="all">
<meta name="google" content="all">
<meta name="googlebot" content="all">
<meta name="verify" content="all">
    <!--Title-->

<title>
    
    Hadoop学习之路（十）HDFS API的使用 |
    
    福星
</title>

<link rel="alternate" href="/atom.xml" title="福星" type="application/atom+xml">


<link rel="icon" href="/favicon.ico">

    

<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7">
<link rel="stylesheet" href="/css/font-awesome.min.css?rev=4.7.0">
<link rel="stylesheet" href="/css/style.css?rev=@@hash">
    
<div class="hide">
    <script type="text/javascript">
    var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
    document.write(unescape("%3Cspan class='cnzz_stat_icon_1263868967 hide' %3E%3Cscript%20src%3D%22https%3A%2F%2Fs95.cnzz.com%2Fz_stat.php%3Fweb_id%3D1272564536%22%3E%3C%2Fscript%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1263868967%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
    </script>
</div>




    

<script>
(function() {
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    } else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

</head></html>
<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->
<body>
    <header class="main-header"  style="background-image:url(
    http://snippet.shenliyang.com/img/banner.jpg)"
     >
    <div class="main-header-box">
        <a class="header-avatar" href="/" title='福 星'>
            <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block">
        </a>
        <div class="branding">
            <!--<h2 class="text-hide">Snippet主题,从未如此简单有趣</h2>-->
            
            <img src="/img/branding.png" alt="Snippet 博客主题" class="img-responsive center-block">
            
        </div>
    </div>
</header>
    <nav class="main-navigation">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                        <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="navbar-brand" href="http://zhangfuxin.cn">
                        福星</a>
                </div>
                <div class="collapse navbar-collapse" id="main-menu">
                    <ul class="menu">
                        
                        <li role="presentation" class="text-center">
                            <a href="/"><i class="fa "></i>
                                首页</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Linux/"><i class="fa "></i>
                                Linux</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Hadoop/"><i class="fa "></i>
                                Hadoop</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Spark/"><i class="fa "></i>
                                Spark</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Java/"><i class="fa "></i>
                                Java</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Python/"><i class="fa "></i>
                                Python</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/algorithm/"><i class="fa "></i>
                                算法</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/工具/"><i class="fa "></i>
                                工具</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/archives/"><i class="fa "></i>
                                时间轴</a>
                        </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="Hadoop学习之路（十）HDFS API的使用">
            
            Hadoop学习之路（十）HDFS API的使用
            
        </h1>
        <div class="post-meta">
    
    <span class="categories-meta fa-wrap">
        <i class="fa fa-folder-open-o"></i>
        <a class="category-link" href="/categories/Hadoop/">Hadoop</a>
    </span>
    
    
    <span class="fa-wrap">
        <i class="fa fa-tags"></i>
        <span class="tags-meta">
            
            <a class="tag-link" href="/tags/Hadoop/">Hadoop</a>
            
        </span>
    </span>
    
    
    
    <span class="fa-wrap">
        <i class="fa fa-clock-o"></i>
        <span class="date-meta">
            2018/04/10</span>
    </span>
    
    <span class="fa-wrap">
        <i class="fa fa-eye"></i>
        <span id="busuanzi_value_page_pv"></span>
    </span>
    
    
</div>
        
        
    </div>
    
    <div class="post-body post-content">
        <p>** Hadoop学习之路（十）HDFS API的使用：** &lt;Excerpt in index | 首页摘要&gt;</p>
<p>​        Hadoop学习之路（十）HDFS API的使用</p>
<a id="more"></a>
<p>&lt;The rest of contents | 余下全文&gt;</p>
<p>HDFS API的高级编程</p>
<p>HDFS的API就两个：<strong>FileSystem 和Configuration</strong></p>
<h2 id="1、文件的上传和下载"><a href="#1、文件的上传和下载" class="headerlink" title="1、文件的上传和下载"></a>1、文件的上传和下载</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"> 1 package com.ghgj.hdfs.api;</span><br><span class="line"> 2 </span><br><span class="line"> 3 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 4 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 5 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 6 </span><br><span class="line"> 7 public class HDFS_GET_AND_PUT &#123;</span><br><span class="line"> 8 </span><br><span class="line"> 9     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">10         </span><br><span class="line">11         </span><br><span class="line">12         Configuration conf = new Configuration();</span><br><span class="line">13         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);</span><br><span class="line">14         conf.set(&quot;dfs.replication&quot;, &quot;2&quot;);</span><br><span class="line">15         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">16         </span><br><span class="line">17         </span><br><span class="line">18         /**</span><br><span class="line">19          * 更改操作用户有两种方式:</span><br><span class="line">20          * </span><br><span class="line">21          * 1、直接设置运行换种的用户名为hadoop</span><br><span class="line">22          * </span><br><span class="line">23          *     VM arguments ;   -DHADOOP_USER_NAME=hadoop</span><br><span class="line">24          * </span><br><span class="line">25          * 2、在代码中进行声明</span><br><span class="line">26          * </span><br><span class="line">27          *  System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">28          */</span><br><span class="line">29         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">30         </span><br><span class="line">31         // 上传</span><br><span class="line">32         fs.copyFromLocalFile(new Path(&quot;c:/sss.txt&quot;), new Path(&quot;/a/ggg.txt&quot;));</span><br><span class="line">33         </span><br><span class="line">34         </span><br><span class="line">35         </span><br><span class="line">36         /**</span><br><span class="line">37          * .crc  ： 校验文件</span><br><span class="line">38          * </span><br><span class="line">39          * 每个块的元数据信息都只会记录合法数据的起始偏移量：  qqq.txt  blk_41838 :  0 - 1100byte</span><br><span class="line">40          * </span><br><span class="line">41          * 如果进行非法的数据追加。最终是能够下载合法数据。</span><br><span class="line">42          * 由于你在数据的中间， 也就是说在 0 -1100 之间的范围进行了数据信息的更改。 造成了采用CRC算法计算出来校验值，和最初存入进HDFS的校验值</span><br><span class="line">43          * 不一致。HDFS就认为当前这个文件被损坏了。</span><br><span class="line">44          */</span><br><span class="line">45         </span><br><span class="line">46         </span><br><span class="line">47         // 下载 </span><br><span class="line">48         fs.copyToLocalFile(new Path(&quot;/a/qqq.txt&quot;), new Path(&quot;c:/qqq3.txt&quot;));</span><br><span class="line">49         </span><br><span class="line">50         </span><br><span class="line">51         /**</span><br><span class="line">52          * 上传和下载的API的底层封装其实就是 ： FileUtil.copy(....)</span><br><span class="line">53          */</span><br><span class="line">54         </span><br><span class="line">55         fs.close();</span><br><span class="line">56     &#125;</span><br><span class="line">57 &#125;</span><br></pre></td></tr></table></figure>

<h2 id="2、配置文件conf"><a href="#2、配置文件conf" class="headerlink" title="2、配置文件conf"></a>2、配置文件conf</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"> 1 package com.exam.hdfs;</span><br><span class="line"> 2 </span><br><span class="line"> 3 import java.io.IOException;</span><br><span class="line"> 4 import java.util.Iterator;</span><br><span class="line"> 5 import java.util.Map.Entry;</span><br><span class="line"> 6 </span><br><span class="line"> 7 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 8 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 9 </span><br><span class="line">10 public class TestConf1 &#123;</span><br><span class="line">11 </span><br><span class="line">12     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">13         </span><br><span class="line">14         </span><br><span class="line">15         /**</span><br><span class="line">16          * 底层会加载一堆的配置文件：</span><br><span class="line">17          * </span><br><span class="line">18          * core-default.xml</span><br><span class="line">19          * hdfs-default.xml</span><br><span class="line">20          * mapred-default.xml</span><br><span class="line">21          * yarn-default.xml</span><br><span class="line">22          */</span><br><span class="line">23         Configuration conf = new Configuration();</span><br><span class="line">24 //        conf.addResource(&quot;hdfs-default.xml&quot;);</span><br><span class="line">25         </span><br><span class="line">26         /**</span><br><span class="line">27          * 当前这个hdfs-site.xml文件就放置在这个项目中的src下。也就是classpath路径下。</span><br><span class="line">28          * 所以 FS在初始化的时候，会把hdfs-site.xml这个文件中的name-value对解析到conf中</span><br><span class="line">29          * </span><br><span class="line">30          * </span><br><span class="line">31          * 但是：</span><br><span class="line">32          * </span><br><span class="line">33          * 1、如果hdfs-site.xml 不在src下， 看是否能加载？？？  不能</span><br><span class="line">34          * </span><br><span class="line">35          * 2、如果文件名不叫做 hdfs-default.xml 或者 hdsf-site.xml  看是否能自动加载？？？  不能</span><br><span class="line">36          * </span><br><span class="line">37          * 得出的结论：</span><br><span class="line">38          * </span><br><span class="line">39          * 如果需要项目代码自动加载配置文件中的信息，那么就必须把配置文件改成-default.xml或者-site.xml的名称</span><br><span class="line">40          * 而且必须放置在src下</span><br><span class="line">41          * </span><br><span class="line">42          * 那如果不叫这个名，或者不在src下，也需要加载这些配置文件中的参数：</span><br><span class="line">43          * </span><br><span class="line">44          * 必须使用conf对象提供的一些方法去手动加载</span><br><span class="line">45          */</span><br><span class="line">46 //        conf.addResource(&quot;hdfs-site.xml&quot;);</span><br><span class="line">47         conf.set(&quot;dfs.replication&quot;, &quot;1&quot;);</span><br><span class="line">48         conf.addResource(&quot;myconfig/hdfs-site.xml&quot;);</span><br><span class="line">49         </span><br><span class="line">50         </span><br><span class="line">51         /**</span><br><span class="line">52          * 依次加载的参数信息的顺序是：</span><br><span class="line">53          * </span><br><span class="line">54          * 1、加载 core/hdfs/mapred/yarn-default.xml</span><br><span class="line">55          * </span><br><span class="line">56          * 2、加载通过conf.addResources()加载的配置文件</span><br><span class="line">57          * </span><br><span class="line">58          * 3、加载conf.set(name, value)</span><br><span class="line">59          */</span><br><span class="line">60         </span><br><span class="line">61         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">62         </span><br><span class="line">63         System.out.println(conf.get(&quot;dfs.replication&quot;));</span><br><span class="line">64 </span><br><span class="line">65         </span><br><span class="line">66         Iterator&lt;Entry&lt;String, String&gt;&gt; iterator = conf.iterator();</span><br><span class="line">67         while(iterator.hasNext())&#123;</span><br><span class="line">68             Entry&lt;String, String&gt; e = iterator.next();</span><br><span class="line">69             System.out.println(e.getKey() + &quot;\t&quot; + e.getValue());</span><br><span class="line">70         &#125;</span><br><span class="line">71     &#125;</span><br><span class="line">72 &#125;</span><br></pre></td></tr></table></figure>

<h2 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br></pre></td><td class="code"><pre><span class="line">  1 log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).</span><br><span class="line">  2 log4j:WARN Please initialize the log4j system properly.</span><br><span class="line">  3 log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span><br><span class="line">  4 1</span><br><span class="line">  5 hadoop.security.groups.cache.secs    300</span><br><span class="line">  6 dfs.datanode.cache.revocation.timeout.ms    900000</span><br><span class="line">  7 dfs.namenode.resource.check.interval    5000</span><br><span class="line">  8 s3.client-write-packet-size    65536</span><br><span class="line">  9 dfs.client.https.need-auth    false</span><br><span class="line"> 10 dfs.replication    1</span><br><span class="line"> 11 hadoop.security.group.mapping.ldap.directory.search.timeout    10000</span><br><span class="line"> 12 dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold    10737418240</span><br><span class="line"> 13 hadoop.work.around.non.threadsafe.getpwuid    false</span><br><span class="line"> 14 dfs.namenode.write-lock-reporting-threshold-ms    5000</span><br><span class="line"> 15 fs.ftp.host.port    21</span><br><span class="line"> 16 dfs.namenode.avoid.read.stale.datanode    false</span><br><span class="line"> 17 dfs.journalnode.rpc-address    0.0.0.0:8485</span><br><span class="line"> 18 hadoop.security.kms.client.encrypted.key.cache.expiry    43200000</span><br><span class="line"> 19 ipc.client.connection.maxidletime    10000</span><br><span class="line"> 20 hadoop.registry.zk.session.timeout.ms    60000</span><br><span class="line"> 21 tfile.io.chunk.size    1048576</span><br><span class="line"> 22 fs.automatic.close    true</span><br><span class="line"> 23 ha.health-monitor.sleep-after-disconnect.ms    1000</span><br><span class="line"> 24 io.map.index.interval    128</span><br><span class="line"> 25 dfs.namenode.https-address    0.0.0.0:50470</span><br><span class="line"> 26 dfs.mover.max-no-move-interval    60000</span><br><span class="line"> 27 io.seqfile.sorter.recordlimit    1000000</span><br><span class="line"> 28 fs.s3n.multipart.uploads.enabled    false</span><br><span class="line"> 29 hadoop.util.hash.type    murmur</span><br><span class="line"> 30 dfs.namenode.replication.min    1</span><br><span class="line"> 31 dfs.datanode.directoryscan.threads    1</span><br><span class="line"> 32 dfs.namenode.fs-limits.min-block-size    1048576</span><br><span class="line"> 33 dfs.datanode.directoryscan.interval    21600</span><br><span class="line"> 34 fs.AbstractFileSystem.file.impl    org.apache.hadoop.fs.local.LocalFs</span><br><span class="line"> 35 dfs.namenode.acls.enabled    false</span><br><span class="line"> 36 dfs.client.short.circuit.replica.stale.threshold.ms    1800000</span><br><span class="line"> 37 net.topology.script.number.args    100</span><br><span class="line"> 38 hadoop.http.authentication.token.validity    36000</span><br><span class="line"> 39 fs.s3.block.size    67108864</span><br><span class="line"> 40 dfs.namenode.resource.du.reserved    104857600</span><br><span class="line"> 41 ha.failover-controller.graceful-fence.rpc-timeout.ms    5000</span><br><span class="line"> 42 s3native.bytes-per-checksum    512</span><br><span class="line"> 43 dfs.namenode.datanode.registration.ip-hostname-check    true</span><br><span class="line"> 44 dfs.namenode.path.based.cache.block.map.allocation.percent    0.25</span><br><span class="line"> 45 dfs.namenode.backup.http-address    0.0.0.0:50105</span><br><span class="line"> 46 hadoop.security.group.mapping    org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback</span><br><span class="line"> 47 dfs.namenode.edits.noeditlogchannelflush    false</span><br><span class="line"> 48 dfs.datanode.cache.revocation.polling.ms    500</span><br><span class="line"> 49 dfs.namenode.audit.loggers    default</span><br><span class="line"> 50 hadoop.security.groups.cache.warn.after.ms    5000</span><br><span class="line"> 51 io.serializations    org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization</span><br><span class="line"> 52 dfs.namenode.lazypersist.file.scrub.interval.sec    300</span><br><span class="line"> 53 fs.s3a.threads.core    15</span><br><span class="line"> 54 hadoop.security.crypto.buffer.size    8192</span><br><span class="line"> 55 hadoop.http.cross-origin.allowed-methods    GET,POST,HEAD</span><br><span class="line"> 56 hadoop.registry.zk.retry.interval.ms    1000</span><br><span class="line"> 57 dfs.http.policy    HTTP_ONLY</span><br><span class="line"> 58 hadoop.registry.secure    false</span><br><span class="line"> 59 dfs.namenode.replication.interval    3</span><br><span class="line"> 60 dfs.namenode.safemode.min.datanodes    0</span><br><span class="line"> 61 dfs.client.file-block-storage-locations.num-threads    10</span><br><span class="line"> 62 nfs.dump.dir    /tmp/.hdfs-nfs</span><br><span class="line"> 63 dfs.namenode.secondary.https-address    0.0.0.0:50091</span><br><span class="line"> 64 hadoop.kerberos.kinit.command    kinit</span><br><span class="line"> 65 dfs.block.access.token.lifetime    600</span><br><span class="line"> 66 dfs.webhdfs.enabled    true</span><br><span class="line"> 67 dfs.client.use.datanode.hostname    false</span><br><span class="line"> 68 dfs.namenode.delegation.token.max-lifetime    604800000</span><br><span class="line"> 69 fs.trash.interval    0</span><br><span class="line"> 70 dfs.datanode.drop.cache.behind.writes    false</span><br><span class="line"> 71 dfs.namenode.avoid.write.stale.datanode    false</span><br><span class="line"> 72 dfs.namenode.num.extra.edits.retained    1000000</span><br><span class="line"> 73 s3.blocksize    67108864</span><br><span class="line"> 74 ipc.client.connect.max.retries.on.timeouts    45</span><br><span class="line"> 75 dfs.datanode.data.dir    /home/hadoop/data/hadoopdata/data</span><br><span class="line"> 76 fs.s3.buffer.dir    $&#123;hadoop.tmp.dir&#125;/s3</span><br><span class="line"> 77 fs.s3n.block.size    67108864</span><br><span class="line"> 78 nfs.exports.allowed.hosts    * rw</span><br><span class="line"> 79 ha.health-monitor.connect-retry-interval.ms    1000</span><br><span class="line"> 80 hadoop.security.instrumentation.requires.admin    false</span><br><span class="line"> 81 hadoop.registry.zk.retry.ceiling.ms    60000</span><br><span class="line"> 82 nfs.rtmax    1048576</span><br><span class="line"> 83 dfs.client.mmap.cache.size    256</span><br><span class="line"> 84 dfs.datanode.data.dir.perm    700</span><br><span class="line"> 85 io.file.buffer.size    4096</span><br><span class="line"> 86 dfs.namenode.backup.address    0.0.0.0:50100</span><br><span class="line"> 87 dfs.client.datanode-restart.timeout    30</span><br><span class="line"> 88 dfs.datanode.readahead.bytes    4194304</span><br><span class="line"> 89 dfs.namenode.xattrs.enabled    true</span><br><span class="line"> 90 io.mapfile.bloom.size    1048576</span><br><span class="line"> 91 ipc.client.connect.retry.interval    1000</span><br><span class="line"> 92 dfs.client-write-packet-size    65536</span><br><span class="line"> 93 dfs.namenode.checkpoint.txns    1000000</span><br><span class="line"> 94 dfs.datanode.bp-ready.timeout    20</span><br><span class="line"> 95 dfs.datanode.transfer.socket.send.buffer.size    131072</span><br><span class="line"> 96 hadoop.security.kms.client.authentication.retry-count    1</span><br><span class="line"> 97 dfs.client.block.write.retries    3</span><br><span class="line"> 98 fs.swift.impl    org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem</span><br><span class="line"> 99 ha.failover-controller.graceful-fence.connection.retries    1</span><br><span class="line">100 hadoop.registry.zk.connection.timeout.ms    15000</span><br><span class="line">101 dfs.namenode.safemode.threshold-pct    0.999f</span><br><span class="line">102 dfs.cachereport.intervalMsec    10000</span><br><span class="line">103 hadoop.security.java.secure.random.algorithm    SHA1PRNG</span><br><span class="line">104 ftp.blocksize    67108864</span><br><span class="line">105 dfs.namenode.list.cache.directives.num.responses    100</span><br><span class="line">106 dfs.namenode.kerberos.principal.pattern    *</span><br><span class="line">107 file.stream-buffer-size    4096</span><br><span class="line">108 dfs.datanode.dns.nameserver    default</span><br><span class="line">109 fs.s3a.max.total.tasks    1000</span><br><span class="line">110 dfs.namenode.replication.considerLoad    true</span><br><span class="line">111 nfs.allow.insecure.ports    true</span><br><span class="line">112 dfs.namenode.edits.journal-plugin.qjournal    org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager</span><br><span class="line">113 dfs.client.write.exclude.nodes.cache.expiry.interval.millis    600000</span><br><span class="line">114 dfs.client.mmap.cache.timeout.ms    3600000</span><br><span class="line">115 ipc.client.idlethreshold    4000</span><br><span class="line">116 io.skip.checksum.errors    false</span><br><span class="line">117 ftp.stream-buffer-size    4096</span><br><span class="line">118 fs.s3a.fast.upload    false</span><br><span class="line">119 dfs.client.failover.connection.retries.on.timeouts    0</span><br><span class="line">120 file.blocksize    67108864</span><br><span class="line">121 ftp.replication    3</span><br><span class="line">122 dfs.namenode.replication.work.multiplier.per.iteration    2</span><br><span class="line">123 hadoop.security.authorization    false</span><br><span class="line">124 hadoop.http.authentication.simple.anonymous.allowed    true</span><br><span class="line">125 s3native.client-write-packet-size    65536</span><br><span class="line">126 hadoop.rpc.socket.factory.class.default    org.apache.hadoop.net.StandardSocketFactory</span><br><span class="line">127 file.bytes-per-checksum    512</span><br><span class="line">128 dfs.datanode.slow.io.warning.threshold.ms    300</span><br><span class="line">129 fs.har.impl.disable.cache    true</span><br><span class="line">130 rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB    org.apache.hadoop.ipc.ProtobufRpcEngine</span><br><span class="line">131 io.seqfile.lazydecompress    true</span><br><span class="line">132 dfs.namenode.reject-unresolved-dn-topology-mapping    false</span><br><span class="line">133 hadoop.common.configuration.version    0.23.0</span><br><span class="line">134 hadoop.security.authentication    simple</span><br><span class="line">135 dfs.datanode.drop.cache.behind.reads    false</span><br><span class="line">136 dfs.image.compression.codec    org.apache.hadoop.io.compress.DefaultCodec</span><br><span class="line">137 dfs.client.read.shortcircuit.streams.cache.size    256</span><br><span class="line">138 file.replication    1</span><br><span class="line">139 dfs.namenode.top.num.users    10</span><br><span class="line">140 dfs.namenode.accesstime.precision    3600000</span><br><span class="line">141 dfs.namenode.fs-limits.max-xattrs-per-inode    32</span><br><span class="line">142 dfs.image.transfer.timeout    60000</span><br><span class="line">143 io.mapfile.bloom.error.rate    0.005</span><br><span class="line">144 nfs.wtmax    1048576</span><br><span class="line">145 hadoop.security.kms.client.encrypted.key.cache.size    500</span><br><span class="line">146 dfs.namenode.edit.log.autoroll.check.interval.ms    300000</span><br><span class="line">147 fs.s3a.multipart.purge    false</span><br><span class="line">148 dfs.namenode.support.allow.format    true</span><br><span class="line">149 hadoop.hdfs.configuration.version    1</span><br><span class="line">150 fs.s3a.connection.establish.timeout    5000</span><br><span class="line">151 hadoop.security.group.mapping.ldap.search.attr.member    member</span><br><span class="line">152 dfs.secondary.namenode.kerberos.internal.spnego.principal    $&#123;dfs.web.authentication.kerberos.principal&#125;</span><br><span class="line">153 dfs.stream-buffer-size    4096</span><br><span class="line">154 hadoop.ssl.client.conf    ssl-client.xml</span><br><span class="line">155 dfs.namenode.invalidate.work.pct.per.iteration    0.32f</span><br><span class="line">156 fs.s3a.multipart.purge.age    86400</span><br><span class="line">157 dfs.journalnode.https-address    0.0.0.0:8481</span><br><span class="line">158 dfs.namenode.top.enabled    true</span><br><span class="line">159 hadoop.security.kms.client.encrypted.key.cache.low-watermark    0.3f</span><br><span class="line">160 dfs.namenode.max.objects    0</span><br><span class="line">161 hadoop.user.group.static.mapping.overrides    dr.who=;</span><br><span class="line">162 fs.s3a.fast.buffer.size    1048576</span><br><span class="line">163 dfs.bytes-per-checksum    512</span><br><span class="line">164 dfs.datanode.max.transfer.threads    4096</span><br><span class="line">165 dfs.block.access.key.update.interval    600</span><br><span class="line">166 ipc.maximum.data.length    67108864</span><br><span class="line">167 tfile.fs.input.buffer.size    262144</span><br><span class="line">168 ha.failover-controller.new-active.rpc-timeout.ms    60000</span><br><span class="line">169 dfs.client.cached.conn.retry    3</span><br><span class="line">170 dfs.client.read.shortcircuit    false</span><br><span class="line">171 hadoop.ssl.hostname.verifier    DEFAULT</span><br><span class="line">172 dfs.datanode.hdfs-blocks-metadata.enabled    false</span><br><span class="line">173 dfs.datanode.directoryscan.throttle.limit.ms.per.sec    0</span><br><span class="line">174 dfs.image.transfer.chunksize    65536</span><br><span class="line">175 hadoop.http.authentication.type    simple</span><br><span class="line">176 dfs.namenode.list.encryption.zones.num.responses    100</span><br><span class="line">177 dfs.client.https.keystore.resource    ssl-client.xml</span><br><span class="line">178 s3native.blocksize    67108864</span><br><span class="line">179 net.topology.impl    org.apache.hadoop.net.NetworkTopology</span><br><span class="line">180 dfs.client.failover.sleep.base.millis    500</span><br><span class="line">181 io.seqfile.compress.blocksize    1000000</span><br><span class="line">182 dfs.namenode.path.based.cache.refresh.interval.ms    30000</span><br><span class="line">183 dfs.namenode.decommission.interval    30</span><br><span class="line">184 dfs.permissions.superusergroup    supergroup</span><br><span class="line">185 dfs.namenode.fs-limits.max-directory-items    1048576</span><br><span class="line">186 hadoop.registry.zk.retry.times    5</span><br><span class="line">187 dfs.ha.log-roll.period    120</span><br><span class="line">188 fs.AbstractFileSystem.ftp.impl    org.apache.hadoop.fs.ftp.FtpFs</span><br><span class="line">189 ftp.bytes-per-checksum    512</span><br><span class="line">190 dfs.user.home.dir.prefix    /user</span><br><span class="line">191 dfs.namenode.checkpoint.edits.dir    $&#123;dfs.namenode.checkpoint.dir&#125;</span><br><span class="line">192 dfs.client.socket.send.buffer.size    131072</span><br><span class="line">193 ipc.client.fallback-to-simple-auth-allowed    false</span><br><span class="line">194 dfs.blockreport.initialDelay    0</span><br><span class="line">195 dfs.namenode.inotify.max.events.per.rpc    1000</span><br><span class="line">196 dfs.namenode.heartbeat.recheck-interval    300000</span><br><span class="line">197 dfs.namenode.safemode.extension    30000</span><br><span class="line">198 dfs.client.failover.sleep.max.millis    15000</span><br><span class="line">199 dfs.namenode.delegation.key.update-interval    86400000</span><br><span class="line">200 dfs.datanode.transfer.socket.recv.buffer.size    131072</span><br><span class="line">201 hadoop.rpc.protection    authentication</span><br><span class="line">202 fs.permissions.umask-mode    022</span><br><span class="line">203 fs.s3.sleepTimeSeconds    10</span><br><span class="line">204 dfs.namenode.fs-limits.max-xattr-size    16384</span><br><span class="line">205 ha.health-monitor.rpc-timeout.ms    45000</span><br><span class="line">206 hadoop.http.staticuser.user    dr.who</span><br><span class="line">207 dfs.datanode.http.address    0.0.0.0:50075</span><br><span class="line">208 fs.s3a.connection.maximum    15</span><br><span class="line">209 fs.s3a.paging.maximum    5000</span><br><span class="line">210 fs.AbstractFileSystem.viewfs.impl    org.apache.hadoop.fs.viewfs.ViewFs</span><br><span class="line">211 dfs.namenode.blocks.per.postponedblocks.rescan    10000</span><br><span class="line">212 fs.ftp.host    0.0.0.0</span><br><span class="line">213 dfs.lock.suppress.warning.interval    10s</span><br><span class="line">214 hadoop.http.authentication.kerberos.keytab    $&#123;user.home&#125;/hadoop.keytab</span><br><span class="line">215 fs.s3a.impl    org.apache.hadoop.fs.s3a.S3AFileSystem</span><br><span class="line">216 hadoop.registry.zk.root    /registry</span><br><span class="line">217 hadoop.jetty.logs.serve.aliases    true</span><br><span class="line">218 dfs.namenode.fs-limits.max-blocks-per-file    1048576</span><br><span class="line">219 dfs.balancer.keytab.enabled    false</span><br><span class="line">220 dfs.client.block.write.replace-datanode-on-failure.enable    true</span><br><span class="line">221 hadoop.http.cross-origin.max-age    1800</span><br><span class="line">222 io.compression.codec.bzip2.library    system-native</span><br><span class="line">223 dfs.namenode.checkpoint.dir    file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary</span><br><span class="line">224 dfs.client.use.legacy.blockreader.local    false</span><br><span class="line">225 dfs.namenode.top.windows.minutes    1,5,25</span><br><span class="line">226 ipc.ping.interval    60000</span><br><span class="line">227 net.topology.node.switch.mapping.impl    org.apache.hadoop.net.ScriptBasedMapping</span><br><span class="line">228 nfs.mountd.port    4242</span><br><span class="line">229 dfs.storage.policy.enabled    true</span><br><span class="line">230 dfs.namenode.list.cache.pools.num.responses    100</span><br><span class="line">231 fs.df.interval    60000</span><br><span class="line">232 nfs.server.port    2049</span><br><span class="line">233 ha.zookeeper.parent-znode    /hadoop-ha</span><br><span class="line">234 hadoop.http.cross-origin.allowed-headers    X-Requested-With,Content-Type,Accept,Origin</span><br><span class="line">235 dfs.datanode.block-pinning.enabled    false</span><br><span class="line">236 dfs.namenode.num.checkpoints.retained    2</span><br><span class="line">237 fs.s3a.attempts.maximum    10</span><br><span class="line">238 s3native.stream-buffer-size    4096</span><br><span class="line">239 io.seqfile.local.dir    $&#123;hadoop.tmp.dir&#125;/io/local</span><br><span class="line">240 fs.s3n.multipart.copy.block.size    5368709120</span><br><span class="line">241 dfs.encrypt.data.transfer.cipher.key.bitlength    128</span><br><span class="line">242 dfs.client.mmap.retry.timeout.ms    300000</span><br><span class="line">243 dfs.datanode.sync.behind.writes    false</span><br><span class="line">244 dfs.namenode.fslock.fair    true</span><br><span class="line">245 hadoop.ssl.keystores.factory.class    org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</span><br><span class="line">246 dfs.permissions.enabled    true</span><br><span class="line">247 fs.AbstractFileSystem.hdfs.impl    org.apache.hadoop.fs.Hdfs</span><br><span class="line">248 dfs.blockreport.split.threshold    1000000</span><br><span class="line">249 dfs.datanode.balance.bandwidthPerSec    1048576</span><br><span class="line">250 dfs.block.scanner.volume.bytes.per.second    1048576</span><br><span class="line">251 hadoop.security.random.device.file.path    /dev/urandom</span><br><span class="line">252 fs.s3.maxRetries    4</span><br><span class="line">253 hadoop.http.filter.initializers    org.apache.hadoop.http.lib.StaticUserWebFilter</span><br><span class="line">254 dfs.namenode.stale.datanode.interval    30000</span><br><span class="line">255 ipc.client.rpc-timeout.ms    0</span><br><span class="line">256 fs.client.resolve.remote.symlinks    true</span><br><span class="line">257 dfs.default.chunk.view.size    32768</span><br><span class="line">258 hadoop.ssl.enabled.protocols    TLSv1</span><br><span class="line">259 dfs.namenode.decommission.blocks.per.interval    500000</span><br><span class="line">260 dfs.namenode.handler.count    10</span><br><span class="line">261 dfs.image.transfer.bandwidthPerSec    0</span><br><span class="line">262 rpc.metrics.quantile.enable    false</span><br><span class="line">263 hadoop.ssl.enabled    false</span><br><span class="line">264 dfs.replication.max    512</span><br><span class="line">265 dfs.namenode.name.dir    /home/hadoop/data/hadoopdata/name</span><br><span class="line">266 dfs.namenode.read-lock-reporting-threshold-ms    5000</span><br><span class="line">267 dfs.datanode.https.address    0.0.0.0:50475</span><br><span class="line">268 dfs.datanode.failed.volumes.tolerated    0</span><br><span class="line">269 ipc.client.kill.max    10</span><br><span class="line">270 fs.s3a.threads.max    256</span><br><span class="line">271 ipc.server.listen.queue.size    128</span><br><span class="line">272 dfs.client.domain.socket.data.traffic    false</span><br><span class="line">273 dfs.block.access.token.enable    false</span><br><span class="line">274 dfs.blocksize    134217728</span><br><span class="line">275 fs.s3a.connection.timeout    50000</span><br><span class="line">276 fs.s3a.threads.keepalivetime    60</span><br><span class="line">277 file.client-write-packet-size    65536</span><br><span class="line">278 dfs.datanode.address    0.0.0.0:50010</span><br><span class="line">279 ha.failover-controller.cli-check.rpc-timeout.ms    20000</span><br><span class="line">280 ha.zookeeper.acl    world:anyone:rwcda</span><br><span class="line">281 ipc.client.connect.max.retries    10</span><br><span class="line">282 dfs.encrypt.data.transfer    false</span><br><span class="line">283 dfs.namenode.write.stale.datanode.ratio    0.5f</span><br><span class="line">284 ipc.client.ping    true</span><br><span class="line">285 dfs.datanode.shared.file.descriptor.paths    /dev/shm,/tmp</span><br><span class="line">286 dfs.short.circuit.shared.memory.watcher.interrupt.check.ms    60000</span><br><span class="line">287 hadoop.tmp.dir    /home/hadoop/data/hadoopdata</span><br><span class="line">288 dfs.datanode.handler.count    10</span><br><span class="line">289 dfs.client.failover.max.attempts    15</span><br><span class="line">290 dfs.balancer.max-no-move-interval    60000</span><br><span class="line">291 dfs.client.read.shortcircuit.streams.cache.expiry.ms    300000</span><br><span class="line">292 dfs.namenode.block-placement-policy.default.prefer-local-node    true</span><br><span class="line">293 hadoop.ssl.require.client.cert    false</span><br><span class="line">294 hadoop.security.uid.cache.secs    14400</span><br><span class="line">295 dfs.client.read.shortcircuit.skip.checksum    false</span><br><span class="line">296 dfs.namenode.resource.checked.volumes.minimum    1</span><br><span class="line">297 hadoop.registry.rm.enabled    false</span><br><span class="line">298 dfs.namenode.quota.init-threads    4</span><br><span class="line">299 dfs.namenode.max.extra.edits.segments.retained    10000</span><br><span class="line">300 dfs.webhdfs.user.provider.user.pattern    ^[A-Za-z_][A-Za-z0-9._-]*[$]?$</span><br><span class="line">301 dfs.client.mmap.enabled    true</span><br><span class="line">302 dfs.client.file-block-storage-locations.timeout.millis    1000</span><br><span class="line">303 dfs.datanode.block.id.layout.upgrade.threads    12</span><br><span class="line">304 dfs.datanode.use.datanode.hostname    false</span><br><span class="line">305 hadoop.fuse.timer.period    5</span><br><span class="line">306 dfs.client.context    default</span><br><span class="line">307 fs.trash.checkpoint.interval    0</span><br><span class="line">308 dfs.journalnode.http-address    0.0.0.0:8480</span><br><span class="line">309 dfs.balancer.address    0.0.0.0:0</span><br><span class="line">310 dfs.namenode.lock.detailed-metrics.enabled    false</span><br><span class="line">311 dfs.namenode.delegation.token.renew-interval    86400000</span><br><span class="line">312 ha.health-monitor.check-interval.ms    1000</span><br><span class="line">313 dfs.namenode.retrycache.heap.percent    0.03f</span><br><span class="line">314 ipc.client.connect.timeout    20000</span><br><span class="line">315 dfs.reformat.disabled    false</span><br><span class="line">316 dfs.blockreport.intervalMsec    21600000</span><br><span class="line">317 fs.s3a.multipart.threshold    2147483647</span><br><span class="line">318 dfs.https.server.keystore.resource    ssl-server.xml</span><br><span class="line">319 hadoop.http.cross-origin.enabled    false</span><br><span class="line">320 io.map.index.skip    0</span><br><span class="line">321 dfs.balancer.block-move.timeout    0</span><br><span class="line">322 io.native.lib.available    true</span><br><span class="line">323 s3.replication    3</span><br><span class="line">324 dfs.namenode.kerberos.internal.spnego.principal    $&#123;dfs.web.authentication.kerberos.principal&#125;</span><br><span class="line">325 fs.AbstractFileSystem.har.impl    org.apache.hadoop.fs.HarFs</span><br><span class="line">326 hadoop.security.kms.client.encrypted.key.cache.num.refill.threads    2</span><br><span class="line">327 fs.s3n.multipart.uploads.block.size    67108864</span><br><span class="line">328 dfs.image.compress    false</span><br><span class="line">329 dfs.datanode.dns.interface    default</span><br><span class="line">330 dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction    0.75f</span><br><span class="line">331 tfile.fs.output.buffer.size    262144</span><br><span class="line">332 fs.du.interval    600000</span><br><span class="line">333 dfs.client.failover.connection.retries    0</span><br><span class="line">334 dfs.namenode.edit.log.autoroll.multiplier.threshold    2.0</span><br><span class="line">335 hadoop.security.group.mapping.ldap.ssl    false</span><br><span class="line">336 dfs.namenode.top.window.num.buckets    10</span><br><span class="line">337 fs.s3a.buffer.dir    $&#123;hadoop.tmp.dir&#125;/s3a</span><br><span class="line">338 dfs.namenode.checkpoint.check.period    60</span><br><span class="line">339 fs.defaultFS    hdfs://hadoop1:9000</span><br><span class="line">340 fs.s3a.multipart.size    104857600</span><br><span class="line">341 dfs.client.slow.io.warning.threshold.ms    30000</span><br><span class="line">342 dfs.datanode.max.locked.memory    0</span><br><span class="line">343 dfs.namenode.retrycache.expirytime.millis    600000</span><br><span class="line">344 hadoop.security.group.mapping.ldap.search.attr.group.name    cn</span><br><span class="line">345 dfs.client.block.write.replace-datanode-on-failure.best-effort    false</span><br><span class="line">346 dfs.ha.fencing.ssh.connect-timeout    30000</span><br><span class="line">347 dfs.datanode.scan.period.hours    504</span><br><span class="line">348 hadoop.registry.zk.quorum    localhost:2181</span><br><span class="line">349 dfs.namenode.fs-limits.max-component-length    255</span><br><span class="line">350 hadoop.http.cross-origin.allowed-origins    *</span><br><span class="line">351 dfs.namenode.enable.retrycache    true</span><br><span class="line">352 dfs.datanode.du.reserved    0</span><br><span class="line">353 dfs.datanode.ipc.address    0.0.0.0:50020</span><br><span class="line">354 hadoop.registry.system.acls    sasl:yarn@, sasl:mapred@, sasl:hdfs@</span><br><span class="line">355 dfs.namenode.path.based.cache.retry.interval.ms    30000</span><br><span class="line">356 hadoop.security.crypto.cipher.suite    AES/CTR/NoPadding</span><br><span class="line">357 dfs.client.block.write.replace-datanode-on-failure.policy    DEFAULT</span><br><span class="line">358 dfs.namenode.http-address    0.0.0.0:50070</span><br><span class="line">359 hadoop.security.crypto.codec.classes.aes.ctr.nopadding    org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec</span><br><span class="line">360 dfs.ha.tail-edits.period    60</span><br><span class="line">361 hadoop.security.groups.negative-cache.secs    30</span><br><span class="line">362 hadoop.ssl.server.conf    ssl-server.xml</span><br><span class="line">363 hadoop.registry.jaas.context    Client</span><br><span class="line">364 s3native.replication    3</span><br><span class="line">365 hadoop.security.group.mapping.ldap.search.filter.group    (objectClass=group)</span><br><span class="line">366 hadoop.http.authentication.kerberos.principal    HTTP/_HOST@LOCALHOST</span><br><span class="line">367 dfs.namenode.startup.delay.block.deletion.sec    0</span><br><span class="line">368 hadoop.security.group.mapping.ldap.search.filter.user    (&amp;(objectClass=user)(sAMAccountName=&#123;0&#125;))</span><br><span class="line">369 dfs.namenode.edits.dir    $&#123;dfs.namenode.name.dir&#125;</span><br><span class="line">370 dfs.namenode.checkpoint.max-retries    3</span><br><span class="line">371 s3.stream-buffer-size    4096</span><br><span class="line">372 ftp.client-write-packet-size    65536</span><br><span class="line">373 dfs.datanode.fsdatasetcache.max.threads.per.volume    4</span><br><span class="line">374 hadoop.security.sensitive-config-keys    password$,fs.s3.*[Ss]ecret.?[Kk]ey,fs.azure.account.key.*,dfs.webhdfs.oauth2.[a-z]+.token,hadoop.security.sensitive-config-keys</span><br><span class="line">375 dfs.namenode.decommission.max.concurrent.tracked.nodes    100</span><br><span class="line">376 dfs.namenode.name.dir.restore    false</span><br><span class="line">377 ipc.server.log.slow.rpc    false</span><br><span class="line">378 dfs.heartbeat.interval    3</span><br><span class="line">379 dfs.namenode.secondary.http-address    hadoop3:50090</span><br><span class="line">380 ha.zookeeper.session-timeout.ms    5000</span><br><span class="line">381 s3.bytes-per-checksum    512</span><br><span class="line">382 fs.s3a.connection.ssl.enabled    true</span><br><span class="line">383 hadoop.http.authentication.signature.secret.file    $&#123;user.home&#125;/hadoop-http-auth-signature-secret</span><br><span class="line">384 hadoop.fuse.connection.timeout    300</span><br><span class="line">385 dfs.namenode.checkpoint.period    3600</span><br><span class="line">386 ipc.server.max.connections    0</span><br><span class="line">387 dfs.ha.automatic-failover.enabled    false</span><br></pre></td></tr></table></figure>

<h2 id="3、列出指定目录下的文件以及块的信息"><a href="#3、列出指定目录下的文件以及块的信息" class="headerlink" title="3、列出指定目录下的文件以及块的信息"></a>3、列出指定目录下的文件以及块的信息</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"> 1 package com.exam.hdfs;</span><br><span class="line"> 2 </span><br><span class="line"> 3 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 4 import org.apache.hadoop.fs.BlockLocation;</span><br><span class="line"> 5 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 6 import org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"> 7 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 8 import org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"> 9 </span><br><span class="line">10 public class TestHDFS1 &#123;</span><br><span class="line">11 </span><br><span class="line">12     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">13 </span><br><span class="line">14         Configuration conf = new Configuration();</span><br><span class="line">15         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">16         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);</span><br><span class="line">17         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">18 </span><br><span class="line">19         /**</span><br><span class="line">20          * 列出指定的目录下的所有文件</span><br><span class="line">21          */</span><br><span class="line">22         RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true);</span><br><span class="line">23         while(listFiles.hasNext())&#123;</span><br><span class="line">24             LocatedFileStatus file = listFiles.next();</span><br><span class="line">25             </span><br><span class="line">26             </span><br><span class="line">27             System.out.println(file.getPath()+&quot;\t&quot;);</span><br><span class="line">28             System.out.println(file.getPath().getName()+&quot;\t&quot;);</span><br><span class="line">29             System.out.println(file.getLen()+&quot;\t&quot;);</span><br><span class="line">30             System.out.println(file.getReplication()+&quot;\t&quot;);</span><br><span class="line">31             </span><br><span class="line">32             /**</span><br><span class="line">33              * blockLocations的长度是几？  是什么意义？</span><br><span class="line">34              * </span><br><span class="line">35              * 块的数量</span><br><span class="line">36              */</span><br><span class="line">37             BlockLocation[] blockLocations = file.getBlockLocations();</span><br><span class="line">38             System.out.println(blockLocations.length+&quot;\t&quot;);</span><br><span class="line">39             </span><br><span class="line">40             for(BlockLocation bl : blockLocations)&#123;</span><br><span class="line">41                 String[] hosts = bl.getHosts();</span><br><span class="line">42                 </span><br><span class="line">43                 System.out.print(hosts[0] + &quot;-&quot; + hosts[1]+&quot;\t&quot;);</span><br><span class="line">44             &#125;</span><br><span class="line">45             System.out.println();</span><br><span class="line">46             </span><br><span class="line">47         &#125;</span><br><span class="line">48         </span><br><span class="line">49         </span><br><span class="line">50     &#125;</span><br><span class="line">51 &#125;</span><br></pre></td></tr></table></figure>

<p>输出结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1 hdfs://hadoop1:9000/aa/bb/cc/hadoop.tar.gz    </span><br><span class="line">2 hadoop.tar.gz    </span><br><span class="line">3 199007110    </span><br><span class="line">4 2    </span><br><span class="line">5 3    </span><br><span class="line">6 hadoop3-hadoop1    hadoop1-hadoop2    hadoop1-hadoop4</span><br></pre></td></tr></table></figure>

<h2 id="4、上传文件"><a href="#4、上传文件" class="headerlink" title="4、上传文件"></a>4、上传文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"> 1 package com.exam.hdfs;</span><br><span class="line"> 2 </span><br><span class="line"> 3 import java.io.File;</span><br><span class="line"> 4 import java.io.FileInputStream;</span><br><span class="line"> 5 import java.io.InputStream;</span><br><span class="line"> 6 </span><br><span class="line"> 7 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 8 import org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"> 9 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">10 import org.apache.hadoop.fs.Path;</span><br><span class="line">11 import org.apache.hadoop.io.IOUtils;</span><br><span class="line">12 </span><br><span class="line">13 public class UploadDataByStream &#123;</span><br><span class="line">14 </span><br><span class="line">15     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">16         </span><br><span class="line">17         </span><br><span class="line">18         Configuration conf = new Configuration();</span><br><span class="line">19         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">20         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);</span><br><span class="line">21         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">22         </span><br><span class="line">23         </span><br><span class="line">24         InputStream in = new FileInputStream(new File(&quot;d:/abc.tar.gz&quot;));</span><br><span class="line">25         FSDataOutputStream out = fs.create(new Path(&quot;/aa/abc.tar.gz&quot;));</span><br><span class="line">26         </span><br><span class="line">27         </span><br><span class="line">28         IOUtils.copyBytes(in, out, 4096, true);</span><br><span class="line">29         </span><br><span class="line">30         fs.close();</span><br><span class="line">31         </span><br><span class="line">32     &#125;</span><br><span class="line">33 &#125;</span><br></pre></td></tr></table></figure>

<h2 id="5、下载文件"><a href="#5、下载文件" class="headerlink" title="5、下载文件"></a>5、下载文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"> 1 package com.exam.hdfs;</span><br><span class="line"> 2 </span><br><span class="line"> 3 import java.io.File;</span><br><span class="line"> 4 import java.io.FileOutputStream;</span><br><span class="line"> 5 import java.io.OutputStream;</span><br><span class="line"> 6 </span><br><span class="line"> 7 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 8 import org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"> 9 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">10 import org.apache.hadoop.fs.Path;</span><br><span class="line">11 import org.apache.hadoop.io.IOUtils;</span><br><span class="line">12 </span><br><span class="line">13 public class DownloadDataByStream &#123;</span><br><span class="line">14 </span><br><span class="line">15     </span><br><span class="line">16     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">17         </span><br><span class="line">18         Configuration conf = new Configuration();</span><br><span class="line">19         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">20         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);</span><br><span class="line">21         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">22         </span><br><span class="line">23         </span><br><span class="line">24         FSDataInputStream in = fs.open(new Path(&quot;/aa/abc.tar.gz&quot;));</span><br><span class="line">25         OutputStream out = new FileOutputStream(new File(&quot;D:/abc.sh&quot;));</span><br><span class="line">26         </span><br><span class="line">27         </span><br><span class="line">28         IOUtils.copyBytes(in, out, 4096, true);</span><br><span class="line">29         </span><br><span class="line">30         fs.close();</span><br><span class="line">31         </span><br><span class="line">32     &#125;</span><br><span class="line">33 &#125;</span><br></pre></td></tr></table></figure>

<h2 id="6、删除某个路径下特定类型的文件，比如class类型文件，比如txt类型文件"><a href="#6、删除某个路径下特定类型的文件，比如class类型文件，比如txt类型文件" class="headerlink" title="6、删除某个路径下特定类型的文件，比如class类型文件，比如txt类型文件"></a>6、删除某个路径下特定类型的文件，比如class类型文件，比如txt类型文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"> 1 package com.exam.hdfs;</span><br><span class="line"> 2 </span><br><span class="line"> 3 import java.net.URI;</span><br><span class="line"> 4 </span><br><span class="line"> 5 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 6 import org.apache.hadoop.fs.FileStatus;</span><br><span class="line"> 7 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 8 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 9 </span><br><span class="line">10 public class HDFS_DELETE_CLASS &#123;</span><br><span class="line">11     </span><br><span class="line">12     public static final String FILETYPE = &quot;tar.gz&quot;;</span><br><span class="line">13     public static final String DELETE_PATH = &quot;/aa&quot;;</span><br><span class="line">14     </span><br><span class="line">15     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">16         </span><br><span class="line">17         new HDFS_DELETE_CLASS().rmrClassFile(new Path(DELETE_PATH));</span><br><span class="line">18     &#125;</span><br><span class="line">19     </span><br><span class="line">20     public void rmrClassFile(Path path) throws Exception&#123;</span><br><span class="line">21         </span><br><span class="line">22         // 首先获取集群必要的信息，以得到FileSystem的示例对象fs</span><br><span class="line">23         Configuration conf = new Configuration();</span><br><span class="line">24         FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop1:9000&quot;), conf, &quot;hadoop&quot;);</span><br><span class="line">25         </span><br><span class="line">26         // 首先检查path本身是文件夹还是目录</span><br><span class="line">27         FileStatus fileStatus = fs.getFileStatus(path);</span><br><span class="line">28         boolean directory = fileStatus.isDirectory();</span><br><span class="line">29         </span><br><span class="line">30         // 根据该目录是否是文件或者文件夹进行相应的操作</span><br><span class="line">31         if(directory)&#123;</span><br><span class="line">32             // 如果是目录</span><br><span class="line">33             checkAndDeleteDirectory(path, fs);</span><br><span class="line">34         &#125;else&#123;</span><br><span class="line">35             // 如果是文件，检查该文件名是不是FILETYPE类型的文件</span><br><span class="line">36             checkAndDeleteFile(path, fs);</span><br><span class="line">37         &#125;</span><br><span class="line">38     &#125;</span><br><span class="line">39     </span><br><span class="line">40     // 处理目录</span><br><span class="line">41     public static void checkAndDeleteDirectory(Path path, FileSystem fs) throws Exception&#123;</span><br><span class="line">42         // 查看该path目录下一级子目录和子文件的状态</span><br><span class="line">43         FileStatus[] listStatus = fs.listStatus(path);</span><br><span class="line">44         for(FileStatus fStatus: listStatus)&#123;</span><br><span class="line">45             Path p = fStatus.getPath();</span><br><span class="line">46             // 如果是文件，并且是以FILETYPE结尾，则删掉，否则继续遍历下一级目录</span><br><span class="line">47             if(fStatus.isFile())&#123;</span><br><span class="line">48                 checkAndDeleteFile(p, fs);</span><br><span class="line">49             &#125;else&#123;</span><br><span class="line">50                 checkAndDeleteDirectory(p, fs);</span><br><span class="line">51             &#125;</span><br><span class="line">52         &#125;</span><br><span class="line">53     &#125;</span><br><span class="line">54     </span><br><span class="line">55     // 檢查文件是否符合刪除要求，如果符合要求則刪除，不符合要求则不做处理</span><br><span class="line">56     public static void checkAndDeleteFile(Path path, FileSystem fs) throws Exception&#123;</span><br><span class="line">57         String name = path.getName();</span><br><span class="line">58         System.out.println(name);</span><br><span class="line">59         /*// 直接判断有没有FILETYPE这个字符串,不是特别稳妥，并且会有误操作，所以得判断是不是以FILETYPE结尾</span><br><span class="line">60         if(name.indexOf(FILETYPE) != -1)&#123;</span><br><span class="line">61             fs.delete(path, true);</span><br><span class="line">62         &#125;*/</span><br><span class="line">63         // 判断是不是以FILETYPE结尾</span><br><span class="line">64         int startIndex = name.length() - FILETYPE.length();</span><br><span class="line">65         int endIndex = name.length();</span><br><span class="line">66         // 求得文件后缀名</span><br><span class="line">67         String fileSuffix = name.substring(startIndex, endIndex);</span><br><span class="line">68         if(fileSuffix.equals(FILETYPE))&#123;</span><br><span class="line">69             fs.delete(path, true);</span><br><span class="line">70         &#125;</span><br><span class="line">71     &#125;</span><br><span class="line">72 &#125;</span><br></pre></td></tr></table></figure>

<h2 id="7、删除HDFS集群中的所有空文件和空目录"><a href="#7、删除HDFS集群中的所有空文件和空目录" class="headerlink" title="7、删除HDFS集群中的所有空文件和空目录"></a>7、删除HDFS集群中的所有空文件和空目录</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line">  1 public class DeleteEmptyDirAndFile &#123;</span><br><span class="line">  2     </span><br><span class="line">  3     static FileSystem fs = null;</span><br><span class="line">  4 </span><br><span class="line">  5     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">  6         </span><br><span class="line">  7         initFileSystem();</span><br><span class="line">  8 </span><br><span class="line">  9 //         创建测试数据</span><br><span class="line"> 10 //        makeTestData();</span><br><span class="line"> 11 </span><br><span class="line"> 12         // 删除测试数据</span><br><span class="line"> 13 //        deleteTestData();</span><br><span class="line"> 14 </span><br><span class="line"> 15         // 删除指定文件夹下的空文件和空文件夹</span><br><span class="line"> 16         deleteEmptyDirAndFile(new Path(&quot;/aa&quot;));</span><br><span class="line"> 17     &#125;</span><br><span class="line"> 18     </span><br><span class="line"> 19     /**</span><br><span class="line"> 20      * 删除指定文件夹下的 空文件 和 空文件夹</span><br><span class="line"> 21      * @throws Exception </span><br><span class="line"> 22      */</span><br><span class="line"> 23     public static void deleteEmptyDirAndFile(Path path) throws Exception &#123;</span><br><span class="line"> 24         </span><br><span class="line"> 25         //当是空文件夹时</span><br><span class="line"> 26         FileStatus[] listStatus = fs.listStatus(path);</span><br><span class="line"> 27         if(listStatus.length == 0)&#123;</span><br><span class="line"> 28             fs.delete(path, true);</span><br><span class="line"> 29             return;</span><br><span class="line"> 30         &#125;</span><br><span class="line"> 31         </span><br><span class="line"> 32         // 该方法的结果：包括指定目录的  文件 和 文件夹</span><br><span class="line"> 33         RemoteIterator&lt;LocatedFileStatus&gt; listLocatedStatus = fs.listLocatedStatus(path);</span><br><span class="line"> 34         </span><br><span class="line"> 35         while (listLocatedStatus.hasNext()) &#123;</span><br><span class="line"> 36             LocatedFileStatus next = listLocatedStatus.next();</span><br><span class="line"> 37 </span><br><span class="line"> 38             Path currentPath = next.getPath();</span><br><span class="line"> 39             // 获取父目录</span><br><span class="line"> 40             Path parent = next.getPath().getParent();</span><br><span class="line"> 41             </span><br><span class="line"> 42             // 如果是文件夹，继续往下遍历，删除符合条件的文件（空文件夹）</span><br><span class="line"> 43             if (next.isDirectory()) &#123;</span><br><span class="line"> 44                 </span><br><span class="line"> 45                 // 如果是空文件夹</span><br><span class="line"> 46                 if(fs.listStatus(currentPath).length == 0)&#123;</span><br><span class="line"> 47                     // 删除掉</span><br><span class="line"> 48                     fs.delete(currentPath, true);</span><br><span class="line"> 49                 &#125;else&#123;</span><br><span class="line"> 50                     // 不是空文件夹，那么则继续遍历</span><br><span class="line"> 51                     if(fs.exists(currentPath))&#123;</span><br><span class="line"> 52                         deleteEmptyDirAndFile(currentPath);</span><br><span class="line"> 53                     &#125;</span><br><span class="line"> 54                 &#125;</span><br><span class="line"> 55                 </span><br><span class="line"> 56             // 如果是文件</span><br><span class="line"> 57             &#125; else &#123;</span><br><span class="line"> 58                 // 获取文件的长度</span><br><span class="line"> 59                 long fileLength = next.getLen();</span><br><span class="line"> 60                 // 当文件是空文件时， 删除</span><br><span class="line"> 61                 if(fileLength == 0)&#123;</span><br><span class="line"> 62                     fs.delete(currentPath, true);</span><br><span class="line"> 63                 &#125;</span><br><span class="line"> 64             &#125;</span><br><span class="line"> 65             </span><br><span class="line"> 66             // 当空文件夹或者空文件删除时，有可能导致父文件夹为空文件夹，</span><br><span class="line"> 67             // 所以每次删除一个空文件或者空文件的时候都需要判断一下，如果真是如此，那么就需要把该文件夹也删除掉</span><br><span class="line"> 68             int length = fs.listStatus(parent).length;</span><br><span class="line"> 69             if(length == 0)&#123;</span><br><span class="line"> 70                 fs.delete(parent, true);</span><br><span class="line"> 71             &#125;</span><br><span class="line"> 72         &#125;</span><br><span class="line"> 73     &#125;</span><br><span class="line"> 74     </span><br><span class="line"> 75     /**</span><br><span class="line"> 76      * 初始化FileSystem对象之用</span><br><span class="line"> 77      */</span><br><span class="line"> 78     public static void initFileSystem() throws Exception&#123;</span><br><span class="line"> 79         Configuration conf = new Configuration();</span><br><span class="line"> 80         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line"> 81         conf.addResource(&quot;config/core-site.xml&quot;);</span><br><span class="line"> 82         conf.addResource(&quot;config/hdfs-site.xml&quot;);</span><br><span class="line"> 83         fs = FileSystem.get(conf);</span><br><span class="line"> 84     &#125;</span><br><span class="line"> 85 </span><br><span class="line"> 86     /**</span><br><span class="line"> 87      * 创建 测试 数据之用</span><br><span class="line"> 88      */</span><br><span class="line"> 89     public static void makeTestData() throws Exception &#123;</span><br><span class="line"> 90         </span><br><span class="line"> 91         String emptyFilePath = &quot;D:\\bigdata\\1704mr_test\\empty.txt&quot;;</span><br><span class="line"> 92         String notEmptyFilePath = &quot;D:\\bigdata\\1704mr_test\\notEmpty.txt&quot;;</span><br><span class="line"> 93 </span><br><span class="line"> 94         // 空文件夹 和 空文件 的目录</span><br><span class="line"> 95         String path1 = &quot;/aa/bb1/cc1/dd1/&quot;;</span><br><span class="line"> 96         fs.mkdirs(new Path(path1));</span><br><span class="line"> 97         fs.mkdirs(new Path(&quot;/aa/bb1/cc1/dd2/&quot;));</span><br><span class="line"> 98         fs.copyFromLocalFile(new Path(emptyFilePath), new Path(path1));</span><br><span class="line"> 99         fs.copyFromLocalFile(new Path(notEmptyFilePath), new Path(path1));</span><br><span class="line">100 </span><br><span class="line">101         // 空文件 的目录</span><br><span class="line">102         String path2 = &quot;/aa/bb1/cc2/dd2/&quot;;</span><br><span class="line">103         fs.mkdirs(new Path(path2));</span><br><span class="line">104         fs.copyFromLocalFile(new Path(emptyFilePath), new Path(path2));</span><br><span class="line">105 </span><br><span class="line">106         // 非空文件 的目录</span><br><span class="line">107         String path3 = &quot;/aa/bb2/cc3/dd3&quot;;</span><br><span class="line">108         fs.mkdirs(new Path(path3));</span><br><span class="line">109         fs.copyFromLocalFile(new Path(notEmptyFilePath), new Path(path3));</span><br><span class="line">110 </span><br><span class="line">111         // 空 文件夹</span><br><span class="line">112         String path4 = &quot;/aa/bb2/cc4/dd4&quot;;</span><br><span class="line">113         fs.mkdirs(new Path(path4));</span><br><span class="line">114 </span><br><span class="line">115         System.out.println(&quot;测试数据创建成功&quot;);</span><br><span class="line">116     &#125;</span><br><span class="line">117 </span><br><span class="line">118     /**</span><br><span class="line">119      * 删除 指定文件夹</span><br><span class="line">120      * @throws Exception </span><br><span class="line">121      */</span><br><span class="line">122     public static void deleteTestData() throws Exception &#123;</span><br><span class="line">123         boolean delete = fs.delete(new Path(&quot;/aa&quot;), true);</span><br><span class="line">124         System.out.println(delete ? &quot;删除数据成功&quot; : &quot;删除数据失败&quot;);</span><br><span class="line">125     &#125;</span><br><span class="line">126 </span><br><span class="line">127 &#125;</span><br></pre></td></tr></table></figure>

<h2 id="8、手动拷贝某个特定的数据块（比如某个文件的第二个数据块）"><a href="#8、手动拷贝某个特定的数据块（比如某个文件的第二个数据块）" class="headerlink" title="8、手动拷贝某个特定的数据块（比如某个文件的第二个数据块）"></a>8、手动拷贝某个特定的数据块（比如某个文件的第二个数据块）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"> 1 /**</span><br><span class="line"> 2      * 手动拷贝某个特定的数据块（比如某个文件的第二个数据块）</span><br><span class="line"> 3      * */</span><br><span class="line"> 4     public static void copyBlock(String str,int num) &#123;</span><br><span class="line"> 5         </span><br><span class="line"> 6         Path path = new Path(str);</span><br><span class="line"> 7         </span><br><span class="line"> 8         BlockLocation[] localtions = new BlockLocation[0] ;</span><br><span class="line"> 9         </span><br><span class="line">10         try &#123;</span><br><span class="line">11             FileStatus fileStatus = fs.getFileStatus(path);</span><br><span class="line">12             </span><br><span class="line">13             localtions = fs.getFileBlockLocations(fileStatus, 0, fileStatus.getLen());</span><br><span class="line">14             </span><br><span class="line">15             /*for(int i=0;i&lt;localtions.length;i++) &#123;</span><br><span class="line">16                 //0,134217728,hadoop1,hadoop3</span><br><span class="line">17                 //134217728,64789382,hadoop3,hadoop1</span><br><span class="line">18                 System.out.println(localtions[i]);</span><br><span class="line">19             &#125;*/</span><br><span class="line">20             </span><br><span class="line">21             /*System.out.println(localtions[num-1].getOffset());</span><br><span class="line">22             System.out.println(localtions[num-1].getLength());</span><br><span class="line">23             String[] hosts = localtions[num-1].getHosts();*/</span><br><span class="line">24             </span><br><span class="line">25             FSDataInputStream open = fs.open(path);</span><br><span class="line">26             open.seek(localtions[num-1].getOffset());</span><br><span class="line">27             OutputStream out = new FileOutputStream(new File(&quot;D:/abc.tar.gz&quot;));</span><br><span class="line">28             IOUtils.copyBytes(open, out,4096,true);</span><br><span class="line">29             </span><br><span class="line">30             </span><br><span class="line">31             </span><br><span class="line">32         &#125; catch (IOException e) &#123;</span><br><span class="line">33             e.printStackTrace();</span><br><span class="line">34         &#125;</span><br><span class="line">35         </span><br><span class="line">36     &#125;</span><br></pre></td></tr></table></figure>

<h2 id="9、编写程序统计出HDFS文件系统中文件大小小于HDFS集群中的默认块大小的文件占比"><a href="#9、编写程序统计出HDFS文件系统中文件大小小于HDFS集群中的默认块大小的文件占比" class="headerlink" title="9、编写程序统计出HDFS文件系统中文件大小小于HDFS集群中的默认块大小的文件占比"></a>9、编写程序统计出HDFS文件系统中文件大小小于HDFS集群中的默认块大小的文件占比</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"> 1 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 2 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 3 import org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"> 4 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 5 import org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"> 6 </span><br><span class="line"> 7 /**</span><br><span class="line"> 8  * </span><br><span class="line"> 9  * 编写程序统计出HDFS文件系统中文件大小小于HDFS集群中的默认块大小的文件占比</span><br><span class="line">10  * 比如：大于等于128M的文件个数为98，小于128M的文件总数为2，所以答案是2%</span><br><span class="line">11  */</span><br><span class="line">12 public class Exam1_SmallFilePercent &#123;</span><br><span class="line">13     </span><br><span class="line">14     private static int DEFAULT_BLOCKSIZE = 128 * 1024 * 1024;</span><br><span class="line">15 </span><br><span class="line">16     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">17         </span><br><span class="line">18         </span><br><span class="line">19         Configuration conf = new Configuration();</span><br><span class="line">20         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);</span><br><span class="line">21         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">22         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">23         </span><br><span class="line">24         </span><br><span class="line">25         Path path = new Path(&quot;/&quot;);</span><br><span class="line">26         float smallFilePercent = getSmallFilePercent(fs, path);</span><br><span class="line">27         System.out.println(smallFilePercent);</span><br><span class="line">28         </span><br><span class="line">29         </span><br><span class="line">30         fs.close();</span><br><span class="line">31     &#125;</span><br><span class="line">32 </span><br><span class="line">33     /**</span><br><span class="line">34      * 该方法求出指定目录下的小文件和总文件数的对比</span><br><span class="line">35      * @throws Exception </span><br><span class="line">36      */</span><br><span class="line">37     private static float getSmallFilePercent(FileSystem fs, Path path) throws Exception &#123;</span><br><span class="line">38         // TODO Auto-generated method stub</span><br><span class="line">39         </span><br><span class="line">40         int smallFile = 0;</span><br><span class="line">41         int totalFile = 0;</span><br><span class="line">42         </span><br><span class="line">43         RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(path, false);</span><br><span class="line">44         while(listFiles.hasNext())&#123;</span><br><span class="line">45             totalFile++;</span><br><span class="line">46             LocatedFileStatus next = listFiles.next();</span><br><span class="line">47             long len = next.getLen();</span><br><span class="line">48             if(len &lt; DEFAULT_BLOCKSIZE)&#123;</span><br><span class="line">49                 smallFile++;</span><br><span class="line">50             &#125;</span><br><span class="line">51         &#125;</span><br><span class="line">52         System.out.println(smallFile+&quot; : &quot;+totalFile);</span><br><span class="line">53         </span><br><span class="line">54         return smallFile * 1f /totalFile;</span><br><span class="line">55     &#125;</span><br><span class="line">56     </span><br><span class="line">57 &#125;</span><br></pre></td></tr></table></figure>

<h2 id="10、编写程序统计出HDFS文件系统中的平均数据块数（数据块总数-文件总数）"><a href="#10、编写程序统计出HDFS文件系统中的平均数据块数（数据块总数-文件总数）" class="headerlink" title="10、编写程序统计出HDFS文件系统中的平均数据块数（数据块总数/文件总数）"></a>10、编写程序统计出HDFS文件系统中的平均数据块数（数据块总数/文件总数）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"> 1 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 2 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 3 import org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"> 4 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 5 import org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"> 6 </span><br><span class="line"> 7 /**</span><br><span class="line"> 8  * </span><br><span class="line"> 9  * 编写程序统计出HDFS文件系统中的平均数据块数（数据块总数/文件总数）</span><br><span class="line">10  * 比如：一个文件有5个块，一个文件有3个块，那么平均数据块数为4</span><br><span class="line">11  * 如果还有一个文件，并且数据块就1个，那么整个HDFS的平均数据块数就是3</span><br><span class="line">12  */</span><br><span class="line">13 public class Exam2_HDSFAvgBlocks &#123;</span><br><span class="line">14     </span><br><span class="line">15     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">16         </span><br><span class="line">17         </span><br><span class="line">18         Configuration conf = new Configuration();</span><br><span class="line">19         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);</span><br><span class="line">20         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">21         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">22         </span><br><span class="line">23         </span><br><span class="line">24         Path path = new Path(&quot;/&quot;);</span><br><span class="line">25         float avgHDFSBlocks = getHDFSAvgBlocks(fs, path);</span><br><span class="line">26         System.out.println(&quot;HDFS的平均数据块个数为：&quot; + avgHDFSBlocks);</span><br><span class="line">27         </span><br><span class="line">28         </span><br><span class="line">29         fs.close();</span><br><span class="line">30     &#125;</span><br><span class="line">31 </span><br><span class="line">32     /**</span><br><span class="line">33      * 求出指定目录下的所有文件的平均数据块个数</span><br><span class="line">34      */</span><br><span class="line">35     private static float getHDFSAvgBlocks(FileSystem fs, Path path) throws Exception &#123;</span><br><span class="line">36         // TODO Auto-generated method stub</span><br><span class="line">37         </span><br><span class="line">38         int totalFiles = 0;        // 总文件数</span><br><span class="line">39         int totalBlocks = 0;    // 总数据块数</span><br><span class="line">40         </span><br><span class="line">41         RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(path, false);</span><br><span class="line">42         </span><br><span class="line">43         while(listFiles.hasNext())&#123;</span><br><span class="line">44             LocatedFileStatus next = listFiles.next();</span><br><span class="line">45             int length = next.getBlockLocations().length;</span><br><span class="line">46             totalBlocks += length;</span><br><span class="line">47             if(next.getLen() != 0)&#123;</span><br><span class="line">48                 totalFiles++;</span><br><span class="line">49             &#125;</span><br><span class="line">50         &#125;</span><br><span class="line">51         System.out.println(totalBlocks+&quot; : &quot;+totalFiles);</span><br><span class="line">52         </span><br><span class="line">53         return totalBlocks * 1f / totalFiles;</span><br><span class="line">54     &#125;</span><br><span class="line">55     </span><br><span class="line">56 &#125;</span><br></pre></td></tr></table></figure>

<h2 id="11、编写程序统计出HDFS文件系统中的平均副本数（副本总数-总数据块数）"><a href="#11、编写程序统计出HDFS文件系统中的平均副本数（副本总数-总数据块数）" class="headerlink" title="11、编写程序统计出HDFS文件系统中的平均副本数（副本总数/总数据块数）"></a>11、编写程序统计出HDFS文件系统中的平均副本数（副本总数/总数据块数）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"> 1 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 2 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 3 import org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"> 4 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 5 import org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"> 6 </span><br><span class="line"> 7 /**</span><br><span class="line"> 8  * 编写程序统计出HDFS文件系统中的平均副本数（副本总数/总数据块数）</span><br><span class="line"> 9  * 比如：总共两个文件，一个文件5个数据块，每个数据块3个副本，第二个文件2个数据块，每个文件2个副本，最终的平均副本数 = （3*3 + 2*2）/（3+2）= 2.8</span><br><span class="line">10  */</span><br><span class="line">11 public class Exam3_HDSFAvgBlockCopys &#123;</span><br><span class="line">12     </span><br><span class="line">13     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">14         </span><br><span class="line">15         </span><br><span class="line">16         Configuration conf = new Configuration();</span><br><span class="line">17         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop02:9000&quot;);</span><br><span class="line">18         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">19         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">20         </span><br><span class="line">21         </span><br><span class="line">22         Path path = new Path(&quot;/&quot;);</span><br><span class="line">23         float avgHDFSBlockCopys = getHDFSAvgBlockCopys(fs, path);</span><br><span class="line">24         System.out.println(&quot;HDFS的平均数据块个数为：&quot; + avgHDFSBlockCopys);</span><br><span class="line">25         </span><br><span class="line">26         </span><br><span class="line">27         fs.close();</span><br><span class="line">28     &#125;</span><br><span class="line">29 </span><br><span class="line">30     /**</span><br><span class="line">31      * 求出指定目录下的所有文件的平均数据块个数</span><br><span class="line">32      */</span><br><span class="line">33     private static float getHDFSAvgBlockCopys(FileSystem fs, Path path) throws Exception &#123;</span><br><span class="line">34         // TODO Auto-generated method stub</span><br><span class="line">35         </span><br><span class="line">36         int totalCopy = 0;        // 总副本数</span><br><span class="line">37         int totalBlocks = 0;    // 总数据块数</span><br><span class="line">38         </span><br><span class="line">39         RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(path, false);</span><br><span class="line">40         </span><br><span class="line">41         while(listFiles.hasNext())&#123;</span><br><span class="line">42             LocatedFileStatus next = listFiles.next();</span><br><span class="line">43 </span><br><span class="line">44             int length = next.getBlockLocations().length;</span><br><span class="line">45             short replication = next.getReplication();</span><br><span class="line">46             </span><br><span class="line">47             totalBlocks += length;</span><br><span class="line">48             totalCopy += length * replication;</span><br><span class="line">49         &#125;</span><br><span class="line">50         System.out.println(totalCopy+&quot; : &quot;+totalBlocks);</span><br><span class="line">51         </span><br><span class="line">52         return totalCopy * 1f / totalBlocks;</span><br><span class="line">53     &#125;</span><br><span class="line">54     </span><br><span class="line">55 &#125;</span><br></pre></td></tr></table></figure>

<h2 id="12、统计HDFS整个文件系统中的不足指定数据块大小的数据块的比例"><a href="#12、统计HDFS整个文件系统中的不足指定数据块大小的数据块的比例" class="headerlink" title="12、统计HDFS整个文件系统中的不足指定数据块大小的数据块的比例"></a>12、统计HDFS整个文件系统中的不足指定数据块大小的数据块的比例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"> 1 import java.io.IOException;</span><br><span class="line"> 2 </span><br><span class="line"> 3 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 4 import org.apache.hadoop.fs.BlockLocation;</span><br><span class="line"> 5 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 6 import org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"> 7 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 8 import org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"> 9 </span><br><span class="line">10 /**</span><br><span class="line">11  * 统计HDFS整个文件系统中的不足指定数据块大小的数据块的比例</span><br><span class="line">12  * 比如指定的数据块大小是128M，总数据块有100个，不是大小为完整的128M的数据块有5个，那么不足指定数据块大小的数据块的比例就为5%</span><br><span class="line">13  * 注意：千万注意考虑不同文件的指定数据块大小可能不一致。所以千万不能用默认的128M一概而论</span><br><span class="line">14  */</span><br><span class="line">15 public class Exam4_LTBlockSize &#123;</span><br><span class="line">16 </span><br><span class="line">17     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">18         </span><br><span class="line">19         Configuration conf = new Configuration();</span><br><span class="line">20         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop02:9000&quot;);</span><br><span class="line">21         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">22         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">23         </span><br><span class="line">24         Path path = new Path(&quot;/&quot;);</span><br><span class="line">25         float avgHDFSBlockCopys = getLessThanBlocksizeBlocks(fs, path);</span><br><span class="line">26         System.out.println(&quot;HDFS的不足指定数据块大小的数据块数目为：&quot; + avgHDFSBlockCopys);</span><br><span class="line">27         </span><br><span class="line">28         fs.close();</span><br><span class="line">29     &#125;</span><br><span class="line">30 </span><br><span class="line">31     private static float getLessThanBlocksizeBlocks(FileSystem fs, Path path) throws Exception &#123;</span><br><span class="line">32         // TODO Auto-generated method stub</span><br><span class="line">33         </span><br><span class="line">34         int totalBlocks = 0;                // 总副本数</span><br><span class="line">35         int lessThenBlocksizeBlocks = 0;    // 总数据块数</span><br><span class="line">36         </span><br><span class="line">37         RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(path, false);</span><br><span class="line">38         </span><br><span class="line">39         while(listFiles.hasNext())&#123;</span><br><span class="line">40             LocatedFileStatus next = listFiles.next();</span><br><span class="line">41 </span><br><span class="line">42             BlockLocation[] blockLocations = next.getBlockLocations();</span><br><span class="line">43             int length = blockLocations.length;</span><br><span class="line">44             </span><br><span class="line">45             if(length != 0)&#123;</span><br><span class="line">46                 totalBlocks += length;</span><br><span class="line">47                 long lastBlockSize = blockLocations[length - 1].getLength();</span><br><span class="line">48                 long blockSize = next.getBlockSize();</span><br><span class="line">49                 if(lastBlockSize &lt; blockSize)&#123;</span><br><span class="line">50                     lessThenBlocksizeBlocks++;</span><br><span class="line">51                 &#125;</span><br><span class="line">52             &#125;</span><br><span class="line">53         &#125;</span><br><span class="line">54         System.out.println(lessThenBlocksizeBlocks+&quot; : &quot;+totalBlocks);</span><br><span class="line">55         </span><br><span class="line">56         return lessThenBlocksizeBlocks * 1f / totalBlocks;</span><br><span class="line">57     &#125;</span><br><span class="line">58 &#125;</span><br></pre></td></tr></table></figure>

<h2 id="13、统计出一个给定数组的蓄水总量（把数组的每个位置的数看是做地势高低）"><a href="#13、统计出一个给定数组的蓄水总量（把数组的每个位置的数看是做地势高低）" class="headerlink" title="13、统计出一个给定数组的蓄水总量（把数组的每个位置的数看是做地势高低）"></a>13、统计出一个给定数组的蓄水总量（把数组的每个位置的数看是做地势高低）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">  1 /**</span><br><span class="line">  2         统计出一个给定数组的蓄水总量（把数组的每个位置的数看是做地势高低）</span><br><span class="line">  3         比如：int[] intArray = new int[]&#123;4,3,2,5,6,4,4,7&#125;</span><br><span class="line">  4         能蓄水：[0,1,2,0,0,2,2,0] 所以总量是：7</span><br><span class="line">  5         </span><br><span class="line">  6     核心思路：把数组切成很多个 01数组，每一层一个01数组，统计每个01数组中的合法0的总个数（数组的左边第一个1的中间区间中的0的个数）即可</span><br><span class="line">  7  */</span><br><span class="line">  8 public class Exam5_WaterStoreOfArray &#123;</span><br><span class="line">  9 </span><br><span class="line"> 10     public static void main(String[] args) &#123;</span><br><span class="line"> 11         </span><br><span class="line"> 12 //        int[] intArray = new int[]&#123;4,3,2,5,6,4,4,7&#125;;</span><br><span class="line"> 13 //        int[] intArray = new int[]&#123;1,2,3,4,5,6&#125;;</span><br><span class="line"> 14         int[] intArray = new int[]&#123;3,1,2,7,3,8,4,9,5,6&#125;;</span><br><span class="line"> 15         </span><br><span class="line"> 16         int totalWater = getArrayWater(intArray);</span><br><span class="line"> 17         System.out.println(totalWater);</span><br><span class="line"> 18     &#125;</span><br><span class="line"> 19     </span><br><span class="line"> 20     /**</span><br><span class="line"> 21      * 求出数组中的水数</span><br><span class="line"> 22      */</span><br><span class="line"> 23     private static int getArrayWater(int[] intArray) &#123;</span><br><span class="line"> 24         </span><br><span class="line"> 25         int findMaxValueOfArray = findMaxValueOfArray(intArray);</span><br><span class="line"> 26         int findMinValueOfArray = findMinValueOfArray(intArray);</span><br><span class="line"> 27         int length = intArray.length;</span><br><span class="line"> 28         </span><br><span class="line"> 29         int totalWater = 0;</span><br><span class="line"> 30         </span><br><span class="line"> 31         // 循环次数就是最大值和最小值的差</span><br><span class="line"> 32         for(int i=findMinValueOfArray; i&lt;findMaxValueOfArray; i++)&#123;</span><br><span class="line"> 33             // 循环构造每一层的01数组</span><br><span class="line"> 34             int[] tempArray = new int[length];</span><br><span class="line"> 35             for(int j=0; j&lt;length; j++)&#123;</span><br><span class="line"> 36                 if(intArray[j] &gt; i)&#123;</span><br><span class="line"> 37                     tempArray[j] = 1;</span><br><span class="line"> 38                 &#125;else&#123;</span><br><span class="line"> 39                     tempArray[j] = 0;</span><br><span class="line"> 40                 &#125;</span><br><span class="line"> 41             &#125;</span><br><span class="line"> 42             // 获取每一个01数组的合法0个数</span><br><span class="line"> 43             int waterOfOneZeroArray = getWaterOfOneZeroArray(tempArray);</span><br><span class="line"> 44             totalWater += waterOfOneZeroArray;</span><br><span class="line"> 45         &#125;</span><br><span class="line"> 46         return totalWater;</span><br><span class="line"> 47     &#125;</span><br><span class="line"> 48     </span><br><span class="line"> 49 </span><br><span class="line"> 50     /**</span><br><span class="line"> 51      * 寻找逻辑是：从左右开始各找一个1，然后这两个1之间的所有0的个数，就是水数</span><br><span class="line"> 52      */</span><br><span class="line"> 53     private static int getWaterOfOneZeroArray(int[] tempArray) &#123;</span><br><span class="line"> 54         </span><br><span class="line"> 55         int length = tempArray.length;</span><br><span class="line"> 56         int toatalWater = 0;</span><br><span class="line"> 57         </span><br><span class="line"> 58         // 找左边的1</span><br><span class="line"> 59         int i = 0;</span><br><span class="line"> 60         while(i &lt; length)&#123;</span><br><span class="line"> 61             if(tempArray[i] == 1)&#123;</span><br><span class="line"> 62                 break;</span><br><span class="line"> 63             &#125;</span><br><span class="line"> 64             i++;</span><br><span class="line"> 65         &#125;</span><br><span class="line"> 66         </span><br><span class="line"> 67         // 从右边开始找1</span><br><span class="line"> 68         int j=length-1;</span><br><span class="line"> 69         while(j &gt;= i)&#123;</span><br><span class="line"> 70             if(tempArray[j] == 1)&#123;</span><br><span class="line"> 71                 break;</span><br><span class="line"> 72             &#125;</span><br><span class="line"> 73             j--;</span><br><span class="line"> 74         &#125;</span><br><span class="line"> 75         </span><br><span class="line"> 76         // 找以上两个1之间的0的个数。</span><br><span class="line"> 77         if(i == j || i + 1 == j)&#123;</span><br><span class="line"> 78             return 0;</span><br><span class="line"> 79         &#125;else&#123;</span><br><span class="line"> 80             for(int k=i+1; k&lt;j; k++)&#123;</span><br><span class="line"> 81                 if(tempArray[k] == 0)&#123;</span><br><span class="line"> 82                     toatalWater++;</span><br><span class="line"> 83                 &#125;</span><br><span class="line"> 84             &#125;</span><br><span class="line"> 85             return toatalWater;</span><br><span class="line"> 86         &#125;</span><br><span class="line"> 87     &#125;</span><br><span class="line"> 88 </span><br><span class="line"> 89     /**</span><br><span class="line"> 90      * </span><br><span class="line"> 91      * 描述：找出一个数组中的最大值</span><br><span class="line"> 92      */</span><br><span class="line"> 93     public static int findMaxValueOfArray(int[] intArray)&#123;</span><br><span class="line"> 94         int length = intArray.length;</span><br><span class="line"> 95         if(length == 0)&#123;</span><br><span class="line"> 96             return 0;</span><br><span class="line"> 97         &#125;else if(length == 1)&#123;</span><br><span class="line"> 98             return intArray[0];</span><br><span class="line"> 99         &#125;else&#123;</span><br><span class="line">100             int max = intArray[0];</span><br><span class="line">101             for(int i=1; i&lt;length; i++)&#123;</span><br><span class="line">102                 if(intArray[i] &gt; max)&#123;</span><br><span class="line">103                     max = intArray[i];</span><br><span class="line">104                 &#125;</span><br><span class="line">105             &#125;</span><br><span class="line">106             return max;</span><br><span class="line">107         &#125;</span><br><span class="line">108     &#125;</span><br><span class="line">109     </span><br><span class="line">110     /**</span><br><span class="line">111      * 找出一个数组中的最小值</span><br><span class="line">112      */</span><br><span class="line">113     public static int findMinValueOfArray(int[] intArray)&#123;</span><br><span class="line">114         int length = intArray.length;</span><br><span class="line">115         if(length == 0)&#123;</span><br><span class="line">116             return 0;</span><br><span class="line">117         &#125;else if(length == 1)&#123;</span><br><span class="line">118             return intArray[0];</span><br><span class="line">119         &#125;else&#123;</span><br><span class="line">120             int min = intArray[0];</span><br><span class="line">121             for(int i=1; i&lt;length; i++)&#123;</span><br><span class="line">122                 if(intArray[i] &lt; min)&#123;</span><br><span class="line">123                     min = intArray[i];</span><br><span class="line">124                 &#125;</span><br><span class="line">125             &#125;</span><br><span class="line">126             return min;</span><br><span class="line">127         &#125;</span><br><span class="line">128     &#125;</span><br><span class="line">129 &#125;</span><br></pre></td></tr></table></figure>
    </div>
    
    <div class="post-footer">
        <div>
            
            转载声明：
            商业转载请联系作者获得授权,非商业转载请注明出处 © <a href="https://github.com/wenxinzhang" target="_blank">福星</a>
            
            
        </div>
        <div>
            
        </div>
    </div>
</article>
<div class="article-nav prev-next-wrap clearfix">
    
    <a href="/2018-04-11-Hadoop学习之路（十一）HDFS的读写详解.html" class="pre-post btn btn-default" title='Hadoop学习之路（十一）HDFS的读写详解'>
        <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
        <span class="hidden-xs">
            Hadoop学习之路（十一）HDFS的读写详解</span>
    </a>
    
    
    <a href="/2018-04-09-Hadoop学习之路（九）HDFS深入理解.html" class="next-post btn btn-default" title='Hadoop学习之路（九）HDFS深入理解'>
        <span class="hidden-lg">下一篇</span>
        <span class="hidden-xs">
            Hadoop学习之路（九）HDFS深入理解</span><i class="fa fa-angle-right fa-fw"></i>
    </a>
    
</div>

<div id="comments">
    

<div id="vcomments" class="valine"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="/assets/valine.min.js"></script>
<script>
new Valine({
    av: AV,
    el: '#vcomments',
    appId: 'UckE9LEIQ8aoa3MH1Kio27rB-gzGzoHsz',
    appKey: '7HC9xCVYQdshKqFRDmULFm5G',
    placeholder: '说点什么吧',
    notify: false,
    verify: true,
    avatar: 'mm',
    meta: 'nick,mail'.split(','),
    pageSize: '10',
    path: window.location.pathname,
    lang: 'zh-CN'.toLowerCase()
})
</script>


</div>

                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">
            文章目录
        </h3>
        
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、文件的上传和下载"><span class="toc-text">1、文件的上传和下载</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、配置文件conf"><span class="toc-text">2、配置文件conf</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#输出结果"><span class="toc-text">输出结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、列出指定目录下的文件以及块的信息"><span class="toc-text">3、列出指定目录下的文件以及块的信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、上传文件"><span class="toc-text">4、上传文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5、下载文件"><span class="toc-text">5、下载文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6、删除某个路径下特定类型的文件，比如class类型文件，比如txt类型文件"><span class="toc-text">6、删除某个路径下特定类型的文件，比如class类型文件，比如txt类型文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7、删除HDFS集群中的所有空文件和空目录"><span class="toc-text">7、删除HDFS集群中的所有空文件和空目录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8、手动拷贝某个特定的数据块（比如某个文件的第二个数据块）"><span class="toc-text">8、手动拷贝某个特定的数据块（比如某个文件的第二个数据块）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9、编写程序统计出HDFS文件系统中文件大小小于HDFS集群中的默认块大小的文件占比"><span class="toc-text">9、编写程序统计出HDFS文件系统中文件大小小于HDFS集群中的默认块大小的文件占比</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10、编写程序统计出HDFS文件系统中的平均数据块数（数据块总数-文件总数）"><span class="toc-text">10、编写程序统计出HDFS文件系统中的平均数据块数（数据块总数/文件总数）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11、编写程序统计出HDFS文件系统中的平均副本数（副本总数-总数据块数）"><span class="toc-text">11、编写程序统计出HDFS文件系统中的平均副本数（副本总数/总数据块数）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12、统计HDFS整个文件系统中的不足指定数据块大小的数据块的比例"><span class="toc-text">12、统计HDFS整个文件系统中的不足指定数据块大小的数据块的比例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13、统计出一个给定数组的蓄水总量（把数组的每个位置的数看是做地势高低）"><span class="toc-text">13、统计出一个给定数组的蓄水总量（把数组的每个位置的数看是做地势高低）</span></a></li></ol>
        
    </div>
</aside>
                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>
<a id="back-to-top" class="icon-btn hide">
    <i class="fa fa-chevron-up"></i>
</a>
    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="busuanzi">
    
    访问量:
    <strong id="busuanzi_value_site_pv">
        <i class="fa fa-spinner fa-spin"></i>
    </strong>
    &nbsp; | &nbsp;
    访客数:
    <strong id="busuanzi_value_site_uv">
        <i class="fa fa-spinner fa-spin"></i>
    </strong>
    
</div>
            </div>
            <div class="col-sm-12">
                <span>Copyright &copy;
                    2018
                </span> |
                <span>
                    Powered by <a href="//hexo.io" class="copyright-links" target="_blank" rel="nofollow">Hexo</a>
                </span> |
                <span>
                    Theme by <a href="//github.com/shenliyang/hexo-theme-snippet.git" class="copyright-links" target="_blank" rel="nofollow">Snippet</a>
                </span>
            </div>
        </div>
    </div>
</div>



<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<script src="/js/app.js?rev=@@hash"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":200,"height":350},"mobile":{"show":true}});</script></body>
</html>