<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Parquet文件存储格式</title>
      <link href="/2019-09-18-Parquet%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F.html"/>
      <url>/2019-09-18-Parquet%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F.html</url>
      
        <content type="html"><![CDATA[<p>** Parquet文件存储格式：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Parquet文件存储格式</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h1 id="一、Parquet的组成"><a href="#一、Parquet的组成" class="headerlink" title="一、Parquet的组成"></a>一、<strong>Parquet的组成</strong></h1><p>Parquet仅仅是一种存储格式，它是语言、平台无关的，并且不需要和任何一种数据处理框架绑定，目前能够和Parquet适配的组件包括下面这些，可以看出基本上通常使用的查询引擎和计算框架都已适配，并且可以很方便的将其它序列化工具生成的数据转换成Parquet格式。</p><ul><li>查询引擎: Hive, Impala, Pig, Presto, Drill, Tajo, HAWQ, IBM Big SQL</li><li>计算框架: MapReduce, Spark, Cascading, Crunch, Scalding, Kite</li><li>数据模型: Avro, Thrift, Protocol Buffers, POJOs</li></ul><h2 id="项目组成"><a href="#项目组成" class="headerlink" title="项目组成"></a>项目组成</h2><p>Parquet项目由以下几个子项目组成:</p><ul><li><a href="https://github.com/apache/parquet-format" target="_blank" rel="noopener">parquet-format</a>项目由java实现，它定义了所有Parquet元数据对象，Parquet的元数据是使用Apache Thrift进行序列化并存储在Parquet文件的尾部。</li><li><a href="https://github.com/apache/parquet-mr" target="_blank" rel="noopener">parquet-format</a>项目由java实现，它包括多个模块，包括实现了读写Parquet文件的功能，并且提供一些和其它组件适配的工具，例如Hadoop Input/Output Formats、Hive Serde(目前Hive已经自带Parquet了)、Pig loaders等。</li><li><a href="https://github.com/Parquet/parquet-compatibility" target="_blank" rel="noopener">parquet-compatibility</a>项目，包含不同编程语言之间(JAVA和C/C++)读写文件的测试代码。</li><li><a href="https://github.com/apache/parquet-cpp" target="_blank" rel="noopener">parquet-cpp</a>项目，它是用于用于读写Parquet文件的C++库。</li></ul><p>下图展示了Parquet各个组件的层次以及从上到下交互的方式。</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017105343302-1717603971.png" alt="img"></p><ul><li>数据存储层定义了Parquet的文件格式，其中元数据在parquet-format中定义，包括Parquet原始类型定义、Page类型、编码类型、压缩类型等等。</li><li>对象转换层完成其他对象模型与Parquet内部数据模型的映射和转换，Parquet的编码方式使用的是striping and assembly算法。</li><li>对象模型层定义了如何读取Parquet文件的内容，这一层转换包括Avro、Thrift、PB等序列化格式、Hive serde等的适配。并且为了帮助大家理解和使用，Parquet提供了org.apache.parquet.example包实现了java对象和Parquet文件的转换。</li></ul><h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a><strong>数据模型</strong></h2><p>Parquet支持嵌套的数据模型，类似于Protocol Buffers，每一个数据模型的schema包含多个字段，每一个字段又可以包含多个字段，每一个字段有三个属性：重复数、数据类型和字段名，重复数可以是以下三种：required(出现1次)，repeated(出现0次或多次)，optional(出现0次或1次)。每一个字段的数据类型可以分成两种：group(复杂类型)和primitive(基本类型)。例如Dremel中提供的Document的schema示例，它的定义如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">message <span class="type">Document</span> &#123;</span><br><span class="line">    required int64 <span class="type">DocId</span>;</span><br><span class="line">    optional group <span class="type">Links</span> &#123;</span><br><span class="line">        repeated int64 <span class="type">Backward</span>;</span><br><span class="line">        repeated int64 <span class="type">Forward</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    repeated group <span class="type">Name</span> &#123;</span><br><span class="line">        repeated group <span class="type">Language</span> &#123;</span><br><span class="line">            required string <span class="type">Code</span>;</span><br><span class="line">            optional string <span class="type">Country</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        optional string <span class="type">Url</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以把这个Schema转换成树状结构，根节点可以理解为repeated类型，如下图: </p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017105738912-789367511.png" alt="img"></p><p>可以看出在Schema中所有的基本类型字段都是叶子节点，在这个Schema中一共存在6个叶子节点，如果把这样的Schema转换成扁平式的关系模型，就可以理解为该表包含六个列。Parquet中没有Map、Array这样的复杂数据结构，但是可以通过repeated和group组合来实现这样的需求。在这个包含6个字段的表中有以下几个字段和每一条记录中它们可能出现的次数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DocId                 int64    只能出现一次 </span><br><span class="line">Links.Backward        int64    可能出现任意多次，但是如果出现0次则需要使用NULL标识 </span><br><span class="line">Links.Forward         int64    同上 </span><br><span class="line">Name.Language.Code    string   同上 </span><br><span class="line">Name.Language.Country string   同上 </span><br><span class="line">Name.Url              string   同上</span><br></pre></td></tr></table></figure><p>由于在一个表中可能存在出现任意多次的列，对于这些列需要标示出现多次或者等于NULL的情况，它是由Striping/Assembly算法实现的。</p><h2 id="Striping-Assembly算法"><a href="#Striping-Assembly算法" class="headerlink" title="Striping/Assembly算法"></a>Striping/Assembly算法</h2><p>上文介绍了Parquet的数据模型，在Document中存在多个非required列，由于Parquet一条记录的数据分散的存储在不同的列中，如何组合不同的列值组成一条记录是由Striping/Assembly算法决定的，在该算法中列的每一个值都包含三部分：value、repetition level和definition level。</p><h3 id="Repetition-Levels"><a href="#Repetition-Levels" class="headerlink" title="Repetition Levels"></a><strong>Repetition Levels</strong></h3><p>为了支持repeated类型的节点，在写入的时候该值等于它和前面的值在哪一层节点是不共享的。在读取的时候根据该值可以推导出哪一层上需要创建一个新的节点，例如对于这样的一个schema和两条记录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">message nested &#123;</span><br><span class="line">     repeated group leve1 &#123;</span><br><span class="line">          repeated string leve2;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line">r1:[[a,b,c,] , [d,e,f,g]]</span><br><span class="line">r2:[[h] , [i,j]]</span><br></pre></td></tr></table></figure><p>计算repetition level值的过程如下：</p><ul><li>value=a是一条记录的开始，和前面的值(已经没有值了)在根节点(第0层)上是不共享的，所以repeated level=0.</li><li>value=b它和前面的值共享了level1这个节点，但是level2这个节点上是不共享的，所以repeated level=2.</li><li>同理value=c, repeated level=2.</li><li>value=d和前面的值共享了根节点(属于相同记录)，但是在level1这个节点上是不共享的，所以repeated level=1.</li><li>value=h和前面的值不属于同一条记录，也就是不共享任何节点，所以repeated level=0.</li></ul><p>根据以上的分析每一个value需要记录的repeated level值如下：</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017111957865-335321751.png" alt="img"></p><p>在读取的时候，顺序的读取每一个值，然后根据它的repeated level创建对象，当读取value=a时repeated level=0，表示需要创建一个新的根节点(新记录)，value=b时repeated level=2，表示需要创建一个新的level2节点，value=d时repeated level=1，表示需要创建一个新的level1节点，当所有列读取完成之后可以创建一条新的记录。本例中当读取文件构建每条记录的结果如下：</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017112045865-798020169.png" alt="img"></p><p>可以看出repeated level=0表示一条记录的开始，并且repeated level的值只是针对路径上的repeated类型的节点，因此在计算该值的时候可以忽略非repeated类型的节点，在写入的时候将其理解为该节点和路径上的哪一个repeated节点是不共享的，读取的时候将其理解为需要在哪一层创建一个新的repeated节点，这样的话每一列最大的repeated level值就等于路径上的repeated节点的个数（不包括根节点）。减小repeated level的好处能够使得在存储使用更加紧凑的编码方式，节省存储空间。</p><h3 id="Definition-Levels"><a href="#Definition-Levels" class="headerlink" title="Definition Levels"></a><strong>Definition Levels</strong></h3><p>有了repeated level我们就可以构造出一个记录了，为什么还需要definition levels呢？由于repeated和optional类型的存在，可能一条记录中某一列是没有值的，假设我们不记录这样的值就会导致本该属于下一条记录的值被当做当前记录的一部分，从而造成数据的错误，因此对于这种情况需要一个占位符标示这种情况。</p><p>definition level的值仅仅对于空值是有效的，表示在该值的路径上第几层开始是未定义的，对于非空的值它是没有意义的，因为非空值在叶子节点是定义的，所有的父节点也肯定是定义的，因此它总是等于该列最大的definition levels。例如下面的schema。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">message <span class="type">ExampleDefinitionLevel</span> &#123;</span><br><span class="line">  optional group a &#123;</span><br><span class="line">    optional group b &#123;</span><br><span class="line">      optional string c;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>它包含一个列a.b.c，这个列的的每一个节点都是optional类型的，当c被定义时a和b肯定都是已定义的，当c未定义时我们就需要标示出在从哪一层开始时未定义的，如下面的值：</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017112121037-646635962.png" alt="img"></p><p>由于definition level只需要考虑未定义的值，而对于repeated类型的节点，只要父节点是已定义的，该节点就必须定义（例如Document中的DocId，每一条记录都该列都必须有值，同样对于Language节点，只要它定义了Code必须有值），所以计算definition level的值时可以忽略路径上的required节点，这样可以减小definition level的最大值，优化存储。</p><h3 id="一个完整的例子"><a href="#一个完整的例子" class="headerlink" title="一个完整的例子"></a><strong>一个完整的例子</strong></h3><p>本节我们使用Dremel论文中给的Document示例和给定的两个值r1和r2展示计算repeated level和definition level的过程，这里把未定义的值记录为NULL，使用R表示repeated level，D表示definition level。</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017112320177-629116810.png" alt="img"></p><p>首先看DocuId这一列，对于r1，DocId=10，由于它是记录的开始并且是已定义的，所以R=0，D=0，同样r2中的DocId=20，R=0，D=0。</p><p>对于Links.Forward这一列，在r1中，它是未定义的但是Links是已定义的，并且是该记录中的第一个值，所以R=0，D=1，在r1中该列有两个值，value1=10，R=0(记录中该列的第一个值)，D=2(该列的最大definition level)。</p><p>对于Name.Url这一列，r1中它有三个值，分别为url1=’<a href="http://a/" target="_blank" rel="noopener">http://A</a>‘，它是r1中该列的第一个值并且是定义的，所以R=0，D=2；value2=’<a href="http://b/" target="_blank" rel="noopener">http://B</a>‘，和上一个值value1在Name这一层是不相同的，所以R=1，D=2；value3=NULL，和上一个值value2在Name这一层是不相同的，所以R=1，但它是未定义的，而Name这一层是定义的，所以D=1。r2中该列只有一个值value3=’<a href="http://c/" target="_blank" rel="noopener">http://C</a>‘，R=0，D=2.</p><p>最后看一下Name.Language.Code这一列，r1中有4个值，value1=’en-us’，它是r1中的第一个值并且是已定义的，所以R=0，D=2(由于Code是required类型，这一列repeated level的最大值等于2)；value2=’en’，它和value1在Language这个节点是不共享的，所以R=2，D=2；value3=NULL，它是未定义的，但是它和前一个值在Name这个节点是不共享的，在Name这个节点是已定义的，所以R=1，D=1；value4=’en-gb’，它和前一个值在Name这一层不共享，所以R=1，D=2。在r2中该列有一个值，它是未定义的，但是Name这一层是已定义的，所以R=0，D=1.</p><h2 id="Parquet文件格式"><a href="#Parquet文件格式" class="headerlink" title="Parquet文件格式"></a>Parquet文件格式</h2><p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。在HDFS文件系统和Parquet文件中存在如下几个概念。</p><ul><li>HDFS块(Block)：它是HDFS上的最小的副本单位，HDFS会把一个Block存储在本地的一个文件并且维护分散在不同的机器上的多个副本，通常情况下一个Block的大小为256M、512M等。</li><li>HDFS文件(File)：一个HDFS的文件，包括数据和元数据，数据分散存储在多个Block中。</li><li>行组(Row Group)：按照行将数据物理上划分为多个单元，每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，Parquet读写的时候会将整个行组缓存在内存中，所以如果每一个行组的大小是由内存大的小决定的，例如记录占用空间比较小的Schema可以在每一个行组中存储更多的行。</li><li>列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。</li><li>页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。</li></ul><h3 id="文件格式"><a href="#文件格式" class="headerlink" title="文件格式"></a><strong>文件格式</strong></h3><p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017112835787-354192287.png" alt="img"></p><p>上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页，但是在后面的版本中增加。</p><p>在执行MR任务的时候可能存在多个Mapper任务的输入是同一个Parquet文件的情况，每一个Mapper通过InputSplit标示处理的文件范围，如果多个InputSplit跨越了一个Row Group，Parquet能够保证一个Row Group只会被一个Mapper任务处理。</p><h3 id="映射下推-Project-PushDown"><a href="#映射下推-Project-PushDown" class="headerlink" title="映射下推(Project PushDown)"></a><strong>映射下推(Project PushDown)</strong></h3><p>说到列式存储的优势，映射下推是最突出的，它意味着在获取表中原始数据时只需要扫描查询中需要的列，由于每一列的所有值都是连续存储的，所以分区取出每一列的所有值就可以实现TableScan算子，而避免扫描整个表文件内容。</p><p>在Parquet中原生就支持映射下推，执行查询的时候可以通过Configuration传递需要读取的列的信息，这些列必须是Schema的子集，映射每次会扫描一个Row Group的数据，然后一次性得将该Row Group里所有需要的列的Cloumn Chunk都读取到内存中，每次读取一个Row Group的数据能够大大降低随机读的次数，除此之外，Parquet在读取的时候会考虑列是否连续，如果某些需要的列是存储位置是连续的，那么一次读操作就可以把多个列的数据读取到内存。</p><h3 id="谓词下推-Predicate-PushDown"><a href="#谓词下推-Predicate-PushDown" class="headerlink" title="谓词下推(Predicate PushDown)"></a><strong>谓词下推(Predicate PushDown)</strong></h3><p>在数据库之类的查询系统中最常用的优化手段就是谓词下推了，通过将一些过滤条件尽可能的在最底层执行可以减少每一层交互的数据量，从而提升性能，例如”select count(1) from A Join B on A.id = B.id where A.a &gt; 10 and B.b &lt; 100”SQL查询中，在处理Join操作之前需要首先对A和B执行TableScan操作，然后再进行Join，再执行过滤，最后计算聚合函数返回，但是如果把过滤条件A.a &gt; 10和B.b &lt; 100分别移到A表的TableScan和B表的TableScan的时候执行，可以大大降低Join操作的输入数据。</p><p>无论是行式存储还是列式存储，都可以在将过滤条件在读取一条记录之后执行以判断该记录是否需要返回给调用者，在Parquet做了更进一步的优化，优化的方法时对每一个Row Group的每一个Column Chunk在存储的时候都计算对应的统计信息，包括该Column Chunk的最大值、最小值和空值个数。通过这些统计值和该列的过滤条件可以判断该Row Group是否需要扫描。另外Parquet未来还会增加诸如Bloom Filter和Index等优化数据，更加有效的完成谓词下推。</p><p>在使用Parquet的时候可以通过如下两种策略提升查询性能：1、类似于关系数据库的主键，对需要频繁过滤的列设置为有序的，这样在导入数据的时候会根据该列的顺序存储数据，这样可以最大化的利用最大值、最小值实现谓词下推。2、减小行组大小和页大小，这样增加跳过整个行组的可能性，但是此时需要权衡由于压缩和编码效率下降带来的I/O负载。</p><h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a><strong>性能</strong></h2><p>相比传统的行式存储，Hadoop生态圈近年来也涌现出诸如RC、ORC、Parquet的列式存储格式，它们的性能优势主要体现在两个方面：1、更高的压缩比，由于相同类型的数据更容易针对不同类型的列使用高效的编码和压缩方式。2、更小的I/O操作，由于映射下推和谓词下推的使用，可以减少一大部分不必要的数据扫描，尤其是表结构比较庞大的时候更加明显，由此也能够带来更好的查询性能</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017122313865-1783718133.png" alt="img"></p><p>上图是展示了使用不同格式存储TPC-H和TPC-DS数据集中两个表数据的文件大小对比，可以看出Parquet较之于其他的二进制文件存储格式能够更有效的利用存储空间，而新版本的Parquet(2.0版本)使用了更加高效的页存储方式，进一步的提升存储空间</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017122414756-63812379.png" alt="img"></p><p>上图展示了Twitter在Impala中使用不同格式文件执行TPC-DS基准测试的结果，测试结果可以看出Parquet较之于其他的行式存储格式有较明显的性能提升。</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017122448099-998445606.png" alt="img"></p><p>上图展示了criteo公司在Hive中使用ORC和Parquet两种列式存储格式执行TPC-DS基准测试的结果，测试结果可以看出在数据存储方面，两种存储格式在都是用snappy压缩的情况下量中存储格式占用的空间相差并不大，查询的结果显示Parquet格式稍好于ORC格式，两者在功能上也都有优缺点，Parquet原生支持嵌套式数据结构，而ORC对此支持的较差，这种复杂的Schema查询也相对较差；而Parquet不支持数据的修改和ACID，但是ORC对此提供支持，但是在OLAP环境下很少会对单条数据修改，更多的则是批量导入。</p><h2 id="项目发展"><a href="#项目发展" class="headerlink" title="项目发展"></a><strong>项目发展</strong></h2><p>自从2012年由Twitter和Cloudera共同研发Parquet开始，该项目一直处于高速发展之中，并且在项目之初就将其贡献给开源社区，2013年，Criteo公司加入开发并且向Hive社区提交了向hive集成Parquet的patch(HIVE-5783)，在Hive 0.13版本之后正式加入了Parquet的支持；之后越来越多的查询引擎对此进行支持，也进一步带动了Parquet的发展。</p><p>目前Parquet正处于向2.0版本迈进的阶段，在新的版本中实现了新的Page存储格式，针对不同的类型优化编码算法，另外丰富了支持的原始类型，增加了Decimal、Timestamp等类型的支持，增加更加丰富的统计信息，例如Bloon Filter，能够尽可能得将谓词下推在元数据层完成。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>本文介绍了一种支持嵌套数据模型对的列式存储系统Parquet，作为大数据系统中OLAP查询的优化方案，它已经被多种查询引擎原生支持，并且部分高性能引擎将其作为默认的文件存储格式。通过数据编码和压缩，以及映射下推和谓词下推功能，Parquet的性能也较之其它文件格式有所提升，可以预见，随着数据模型的丰富和Ad hoc查询的需求，Parquet将会被更广泛的使用。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据集网站汇总</title>
      <link href="/2019-09-17-%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BD%91%E7%AB%99%E6%B1%87%E6%80%BB.html"/>
      <url>/2019-09-17-%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BD%91%E7%AB%99%E6%B1%87%E6%80%BB.html</url>
      
        <content type="html"><![CDATA[<p>** 数据集网站汇总：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        如果用一个句子总结学习数据科学的本质，那就是： 学习数据科学的最佳方法就是应用数据科学。 如果你是一个初学者，你每完成一个新项目后自身能力都会有极大的提高，如果你是一个有经验的数据科学专家，你已经知道这里所蕴含的价值。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/GO1fHyfFYWF82xt3de5wIJs2RUMBRe1YicaOF4Ck0W6cVhfZUfcjbiaUwN0z3WAwia5Mw26cicQCDO7ChQFgI6r9aw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>一.如何使用这些资源?</strong></p><p>如何使用这些数据源是没有限制的，应用和使用只受到您的创造力和实际应用。使用它们最简单的方法是进行数据项目并在网站上发布它们。这不仅能提高你的数据和可视化技能，还能改善你的结构化思维。</p><p>另一方面，如果你正在考虑/处理基于数据的产品，这些数据集可以通过提供额外的/新的输入数据来增加您的产品的功能。所以，继续在这些项目上工作吧，与更大的世界分享它们，以展示你的数据能力!我们已经在不同的部分中划分了这些数据源，以帮助你根据应用程序对数据源进行分类。</p><p>我们从简单、通用和易于处理数据集开始，然后转向大型/行业相关数据集。然后，我们为特定的目的——文本挖掘、图像分类、推荐引擎等提供数据集的链接。这将为您提供一个完整的数据资源列表。如果你能想到这些数据集的任何应用，或者知道我们漏掉了什么流行的资源，请在下面的评论中与我们分享。(部分可能需要翻墙)</p><p><strong>二.由简单和通用的数据集开始</strong></p><p><strong>1.data.gov ( <a href="https://www.data.gov/" target="_blank" rel="noopener">https://www.data.gov/</a> )</strong> 这是美国政府公开数据的所在地，该站点包含了超过19万的数据点。这些数据集不同于气候、教育、能源、金融和更多领域的数据。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/fYsJGst9n2SFp5EEPia57ExkNWwXWsicrnJTTCUmh7E71ULmu3QO5RMv88CEoCDYZwenOrOtZqQ1shvY7VnibicCAg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>2.data.gov.in ( <a href="https://data.gov.in/" target="_blank" rel="noopener">https://data.gov.in/</a> )</strong> 这是印度政府公开数据的所在地，通过各种行业、气候、医疗保健等来寻找数据，你可以在这里找到一些灵感。根据你居住的国家的不同，你也可以从其他一些网站上浏览类似的网站。</p><p><strong>3.World Bank( <a href="http://data.worldbank.org/" target="_blank" rel="noopener">http://data.worldbank.org/</a> )</strong> 世界银行的开放数据。该平台提供 Open Data Catalog，世界发展指数，教育指数等几个工具。</p><p><strong>4.RBI ( <a href="https://rbi.org.in/Scripts/Statistics.aspx" target="_blank" rel="noopener">https://rbi.org.in/Scripts/Statistics.aspx</a> )</strong> 印度储备银行提供的数据。这包括了货币市场操作、收支平衡、银行使用和一些产品的几个指标。</p><p><strong>5.Five Thirty Eight Datasets ( <a href="https://github.com/fivethirtyeight/data" target="_blank" rel="noopener">https://github.com/fivethirtyeight/data</a> ) Five Thirty Eight，</strong>亦称作 538，专注与民意调查分析，政治，经济与体育的博客。该数据集为 Five Thirty Eight Datasets 使用的数据集。每个数据集包括数据，解释数据的字典和Five Thirty Eight 文章的链接。如果你想学习如何创建数据故事，没有比这个更好。</p><p><strong>三.大型数据集</strong></p><p><strong>1.Amazon Web Services(AWS)datasets ( <a href="https://aws.amazon.com/cn/datasets/" target="_blank" rel="noopener">https://aws.amazon.com/cn/datasets/</a> )</strong>Amazon提供了一些大数据集，可以在他们的平台上使用，也可以在本地计算机上使用。您还可以通过EMR使用EC2和Hadoop来分析云中的数据。在亚马逊上流行的数据集包括完整的安然电子邮件数据集，Google Books n-gram，NASA NEX 数据集，百万歌曲数据集等。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/fYsJGst9n2SFp5EEPia57ExkNWwXWsicrnDT397LtcdcHrCTC3ktMqELeEGypv5YeUq5lN21WcJSr7V5fXoicPMMQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>2.Google datasets ( <a href="https://cloud.google.com/bigquery/public-data/" target="_blank" rel="noopener">https://cloud.google.com/bigquery/public-data/</a> )</strong> Google 提供了一些数据集作为其 Big Query 工具的一部分。包括 GitHub 公共资料库的数据，Hacker News 的所有故事和评论。</p><p><strong>3.Youtube labeled Video Dataset ( <a href="https://research.google.com/youtube8m/" target="_blank" rel="noopener">https://research.google.com/youtube8m/</a> )</strong> 几个月前，谷歌研究小组发布了YouTube上的“数据集”，它由800万个YouTube视频id和4800个视觉实体的相关标签组成。它来自数十亿帧的预先计算的，最先进的视觉特征。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/fYsJGst9n2SFp5EEPia57ExkNWwXWsicrnJTzJa7jgNoyANvhLkHMBeBj9Q35gTJVgEgxn3HryJqILaFcwRDiadQA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>四.预测建模与机器学习数据集</strong></p><p><strong>1.UCI Machine Learning Repository ( <a href="https://archive.ics.uci.edu/ml/datasets.html" target="_blank" rel="noopener">https://archive.ics.uci.edu/ml/datasets.html</a> )</strong>UCI机器学习库显然是最著名的数据存储库。如果您正在寻找与机器学习存储库相关的数据集，通常是首选的地方。这些数据集包括了各种各样的数据集，从像Iris和泰坦尼克这样的流行数据集到最近的贡献，比如空气质量和GPS轨迹。存储库包含超过350个与域名类似的数据集(分类/回归)。您可以使用这些过滤器来确定您需要的数据集。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/fYsJGst9n2SFp5EEPia57ExkNWwXWsicrnibJib29micqeLe5P2s97EKaKsK7icibTFssTU1vA6CSKrSYUeZG6VVZTW2g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>2.Kaggle ( <a href="https://www.kaggle.com/datasets" target="_blank" rel="noopener">https://www.kaggle.com/datasets</a> )</strong> Kaggle提出了一个平台，人们可以贡献数据集，其他社区成员可以投票并运行内核/脚本。他们总共有超过350个数据集——有超过200个特征数据集。虽然一些最初的数据集通常出现在其他地方，但我在平台上看到了一些有趣的数据集，而不是在其他地方出现。与新的数据集一起，界面的另一个好处是，您可以在相同的界面上看到来自社区成员的脚本和问题。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/fYsJGst9n2SFp5EEPia57ExkNWwXWsicrn8GYRYjzkIDAcpeG3nVeMlp1WMicVgoZrYEFv6oWFvxpPibkAkofUqRVQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>3.Analytics Vidhya (<a href="https://datahack.analyticsvidhya.com/contest/all/" target="_blank" rel="noopener">https://datahack.analyticsvidhya.com/contest/all/</a> )</strong> 您可以从我们的实践问题和黑客马拉松问题中参与和下载数据集。问题数据集基于真实的行业问题，并且相对较小，因为它们意味着2 - 7天的黑客马拉松。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/fYsJGst9n2SFp5EEPia57ExkNWwXWsicrnZ0VAM13phGYueXFS3x02bMbS5PAwSJfEaP5a4qBZLMKjQoLVUnsfIA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>4.Quandl ( <a href="https://www.quandl.com/" target="_blank" rel="noopener">https://www.quandl.com/</a> )</strong> Quandl 通过起网站、API 或一些工具的直接集成提供了不同来源的财务、经济和替代数据。他们的数据集分为开放和付费。所有开放数据集为免费，但高级数据集需要付费。通过搜索仍然可以在平台上找到优质数据集。例如，来自印度的证券交易所数据是免费的。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/fYsJGst9n2SFp5EEPia57ExkNWwXWsicrnc7mnwosDNnYiaSUdIZrAOAdh0lgBlaf8H9Orelt6o535sRFC5aGyMMQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>5.Past KDD Cups ( <a href="http://www.kdd.org/kdd-cup" target="_blank" rel="noopener">http://www.kdd.org/kdd-cup</a> )</strong> KDD Cup 是 ACM Special Interest Group 组织的年度数据挖掘和知识发现竞赛。</p><p><strong>6.Driven Data ( <a href="https://www.drivendata.org/" target="_blank" rel="noopener">https://www.drivendata.org/</a> )</strong> Driven Data 发现运用数据科学带来积极社会影响的现实问题。然后，他们为数据科学家组织在线模拟竞赛，从而开发出最好的模型来解决这些问题。</p><p><strong>五.图像分类数据集</strong></p><p><strong>1.The MNIST Database ( <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/</a> )</strong> 最流行的图像识别数据集，使用手写数字。它包括6万个示例和1万个示例的测试集。这通常是第一个进行图像识别的数据集。</p><p><strong>2.Chars74K (<a href="http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/" target="_blank" rel="noopener">http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/</a> )</strong> 这里是下一阶段的进化，如果你已经通过了手写的数字。该数据集包括自然图像中的字符识别。数据集包含74,000个图像，因此数据集的名称。</p><p><strong>3.Frontal Face Images (<a href="http://vasc.ri.cmu.edu//idb/html/face/frontal_images/index.html" target="_blank" rel="noopener">http://vasc.ri.cmu.edu//idb/html/face/frontal_images/index.html</a> )</strong> 如果你已经完成了前两个项目，并且能够识别数字和字符，这是图像识别中的下一个挑战级别——正面人脸图像。这些图像是由CMU &amp; MIT收集的，排列在四个文件夹中。</p><p><strong>4.ImageNet ( <a href="http://image-net.org/" target="_blank" rel="noopener">http://image-net.org/</a> )</strong> 现在是时候构建一些通用的东西了。根据WordNet层次结构组织的图像数据库(目前仅为名词)。层次结构的每个节点都由数百个图像描述。目前，该集合平均每个节点有超过500个图像(而且还在增加)。</p><p><strong>六.文本分类数据集</strong></p><p><strong>1.Spam – Non Spam (<a href="http://www.esp.uem.es/jmgomez/smsspamcorpus/" target="_blank" rel="noopener">http://www.esp.uem.es/jmgomez/smsspamcorpus/</a>)</strong> 区分短信是否为垃圾邮件是一个有趣的问题。你需要构建一个分类器将短信进行分类。</p><p><strong>2.Twitter Sentiment Analysis (<a href="http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/" target="_blank" rel="noopener">http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/</a>)</strong> 该数据集包含 1578627 个分类推文，每行被标记为1的积极情绪，0位负面情绪。数据依次基于 Kaggle 比赛和 Nick Sanders 的分析。</p><p><strong>3.Movie Review Data (<a href="http://www.cs.cornell.edu/People/pabo/movie-review-data/" target="_blank" rel="noopener">http://www.cs.cornell.edu/People/pabo/movie-review-data/</a>)</strong> 这个网站提供了一系列的电影评论文件，这些文件标注了他们的总体情绪极性(正面或负面)或主观评价(例如，“两个半明星”)和对其主观性地位(主观或客观)或极性的标签。</p><p><strong>七.推荐引擎数据集</strong></p><p><strong>1.MovieLens ( <a href="https://grouplens.org/" target="_blank" rel="noopener">https://grouplens.org/</a> ) MovieLens</strong> 是一个帮助人们查找电影的网站。它有成千上万的注册用户。他们进行自动内容推荐，推荐界面，基于标签的推荐页面等在线实验。这些数据集可供下载，可用于创建自己的推荐系统。</p><p><strong>2.Jester (<a href="http://www.ieor.berkeley.edu/~goldberg/jester-data/" target="_blank" rel="noopener">http://www.ieor.berkeley.edu/~goldberg/jester-data/</a>)</strong> 在线笑话推荐系统。</p><p><strong>八.各种来源的数据集网站</strong></p><p><strong>1.KDNuggets (<a href="http://www.kdnuggets.com/datasets/index.html" target="_blank" rel="noopener">http://www.kdnuggets.com/datasets/index.html</a>)</strong> KDNuggets 的数据集页面一直是人们搜索数据集的参考。列表全面，但是某些来源不再提供数据集。因此，需要谨慎选择数据集和来源。</p><p><strong>2.Awesome Public Datasets (<a href="https://github.com/caesar0301/awesome-public-datasets" target="_blank" rel="noopener">https://github.com/caesar0301/awesome-public-datasets</a>)</strong> 一个GitHub存储库，它包含一个由域分类的完整的数据集列表。数据集被整齐地分类在不同的领域，这是非常有用的。但是，对于存储库本身的数据集没有描述，这可能使它非常有用。</p><p><strong>3.Reddit Datasets Subreddit (<a href="https://www.reddit.com/r/datasets/" target="_blank" rel="noopener">https://www.reddit.com/r/datasets/</a>)</strong> 由于这是一个社区驱动的论坛，它可能会遇到一些麻烦(与之前的两个来源相比)。但是，您可以通过流行/投票来对数据集进行排序，以查看最流行的数据集。另外，它还有一些有趣的数据集和讨论。</p><p><strong>九.结尾的话</strong></p><p>我们希望这一资源清单对于那些想项目的人来说是非常有用的。这绝对是一个金矿，好好加以利用吧!</p><p>转自：<a href="https://mp.weixin.qq.com/s?__biz=MzI2MjM2MDEzNQ==&amp;mid=2247489072&amp;idx=1&amp;sn=2ac46ef358be4eef43f3de8670086746&amp;chksm=ea4d0b18dd3a820ef82122648806c8516970e8e7323efb5475aa0db1da1752d22ee8c38ec604&amp;mpshare=1&amp;scene=23&amp;srcid=042625ULmfK6xU66wcmkCf1G#rd" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzI2MjM2MDEzNQ==&amp;mid=2247489072&amp;idx=1&amp;sn=2ac46ef358be4eef43f3de8670086746&amp;chksm=ea4d0b18dd3a820ef82122648806c8516970e8e7323efb5475aa0db1da1752d22ee8c38ec604&amp;mpshare=1&amp;scene=23&amp;srcid=042625ULmfK6xU66wcmkCf1G#rd</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vim常用按键大全</title>
      <link href="/2019-09-16-Vim%E5%B8%B8%E7%94%A8%E6%8C%89%E9%94%AE%E5%A4%A7%E5%85%A8.html"/>
      <url>/2019-09-16-Vim%E5%B8%B8%E7%94%A8%E6%8C%89%E9%94%AE%E5%A4%A7%E5%85%A8.html</url>
      
        <content type="html"><![CDATA[<p>** Vim常用按键大全：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Vim常用按键大全</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>Vim完全可以用键盘进行操作。本文将常用的按键归纳总结。</p><h4 id="第一部分：一般模式可用的按钮，如光标移动、复制粘贴、查找替换等"><a href="#第一部分：一般模式可用的按钮，如光标移动、复制粘贴、查找替换等" class="headerlink" title="第一部分：一般模式可用的按钮，如光标移动、复制粘贴、查找替换等"></a>第一部分：一般模式可用的按钮，如光标移动、复制粘贴、查找替换等</h4><h5 id="移动光标的方法"><a href="#移动光标的方法" class="headerlink" title="移动光标的方法"></a>移动光标的方法</h5><table><thead><tr><th>h, j, k, l</th><th>光标向左，下，上，右移动</th></tr></thead><tbody><tr><td>Ctrl + f / b</td><td>屏幕向下/上移动</td></tr><tr><td>Ctrl + d / u</td><td>屏幕向下/上移动半页</td></tr><tr><td>0</td><td>移动到一行的最前面</td></tr><tr><td>$</td><td>移动到一行的最后面字符</td></tr><tr><td>H / M / L</td><td>移动到屏幕最上方/中央/最下方那一行的第一个字符</td></tr><tr><td>G</td><td>移动到文件的最后一行</td></tr><tr><td>nG / ngg</td><td>移动到文件的第n行</td></tr><tr><td>gg</td><td>移动到文件的第一行</td></tr><tr><td>n[Enter]</td><td>向下移动n行</td></tr></tbody></table><h4 id="查找与替换"><a href="#查找与替换" class="headerlink" title="查找与替换"></a>查找与替换</h4><table><thead><tr><th>/word</th><th>向下查找word字符串</th></tr></thead><tbody><tr><td>?word</td><td>向上查找word字符串</td></tr><tr><td>n</td><td>代表重复前一个查找动作</td></tr><tr><td>N</td><td>代表反向重复前一个查找动作</td></tr><tr><td>: s/old/new</td><td>将第一个old替换为new</td></tr><tr><td>: s/old/new/g</td><td>将一行中所有的old替换为new</td></tr><tr><td>:n1, n2s/word1/word2/g</td><td>将行n1与n2之间的word1替换为word2</td></tr><tr><td>:%s/old/new/g</td><td>将文件所有的old替换为new</td></tr><tr><td>:%s/old/new/gc</td><td>替换前要求确认</td></tr></tbody></table><h5 id="删除、复制与粘贴"><a href="#删除、复制与粘贴" class="headerlink" title="删除、复制与粘贴"></a>删除、复制与粘贴</h5><table><thead><tr><th>x/X</th><th>向后/前删除一个字符</th></tr></thead><tbody><tr><td>nx</td><td>连续删除n个字符</td></tr><tr><td>dd</td><td>删除整行</td></tr><tr><td>ndd</td><td>删除n行</td></tr><tr><td>d1G</td><td>删除光标所在到第一行数据</td></tr><tr><td>dG</td><td>删除光标所在到最后一行数据</td></tr><tr><td>d$</td><td>删除光标所在到该行最后一个字符</td></tr><tr><td>d0</td><td>删除光标所在到该行最前面一个字符</td></tr><tr><td>yy</td><td>复制光标所在的一行</td></tr><tr><td>nyy</td><td>向下复制n行</td></tr><tr><td>y1G</td><td>复制光标所在到第一行数据</td></tr><tr><td>yG</td><td>复制光标所在到最后一行数据</td></tr><tr><td>y$</td><td>复制光标所在到该行最后一个字符</td></tr><tr><td>y0</td><td>复制光标所在到该行最前面一个字符</td></tr><tr><td>p/P</td><td>粘贴数据在光标下/上一行</td></tr><tr><td>J</td><td>将光标所在行与下一行数据结合成同一行</td></tr><tr><td>u</td><td>回撤前一操作</td></tr><tr><td>Ctrl + r</td><td>重做前一操作</td></tr><tr><td>.</td><td>重复前一个操作</td></tr></tbody></table><h4 id="第二部分：一般模式切换到编辑模式"><a href="#第二部分：一般模式切换到编辑模式" class="headerlink" title="第二部分：一般模式切换到编辑模式"></a>第二部分：一般模式切换到编辑模式</h4><h4 id="进入插入或替换的编辑模式"><a href="#进入插入或替换的编辑模式" class="headerlink" title="进入插入或替换的编辑模式"></a>进入插入或替换的编辑模式</h4><table><thead><tr><th>i, I</th><th>进入插入模式： i从当前光标所在处插入，I在目前所在行的第一个非空格符处插入</th></tr></thead><tbody><tr><td>a, A</td><td>进入插入模式： a从当前光标所在的下一个字符插入，A从光标所在行的最后一个字符后插入</td></tr><tr><td>o, O</td><td>进入插入模式： o从当前光标所在行的下一行插入新的一行；O正好相反，从上一行插入新行</td></tr><tr><td>r, R</td><td>进入替换模式： r只会替换光标所在的那一个字符一次；R会一直替换光标所在文字，直到Esc</td></tr></tbody></table><h4 id="块选择"><a href="#块选择" class="headerlink" title="块选择"></a>块选择</h4><table><thead><tr><th>v</th><th>字符选择，将光标经过的地方反白选择</th></tr></thead><tbody><tr><td>V</td><td>行选择，将光标经过的行反白选择</td></tr><tr><td>Ctrl + v</td><td>块选择，可以用长方形选择数据</td></tr><tr><td>y</td><td>将反白的地方复制</td></tr><tr><td>d</td><td>删除反白的地方</td></tr></tbody></table><h4 id="多窗口"><a href="#多窗口" class="headerlink" title="多窗口"></a>多窗口</h4><table><thead><tr><th>：sp filename</th><th>打开新窗口，如果有加filename,新窗口打开新文件，否则打开相同文件</th></tr></thead><tbody><tr><td>Ctrl + w + s/v</td><td>水平/垂直分割打开新窗口</td></tr><tr><td>Ctrl + w + h/j/k/l</td><td>光标移动到左/下/上/右窗口</td></tr><tr><td>Ctrl + w + q</td><td>退出窗口</td></tr></tbody></table><h4 id="vim常用命令示意图"><a href="#vim常用命令示意图" class="headerlink" title="vim常用命令示意图"></a>vim常用命令示意图</h4><p><a href="http://images2015.cnblogs.com/blog/435059/201512/435059-20151225151800734-1593095043.jpg" target="_blank" rel="noopener"><img src="https://images2015.cnblogs.com/blog/435059/201512/435059-20151225151801609-170255026.jpg" alt="vim-commands"></a></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CM+CDH离线安装</title>
      <link href="/CDH-hadoop.html"/>
      <url>/CDH-hadoop.html</url>
      
        <content type="html"><![CDATA[<p>** CM+CDH离线安装：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Cloudera Manager可以轻松管理任何生产规模的Hadoop部署。通过直观的用户界面快速部署，配置和监控群集 - 完成滚动升级，备份和灾难恢复以及可定制警报。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1-Cloudera-简介"><a href="#1-1-Cloudera-简介" class="headerlink" title="1.1 Cloudera 简介"></a>1.1 Cloudera 简介</h2><h3 id="1-1-1Cloudera-简介"><a href="#1-1-1Cloudera-简介" class="headerlink" title="1.1.1Cloudera 简介"></a>1.1.1Cloudera 简介</h3><p>官网：<a href="https://www.cloudera.com/" target="_blank" rel="noopener">https://www.cloudera.com/</a></p><p>文档：<a href="https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_intro.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_intro.html</a></p><p>​        CDH是Apache Hadoop和相关项目中最完整，经过测试和最流行的发行版。CDH提供了Hadoop的核心元素 - 可扩展存储和分布式计算 - 以及基于Web的用户界面和重要的企业功能。CDH是Apache许可的开源软件，是唯一提供统一批处理，交互式SQL和交互式搜索以及基于角色的访问控制的Hadoop解决方案。</p><p>CDH提供：</p><ul><li><p>灵活性 - 存储任何类型的数据并使用各种不同的计算框架对其进行操作，包括批处理，交互式SQL，自由文本搜索，机器学习和统计计算。</p></li><li><p>集成 - 在完整的Hadoop平台上快速启动和运行，该平台可与各种硬件和软件解决方案配合使用。</p></li><li><p>安全 - 处理和控制敏感数据。</p></li><li><p>可扩展性 - 支持广泛的应用程序，并扩展和扩展它们以满足您的要求。</p></li><li><p>高可用性 - 充满信心地执行任务关键型业务任务。</p></li><li><p>兼容性 - 利用您现有的IT基础架构和投资。</p><p><img src="https://www.cloudera.com/documentation/enterprise/latest/images/cdh.png" alt="img"></p></li></ul><h3 id="1-1-2Hadoop起源"><a href="#1-1-2Hadoop起源" class="headerlink" title="1.1.2Hadoop起源"></a>1.1.2Hadoop起源</h3><p>​        2004年，Cutting和同为程序员出身的Mike Cafarella决定开发一款可以代替当时的主流搜索产品的开源搜索引擎，这个项目被命名为Nutch。Nutch基于Lucence实现的搜索引擎，能够从互联网上抓取网页数据。抓取来的海量数据的存储问题。但是，这些海量数据都是非结构化数据，不能存在关系型数据库里。如果连数据的存储和管理都解决不了化，就谈不上后续为用户提供搜索服务，包括通过算法去优化检索速度。那时的Cutting正处在Freelancer的生涯 中，所以他希望自己的项目能通过一种低开销的方式来构建网页中的大量算法。幸运的是，Google这时正好发布了一项研究报告，报告中介绍了两款Google为支持自家的搜索引擎而开发的软件平台。这两个平台一个是GFS（Google File System），用于存储不同设备所产生的海量数据；另一个是MapReduce，它运行在GFS之上，负责分布式大规模数据。</p><p>​        2006年cutting根据《Google File System》设计了Nutch的HDFS,hadoop distributed file system。</p><p>Hadoop最开始是nutch的子项目，目的是解决nutch的海量数据存储问题。在nutch 0.8版本之后，Hadoop独立处理，成为一个独立的项目。后来，又根据《Google MapReduce》设计了基于HDFS的MapRedce计算框架。</p><p>2006年之后，cutting带着Hadoop去了雅虎，当时有100多人的团队共同帮cutting完善hadoop。后来yahoo把Hadoop贡献了Apache。所以，现在Hadoop是Apache的顶级项目。</p><h2 id="2-1Hadoop搭建"><a href="#2-1Hadoop搭建" class="headerlink" title="2.1Hadoop搭建"></a>2.1Hadoop搭建</h2><p><strong>Hadoop的三种运行模式</strong> ：</p><ol><li><p>独立（本地）运行模式：无需任何守护进程，所有的程序都运行在同一个JVM上执行。在独立模式下调试MR程序非常高效方便。所以一般该模式主要是在学习或者开发阶段调试使用 。</p></li><li><p>伪分布式模式：  Hadoop守护进程运行在本地机器上，模拟一个小规模的集群，换句话说，可以配置一台机器的Hadoop集群,伪分布式是完全分布式的一个特例。</p></li><li><p>完全分布式模式：Hadoop守护进程运行在一个集群上。</p></li></ol><h2 id="3-1-单机伪分布模式"><a href="#3-1-单机伪分布模式" class="headerlink" title="3.1 单机伪分布模式"></a>3.1 单机伪分布模式</h2><p>​    只支持MapReduce，不支持HDFS。这种模式一般用于调试MapReduce任务用的。</p><h3 id="3-1-1-准备Linux环境，最低的工作内存1G"><a href="#3-1-1-准备Linux环境，最低的工作内存1G" class="headerlink" title="3.1.1 准备Linux环境，最低的工作内存1G"></a>3.1.1 准备Linux环境，最低的工作内存1G</h3><p>内容详见：Vmware安装Centos6.9文档</p><h3 id="3-1-2-关闭防火墙"><a href="#3-1-2-关闭防火墙" class="headerlink" title="3.1.2  关闭防火墙"></a>3.1.2  关闭防火墙</h3><p>临时关闭防火墙：service iptables stop</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br></pre></td></tr></table></figure><p>永久关闭防火墙：chkconfig iptables off </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font>永久修改防火墙需要重启，永久和临时同时执行，可以不用重启。</p><h3 id="3-1-3-配置主机名"><a href="#3-1-3-配置主机名" class="headerlink" title="3.1.3 配置主机名"></a>3.1.3 配置主机名</h3><p>查询主机名称：hostname</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname</span><br></pre></td></tr></table></figure><p>临时修改主机名：hostname  <strong><name></name></strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname &lt;name&gt;</span><br></pre></td></tr></table></figure><p>永久修改主机名：vim /etc/sysconfig/network</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><h3 id="3-1-4-配置hosts文件"><a href="#3-1-4-配置hosts文件" class="headerlink" title="3.1.4 配置hosts文件"></a>3.1.4 配置hosts文件</h3><p>执行: vim /etc/hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><ol><li>不要删除前两行内容。</li><li>IP在前，主机名在后。</li></ol><h3 id="3-1-5-配置免密码登录"><a href="#3-1-5-配置免密码登录" class="headerlink" title="3.1.5 配置免密码登录"></a>3.1.5 配置免密码登录</h3><h4 id="3-1-5-1-免密登陆原理"><a href="#3-1-5-1-免密登陆原理" class="headerlink" title="3.1.5.1 免密登陆原理"></a>3.1.5.1 免密登陆原理</h4><ol><li><p>A机器生成公钥和私钥</p></li><li><p>机器A发送自己的公钥到机器B，这个时候机器B有了机器A的公钥</p></li><li><p>机器B发送一个随机的字符串向机器A</p></li><li><p>机器A利用自己的私钥把字符串加密</p></li><li><p>机器A把加密后的字符串再次发送给机器B</p></li><li><p>机器B利用公钥解密字符串，如果和原来的一样，则OK。</p></li></ol><h4 id="3-1-5-1-免密登陆实现"><a href="#3-1-5-1-免密登陆实现" class="headerlink" title="3.1.5.1 免密登陆实现"></a>3.1.5.1 免密登陆实现</h4><ol><li><p>生成自己的公钥和私钥  ssh-keygen</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure></li><li><p>把生成的公钥copy到远程机器上 ssh-copy-id root@hadoop01</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id root@hadoop01</span><br></pre></td></tr></table></figure></li></ol><p><font color="red">注意：</font>如果是单机的伪分布式环境，自己节点也需要配置免密登录。</p><h3 id="3-1-6-安装和配置jdk"><a href="#3-1-6-安装和配置jdk" class="headerlink" title="3.1.6 安装和配置jdk"></a>3.1.6 安装和配置jdk</h3><ol><li><p>执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>在尾行添加 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASS_PATH</span><br></pre></td></tr></table></figure></li></ol><p>保存退出  :wq</p><ol start="3"><li><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>java -version 查看JDK版本信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-7-上传和安装hadoop"><a href="#3-1-7-上传和安装hadoop" class="headerlink" title="3.1.7 上传和安装hadoop"></a>3.1.7 上传和安装hadoop</h3><p>下载地址：<a href="http://hadoop.apache.org/releases.html" target="_blank" rel="noopener">http://hadoop.apache.org/releases.html</a></p><p><font color="red">注意：</font></p><p>source表示源码</p><p>binary表示二级制包（安装包）</p><h4 id="3-1-7-1-解压Hadoop文件包"><a href="#3-1-7-1-解压Hadoop文件包" class="headerlink" title="3.1.7.1 解压Hadoop文件包"></a>3.1.7.1 解压Hadoop文件包</h4><p>执行：tar -zxvf hadoop-2.7.1_64bit.tar.gz</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-2.7.1_64bit.tar.gz</span><br></pre></td></tr></table></figure><h4 id="3-1-7-2-Hadoop目录说明"><a href="#3-1-7-2-Hadoop目录说明" class="headerlink" title="3.1.7.2 Hadoop目录说明"></a>3.1.7.2 Hadoop目录说明</h4><p>bin目录：命令脚本</p><p>etc/hadoop:存放hadoop的配置文件</p><p>lib目录：hadoop运行的依赖jar包</p><p>sbin目录：启动和关闭hadoop等命令都在这里</p><p>libexec目录：存放的也是hadoop命令，但一般不常用</p><p><font color="red">注意：</font>最常用的就是bin和etc目录。</p><h3 id="3-1-8-配置hadoop配置文件"><a href="#3-1-8-配置hadoop配置文件" class="headerlink" title="3.1.8 配置hadoop配置文件"></a>3.1.8 配置hadoop配置文件</h3><p>Hadoop目录下<strong>/home/hadoop-2.7.1/etc/hadoop/</strong>目录下<strong>6个文件</strong></p><h4 id="3-1-8-1-hadoop-env-sh"><a href="#3-1-8-1-hadoop-env-sh" class="headerlink" title="3.1.8.1 hadoop-env.sh"></a>3.1.8.1 hadoop-env.sh</h4><p>执行：vim hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br></pre></td></tr></table></figure><p> 修改：修改java_home路径和hadoop_conf_dir 路径  25行  33行</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#25行</span><br><span class="line">export JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">#33行</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop-2.7.1/etc/hadoop</span><br></pre></td></tr></table></figure><p> 然后执行：source hadoop-env.sh编译文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source hadoop-env.sh</span><br></pre></td></tr></table></figure><h4 id="3-1-8-2-core-site-xml"><a href="#3-1-8-2-core-site-xml" class="headerlink" title="3.1.8.2 core-site.xml"></a>3.1.8.2 core-site.xml</h4><p>命令行执行：vim core-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim core-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hdfs的老大，namenode的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://tedu:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hadoop运行时产生文件的存放目录--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop-2.7.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-3-hdfs-site-xml"><a href="#3-1-8-3-hdfs-site-xml" class="headerlink" title="3.1.8.3 hdfs-site .xml"></a>3.1.8.3 hdfs-site .xml</h4><p>命令行执行：vim hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hdfs-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定hdfs保存数据副本的数量，包括自己，默认值是3--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--如果是伪分布模式，此值是1--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--设置hdfs的操作权限，false表示任何用户都可以在hdfs上操作文件--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-4-mapred-site-xml"><a href="#3-1-8-4-mapred-site-xml" class="headerlink" title="3.1.8.4 mapred-site.xml"></a>3.1.8.4 mapred-site.xml</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line"></span><br><span class="line">vim mapred-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定mapreduce运行在yarn上--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-5-yarn-site-xml"><a href="#3-1-8-5-yarn-site-xml" class="headerlink" title="3.1.8.5 yarn-site.xml"></a>3.1.8.5 yarn-site.xml</h4><p>命令行执行：vim yarn-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim yarn-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定yarn的老大 resoucemanager的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>tedu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--NodeManager获取数据的方式--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-  services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-6-slaves"><a href="#3-1-8-6-slaves" class="headerlink" title="3.1.8.6 slaves"></a>3.1.8.6 slaves</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves</span><br></pre></td></tr></table></figure><p><strong>修改主机名</strong></p><h3 id="3-1-9-配置hadoop的环境变量"><a href="#3-1-9-配置hadoop的环境变量" class="headerlink" title="3.1.9 配置hadoop的环境变量"></a>3.1.9 配置hadoop的环境变量</h3><ol><li><p>文件最后追加文件</p><p><strong>HADOOP_HOME=/home/hadoop-2.7.1</strong></p><p><strong>export HADOOP_HOME</strong></p></li><li><p>source /etc/profile 使更改的配置立即生效。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">HADOOP_HOME=/home/hadoop-2.7.1</span><br><span class="line">PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASSPATH HADOOP_HOME</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-10-格式化Namenode"><a href="#3-1-10-格式化Namenode" class="headerlink" title="3.1.10 格式化Namenode"></a>3.1.10 格式化Namenode</h3><p>执行：hdfs namenode -format</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>如果不好使，可以重启linux</p><p>当出现：successfully，证明格式化成功。</p><h3 id="3-1-11-启动Hadoop"><a href="#3-1-11-启动Hadoop" class="headerlink" title="3.1.11 启动Hadoop"></a>3.1.11 启动Hadoop</h3><p>在/home/hadoop-2.7.1/sbin目录下</p><p>执行:./start-all.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><h3 id="3-1-12-验证启动成功"><a href="#3-1-12-验证启动成功" class="headerlink" title="3.1.12 验证启动成功"></a>3.1.12 验证启动成功</h3><p>可以访问网址： <a href="http://192.168.220.128:50070" target="_blank" rel="noopener">http://192.168.220.128:50070</a></p>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop伪分布式搭建</title>
      <link href="/hadoop-single.html"/>
      <url>/hadoop-single.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop伪分布式搭建：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        大数据（Big Data），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。<br>​        大数据的定义是4Vs：数据量大、处理速度快、数据源多样、真实性。用中文简单描述就是大、快、多、真。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1Hadoop简介"><a href="#1-1Hadoop简介" class="headerlink" title="1.1Hadoop简介"></a>1.1Hadoop简介</h2><h3 id="1-1-1Hadoop创始人"><a href="#1-1-1Hadoop创始人" class="headerlink" title="1.1.1Hadoop创始人"></a>1.1.1Hadoop创始人</h3><p>​        1985年，<strong>Doug Cutting</strong>毕业于美国斯坦福大学。他并不是一开始就决心投身IT行业的，在大学时代的头两年，Cutting学习了诸如物理、地理等常规课程。因为学费的压力，Cutting开始意识到，自己必须学习一些更加实用、有趣的技能。这样，一方面可以帮助自己还清贷款，另一方面，也是为自己未来的生活做打算。因为斯坦福大学座落在IT行业的“圣地”硅谷，所以学习软件对年轻人来说是再自然不过的事情了。 1997年底，Cutting开始以每周两天的时间投入，在家里试着用Java把这个想法变成现实，不久之后，Lucene诞生了。作为第一个提供全文文本搜索的开源函数库，Lucene的伟大自不必多言。</p><p>Doug Cutting是<strong>Lucence,Nutch,Hadoop</strong>的创始人。</p><h3 id="1-1-2Hadoop起源"><a href="#1-1-2Hadoop起源" class="headerlink" title="1.1.2Hadoop起源"></a>1.1.2Hadoop起源</h3><p>​        2004年，Cutting和同为程序员出身的Mike Cafarella决定开发一款可以代替当时的主流搜索产品的开源搜索引擎，这个项目被命名为Nutch。Nutch基于Lucence实现的搜索引擎，能够从互联网上抓取网页数据。抓取来的海量数据的存储问题。但是，这些海量数据都是非结构化数据，不能存在关系型数据库里。如果连数据的存储和管理都解决不了化，就谈不上后续为用户提供搜索服务，包括通过算法去优化检索速度。那时的Cutting正处在Freelancer的生涯 中，所以他希望自己的项目能通过一种低开销的方式来构建网页中的大量算法。幸运的是，Google这时正好发布了一项研究报告，报告中介绍了两款Google为支持自家的搜索引擎而开发的软件平台。这两个平台一个是GFS（Google File System），用于存储不同设备所产生的海量数据；另一个是MapReduce，它运行在GFS之上，负责分布式大规模数据。</p><p>​        2006年cutting根据《Google File System》设计了Nutch的HDFS,hadoop distributed file system。</p><p>Hadoop最开始是nutch的子项目，目的是解决nutch的海量数据存储问题。在nutch 0.8版本之后，Hadoop独立处理，成为一个独立的项目。后来，又根据《Google MapReduce》设计了基于HDFS的MapRedce计算框架。</p><p>2006年之后，cutting带着Hadoop去了雅虎，当时有100多人的团队共同帮cutting完善hadoop。后来yahoo把Hadoop贡献了Apache。所以，现在Hadoop是Apache的顶级项目。</p><h2 id="2-1Hadoop搭建"><a href="#2-1Hadoop搭建" class="headerlink" title="2.1Hadoop搭建"></a>2.1Hadoop搭建</h2><p><strong>Hadoop的三种运行模式</strong> ：</p><ol><li><p>独立（本地）运行模式：无需任何守护进程，所有的程序都运行在同一个JVM上执行。在独立模式下调试MR程序非常高效方便。所以一般该模式主要是在学习或者开发阶段调试使用 。</p></li><li><p>伪分布式模式：  Hadoop守护进程运行在本地机器上，模拟一个小规模的集群，换句话说，可以配置一台机器的Hadoop集群,伪分布式是完全分布式的一个特例。</p></li><li><p>完全分布式模式：Hadoop守护进程运行在一个集群上。</p></li></ol><h2 id="3-1-单机伪分布模式"><a href="#3-1-单机伪分布模式" class="headerlink" title="3.1 单机伪分布模式"></a>3.1 单机伪分布模式</h2><p>​    只支持MapReduce，不支持HDFS。这种模式一般用于调试MapReduce任务用的。</p><h3 id="3-1-1-准备Linux环境，最低的工作内存1G"><a href="#3-1-1-准备Linux环境，最低的工作内存1G" class="headerlink" title="3.1.1 准备Linux环境，最低的工作内存1G"></a>3.1.1 准备Linux环境，最低的工作内存1G</h3><p>内容详见：Vmware安装Centos6.9文档</p><h3 id="3-1-2-关闭防火墙"><a href="#3-1-2-关闭防火墙" class="headerlink" title="3.1.2  关闭防火墙"></a>3.1.2  关闭防火墙</h3><p>临时关闭防火墙：service iptables stop</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br></pre></td></tr></table></figure><p>永久关闭防火墙：chkconfig iptables off </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font>永久修改防火墙需要重启，永久和临时同时执行，可以不用重启。</p><h3 id="3-1-3-配置主机名"><a href="#3-1-3-配置主机名" class="headerlink" title="3.1.3 配置主机名"></a>3.1.3 配置主机名</h3><p>查询主机名称：hostname</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname</span><br></pre></td></tr></table></figure><p>临时修改主机名：hostname  <strong><name></name></strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname &lt;name&gt;</span><br></pre></td></tr></table></figure><p>永久修改主机名：vim /etc/sysconfig/network</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><h3 id="3-1-4-配置hosts文件"><a href="#3-1-4-配置hosts文件" class="headerlink" title="3.1.4 配置hosts文件"></a>3.1.4 配置hosts文件</h3><p>执行: vim /etc/hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><ol><li>不要删除前两行内容。</li><li>IP在前，主机名在后。</li></ol><h3 id="3-1-5-配置免密码登录"><a href="#3-1-5-配置免密码登录" class="headerlink" title="3.1.5 配置免密码登录"></a>3.1.5 配置免密码登录</h3><h4 id="3-1-5-1-免密登陆原理"><a href="#3-1-5-1-免密登陆原理" class="headerlink" title="3.1.5.1 免密登陆原理"></a>3.1.5.1 免密登陆原理</h4><ol><li><p>A机器生成公钥和私钥</p></li><li><p>机器A发送自己的公钥到机器B，这个时候机器B有了机器A的公钥</p></li><li><p>机器B发送一个随机的字符串向机器A</p></li><li><p>机器A利用自己的私钥把字符串加密</p></li><li><p>机器A把加密后的字符串再次发送给机器B</p></li><li><p>机器B利用公钥解密字符串，如果和原来的一样，则OK。</p></li></ol><h4 id="3-1-5-1-免密登陆实现"><a href="#3-1-5-1-免密登陆实现" class="headerlink" title="3.1.5.1 免密登陆实现"></a>3.1.5.1 免密登陆实现</h4><ol><li><p>生成自己的公钥和私钥  ssh-keygen</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure></li><li><p>把生成的公钥copy到远程机器上 ssh-copy-id root@hadoop01</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id root@hadoop01</span><br></pre></td></tr></table></figure></li></ol><p><font color="red">注意：</font>如果是单机的伪分布式环境，自己节点也需要配置免密登录。</p><h3 id="3-1-6-安装和配置jdk"><a href="#3-1-6-安装和配置jdk" class="headerlink" title="3.1.6 安装和配置jdk"></a>3.1.6 安装和配置jdk</h3><ol><li><p>执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>在尾行添加 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASS_PATH</span><br></pre></td></tr></table></figure></li></ol><p>保存退出  :wq</p><ol start="3"><li><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>java -version 查看JDK版本信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-7-上传和安装hadoop"><a href="#3-1-7-上传和安装hadoop" class="headerlink" title="3.1.7 上传和安装hadoop"></a>3.1.7 上传和安装hadoop</h3><p>下载地址：<a href="http://hadoop.apache.org/releases.html" target="_blank" rel="noopener">http://hadoop.apache.org/releases.html</a></p><p><font color="red">注意：</font></p><p>source表示源码</p><p>binary表示二级制包（安装包）</p><h4 id="3-1-7-1-解压Hadoop文件包"><a href="#3-1-7-1-解压Hadoop文件包" class="headerlink" title="3.1.7.1 解压Hadoop文件包"></a>3.1.7.1 解压Hadoop文件包</h4><p>执行：tar -zxvf hadoop-2.7.1_64bit.tar.gz</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-2.7.1_64bit.tar.gz</span><br></pre></td></tr></table></figure><h4 id="3-1-7-2-Hadoop目录说明"><a href="#3-1-7-2-Hadoop目录说明" class="headerlink" title="3.1.7.2 Hadoop目录说明"></a>3.1.7.2 Hadoop目录说明</h4><p>bin目录：命令脚本</p><p>etc/hadoop:存放hadoop的配置文件</p><p>lib目录：hadoop运行的依赖jar包</p><p>sbin目录：启动和关闭hadoop等命令都在这里</p><p>libexec目录：存放的也是hadoop命令，但一般不常用</p><p><font color="red">注意：</font>最常用的就是bin和etc目录。</p><h3 id="3-1-8-配置hadoop配置文件"><a href="#3-1-8-配置hadoop配置文件" class="headerlink" title="3.1.8 配置hadoop配置文件"></a>3.1.8 配置hadoop配置文件</h3><p>Hadoop目录下<strong>/home/hadoop-2.7.1/etc/hadoop/</strong>目录下<strong>6个文件</strong></p><h4 id="3-1-8-1-hadoop-env-sh"><a href="#3-1-8-1-hadoop-env-sh" class="headerlink" title="3.1.8.1 hadoop-env.sh"></a>3.1.8.1 hadoop-env.sh</h4><p>执行：vim hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br></pre></td></tr></table></figure><p> 修改：修改java_home路径和hadoop_conf_dir 路径  25行  33行</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#25行</span><br><span class="line">export JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">#33行</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop-2.7.1/etc/hadoop</span><br></pre></td></tr></table></figure><p> 然后执行：source hadoop-env.sh编译文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source hadoop-env.sh</span><br></pre></td></tr></table></figure><h4 id="3-1-8-2-core-site-xml"><a href="#3-1-8-2-core-site-xml" class="headerlink" title="3.1.8.2 core-site.xml"></a>3.1.8.2 core-site.xml</h4><p>命令行执行：vim core-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim core-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hdfs的老大，namenode的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://tedu:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hadoop运行时产生文件的存放目录--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop-2.7.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-3-hdfs-site-xml"><a href="#3-1-8-3-hdfs-site-xml" class="headerlink" title="3.1.8.3 hdfs-site .xml"></a>3.1.8.3 hdfs-site .xml</h4><p>命令行执行：vim hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hdfs-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定hdfs保存数据副本的数量，包括自己，默认值是3--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--如果是伪分布模式，此值是1--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--设置hdfs的操作权限，false表示任何用户都可以在hdfs上操作文件--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-4-mapred-site-xml"><a href="#3-1-8-4-mapred-site-xml" class="headerlink" title="3.1.8.4 mapred-site.xml"></a>3.1.8.4 mapred-site.xml</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line"></span><br><span class="line">vim mapred-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定mapreduce运行在yarn上--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-5-yarn-site-xml"><a href="#3-1-8-5-yarn-site-xml" class="headerlink" title="3.1.8.5 yarn-site.xml"></a>3.1.8.5 yarn-site.xml</h4><p>命令行执行：vim yarn-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim yarn-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定yarn的老大 resoucemanager的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>tedu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--NodeManager获取数据的方式--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-  services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-6-slaves"><a href="#3-1-8-6-slaves" class="headerlink" title="3.1.8.6 slaves"></a>3.1.8.6 slaves</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves</span><br></pre></td></tr></table></figure><p><strong>修改主机名</strong></p><h3 id="3-1-9-配置hadoop的环境变量"><a href="#3-1-9-配置hadoop的环境变量" class="headerlink" title="3.1.9 配置hadoop的环境变量"></a>3.1.9 配置hadoop的环境变量</h3><ol><li><p>文件最后追加文件</p><p><strong>HADOOP_HOME=/home/hadoop-2.7.1</strong></p><p><strong>export HADOOP_HOME</strong></p></li><li><p>source /etc/profile 使更改的配置立即生效。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">HADOOP_HOME=/home/hadoop-2.7.1</span><br><span class="line">PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASSPATH HADOOP_HOME</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-10-格式化Namenode"><a href="#3-1-10-格式化Namenode" class="headerlink" title="3.1.10 格式化Namenode"></a>3.1.10 格式化Namenode</h3><p>执行：hdfs namenode -format</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>如果不好使，可以重启linux</p><p>当出现：successfully，证明格式化成功。</p><h3 id="3-1-11-启动Hadoop"><a href="#3-1-11-启动Hadoop" class="headerlink" title="3.1.11 启动Hadoop"></a>3.1.11 启动Hadoop</h3><p>在/home/hadoop-2.7.1/sbin目录下</p><p>执行:./start-all.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><h3 id="3-1-12-验证启动成功"><a href="#3-1-12-验证启动成功" class="headerlink" title="3.1.12 验证启动成功"></a>3.1.12 验证启动成功</h3><p>可以访问网址： <a href="http://192.168.220.128:50070" target="_blank" rel="noopener">http://192.168.220.128:50070</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何选购合适的电脑</title>
      <link href="/buy-computer.html"/>
      <url>/buy-computer.html</url>
      
        <content type="html"><![CDATA[<p>** 购买合适的电脑：** &lt;Excerpt in index | 首页摘要&gt;<br>随着时代的发展，人们生活水平的提高，计算机也成为了我们工作必不可少的生产力，办公，打游戏，看电影，购物等等。市面上电脑种类根据用途又可以分为很多类，台式机、笔记本、IPAD（平板）。购买一台适合自己的电脑工作时可以如虎添翼，电脑用着舒服，心情也会舒畅许多。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="买电脑主要需求"><a href="#买电脑主要需求" class="headerlink" title="买电脑主要需求"></a>买电脑主要需求</h2><ol><li>看电影，上网（购物）   </li><li>打游戏</li><li>办公（移动办公）</li><li>平面设计（CAD）</li><li>UI(影视剪辑)</li><li>编程</li><li>其他</li></ol><h2 id="电脑配置说明"><a href="#电脑配置说明" class="headerlink" title="电脑配置说明"></a>电脑配置说明</h2><p>目前电脑配置的CPU（绝对过剩），内存Win10最低要8个G，显卡要根据自己需求一般显卡基本够用，电脑最大的瓶颈都是在硬盘上，所以现在买电脑带不带固态硬盘是我首选的配置（我对固态硬盘定义最低要128G,512G固态才是标配，毕竟固态大小会影响到一定的读写速率，还有为了保证固态寿命做系统时会留出10%的空间不划分到分区中），至于买笔记本还是台式机需要根据不同应用场景来定。台式机性能肯定远超同价位笔记本，这个是毋庸置疑的。</p><h2 id="看电影，上网（购物）"><a href="#看电影，上网（购物）" class="headerlink" title="看电影，上网（购物）"></a>看电影，上网（购物）</h2><p>对于这方面需求的一般一女生居多，看电影上网，对电脑配置要求比较低的，一般普通双核CPU，AMD、酷睿i3都可以（最好是i5），内存8G就够了（win7的话4G就够，但是Win7现在不支持更新了）。要是女生最重要的是漂亮，这里推荐DELL或者HP相对性价比会比较合适。毕竟要是要以轻薄、美观为主。要是资金充足可以考虑各家品牌的超级本。要是父母的需求的话其实买笔记本或者台式机都可以。这里不推荐苹果笔记本，因为用苹果看电影会容易热，要是妹子是苹果控或者周边产品都是苹果产品，苹果笔记本也可在考虑之列。</p><h2 id="打游戏"><a href="#打游戏" class="headerlink" title="打游戏"></a>打游戏</h2><p>游戏主机两个最主要的要求配置和扩展性，主要是CPU和显卡，我们又称之为“双烧”，建议买台式机。要是需要便携的话，外星人品牌是一个不错的考虑，笔记本显卡最好不要超过GTX2070以上，也许你会问为什么不买笔记本GTX2080的本子，一方面是贵，价格会差很多。还有就是散热问题。为了更好体验还是台式机加水冷。</p><ul><li>一般的主流网游：i5或i7处理器，内存16G，中端显卡就可以了，硬盘128G固态+1T机械起</li><li>大型单机：i7或i9处理器（水冷），内存16-32G，，显卡中高端GTX1060起，要是玩刺客信条奥德赛GTX2080Ti不用犹豫，硬盘512三星固态+1T机械（最好在配置1T的固态，毕竟游戏不小）起</li><li>发烧友：i9处理器（水冷），内存32G-64G，显卡高端GTX2080或者是多显交火，硬盘512G（三星固态PRO系列）+1T固态</li></ul><h2 id="办公"><a href="#办公" class="headerlink" title="办公"></a>办公</h2><p>用于办公的大多是商务人士，对笔记本的性能要求一般，最主要的是便携性，各大品牌的超极本都很合适，还能衬托气质，最推荐的还是联想的thinkpad系列，没钱买个E系类（基本三年就会坏），要是有资金充裕T系列或者X系列是首选配置（尤其是X系列）。</p><h2 id="平面设计（CAD）"><a href="#平面设计（CAD）" class="headerlink" title="平面设计（CAD）"></a>平面设计（CAD）</h2><p>这个是专业领域的需求，对CPU、显卡和内存、显示器都较高，能好一点就好一点。    </p><h2 id="UI-影视剪辑"><a href="#UI-影视剪辑" class="headerlink" title="UI(影视剪辑)"></a>UI(影视剪辑)</h2><p>苹果的Macbookpro 16G，512SSD（固态太小用久了会后悔的），i7处理器 最为合适。没有比苹果更适合做平面设计的电脑。Windows系统和苹果系统没得比。</p><h2 id="编程"><a href="#编程" class="headerlink" title="编程"></a>编程</h2><p>苹果的Macbookpro 16G、512SSD、i7处理器。个人推荐MAC的笔记本做编程，一用就停不下来，会上瘾。Windows系统用来打游戏就好了。<br>推荐配置：Macbookpro 16G、i7处理器（i9也是阉割版没意义）、512SSD（固态真的不能太小，512G就不大，考虑到价格没办法）、最好是能带键盘灯、Air pods耳机还是要有一个的，用了就知道不亏。经济允许最好是配置一个IPAD PRO做分屏开发可以调高效率。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>IPAD我对它的定义就是一台游戏机，不建议用IPAD看电影（用久了手会麻）。因为我不做UI我也没有体会到那只笔的好处。</p><p>还有一个设备一点光要说一下就是亚马逊的Kindle，要是你经常看小说，或者是看英文，建议有一个（前期是你不是必须要纸质书）还是很方便的，尤其是书多了的时候。IPAD优势在于pdf文档做笔记。用了就会知道两个不一样。Kindle看电子书是生活品质提升的表现。</p>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数码产品 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （十九）SparkSQL的自定义函数UDF</title>
      <link href="/2019-06-19-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89SparkSQL%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0UDF.html"/>
      <url>/2019-06-19-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89SparkSQL%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0UDF.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （十九）SparkSQL的自定义函数UDF：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （十九）SparkSQL的自定义函数UDF</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>在Spark中，也支持Hive中的自定义函数。自定义函数大致可以分为三种：</p><ul><li>UDF(User-Defined-Function)，即最基本的自定义函数，类似to_char,to_date等</li><li>UDAF（User- Defined Aggregation Funcation），用户自定义聚合函数，类似在group by之后使用的sum,avg等</li><li>UDTF(User-Defined Table-Generating Functions),用户自定义生成函数，有点像stream里面的flatMap</li></ul><p>自定义一个UDF函数需要继承UserDefinedAggregateFunction类，并实现其中的8个方法</p><p>示例</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">DataType</span>, <span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GetDistinctCityUDF</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span></span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 输入的数据类型</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"status"</span>,<span class="type">StringType</span>,<span class="literal">true</span>) :: <span class="type">Nil</span></span><br><span class="line">  )</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 缓存字段类型</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">StructType</span>(</span><br><span class="line">      <span class="type">Array</span>(</span><br><span class="line">        <span class="type">StructField</span>(<span class="string">"buffer_city_info"</span>,<span class="type">StringType</span>,<span class="literal">true</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 输出结果类型</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">StringType</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 输入类型和输出类型是否一致</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 对辅助字段进行初始化</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer.update(<span class="number">0</span>,<span class="string">""</span>)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *修改辅助字段的值</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//获取最后一次的值</span></span><br><span class="line">    <span class="keyword">var</span> last_str = buffer.getString(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">//获取当前的值</span></span><br><span class="line">    <span class="keyword">val</span> current_str = input.getString(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">//判断最后一次的值是否包含当前的值</span></span><br><span class="line">    <span class="keyword">if</span>(!last_str.contains(current_str))&#123;</span><br><span class="line">      <span class="comment">//判断是否是第一个值，是的话走if赋值，不是的话走else追加</span></span><br><span class="line">      <span class="keyword">if</span>(last_str.equals(<span class="string">""</span>))&#123;</span><br><span class="line">        last_str = current_str</span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        last_str += <span class="string">","</span> + current_str</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    buffer.update(<span class="number">0</span>,last_str)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *对分区结果进行合并</span></span><br><span class="line"><span class="comment">  * buffer1是机器hadoop1上的结果</span></span><br><span class="line"><span class="comment">  * buffer2是机器Hadoop2上的结果</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> buf1 = buffer1.getString(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> buf2 = buffer2.getString(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">//将buf2里面存在的数据而buf1里面没有的数据追加到buf1</span></span><br><span class="line">    <span class="comment">//buf2的数据按照，进行切分</span></span><br><span class="line">    <span class="keyword">for</span>(s &lt;- buf2.split(<span class="string">","</span>))&#123;</span><br><span class="line">      <span class="keyword">if</span>(!buf1.contains(s))&#123;</span><br><span class="line">        <span class="keyword">if</span>(buf1.equals(<span class="string">""</span>))&#123;</span><br><span class="line">          buf1 = s</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">          buf1 += s</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    buffer1.update(<span class="number">0</span>,buf1)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 最终的计算结果</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    buffer.getString(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注册自定义的UDF函数为临时函数</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 第一步 创建程序入口</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"AralHotProductSpark"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> hiveContext = <span class="keyword">new</span> <span class="type">HiveContext</span>(sc)　　<span class="comment">//注册成为临时函数</span></span><br><span class="line">    hiveContext.udf.register(<span class="string">"get_distinct_city"</span>,<span class="type">GetDistinctCityUDF</span>)</span><br><span class="line">　　<span class="comment">//注册成为临时函数</span></span><br><span class="line">    hiveContext.udf.register(<span class="string">"get_product_status"</span>,(str:<span class="type">String</span>) =&gt;&#123;</span><br><span class="line">      <span class="keyword">var</span> status = <span class="number">0</span></span><br><span class="line">      <span class="keyword">for</span>(s &lt;- str.split(<span class="string">","</span>))&#123;</span><br><span class="line">        <span class="keyword">if</span>(s.contains(<span class="string">"product_status"</span>))&#123;</span><br><span class="line">          status = s.split(<span class="string">":"</span>)(<span class="number">1</span>).toInt</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RDD、DataFrame和DataSet的区别是什么</title>
      <link href="/2019-06-18-DataSet%E5%92%8CDataFrame%E5%8C%BA%E5%88%AB%E5%92%8C%E8%BD%AC%E6%8D%A2.html"/>
      <url>/2019-06-18-DataSet%E5%92%8CDataFrame%E5%8C%BA%E5%88%AB%E5%92%8C%E8%BD%AC%E6%8D%A2.html</url>
      
        <content type="html"><![CDATA[<p>** RDD、DataFrame和DataSet的区别是什么：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        RDD、DataFrame和DataSet是容易产生混淆的概念，必须对其相互之间对比，才可以知道其中异同：DataFrame多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合。DataFrame是分布式的Row对象的集合。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>RDD、DataFrame和DataSet是容易产生混淆的概念，必须对其相互之间对比，才可以知道其中异同。</p><h2 id="RDD和DataFrame"><a href="#RDD和DataFrame" class="headerlink" title="RDD和DataFrame"></a>RDD和DataFrame</h2><p><img src="https://upload-images.jianshu.io/upload_images/2160494-08d7d2c7495fd300.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/607/format/webp" alt="img"></p><p>RDD-DataFrame</p><p>上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解 Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。</p><h3 id="提升执行效率"><a href="#提升执行效率" class="headerlink" title="提升执行效率"></a>提升执行效率</h3><p>RDD API是函数式的，强调不变性，在大部分场景下倾向于创建新对象而不是修改老对象。这一特点虽然带来了干净整洁的API，却也使得Spark应用程序在运行期倾向于创建大量临时对象，对GC造成压力。在现有RDD API的基础之上，我们固然可以利用mapPartitions方法来重载RDD单个分片内的数据创建方式，用复用可变对象的方式来减小对象分配和GC的开销，但这牺牲了代码的可读性，而且要求开发者对Spark运行时机制有一定的了解，门槛较高。另一方面，Spark SQL在框架内部已经在各种可能的情况下尽量重用对象，这样做虽然在内部会打破了不变性，但在将数据返回给用户时，还会重新转为不可变数据。利用 DataFrame API进行开发，可以免费地享受到这些优化效果。</p><h3 id="减少数据读取"><a href="#减少数据读取" class="headerlink" title="减少数据读取"></a>减少数据读取</h3><p>分析大数据，最快的方法就是 ——忽略它。这里的“忽略”并不是熟视无睹，而是根据查询条件进行恰当的剪枝。</p><p>上文讨论分区表时提到的分区剪 枝便是其中一种——当查询的过滤条件中涉及到分区列时，我们可以根据查询条件剪掉肯定不包含目标数据的分区目录，从而减少IO。</p><p>对于一些“智能”数据格 式，Spark SQL还可以根据数据文件中附带的统计信息来进行剪枝。简单来说，在这类数据格式中，数据是分段保存的，每段数据都带有最大值、最小值、null值数量等 一些基本的统计信息。当统计信息表名某一数据段肯定不包括符合查询条件的目标数据时，该数据段就可以直接跳过(例如某整数列a某段的最大值为100，而查询条件要求a &gt; 200)。</p><p>此外，Spark SQL也可以充分利用RCFile、ORC、Parquet等列式存储格式的优势，仅扫描查询真正涉及的列，忽略其余列的数据。</p><h3 id="执行优化"><a href="#执行优化" class="headerlink" title="执行优化"></a>执行优化</h3><p><img src="https://upload-images.jianshu.io/upload_images/2160494-c2423230fcc3841d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/571/format/webp" alt="img"></p><p>人口数据分析示例</p><p>为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter 下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。</p><p>得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。</p><p>对于普通开发者而言，查询优化 器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。</p><h2 id="RDD和DataSet"><a href="#RDD和DataSet" class="headerlink" title="RDD和DataSet"></a>RDD和DataSet</h2><p>DataSet以Catalyst逻辑执行计划表示，并且数据以编码的二进制形式被存储，不需要反序列化就可以执行sorting、shuffle等操作。</p><p>DataSet创立需要一个显式的Encoder，把对象序列化为二进制，可以把对象的scheme映射为SparkSQl类型，然而RDD依赖于运行时反射机制。</p><p>通过上面两点，DataSet的性能比RDD的要好很多。</p><h2 id="DataFrame和DataSet"><a href="#DataFrame和DataSet" class="headerlink" title="DataFrame和DataSet"></a>DataFrame和DataSet</h2><p>DataSet跟DataFrame还是有挺大区别的，DataFrame开发都是写sql，但是DataSet是使用类似RDD的API。主要区别是Dataset每一个record存储的是一个强类型值而不是一个Row。</p><h3 id="1-相同点："><a href="#1-相同点：" class="headerlink" title="(1)相同点："></a>(1)相同点：</h3><p>都是分布式数据集</p><p>DataFrame底层是RDD，但是DataSet不是，不过他们最后都是转换成RDD运行</p><p>DataSet和DataFrame的相同点都是有数据特征、数据类型的分布式数据集(schema)</p><h3 id="2-不同点："><a href="#2-不同点：" class="headerlink" title="(2)不同点："></a>(2)不同点：</h3><p><strong>(a)schema信息：</strong></p><p>RDD中的数据是没有数据类型的</p><p>DataFrame中的数据是<strong>弱数据类型</strong>，不会做数据类型检查</p><p>虽然有schema规定了数据类型，但是编译时是不会报错的，运行时才会报错</p><p>DataSet中的数据类型是<strong>强数据类型</strong></p><p><strong>(b)序列化机制：</strong></p><p>RDD和DataFrame默认的序列化机制是java的序列化，可以修改为Kyro的机制</p><p>DataSet使用自定义的数据编码器进行序列化和反序列化</p><h2 id="创建方式："><a href="#创建方式：" class="headerlink" title="创建方式："></a>创建方式：</h2><h3 id="1-要使用toDS之前"><a href="#1-要使用toDS之前" class="headerlink" title="(1)要使用toDS之前"></a>(1)要使用toDS之前</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br></pre></td></tr></table></figure><h3 id="2-将内存中的数据转换成DataSet"><a href="#2-将内存中的数据转换成DataSet" class="headerlink" title="(2)将内存中的数据转换成DataSet"></a>(2)将内存中的数据转换成DataSet</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Encoders for most common types are automatically provided by importing</span></span><br><span class="line"></span><br><span class="line">sqlContext.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).toDS()</span><br><span class="line">ds.map(_ + <span class="number">1</span>).collect() <span class="comment">// Returns: Array(2, 3, 4)</span></span><br></pre></td></tr></table></figure><p>其中：</p><p>collect()：返回一个Array，包含所有行信息</p><p>Returns an array that contains all rows in this Dataset.</p><h3 id="3-可以直接把case-class对象转化成DataSet"><a href="#3-可以直接把case-class对象转化成DataSet" class="headerlink" title="(3)可以直接把case class对象转化成DataSet"></a>(3)可以直接把case class对象转化成DataSet</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Encoders are also created for case classes.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">ds</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Andy"</span>, <span class="number">32</span>)).toDS()</span><br></pre></td></tr></table></figure><h3 id="4-将DataFrame转换成DataSet，不过要求是DataFrame的数据类型必须是case-class"><a href="#4-将DataFrame转换成DataSet，不过要求是DataFrame的数据类型必须是case-class" class="headerlink" title="(4)将DataFrame转换成DataSet，不过要求是DataFrame的数据类型必须是case class"></a>(4)将DataFrame转换成DataSet，不过要求是DataFrame的数据类型必须是case class</h3><p>并且要求DataFrame的数据类型必须和case class一致(顺序也必须一致)</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> _0729DF</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//import org.apache.spark</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Dataset</span> <span class="keyword">extends</span> <span class="title">App</span></span>&#123;</span><br><span class="line"><span class="comment">// import spark.implicits._</span></span><br><span class="line"><span class="comment">// val ds = Seq(1, 2, 3).toDS()</span></span><br><span class="line"><span class="comment">// ds.map(_ + 1).collect() // Returns: Array(2, 3, 4)</span></span><br><span class="line"><span class="comment">// // Encoders are also created for case classes.</span></span><br><span class="line"><span class="comment">// case class Person(name: String, age: Long)</span></span><br><span class="line"><span class="comment">// val ds = Seq(Person("Andy", 32)).toDS()</span></span><br><span class="line"><span class="comment">// ds.show</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> session = <span class="type">SparkSession</span>.builder()</span><br><span class="line">.appName(<span class="string">"app"</span>)</span><br><span class="line">.master(<span class="string">"local"</span>)</span><br><span class="line">.getOrCreate()</span><br><span class="line"><span class="keyword">val</span> sqlContext = session.sqlContext</span><br><span class="line"><span class="keyword">val</span> wcDs = sqlContext.read.textFile(<span class="string">"datas/halibote.txt"</span>)</span><br><span class="line"><span class="comment">// 导入隐式转换</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> session.implicits._</span><br><span class="line"><span class="keyword">val</span> wordData=wcDs.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">wordData.createTempView(<span class="string">"t_word"</span>)</span><br><span class="line">wordData.show()</span><br><span class="line">    </span><br><span class="line"><span class="comment">//wordData.printSchema()</span></span><br><span class="line"><span class="comment">// Encoders for most common types are automatically provided by importing sqlContext.implicits._</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds=<span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>).toDS()</span><br><span class="line">ds.map(_ + <span class="number">1</span>).collect() <span class="comment">// Returns: Array(2, 3, 4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// // Encoders are also created for case classes.</span></span><br><span class="line"><span class="comment">// case class Person(name: String, age: Long)</span></span><br><span class="line"><span class="comment">// val ds = Seq(Person("Andy", 32)).toDS()</span></span><br><span class="line"><span class="comment">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">age:<span class="type">Long</span>,name:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">path</span> </span>= <span class="string">"datas/people.json"</span></span><br><span class="line"><span class="keyword">val</span> people: <span class="type">Dataset</span>[<span class="type">Person</span>] = sqlContext.read.json(path).as[<span class="type">Person</span>]</span><br><span class="line">people.show()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>用wordcount举例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DataFrame</span></span><br><span class="line"><span class="comment">// Load a text file and interpret each line as a java.lang.String</span></span><br><span class="line"><span class="keyword">val</span> ds = sqlContext.read.text(<span class="string">"/home/spark/1.6/lines"</span>).as[<span class="type">String</span>]</span><br><span class="line"><span class="keyword">val</span> result = ds</span><br><span class="line">  .flatMap(_.split(<span class="string">" "</span>))              <span class="comment">// Split on whitespace</span></span><br><span class="line">  .filter(_ != <span class="string">""</span>)                    <span class="comment">// Filter empty words</span></span><br><span class="line">  .toDF()                              <span class="comment">// Convert to DataFrame to perform aggregation / sorting</span></span><br><span class="line">  .groupBy($<span class="string">"value"</span>)                  <span class="comment">// Count number of occurences of each word</span></span><br><span class="line">  .agg(count(<span class="string">"*"</span>) as <span class="string">"numOccurances"</span>)</span><br><span class="line">  .orderBy($<span class="string">"numOccurances"</span> desc)      <span class="comment">// Show most common words first</span></span><br></pre></td></tr></table></figure><p>后面版本DataFrame会继承DataSet，DataFrame是面向Spark SQL的接口。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DataSet,完全使用scala编程，不要切换到DataFrame</span></span><br><span class="line"><span class="keyword">val</span> wordCount = </span><br><span class="line">ds.flatMap(.split(<span class="string">" "</span>))</span><br><span class="line">  .filter( != <span class="string">""</span>)</span><br><span class="line">  .groupBy(_.toLowerCase())  <span class="comment">// Instead of grouping on a column expression (i.e. $"value") we pass a lambda function</span></span><br><span class="line">  .count()</span><br></pre></td></tr></table></figure><p>DataFrame和DataSet可以相互转化， df.as[ElementType] 这样可以把DataFrame转化为DataSet， ds.toDF() 这样可以把DataSet转化为DataFrame。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （十八）SparkSQL简单使用</title>
      <link href="/2019-06-18-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89SparkSQL%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8.html"/>
      <url>/2019-06-18-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89SparkSQL%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （十八）SparkSQL简单使用：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （十八）SparkSQL简单使用</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、SparkSQL的进化之路"><a href="#一、SparkSQL的进化之路" class="headerlink" title="一、SparkSQL的进化之路"></a>一、SparkSQL的进化之路</h2><p>1.0以前：    Shark</p><p>1.1.x：         SparkSQL(只是测试性的)  SQL</p><p>1.3.x:            SparkSQL(正式版本)+Dataframe</p><p>1.5.x:            SparkSQL 钨丝计划</p><p>1.6.x：         SparkSQL+DataFrame+DataSet(测试版本)</p><p>2.x   :            SparkSQL+DataFrame+DataSet(正式版本)</p><p>​                      SparkSQL:还有其他的优化</p><p>​                      StructuredStreaming(DataSet)</p><h2 id="二、认识SparkSQL"><a href="#二、认识SparkSQL" class="headerlink" title="二、认识SparkSQL"></a>二、认识SparkSQL</h2><h3 id="2-1-什么是SparkSQL"><a href="#2-1-什么是SparkSQL" class="headerlink" title="2.1　什么是SparkSQL?"></a>2.1　什么是SparkSQL?</h3><p>spark SQL是spark的一个模块，主要用于进行结构化数据的处理。它提供的最核心的编程抽象就是DataFrame。</p><h3 id="2-2-SparkSQL的作用"><a href="#2-2-SparkSQL的作用" class="headerlink" title="2.2　SparkSQL的作用"></a>2.2　SparkSQL的作用</h3><p>提供一个编程抽象（DataFrame） 并且作为分布式 SQL 查询引擎</p><p>DataFrame：它可以根据很多源进行构建，包括：<strong>结构化的数据文件，hive中的表，外部的关系型数据库，以及RDD</strong></p><h3 id="2-3-运行原理"><a href="#2-3-运行原理" class="headerlink" title="2.3　运行原理"></a>2.3　运行原理</h3><p>将 Spark SQL 转化为 RDD， 然后提交到集群执行</p><h3 id="2-4-特点"><a href="#2-4-特点" class="headerlink" title="2.4　特点"></a>2.4　特点</h3><p>（1）容易整合</p><p>（2）统一的数据访问方式</p><p>（3）兼容 Hive</p><p>（4）标准的数据连接</p><h3 id="2-5-SparkSession"><a href="#2-5-SparkSession" class="headerlink" title="2.5　SparkSession"></a>2.5　SparkSession</h3><p>SparkSession是Spark 2.0引如的新概念。SparkSession为用户提供了统一的切入点，来让用户学习spark的各项功能。<br>  在spark的早期版本中，SparkContext是spark的主要切入点，由于RDD是主要的API，我们通过sparkcontext来创建和操作RDD。对于每个其他的API，我们需要使用不同的context。例如，对于Streming，我们需要使用StreamingContext；对于sql，使用sqlContext；对于Hive，使用hiveContext。但是随着DataSet和DataFrame的API逐渐成为标准的API，就需要为他们建立接入点。所以在spark2.0中，引入SparkSession作为DataSet和DataFrame API的切入点，SparkSession封装了SparkConf、SparkContext和SQLContext。为了向后兼容，SQLContext和HiveContext也被保存下来。<br>　　<br>　　SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。</p><p>特点：</p><p>　　 <strong>—-</strong> <strong>为用户提供一个统一的切入点使用Spark 各项功能</strong></p><p>​        <strong>—-</strong> <strong>允许用户通过它调用 DataFrame 和 Dataset 相关 API 来编写程序</strong></p><p>​        <strong>—-</strong> <strong>减少了用户需要了解的一些概念，可以很容易的与 Spark 进行交互</strong></p><p>​        <strong>—-</strong> <strong>与 Spark 交互之时不需要显示的创建 SparkConf, SparkContext 以及 SQlContext，这些对象已经封闭在 SparkSession 中</strong></p><h3 id="2-7-DataFrames"><a href="#2-7-DataFrames" class="headerlink" title="2.7　DataFrames"></a><strong>2.7　DataFrames</strong></h3><p>在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503195056657-1280315007.png" alt="img"></p><h2 id="三、RDD转换成为DataFrame"><a href="#三、RDD转换成为DataFrame" class="headerlink" title="三、RDD转换成为DataFrame"></a>三、RDD转换成为DataFrame</h2><p>使用spark1.x版本的方式</p><p>测试数据目录：/home/hadoop/apps/spark/examples/src/main/resources（spark的安装目录里面）</p><p>people.txt</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503195656260-525846269.png" alt="img"></p><h3 id="3-1-方式一：通过-case-class-创建-DataFrames（反射）"><a href="#3-1-方式一：通过-case-class-创建-DataFrames（反射）" class="headerlink" title="3.1　方式一：通过 case class 创建 DataFrames（反射）"></a>3.1　方式一：<strong>通过 case class 创建 DataFrames（反射）</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">//定义case class，相当于表结构</span><br><span class="line">case class People(var name:String,var age:Int)</span><br><span class="line">object TestDataFrame1 &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setAppName("RDDToDataFrame").setMaster("local")</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val context = new SQLContext(sc)</span><br><span class="line">    // 将本地的数据读入 RDD， 并将 RDD 与 case class 关联</span><br><span class="line">    val peopleRDD = sc.textFile("E:\\666\\people.txt")</span><br><span class="line">      .map(line =&gt; People(line.split(",")(0), line.split(",")(1).trim.toInt))</span><br><span class="line">    import context.implicits._</span><br><span class="line">    // 将RDD 转换成 DataFrames</span><br><span class="line">    val df = peopleRDD.toDF</span><br><span class="line">    //将DataFrames创建成一个临时的视图</span><br><span class="line">    df.createOrReplaceTempView("people")</span><br><span class="line">    //使用SQL语句进行查询</span><br><span class="line">    context.sql("select * from people").show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503202629907-1000361533.png" alt="img"></p><h3 id="3-2-方式二：通过-structType-创建-DataFrames（编程接口）"><a href="#3-2-方式二：通过-structType-创建-DataFrames（编程接口）" class="headerlink" title="3.2　方式二：通过 structType 创建 DataFrames（编程接口）"></a>3.2　方式二：<strong>通过 structType 创建 DataFrames（编程接口）</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">object TestDataFrame2 &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setAppName("TestDataFrame2").setMaster("local")</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val sqlContext = new SQLContext(sc)</span><br><span class="line">    val fileRDD = sc.textFile("E:\\666\\people.txt")</span><br><span class="line">    // 将 RDD 数据映射成 Row，需要 import org.apache.spark.sql.Row</span><br><span class="line">    val rowRDD: RDD[Row] = fileRDD.map(line =&gt; &#123;</span><br><span class="line">      val fields = line.split(",")</span><br><span class="line">      Row(fields(0), fields(1).trim.toInt)</span><br><span class="line">    &#125;)</span><br><span class="line">    // 创建 StructType 来定义结构</span><br><span class="line">    val structType: StructType = StructType(</span><br><span class="line">      //字段名，字段类型，是否可以为空</span><br><span class="line">      StructField("name", StringType, true) ::</span><br><span class="line">        StructField("age", IntegerType, true) :: Nil</span><br><span class="line">    )</span><br><span class="line">    /**</span><br><span class="line">      * rows: java.util.List[Row],</span><br><span class="line">      * schema: StructType</span><br><span class="line">      * */</span><br><span class="line">    val df: DataFrame = sqlContext.createDataFrame(rowRDD,structType)</span><br><span class="line">    df.createOrReplaceTempView("people")</span><br><span class="line">    sqlContext.sql("select * from people").show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503202905865-1517500300.png" alt="img"></p><h3 id="3-3-方式三：通过-json-文件创建-DataFrames"><a href="#3-3-方式三：通过-json-文件创建-DataFrames" class="headerlink" title="3.3　方式三：通过 json 文件创建 DataFrames"></a>3.3　方式三：<strong>通过 json 文件创建 DataFrames</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">object TestDataFrame3 &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setAppName("TestDataFrame2").setMaster("local")</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val sqlContext = new SQLContext(sc)</span><br><span class="line">    val df: DataFrame = sqlContext.read.json("E:\\666\\people.json")</span><br><span class="line">    df.createOrReplaceTempView("people")</span><br><span class="line">    sqlContext.sql("select * from people").show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503203759442-1628348320.png" alt="img"></p><h2 id="四、DataFrame的read和save和savemode"><a href="#四、DataFrame的read和save和savemode" class="headerlink" title="四、DataFrame的read和save和savemode"></a>四、DataFrame的read和save和savemode</h2><h3 id="4-1-数据的读取"><a href="#4-1-数据的读取" class="headerlink" title="4.1　数据的读取"></a>4.1　数据的读取</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">object TestRead &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setAppName("TestDataFrame2").setMaster("local")</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val sqlContext = new SQLContext(sc)</span><br><span class="line">    //方式一</span><br><span class="line">    val df1 = sqlContext.read.json("E:\\666\\people.json")</span><br><span class="line">    val df2 = sqlContext.read.parquet("E:\\666\\users.parquet")</span><br><span class="line">    //方式二</span><br><span class="line">    val df3 = sqlContext.read.format("json").load("E:\\666\\people.json")</span><br><span class="line">    val df4 = sqlContext.read.format("parquet").load("E:\\666\\users.parquet")</span><br><span class="line">    //方式三，默认是parquet格式</span><br><span class="line">    val df5 = sqlContext.load("E:\\666\\users.parquet")</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-2-数据的保存"><a href="#4-2-数据的保存" class="headerlink" title="4.2　数据的保存"></a>4.2　数据的保存</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">object TestSave &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setAppName("TestDataFrame2").setMaster("local")</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val sqlContext = new SQLContext(sc)</span><br><span class="line">    val df1 = sqlContext.read.json("E:\\666\\people.json")</span><br><span class="line">    //方式一</span><br><span class="line">    df1.write.json("E:\\111")</span><br><span class="line">    df1.write.parquet("E:\\222")</span><br><span class="line">    //方式二</span><br><span class="line">    df1.write.format("json").save("E:\\333")</span><br><span class="line">    df1.write.format("parquet").save("E:\\444")</span><br><span class="line">    //方式三</span><br><span class="line">    df1.write.save("E:\\555")</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-3-数据的保存模式"><a href="#4-3-数据的保存模式" class="headerlink" title="4.3　数据的保存模式"></a>4.3　数据的保存模式</h3><p>使用mode</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1.write.format(&quot;parquet&quot;).mode(SaveMode.Ignore).save(&quot;E:\\444&quot;)</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503211638036-705055493.png" alt="img"></p><h2 id="五、数据源"><a href="#五、数据源" class="headerlink" title="五、数据源"></a>五、数据源</h2><h3 id="5-1-数据源只json"><a href="#5-1-数据源只json" class="headerlink" title="5.1　数据源只json"></a>5.1　数据源只json</h3><p>参考4.1</p><h3 id="5-2-数据源之parquet"><a href="#5-2-数据源之parquet" class="headerlink" title="5.2　数据源之parquet"></a>5.2　数据源之parquet</h3><p>参考4.1</p><h3 id="5-3-数据源之Mysql"><a href="#5-3-数据源之Mysql" class="headerlink" title="5.3　数据源之Mysql"></a>5.3　数据源之Mysql</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestMysql</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"TestMysql"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> url = <span class="string">"jdbc:mysql://192.168.123.102:3306/hivedb"</span></span><br><span class="line">    <span class="keyword">val</span> table = <span class="string">"dbs"</span></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.setProperty(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">    properties.setProperty(<span class="string">"password"</span>,<span class="string">"root"</span>)</span><br><span class="line">    <span class="comment">//需要传入Mysql的URL、表明、properties（连接数据库的用户名密码）</span></span><br><span class="line">    <span class="keyword">val</span> df = sqlContext.read.jdbc(url,table,properties)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"dbs"</span>)</span><br><span class="line">    sqlContext.sql(<span class="string">"select * from dbs"</span>).show()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503215248558-1665335084.png" alt="img"></p><h3 id="5-4-数据源之Hive"><a href="#5-4-数据源之Hive" class="headerlink" title="5.4　数据源之Hive"></a>5.4　数据源之Hive</h3><h4 id="（1）准备工作"><a href="#（1）准备工作" class="headerlink" title="（1）准备工作"></a>（1）准备工作</h4><p>在pom.xml文件中添加依赖</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- https:<span class="comment">//mvnrepository.com/artifact/org.apache.spark/spark-hive --&gt;</span></span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-hive_2<span class="number">.11</span>&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;<span class="number">2.3</span><span class="number">.0</span>&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>开发环境则把resource文件夹下添加hive-site.xml文件，集群环境把hive的配置文件要发到$SPARK_HOME/conf目录下</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180504184547333-1552631388.png" alt="img"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;jdbc:mysql://localhost:3306/hivedb?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">                &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">                &lt;!-- 如果 mysql 和 hive 在同一个服务器节点，那么请更改 hadoop02 为 localhost --&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">                &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">                &lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/hive/warehouse&lt;/value&gt;</span><br><span class="line">                &lt;description&gt;hive default warehouse, if nessecory, change it&lt;/description&gt;</span><br><span class="line">        &lt;/property&gt;  </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="（2）测试代码"><a href="#（2）测试代码" class="headerlink" title="（2）测试代码"></a>（2）测试代码</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">object TestHive &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setMaster("local").setAppName(this.getClass.getSimpleName)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val sqlContext = new HiveContext(sc)</span><br><span class="line">    sqlContext.sql("select * from myhive.student").show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180504192745282-1160176093.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （十七）Spark分区</title>
      <link href="/2019-06-17-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89Spark%E5%88%86%E5%8C%BA.html"/>
      <url>/2019-06-17-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89Spark%E5%88%86%E5%8C%BA.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （十七）Spark分区：** &lt;Excerpt in index | 首页摘要&gt;</p><p>　　分区是RDD内部并行计算的一个计算单元，RDD的数据集在逻辑上被划分为多个分片，每一个分片称为分区，分区的格式决定了并行计算的粒度，而每个分区的数值计算都是在一个任务中进行的，因此任务的个数，也是由RDD(准确来说是作业最后一个RDD)的分区数决定。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、为什么要进行分区"><a href="#一、为什么要进行分区" class="headerlink" title="一、为什么要进行分区"></a>一、为什么要进行分区</h2><p>　　数据分区，在分布式集群里，网络通信的代价很大，减少网络传输可以极大提升性能。mapreduce框架的性能开支主要在io和网络传输，io因为要大量读写文件，它是不可避免的，但是网络传输是可以避免的，把大文件压缩变小文件，   从而减少网络传输，但是增加了cpu的计算负载。</p><p>　　<strong>Spark</strong>里面io也是不可避免的，但是网络传输spark里面进行了优化：</p><p>　　Spark把rdd进行分区（分片），放在集群上并行计算。同一个rdd分片100个，10个节点，平均一个节点10个分区，当进行sum型的计算的时候，先进行每个分区的sum，然后把sum值shuffle传输到主程序进行全局sum，所以进行sum型计算对网络传输非常小。但对于进行join型的计算的时候，需要把数据本身进行shuffle，网络开销很大。</p><p>spark是如何优化这个问题的呢？</p><p>　　Spark把key－value rdd通过key的hashcode进行分区，而且保证相同的key存储在同一个节点上，这样对改rdd进行key聚合时，就不需要shuffle过程，我们进行mapreduce计算的时候为什么要进行shuffle？，就是说mapreduce里面网络传输主要在shuffle阶段，<strong>shuffle的根本原因是相同的key存在不同的节点上，按key进行聚合的时候不得不进行shuffle</strong>。shuffle是非常影响网络的，它要把所有的数据混在一起走网络，然后它才能把相同的key走到一起。<strong>要进行shuffle是存储决定的。</strong></p><p>　　Spark从这个教训中得到启发，spark会把key进行分区，也就是key的hashcode进行分区，相同的key，hashcode肯定是一样的，所以它进行分区的时候100t的数据分成10分，每部分10个t，它能确保相同的key肯定在一个分区里面，而且它能保证存储的时候相同的key能够存在同一个节点上。比如一个rdd分成了100份，集群有10个节点，所以每个节点存10份，每一分称为每个分区，spark能保证相同的key存在同一个节点上，实际上相同的key存在同一个分区。</p><p>　　key的分布不均决定了有的分区大有的分区小。没法分区保证完全相等，但它会保证在一个接近的范围。所以mapreduce里面做的某些工作里边，spark就不需要shuffle了，spark解决网络传输这块的根本原理就是这个。</p><p>　　进行join的时候是两个表，不可能把两个表都分区好，通常情况下是把用的频繁的大表事先进行分区，小表进行关联它的时候小表进行shuffle过程。</p><p>　　大表不需要shuffle。　　</p><p>　　需要在工作节点间进行数据混洗的转换极大地受益于分区。这样的转换是  cogroup，groupWith，join，leftOuterJoin，rightOuterJoin，groupByKey，reduceByKey，combineByKey 和lookup。</p><p>　　<strong>分区是可配置的，只要RDD是基于键值对的即可</strong>。</p><h2 id="二、Spark分区原则及方法"><a href="#二、Spark分区原则及方法" class="headerlink" title="二、Spark分区原则及方法"></a>二、Spark分区原则及方法</h2><p>RDD分区的一个<strong>分区原则：尽可能是得分区的个数等于集群核心数目</strong></p><p>无论是本地模式、Standalone模式、YARN模式或Mesos模式，我们都可以<strong>通过spark.default.parallelism来配置其默认分区个数</strong>，若没有设置该值，则根据不同的集群环境确定该值</p><h3 id="2-1-本地模式"><a href="#2-1-本地模式" class="headerlink" title="2.1　本地模式"></a>2.1　本地模式</h3><h4 id="（1）默认方式"><a href="#（1）默认方式" class="headerlink" title="（1）默认方式"></a>（1）默认方式</h4><p>以下这种默认方式就一个分区</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503184117132-933712151.png" alt="img"></p><p>结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503184211834-340940238.png" alt="img"></p><h4 id="（2）手动设置"><a href="#（2）手动设置" class="headerlink" title="（2）手动设置"></a>（2）手动设置</h4><p>设置了几个分区就是几个分区</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503184315304-1438737967.png" alt="img"></p><p>结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503184417483-992690743.png" alt="img"></p><h4 id="（3）跟local-n-有关"><a href="#（3）跟local-n-有关" class="headerlink" title="（3）跟local[n] 有关"></a>（3）跟local[n] 有关</h4><p>n等于几默认就是几个分区</p><p>如果n=* 那么分区个数就等于cpu core的个数</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503184602132-1762283216.png" alt="img"></p><p>结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503184628115-1042310352.png" alt="img"></p><p>本机电脑查看cpu core，我的电脑–》右键管理–》设备管理器–》处理器</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503184724600-1103843046.png" alt="img"></p><h4 id="（4）参数控制"><a href="#（4）参数控制" class="headerlink" title="（4）参数控制"></a>（4）参数控制</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503185007050-446009891.png" alt="img"></p><p>结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503185028293-1238150539.png" alt="img"></p><h3 id="2-2-YARN模式"><a href="#2-2-YARN模式" class="headerlink" title="2.2　YARN模式"></a>2.2　YARN模式</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503190552013-885991110.png" alt="img"></p><p> 进入defaultParallelism方法</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503190651046-910979790.png" alt="img"></p><p>继续进入defaultParallelism方法</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503190749812-1860089737.png" alt="img"></p><p>这个一个trait，其实现类是（Ctrl+h）</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503190853986-1549098382.png" alt="img"></p><p>进入TaskSchedulerImpl类找到defaultParallelism方法</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503190937719-848606303.png" alt="img"></p><p>继续进入defaultParallelism方法，又是一个trait，看其实现类</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503191109902-1222100636.png" alt="img"></p><p>Ctrl+h看SchedulerBackend类的实现类</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503191213525-716329324.png" alt="img"></p><p>进入CoarseGrainedSchedulerBackend找到defaultParallelism</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503191320589-992961865.png" alt="img"></p><p><strong>totalCoreCount.get()是所有executor使用的core总数，和2比较去较大值</strong></p><p><strong>如果正常的情况下，那你设置了多少就是多少</strong></p><h2 id="四、分区器"><a href="#四、分区器" class="headerlink" title="四、分区器"></a>四、分区器</h2><p>（1）如果是从HDFS里面读取出来的数据，不需要分区器。因为HDFS本来就分好区了。</p><p>　　  分区数我们是可以控制的，但是没必要有分区器。</p><p>（2）非key-value RDD分区，没必要设置分区器</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">al testRDD = sc.textFile(<span class="string">"C:\\Users\\Administrator\\IdeaProjects\\myspark\\src\\main\\hello.txt"</span>)</span><br><span class="line">  .flatMap(line =&gt; line.split(<span class="string">","</span>))</span><br><span class="line">  .map(word =&gt; (word, <span class="number">1</span>)).partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">　　没必要设置，但是非要设置也行。</span><br></pre></td></tr></table></figure><p>（3）Key-value形式的时候，我们就有必要了。</p><p><strong>HashPartitioner</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resultRDD = testRDD.reduceByKey(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>),(x:<span class="type">Int</span>,y:<span class="type">Int</span>) =&gt; x+ y)</span><br><span class="line"><span class="comment">//如果不设置默认也是HashPartitoiner，分区数跟spark.default.parallelism一样</span></span><br><span class="line">println(resultRDD.partitioner)</span><br><span class="line">println(<span class="string">"resultRDD"</span>+resultRDD.getNumPartitions)</span><br></pre></td></tr></table></figure><p><strong>RangePartitioner</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resultRDD = testRDD.reduceByKey((x:<span class="type">Int</span>,y:<span class="type">Int</span>) =&gt; x+ y)</span><br><span class="line"><span class="keyword">val</span> newresultRDD=resultRDD.partitionBy(<span class="keyword">new</span> <span class="type">RangePartitioner</span>[<span class="type">String</span>,<span class="type">Int</span>](<span class="number">3</span>,resultRDD))</span><br><span class="line">println(newresultRDD.partitioner)</span><br><span class="line">println(<span class="string">"newresultRDD"</span>+newresultRDD.getNumPartitions)</span><br><span class="line">注：按照范围进行分区的，如果是字符串，那么就按字典顺序的范围划分。如果是数字，就按数据自的范围划分。</span><br></pre></td></tr></table></figure><p><strong>自定义分区</strong></p><p><strong>需要实现2个方法</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartitoiner</span>(<span class="params">val numParts:<span class="type">Int</span></span>) <span class="keyword">extends</span>  <span class="title">Partitioner</span></span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = numParts</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> domain = <span class="keyword">new</span> <span class="type">URL</span>(key.toString).getHost</span><br><span class="line">    <span class="keyword">val</span> code = (domain.hashCode % numParts)</span><br><span class="line">    <span class="keyword">if</span> (code &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      code + numParts</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      code</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DomainNamePartitioner</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"word count"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> urlRDD = sc.makeRDD(<span class="type">Seq</span>((<span class="string">"http://baidu.com/test"</span>, <span class="number">2</span>),</span><br><span class="line">      (<span class="string">"http://baidu.com/index"</span>, <span class="number">2</span>), (<span class="string">"http://ali.com"</span>, <span class="number">3</span>), (<span class="string">"http://baidu.com/tmmmm"</span>, <span class="number">4</span>),</span><br><span class="line">      (<span class="string">"http://baidu.com/test"</span>, <span class="number">4</span>)))</span><br><span class="line">    <span class="comment">//Array[Array[(String, Int)]]</span></span><br><span class="line">    <span class="comment">// = Array(Array(),</span></span><br><span class="line">    <span class="comment">// Array((http://baidu.com/index,2), (http://baidu.com/tmmmm,4),</span></span><br><span class="line">    <span class="comment">// (http://baidu.com/test,4), (http://baidu.com/test,2), (http://ali.com,3)))</span></span><br><span class="line">    <span class="keyword">val</span> hashPartitionedRDD = urlRDD.partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">    hashPartitionedRDD.glom().collect()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用spark-shell --jar的方式将这个partitioner所在的jar包引进去，然后测试下面的代码</span></span><br><span class="line">    <span class="comment">// spark-shell --master spark://master:7077 --jars spark-rdd-1.0-SNAPSHOT.jar</span></span><br><span class="line">    <span class="keyword">val</span> partitionedRDD = urlRDD.partitionBy(<span class="keyword">new</span> <span class="type">MyPartitoiner</span>(<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">val</span> array = partitionedRDD.glom().collect()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本</title>
      <link href="/2019-06-16-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89SparkCore%E7%9A%84%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%BA%8C%EF%BC%89spark-submit%E6%8F%90%E4%BA%A4%E8%84%9A%E6%9C%AC.html"/>
      <url>/2019-06-16-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89SparkCore%E7%9A%84%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%BA%8C%EF%BC%89spark-submit%E6%8F%90%E4%BA%A4%E8%84%9A%E6%9C%AC.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>上一篇主要是介绍了spark启动的一些脚本，这篇主要分析一下Spark源码中提交任务脚本的处理逻辑，从spark-submit一步步深入进去看看任务提交的整体流程,首先看一下整体的流程概要图：<br><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180502085945492-811482545.png" alt="img" style="zoom:200%;"></p><h2 id="二、源码解读"><a href="#二、源码解读" class="headerlink" title="二、源码解读"></a>二、源码解读</h2><p>2.1　spark-submit</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> -z是检查后面变量是否为空（空则真） shell可以在双引号之内引用变量，单引号不可</span></span><br><span class="line"><span class="meta">#</span><span class="bash">这一步作用是检查SPARK_HOME变量是否为空，为空则执行<span class="keyword">then</span>后面程序</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">source</span>命令： <span class="built_in">source</span> filename作用在当前bash环境下读取并执行filename中的命令</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="variable">$0</span>代表shell脚本文件本身的文件名，这里即使spark-submit</span></span><br><span class="line"><span class="meta">#</span><span class="bash">dirname用于取得脚本文件所在目录 dirname <span class="variable">$0</span>取得当前脚本文件所在目录</span></span><br><span class="line"><span class="meta">#</span><span class="bash">$(命令)表示返回该命令的结果</span></span><br><span class="line"><span class="meta">#</span><span class="bash">故整个<span class="keyword">if</span>语句的含义是：如果SPARK_HOME变量没有设置值，则执行当前目录下的find-spark-home脚本文件，设置SPARK_HOME值</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">disable</span> randomized <span class="built_in">hash</span> <span class="keyword">for</span> string <span class="keyword">in</span> Python 3.3+</span></span><br><span class="line">export PYTHONHASHSEED=0</span><br><span class="line"><span class="meta">#</span><span class="bash">执行spark-class脚本，传递参数org.apache.spark.deploy.SparkSubmit 和<span class="string">"<span class="variable">$@</span>"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">这里<span class="variable">$@</span>表示之前spark-submit接收到的全部参数</span></span><br><span class="line">exec "$&#123;SPARK_HOME&#125;"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"</span><br></pre></td></tr></table></figure><p>所以spark-submit脚本的整体逻辑就是：<br>首先 检查SPARK_HOME是否设置；if 已经设置 执行spark-class文件 否则加载执行find-spark-home文件 </p><h3 id="2-2-find-spark-home"><a href="#2-2-find-spark-home" class="headerlink" title="2.2　find-spark-home"></a>2.2　find-spark-home</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">定义一个变量用于后续判断是否存在定义SPARK_HOME的python脚本文件</span></span><br><span class="line">FIND_SPARK_HOME_PYTHON_SCRIPT="$(cd "$(dirname "$0")"; pwd)/find_spark_home.py"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Short cirtuit <span class="keyword">if</span> the user already has this <span class="built_in">set</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#如果SPARK_HOME为不为空值，成功退出程序</span></span></span><br><span class="line">if [ ! -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">   exit 0</span><br><span class="line"><span class="meta">#</span><span class="bash"> -f用于判断这个文件是否存在并且是否为常规文件，是的话为真，这里不存在为假，执行下面语句，给SPARK_HOME变量赋值</span></span><br><span class="line">elif [ ! -f "$FIND_SPARK_HOME_PYTHON_SCRIPT" ]; then</span><br><span class="line"><span class="meta">  #</span><span class="bash"> If we are not <span class="keyword">in</span> the same directory as find_spark_home.py we are not pip installed so we don<span class="string">'t</span></span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> need to search the different Python directories <span class="keyword">for</span> a Spark installation.</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> Note only that, <span class="keyword">if</span> the user has pip installed PySpark but is directly calling pyspark-shell or</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> spark-submit <span class="keyword">in</span> another directory we want to use that version of PySpark rather than the</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> pip installed version of PySpark.</span></span><br><span class="line">  export SPARK_HOME="$(cd "$(dirname "$0")"/..; pwd)"</span><br><span class="line">else</span><br><span class="line"><span class="meta">  #</span><span class="bash"> We are pip installed, use the Python script to resolve a reasonable SPARK_HOME</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> Default to standard python interpreter unless told otherwise</span></span><br><span class="line">  if [[ -z "$PYSPARK_DRIVER_PYTHON" ]]; then</span><br><span class="line">     PYSPARK_DRIVER_PYTHON="$&#123;PYSPARK_PYTHON:-"python"&#125;"</span><br><span class="line">  fi</span><br><span class="line">  export SPARK_HOME=$($PYSPARK_DRIVER_PYTHON "$FIND_SPARK_HOME_PYTHON_SCRIPT")</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>可以看到，如果事先用户没有设定SPARK_HOME的值，这里程序也会自动设置并且将其注册为环境变量，供后面程序使用</p><p>当SPARK_HOME的值设定完成之后，就会执行Spark-class文件，这也是我们分析的重要部分，源码如下：</p><h3 id="2-3-spark-class"><a href="#2-3-spark-class" class="headerlink" title="2.3　spark-class"></a>2.3　spark-class</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/usr/bin/env bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">依旧是检查设置SPARK_HOME的值</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">执行load-spark-env.sh脚本文件，主要目的在于加载设定一些变量值</span></span><br><span class="line"><span class="meta">#</span><span class="bash">设定spark-env.sh中的变量值到环境变量中，供后续使用</span></span><br><span class="line"><span class="meta">#</span><span class="bash">设定scala版本变量值</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;"/bin/load-spark-env.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find the java binary</span></span><br><span class="line"><span class="meta">#</span><span class="bash">检查设定java环境值</span></span><br><span class="line"><span class="meta">#</span><span class="bash">-n代表检测变量长度是否为0，不为0时候为真</span></span><br><span class="line"><span class="meta">#</span><span class="bash">如果已经安装Java没有设置JAVA_HOME,<span class="built_in">command</span> -v java返回的值为<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin/java</span></span><br><span class="line">if [ -n "$&#123;JAVA_HOME&#125;" ]; then</span><br><span class="line">  RUNNER="$&#123;JAVA_HOME&#125;/bin/java"</span><br><span class="line">else</span><br><span class="line">  if [ "$(command -v java)" ]; then</span><br><span class="line">    RUNNER="java"</span><br><span class="line">  else</span><br><span class="line">    echo "JAVA_HOME is not set" &gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find Spark jars.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">-d检测文件是否为目录，若为目录则为真</span></span><br><span class="line"><span class="meta">#</span><span class="bash">设置一些关联Class文件</span></span><br><span class="line">if [ -d "$&#123;SPARK_HOME&#125;/jars" ]; then</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/jars"</span><br><span class="line">else</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ ! -d "$SPARK_JARS_DIR" ] &amp;&amp; [ -z "$SPARK_TESTING$SPARK_SQL_TESTING" ]; then</span><br><span class="line">  echo "Failed to find Spark jars directory ($SPARK_JARS_DIR)." 1&gt;&amp;2</span><br><span class="line">  echo "You need to build Spark with the target \"package\" before running this program." 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">else</span><br><span class="line">  LAUNCH_CLASSPATH="$SPARK_JARS_DIR/*"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Add the launcher build dir to the classpath <span class="keyword">if</span> requested.</span></span><br><span class="line">if [ -n "$SPARK_PREPEND_CLASSES" ]; then</span><br><span class="line">  LAUNCH_CLASSPATH="$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> For tests</span></span><br><span class="line">if [[ -n "$SPARK_TESTING" ]]; then</span><br><span class="line">  unset YARN_CONF_DIR</span><br><span class="line">  unset HADOOP_CONF_DIR</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The launcher library will <span class="built_in">print</span> arguments separated by a NULL character, to allow arguments with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> characters that would be otherwise interpreted by the shell. Read that <span class="keyword">in</span> a <span class="keyword">while</span> loop, populating</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> an array that will be used to <span class="built_in">exec</span> the final <span class="built_in">command</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The <span class="built_in">exit</span> code of the launcher is appended to the output, so the parent shell removes it from the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">command</span> array and checks the value to see <span class="keyword">if</span> the launcher succeeded.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">执行类文件org.apache.spark.launcher.Main，返回解析后的参数</span></span><br><span class="line">build_command() &#123;</span><br><span class="line">  "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"</span><br><span class="line">  printf "%d\0" $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Turn off posix mode since it does not allow process substitution</span></span><br><span class="line"><span class="meta">#</span><span class="bash">将build_command方法解析后的参数赋给CMD</span></span><br><span class="line">set +o posix</span><br><span class="line">CMD=()</span><br><span class="line">while IFS= read -d '' -r ARG; do</span><br><span class="line">  CMD+=("$ARG")</span><br><span class="line">done &lt; &lt;(build_command "$@")</span><br><span class="line"></span><br><span class="line">COUNT=$&#123;#CMD[@]&#125;</span><br><span class="line">LAST=$((COUNT - 1))</span><br><span class="line">LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Certain JVM failures result <span class="keyword">in</span> errors being printed to stdout (instead of stderr), <span class="built_in">which</span> causes</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the code that parses the output of the launcher to get confused. In those cases, check <span class="keyword">if</span> the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">exit</span> code is an <span class="built_in">integer</span>, and <span class="keyword">if</span> it<span class="string">'s not, handle it as a special error case.</span></span></span><br><span class="line">if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then</span><br><span class="line">  echo "$&#123;CMD[@]&#125;" | head -n-1 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $LAUNCHER_EXIT_CODE != 0 ]; then</span><br><span class="line">  exit $LAUNCHER_EXIT_CODE</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">CMD=("$&#123;CMD[@]:0:$LAST&#125;")</span><br><span class="line"><span class="meta">#</span><span class="bash">执行CMD中的某个参数类org.apache.spark.deploy.SparkSubmit</span></span><br><span class="line">exec "$&#123;CMD[@]&#125;"</span><br></pre></td></tr></table></figure><p>spark-class文件的执行逻辑稍显复杂，总体上应该是这样的：</p><p>检查SPARK_HOME的值—-》执行load-spark-env.sh文件，设定一些需要用到的环境变量，如scala环境值，这其中也加载了spark-env.sh文件——-》检查设定java的执行路径变量值——-》寻找spark jars,设定一些引用相关类的位置变量——》执行类文件org.apache.spark.launcher.Main，返回解析后的参数给CMD——-》判断解析参数是否正确（代表了用户设置的参数是否正确）——–》正确的话执行org.apache.spark.deploy.SparkSubmit这个类</p><h3 id="2-4-SparkSubmit"><a href="#2-4-SparkSubmit" class="headerlink" title="2.4　SparkSubmit"></a>2.4　SparkSubmit</h3><p>2.1最后提交语句，D:\src\spark-2.3.0\core\src\main\scala\org\apache\spark\deploy\SparkSubmit.scala</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exec "$&#123;SPARK_HOME&#125;"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Initialize logging if it hasn't been done yet. Keep track of whether logging needs to</span></span><br><span class="line">    <span class="comment">// be reset before the application starts.</span></span><br><span class="line">    <span class="keyword">val</span> uninitLog = initializeLogIfNecessary(<span class="literal">true</span>, silent = <span class="literal">true</span>)</span><br><span class="line">    <span class="comment">//拿到submit脚本传入的参数</span></span><br><span class="line">    <span class="keyword">val</span> appArgs = <span class="keyword">new</span> <span class="type">SparkSubmitArguments</span>(args)</span><br><span class="line">    <span class="keyword">if</span> (appArgs.verbose) &#123;</span><br><span class="line">      <span class="comment">// scalastyle:off println</span></span><br><span class="line">      printStream.println(appArgs)</span><br><span class="line">      <span class="comment">// scalastyle:on println</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//根据传入的参数匹配对应的执行方法</span></span><br><span class="line">    appArgs.action <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="comment">//根据传入的参数提交命令</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">SUBMIT</span> =&gt; submit(appArgs, uninitLog)</span><br><span class="line">        <span class="comment">//只有standalone和mesos集群模式才触发</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">KILL</span> =&gt; kill(appArgs)</span><br><span class="line">      <span class="comment">//只有standalone和mesos集群模式才触发</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">REQUEST_STATUS</span> =&gt; requestStatus(appArgs)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h4 id="2-4-1-submit十分关键，主要分为两步骤"><a href="#2-4-1-submit十分关键，主要分为两步骤" class="headerlink" title="2.4.1　submit十分关键，主要分为两步骤"></a>2.4.1　submit十分关键，主要分为两步骤</h4><p>（1）调用prepareSubmitEnvironment</p><p>（2）调用doRunMain</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180502185922203-694329056.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本</title>
      <link href="/2019-06-15-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89SparkCore%E7%9A%84%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%B8%80%EF%BC%89%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC.html"/>
      <url>/2019-06-15-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89SparkCore%E7%9A%84%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%B8%80%EF%BC%89%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、启动脚本分析"><a href="#一、启动脚本分析" class="headerlink" title="一、启动脚本分析"></a>一、启动脚本分析</h2><p>独立部署模式下，主要由master和slaves组成，master可以利用zk实现高可用性，其driver，work，app等信息可以持久化到zk上；slaves由一台至多台主机构成。Driver通过向Master申请资源获取运行环境。</p><p>启动master和slaves主要是执行/usr/dahua/spark/sbin目录下的start-master.sh和start-slaves.sh，或者执行</p><p>start-all.sh，其中star-all.sh本质上就是调用start-master.sh和start-slaves.sh</p><h3 id="1-1-start-all-sh"><a href="#1-1-start-all-sh" class="headerlink" title="1.1　start-all.sh"></a>1.1　start-all.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见以下分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3.执行<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/start-master.sh，见以下分析</span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin"/start-master.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4.执行<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/start-slaves.sh，见以下分析</span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin"/star`t-slaves.sh</span><br></pre></td></tr></table></figure><p>其中start-master.sh和start-slave.sh分别调用的是</p><p>org.apache.spark.deploy.master.Master和org.apache.spark.deploy.worker.Worker</p><h3 id="1-2-start-master-sh"><a href="#1-2-start-master-sh" class="headerlink" title="1.2　start-master.sh"></a>1.2　start-master.sh</h3><p>start-master.sh调用了spark-daemon.sh，注意这里指定了启动的类</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> NOTE: This exact class name is matched downstream by SparkSubmit.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Any changes need to be reflected there.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">2.设置CLASS=<span class="string">"org.apache.spark.deploy.master.Master"</span></span></span><br><span class="line">CLASS="org.apache.spark.deploy.master.Master"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3.如果参数结尾包含--<span class="built_in">help</span>或者-h则打印帮助信息，并退出</span></span><br><span class="line">if [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then</span><br><span class="line">  echo "Usage: ./sbin/start-master.sh [options]"</span><br><span class="line">  pattern="Usage:"</span><br><span class="line">  pattern+="\|Using Spark's default log4j profile:"</span><br><span class="line">  pattern+="\|Registered signal handlers for"</span><br><span class="line"></span><br><span class="line">  "$&#123;SPARK_HOME&#125;"/bin/spark-class $CLASS --help 2&gt;&amp;1 | grep -v "$pattern" 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4.设置ORIGINAL_ARGS为所有参数</span></span><br><span class="line">ORIGINAL_ARGS="$@"</span><br><span class="line"><span class="meta">#</span><span class="bash">5.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">6.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">7.SPARK_MASTER_PORT为空则赋值7077</span></span><br><span class="line">if [ "$SPARK_MASTER_PORT" = "" ]; then</span><br><span class="line">  SPARK_MASTER_PORT=7077</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">8.SPARK_MASTER_HOST为空则赋值本主机名(hostname)</span></span><br><span class="line">if [ "$SPARK_MASTER_HOST" = "" ]; then</span><br><span class="line">  case `uname` in</span><br><span class="line">      (SunOS)</span><br><span class="line">      SPARK_MASTER_HOST="`/usr/sbin/check-hostname | awk '&#123;print $NF&#125;'`"</span><br><span class="line">      ;;</span><br><span class="line">      (*)</span><br><span class="line">      SPARK_MASTER_HOST="`hostname -f`"</span><br><span class="line">      ;;</span><br><span class="line">  esac</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">9.SPARK_MASTER_WEBUI_PORT为空则赋值8080</span></span><br><span class="line">if [ "$SPARK_MASTER_WEBUI_PORT" = "" ]; then</span><br><span class="line">  SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">10.执行脚本</span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start $CLASS 1 \</span><br><span class="line">  --host $SPARK_MASTER_HOST --port $SPARK_MASTER_PORT --webui-port $SPARK_MASTER_WEBUI_PORT \</span><br><span class="line"><span class="meta">  $</span><span class="bash">ORIGINAL_ARGS</span></span><br></pre></td></tr></table></figure><p>其中10肯定是重点，分析之前我们看看5，6都干了些啥，最后直译出最后一个脚本</p><h3 id="1-3-spark-config-sh-1-2的第5步"><a href="#1-3-spark-config-sh-1-2的第5步" class="headerlink" title="1.3　spark-config.sh(1.2的第5步)"></a>1.3　spark-config.sh(1.2的第5步)</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">SPARK_CONF_DIR存在就用此目录，不存在用<span class="variable">$&#123;SPARK_HOME&#125;</span>/conf</span></span><br><span class="line">export SPARK_CONF_DIR="$&#123;SPARK_CONF_DIR:-"$&#123;SPARK_HOME&#125;/conf"&#125;"</span><br><span class="line"><span class="meta">#</span><span class="bash"> Add the PySpark classes to the PYTHONPATH:</span></span><br><span class="line">if [ -z "$&#123;PYSPARK_PYTHONPATH_SET&#125;" ]; then</span><br><span class="line">  export PYTHONPATH="$&#123;SPARK_HOME&#125;/python:$&#123;PYTHONPATH&#125;"</span><br><span class="line">  export PYTHONPATH="$&#123;SPARK_HOME&#125;/python/lib/py4j-0.10.6-src.zip:$&#123;PYTHONPATH&#125;"</span><br><span class="line">  export PYSPARK_PYTHONPATH_SET=1</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="1-4-load-spark-env-sh-1-2的第6步"><a href="#1-4-load-spark-env-sh-1-2的第6步" class="headerlink" title="1.4　load-spark-env.sh(1.2的第6步)"></a>1.4　load-spark-env.sh(1.2的第6步)</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">2.判断SPARK_ENV_LOADED是否有值，没有将其设置为1</span></span><br><span class="line">if [ -z "$SPARK_ENV_LOADED" ]; then</span><br><span class="line">  export SPARK_ENV_LOADED=1</span><br><span class="line"><span class="meta">#</span><span class="bash">3.设置user_conf_dir为SPARK_CONF_DIR或SPARK_HOME/conf</span></span><br><span class="line">  export SPARK_CONF_DIR="$&#123;SPARK_CONF_DIR:-"$&#123;SPARK_HOME&#125;"/conf&#125;"</span><br><span class="line"><span class="meta">#</span><span class="bash">4.执行<span class="string">"<span class="variable">$&#123;user_conf_dir&#125;</span>/spark-env.sh"</span> [注：<span class="built_in">set</span> -/+a含义再做研究]</span></span><br><span class="line">  if [ -f "$&#123;SPARK_CONF_DIR&#125;/spark-env.sh" ]; then</span><br><span class="line">    # Promote all variable declarations to environment (exported) variables</span><br><span class="line">    set -a</span><br><span class="line">    . "$&#123;SPARK_CONF_DIR&#125;/spark-env.sh"</span><br><span class="line">    set +a</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Setting SPARK_SCALA_VERSION <span class="keyword">if</span> not already <span class="built_in">set</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">5.选择scala版本，2.11和2.12都存在的情况下，优先选择2.11</span></span><br><span class="line">if [ -z "$SPARK_SCALA_VERSION" ]; then</span><br><span class="line"></span><br><span class="line">  ASSEMBLY_DIR2="$&#123;SPARK_HOME&#125;/assembly/target/scala-2.11"</span><br><span class="line">  ASSEMBLY_DIR1="$&#123;SPARK_HOME&#125;/assembly/target/scala-2.12"</span><br><span class="line"></span><br><span class="line">  if [[ -d "$ASSEMBLY_DIR2" &amp;&amp; -d "$ASSEMBLY_DIR1" ]]; then</span><br><span class="line">    echo -e "Presence of build for multiple Scala versions detected." 1&gt;&amp;2</span><br><span class="line">    echo -e 'Either clean one of them or, export SPARK_SCALA_VERSION in spark-env.sh.' 1&gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line"></span><br><span class="line">  if [ -d "$ASSEMBLY_DIR2" ]; then</span><br><span class="line">    export SPARK_SCALA_VERSION="2.11"</span><br><span class="line">  else</span><br><span class="line">    export SPARK_SCALA_VERSION="2.12"</span><br><span class="line">  fi</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="1-5-spark-env-sh"><a href="#1-5-spark-env-sh" class="headerlink" title="1.5　spark-env.sh"></a>1.5　spark-env.sh</h3><p>列举很多种模式的选项配置</p><h3 id="1-6-spark-daemon-sh"><a href="#1-6-spark-daemon-sh" class="headerlink" title="1.6　spark-daemon.sh"></a>1.6　spark-daemon.sh</h3><p>回过头来看看<strong>1.2第10步</strong>中需要直译出的最后一个脚本,如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/spark-daemon.sh start org.apache.spark.deploy.master.Master 1 --host hostname --port 7077 --webui-port 8080</span><br></pre></td></tr></table></figure><p>上面搞了半天只是设置了变量，最终才进入主角，继续分析spark-daemon.sh脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.参数个数小于等于1，打印帮助</span></span><br><span class="line">if [ $# -le 1 ]; then</span><br><span class="line">  echo $usage</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">2.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">3.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见上述分析 [类似脚本是否有重复？原因是有的人是直接用spark-daemon.sh启动的服务，反正重复设置下变量不需要什么代价]</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> get arguments</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Check <span class="keyword">if</span> --config is passed as an argument. It is an optional parameter.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Exit <span class="keyword">if</span> the argument is not a directory.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4.判断第一个参数是否是--config,如果是取空格后一个字符串，然后判断该目录是否存在，不存在则打印错误信息并退出，存在设置SPARK_CONF_DIR为该目录,<span class="built_in">shift</span>到下一个参数<span class="comment">#[注：--config只能用在第一参数上]</span></span></span><br><span class="line">if [ "$1" == "--config" ]</span><br><span class="line">then</span><br><span class="line">  shift</span><br><span class="line">  conf_dir="$1"</span><br><span class="line">  if [ ! -d "$conf_dir" ]</span><br><span class="line">  then</span><br><span class="line">    echo "ERROR : $conf_dir is not a directory"</span><br><span class="line">    echo $usage</span><br><span class="line">    exit 1</span><br><span class="line">  else</span><br><span class="line">    export SPARK_CONF_DIR="$conf_dir"</span><br><span class="line">  fi</span><br><span class="line">  shift</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">5.分别设置option、<span class="built_in">command</span>、instance为后面的三个参数(如：option=start,<span class="built_in">command</span>=org.apache.spark.deploy.master.Master,instance=1)<span class="comment">#[注：很多人用spark-daemon.sh启动服务不成功的原因是名字不全]</span></span></span><br><span class="line">option=$1</span><br><span class="line">shift</span><br><span class="line">command=$1</span><br><span class="line">shift</span><br><span class="line">instance=$1</span><br><span class="line">shift</span><br><span class="line"><span class="meta">#</span><span class="bash">6.日志回滚函数，主要用于更改日志名，如<span class="built_in">log</span>--&gt;log.1等，略过</span></span><br><span class="line">spark_rotate_log ()</span><br><span class="line">&#123;</span><br><span class="line">    log=$1;</span><br><span class="line">    num=5;</span><br><span class="line">    if [ -n "$2" ]; then</span><br><span class="line">    num=$2</span><br><span class="line">    fi</span><br><span class="line">    if [ -f "$log" ]; then # rotate logs</span><br><span class="line">    while [ $num -gt 1 ]; do</span><br><span class="line">        prev=`expr $num - 1`</span><br><span class="line">        [ -f "$log.$prev" ] &amp;&amp; mv "$log.$prev" "$log.$num"</span><br><span class="line">        num=$prev</span><br><span class="line">    done</span><br><span class="line">    mv "$log" "$log.$num";</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">7.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">8.判断SPARK_IDENT_STRING是否有值，没有将其设置为<span class="variable">$USER</span>(linux用户)</span></span><br><span class="line">if [ "$SPARK_IDENT_STRING" = "" ]; then</span><br><span class="line">  export SPARK_IDENT_STRING="$USER"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">9.设置SPARK_PRINT_LAUNCH_COMMAND=1</span></span><br><span class="line">export SPARK_PRINT_LAUNCH_COMMAND="1"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> get <span class="built_in">log</span> directory</span></span><br><span class="line"><span class="meta">#</span><span class="bash">10.判断SPARK_LOG_DIR是否有值，没有将其设置为<span class="variable">$&#123;SPARK_HOME&#125;</span>/logs，并创建改目录，测试创建文件，修改权限</span></span><br><span class="line">if [ "$SPARK_LOG_DIR" = "" ]; then</span><br><span class="line">  export SPARK_LOG_DIR="$&#123;SPARK_HOME&#125;/logs"</span><br><span class="line">fi</span><br><span class="line">mkdir -p "$SPARK_LOG_DIR"</span><br><span class="line">touch "$SPARK_LOG_DIR"/.spark_test &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">TEST_LOG_DIR=$?</span><br><span class="line">if [ "$&#123;TEST_LOG_DIR&#125;" = "0" ]; then</span><br><span class="line">  rm -f "$SPARK_LOG_DIR"/.spark_test</span><br><span class="line">else</span><br><span class="line">  chown "$SPARK_IDENT_STRING" "$SPARK_LOG_DIR"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">11.判断SPARK_PID_DIR是否有值，没有将其设置为/tmp</span></span><br><span class="line">if [ "$SPARK_PID_DIR" = "" ]; then</span><br><span class="line">  SPARK_PID_DIR=/tmp</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> some variables</span></span><br><span class="line"><span class="meta">#</span><span class="bash">12.设置<span class="built_in">log</span>和pid</span></span><br><span class="line">log="$SPARK_LOG_DIR/spark-$SPARK_IDENT_STRING-$command-$instance-$HOSTNAME.out"</span><br><span class="line">pid="$SPARK_PID_DIR/spark-$SPARK_IDENT_STRING-$command-$instance.pid"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Set default scheduling priority</span></span><br><span class="line"><span class="meta">#</span><span class="bash">13.判断SPARK_NICENESS是否有值，没有将其设置为0 [注：调度优先级，见后面]</span></span><br><span class="line">if [ "$SPARK_NICENESS" = "" ]; then</span><br><span class="line">    export SPARK_NICENESS=0</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">14.execute_command()函数，暂且略过，调用时再作分析</span></span><br><span class="line">execute_command() &#123;</span><br><span class="line">  if [ -z $&#123;SPARK_NO_DAEMONIZE+set&#125; ]; then</span><br><span class="line">      nohup -- "$@" &gt;&gt; $log 2&gt;&amp;1 &lt; /dev/null &amp;</span><br><span class="line">      newpid="$!"</span><br><span class="line"></span><br><span class="line">      echo "$newpid" &gt; "$pid"</span><br><span class="line"></span><br><span class="line">      # Poll for up to 5 seconds for the java process to start</span><br><span class="line">      for i in &#123;1..10&#125;</span><br><span class="line">      do</span><br><span class="line">        if [[ $(ps -p "$newpid" -o comm=) =~ "java" ]]; then</span><br><span class="line">           break</span><br><span class="line">        fi</span><br><span class="line">        sleep 0.5</span><br><span class="line">      done</span><br><span class="line"></span><br><span class="line">      sleep 2</span><br><span class="line">      # Check if the process has died; in that case we'll tail the log so the user can see</span><br><span class="line">      if [[ ! $(ps -p "$newpid" -o comm=) =~ "java" ]]; then</span><br><span class="line">        echo "failed to launch: $@"</span><br><span class="line">        tail -10 "$log" | sed 's/^/  /'</span><br><span class="line">        echo "full log in $log"</span><br><span class="line">      fi</span><br><span class="line">  else</span><br><span class="line">      "$@"</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">15.进入<span class="keyword">case</span>语句，判断option值，进入该分支，我们以start为例</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   执行run_command class <span class="string">"<span class="variable">$@</span>"</span>，其中<span class="variable">$@</span>此时为空，经验证，启动带上此参数后，关闭也需，不然关闭不了，后面再分析此参数作用</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   我们正式进入run_command()函数，分析</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   I.设置mode=class,创建SPARK_PID_DIR，上面的pid文件是否存在，</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   II.SPARK_MASTER不为空，同步删除某些文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   III.回滚<span class="built_in">log</span>日志</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   IV.进入<span class="keyword">case</span>，<span class="built_in">command</span>=org.apache.spark.deploy.master.Master，最终执行</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       nohup nice -n <span class="string">"<span class="variable">$SPARK_NICENESS</span>"</span> <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/spark-class <span class="variable">$command</span> <span class="string">"<span class="variable">$@</span>"</span> &gt;&gt; <span class="string">"<span class="variable">$log</span>"</span> 2&gt;&amp;1 &lt; /dev/null &amp;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       newpid=<span class="string">"$!"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">       <span class="built_in">echo</span> <span class="string">"<span class="variable">$newpid</span>"</span> &gt; <span class="string">"<span class="variable">$pid</span>"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">   重点转向bin/spark-class org.apache.spark.deploy.master.Master</span></span><br><span class="line">run_command() &#123;</span><br><span class="line">  mode="$1"</span><br><span class="line">  shift</span><br><span class="line"></span><br><span class="line">  mkdir -p "$SPARK_PID_DIR"</span><br><span class="line"></span><br><span class="line">  if [ -f "$pid" ]; then</span><br><span class="line">    TARGET_ID="$(cat "$pid")"</span><br><span class="line">    if [[ $(ps -p "$TARGET_ID" -o comm=) =~ "java" ]]; then</span><br><span class="line">      echo "$command running as process $TARGET_ID.  Stop it first."</span><br><span class="line">      exit 1</span><br><span class="line">    fi</span><br><span class="line">  fi</span><br><span class="line"></span><br><span class="line">  if [ "$SPARK_MASTER" != "" ]; then</span><br><span class="line">    echo rsync from "$SPARK_MASTER"</span><br><span class="line">    rsync -a -e ssh --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' "$SPARK_MASTER/" "$&#123;SPARK_HOME&#125;"</span><br><span class="line">  fi</span><br><span class="line"></span><br><span class="line">  spark_rotate_log "$log"</span><br><span class="line">  echo "starting $command, logging to $log"</span><br><span class="line"></span><br><span class="line">  case "$mode" in</span><br><span class="line">    (class)</span><br><span class="line">      execute_command nice -n "$SPARK_NICENESS" "$&#123;SPARK_HOME&#125;"/bin/spark-class "$command" "$@"</span><br><span class="line">      ;;</span><br><span class="line"></span><br><span class="line">    (submit)</span><br><span class="line">      execute_command nice -n "$SPARK_NICENESS" bash "$&#123;SPARK_HOME&#125;"/bin/spark-submit --class "$command" "$@"</span><br><span class="line">      ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">      echo "unknown mode: $mode"</span><br><span class="line">      exit 1</span><br><span class="line">      ;;</span><br><span class="line">  esac</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $option in</span><br><span class="line"></span><br><span class="line">  (submit)</span><br><span class="line">    run_command submit "$@"</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">  (start)</span><br><span class="line">    run_command class "$@"</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">  (stop)</span><br><span class="line"></span><br><span class="line">    if [ -f $pid ]; then</span><br><span class="line">      TARGET_ID="$(cat "$pid")"</span><br><span class="line">      if [[ $(ps -p "$TARGET_ID" -o comm=) =~ "java" ]]; then</span><br><span class="line">        echo "stopping $command"</span><br><span class="line">        kill "$TARGET_ID" &amp;&amp; rm -f "$pid"</span><br><span class="line">      else</span><br><span class="line">        echo "no $command to stop"</span><br><span class="line">      fi</span><br><span class="line">    else</span><br><span class="line">      echo "no $command to stop"</span><br><span class="line">    fi</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">  (status)</span><br><span class="line"></span><br><span class="line">    if [ -f $pid ]; then</span><br><span class="line">      TARGET_ID="$(cat "$pid")"</span><br><span class="line">      if [[ $(ps -p "$TARGET_ID" -o comm=) =~ "java" ]]; then</span><br><span class="line">        echo $command is running.</span><br><span class="line">        exit 0</span><br><span class="line">      else</span><br><span class="line">        echo $pid file is present but $command not running</span><br><span class="line">        exit 1</span><br><span class="line">      fi</span><br><span class="line">    else</span><br><span class="line">      echo $command not running.</span><br><span class="line">      exit 2</span><br><span class="line">    fi</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">  (*)</span><br><span class="line">    echo $usage</span><br><span class="line">    exit 1</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h3 id="1-7-spark-class"><a href="#1-7-spark-class" class="headerlink" title="1.7　spark-class"></a>1.7　spark-class</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;"/bin/load-spark-env.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find the java binary</span></span><br><span class="line"><span class="meta">#</span><span class="bash">3.判断JAVA_HOME是否为NULL，不是则设置RUNNER=<span class="string">"<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin/java"</span>，否则找系统自带，在没有则报未设置，并退出</span></span><br><span class="line">if [ -n "$&#123;JAVA_HOME&#125;" ]; then</span><br><span class="line">  RUNNER="$&#123;JAVA_HOME&#125;/bin/java"</span><br><span class="line">else</span><br><span class="line">  if [ "$(command -v java)" ]; then</span><br><span class="line">    RUNNER="java"</span><br><span class="line">  else</span><br><span class="line">    echo "JAVA_HOME is not set" &gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find Spark jars.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">4.查找SPARK_JARS_DIR，若<span class="variable">$&#123;SPARK_HOME&#125;</span>/RELEASE文件存在，则SPARK_JARS_DIR=<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/jars"</span>，否则</span></span><br><span class="line"><span class="meta">#</span><span class="bash">SPARK_JARS_DIR=<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/assembly/target/scala-<span class="variable">$SPARK_SCALA_VERSION</span>/jars"</span></span></span><br><span class="line">if [ -d "$&#123;SPARK_HOME&#125;/jars" ]; then</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/jars"</span><br><span class="line">else</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5.若SPARK_JARS_DIR不存在且<span class="variable">$SPARK_TESTING</span><span class="variable">$SPARK_SQL_TESTING</span>有值[注：一般我们不设置这两变量]，报错退出，否则LAUNCH_CLASSPATH=<span class="string">"<span class="variable">$SPARK_JARS_DIR</span>/*"</span></span></span><br><span class="line">if [ ! -d "$SPARK_JARS_DIR" ] &amp;&amp; [ -z "$SPARK_TESTING$SPARK_SQL_TESTING" ]; then</span><br><span class="line">  echo "Failed to find Spark jars directory ($SPARK_JARS_DIR)." 1&gt;&amp;2</span><br><span class="line">  echo "You need to build Spark with the target \"package\" before running this program." 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">else</span><br><span class="line">  LAUNCH_CLASSPATH="$SPARK_JARS_DIR/*"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Add the launcher build dir to the classpath <span class="keyword">if</span> requested.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">6.SPARK_PREPEND_CLASSES不是NULL，则LAUNCH_CLASSPATH=<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/launcher/target/scala-<span class="variable">$SPARK_SCALA_VERSION</span>/classes:<span class="variable">$LAUNCH_CLASSPATH</span>"</span>，<span class="comment">#添加编译相关至LAUNCH_CLASSPATH</span></span></span><br><span class="line">if [ -n "$SPARK_PREPEND_CLASSES" ]; then</span><br><span class="line">  LAUNCH_CLASSPATH="$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> For tests</span></span><br><span class="line"><span class="meta">#</span><span class="bash">7.SPARK_TESTING不是NULL，则<span class="built_in">unset</span> YARN_CONF_DIR和<span class="built_in">unset</span> HADOOP_CONF_DIR，暂且当做是为了某种测试</span></span><br><span class="line">if [[ -n "$SPARK_TESTING" ]]; then</span><br><span class="line">  unset YARN_CONF_DIR</span><br><span class="line">  unset HADOOP_CONF_DIR</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">8.build_command函数，略过</span></span><br><span class="line">build_command() &#123;</span><br><span class="line">  "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"</span><br><span class="line">  printf "%d\0" $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Turn off posix mode since it does not allow process substitution</span></span><br><span class="line">set +o posix</span><br><span class="line">CMD=()</span><br><span class="line">while IFS= read -d '' -r ARG; do</span><br><span class="line">  CMD+=("$ARG")</span><br><span class="line"><span class="meta">  #</span><span class="bash">9.最终调用<span class="string">"<span class="variable">$RUNNER</span>"</span> -Xmx128m -cp <span class="string">"<span class="variable">$LAUNCH_CLASSPATH</span>"</span> org.apache.spark.launcher.Main <span class="string">"<span class="variable">$@</span>"</span>，</span></span><br><span class="line"><span class="meta">  #</span><span class="bash">直译：java -Xmx128m -cp <span class="string">"<span class="variable">$LAUNCH_CLASSPATH</span>"</span> org.apache.spark.launcher.Main <span class="string">"<span class="variable">$@</span>"</span></span></span><br><span class="line"><span class="meta">  #</span><span class="bash">转向java类org.apache.spark.launcher.Main，这就是java入口类</span></span><br><span class="line">done &lt; &lt;(build_command "$@")</span><br><span class="line"></span><br><span class="line">COUNT=$&#123;#CMD[@]&#125;</span><br><span class="line">LAST=$((COUNT - 1))</span><br><span class="line">LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Certain JVM failures result <span class="keyword">in</span> errors being printed to stdout (instead of stderr), <span class="built_in">which</span> causes</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the code that parses the output of the launcher to get confused. In those cases, check <span class="keyword">if</span> the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">exit</span> code is an <span class="built_in">integer</span>, and <span class="keyword">if</span> it<span class="string">'s not, handle it as a special error case.</span></span></span><br><span class="line">if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then</span><br><span class="line">  echo "$&#123;CMD[@]&#125;" | head -n-1 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $LAUNCHER_EXIT_CODE != 0 ]; then</span><br><span class="line">  exit $LAUNCHER_EXIT_CODE</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">CMD=("$&#123;CMD[@]:0:$LAST&#125;")</span><br><span class="line">exec "$&#123;CMD[@]&#125;"</span><br></pre></td></tr></table></figure><h3 id="1-8-start-slaves-sh"><a href="#1-8-start-slaves-sh" class="headerlink" title="1.8　start-slaves.sh"></a>1.8　start-slaves.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find the port number <span class="keyword">for</span> the master</span></span><br><span class="line"><span class="meta">#</span><span class="bash">4.SPARK_MASTER_PORT为空则设置为7077</span></span><br><span class="line">if [ "$SPARK_MASTER_PORT" = "" ]; then</span><br><span class="line">  SPARK_MASTER_PORT=7077</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5.SPARK_MASTER_HOST为空则设置为`hostname`</span></span><br><span class="line">if [ "$SPARK_MASTER_HOST" = "" ]; then</span><br><span class="line">  case `uname` in</span><br><span class="line">      (SunOS)</span><br><span class="line">      SPARK_MASTER_HOST="`/usr/sbin/check-hostname | awk '&#123;print $NF&#125;'`"</span><br><span class="line">      ;;</span><br><span class="line">      (*)</span><br><span class="line">      SPARK_MASTER_HOST="`hostname -f`"</span><br><span class="line">      ;;</span><br><span class="line">  esac</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Launch the slaves</span></span><br><span class="line"><span class="meta">#</span><span class="bash">6.启动slaves，</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/slaves.sh"</span> <span class="built_in">cd</span> <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span> \; <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/start-slave.sh"</span> <span class="string">"spark://<span class="variable">$SPARK_MASTER_HOST</span>:<span class="variable">$SPARK_MASTER_PORT</span>"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">   遍历conf/slaves中主机，其中有设置SPARK_SSH_OPTS，ssh每一台机器执行<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/start-slave.sh"</span> <span class="string">"spark://<span class="variable">$SPARK_MASTER_HOST</span>:<span class="variable">$SPARK_MASTER_PORT</span>"</span></span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin/slaves.sh" cd "$&#123;SPARK_HOME&#125;" \; "$&#123;SPARK_HOME&#125;/sbin/start-slave.sh" "spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT"</span><br></pre></td></tr></table></figure><h3 id="1-9-转向start-slave-sh"><a href="#1-9-转向start-slave-sh" class="headerlink" title="1.9　转向start-slave.sh"></a>1.9　转向start-slave.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.设置CLASS=<span class="string">"org.apache.spark.deploy.worker.Worker"</span></span></span><br><span class="line">CLASS="org.apache.spark.deploy.worker.Worker"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3.如果参数结尾包含--<span class="built_in">help</span>或者-h则打印帮助信息，并退出</span></span><br><span class="line">if [[ $# -lt 1 ]] || [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then</span><br><span class="line">  echo "Usage: ./sbin/start-slave.sh [options] &lt;master&gt;"</span><br><span class="line">  pattern="Usage:"</span><br><span class="line">  pattern+="\|Using Spark's default log4j profile:"</span><br><span class="line">  pattern+="\|Registered signal handlers for"</span><br><span class="line"></span><br><span class="line">  "$&#123;SPARK_HOME&#125;"/bin/spark-class $CLASS --help 2&gt;&amp;1 | grep -v "$pattern" 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">5.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">6.MASTER=<span class="variable">$1</span>,这里MASTER=spark://hostname:7077，然后<span class="built_in">shift</span>，也就是说单独启动单个slave使用start-slave.sh spark://hostname:7077</span></span><br><span class="line">MASTER=$1</span><br><span class="line">shift</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">7.SPARK_WORKER_WEBUI_PORT为空则设置为8081</span></span><br><span class="line">if [ "$SPARK_WORKER_WEBUI_PORT" = "" ]; then</span><br><span class="line">  SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">8.函数start_instance，略过</span></span><br><span class="line">function start_instance &#123;</span><br><span class="line"><span class="meta">#</span><span class="bash">设置WORKER_NUM=<span class="variable">$1</span></span></span><br><span class="line">  WORKER_NUM=$1</span><br><span class="line">  shift</span><br><span class="line"></span><br><span class="line">  if [ "$SPARK_WORKER_PORT" = "" ]; then</span><br><span class="line">    PORT_FLAG=</span><br><span class="line">    PORT_NUM=</span><br><span class="line">  else</span><br><span class="line">    PORT_FLAG="--port"</span><br><span class="line">    PORT_NUM=$(( $SPARK_WORKER_PORT + $WORKER_NUM - 1 ))</span><br><span class="line">  fi</span><br><span class="line">  WEBUI_PORT=$(( $SPARK_WORKER_WEBUI_PORT + $WORKER_NUM - 1 ))</span><br><span class="line"></span><br><span class="line"><span class="meta">  #</span><span class="bash">直译：spark-daemon.sh start org.apache.spark.deploy.worker.Worker 1 --webui-port 7077 spark://hostname:7077</span></span><br><span class="line"><span class="meta">  #</span><span class="bash">代码再次转向spark-daemon.sh，见上诉分析</span></span><br><span class="line">  "$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start $CLASS $WORKER_NUM \</span><br><span class="line">     --webui-port "$WEBUI_PORT" $PORT_FLAG $PORT_NUM $MASTER "$@"</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">9.判断SPARK_WORKER_INSTANCES(可以认为是单节点Worker进程数)是否为空</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   为空，则start_instance 1 <span class="string">"<span class="variable">$@</span>"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">   不为空，则循环</span></span><br><span class="line"><span class="meta">#</span><span class="bash">         <span class="keyword">for</span> ((i=0; i&lt;<span class="variable">$SPARK_WORKER_INSTANCES</span>; i++)); <span class="keyword">do</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">           start_instance $(( 1 + <span class="variable">$i</span> )) <span class="string">"<span class="variable">$@</span>"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">         <span class="keyword">done</span></span></span><br><span class="line">if [ "$SPARK_WORKER_INSTANCES" = "" ]; then</span><br><span class="line">  start_instance 1 "$@"</span><br><span class="line">else</span><br><span class="line">  for ((i=0; i&lt;$SPARK_WORKER_INSTANCES; i++)); do</span><br><span class="line"><span class="meta">  #</span><span class="bash">10.转向start_instance函数</span></span><br><span class="line">    start_instance $(( 1 + $i )) "$@"</span><br><span class="line">  done</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h2 id="二、其他脚本"><a href="#二、其他脚本" class="headerlink" title="二、其他脚本"></a>二、其他脚本</h2><h3 id="2-1-start-history-server-sh"><a href="#2-1-start-history-server-sh" class="headerlink" title="2.1　start-history-server.sh"></a>2.1　start-history-server.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">3.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">4.exec <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/spark-daemon.sh start org.apache.spark.deploy.history.HistoryServer 1 <span class="variable">$@</span> ，见上诉分析</span></span><br><span class="line">exec "$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start org.apache.spark.deploy.history.HistoryServer 1 "$@"</span><br></pre></td></tr></table></figure><h3 id="2-2-start-shuffle-service-sh"><a href="#2-2-start-shuffle-service-sh" class="headerlink" title="2.2　start-shuffle-service.sh"></a>2.2　start-shuffle-service.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">3.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">4.exec <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/spark-daemon.sh start org.apache.spark.deploy.ExternalShuffleService 1 ，见上诉分析</span></span><br><span class="line">exec "$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start org.apache.spark.deploy.ExternalShuffleService 1</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器</title>
      <link href="/2019-06-14-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98JVM%E7%9A%84GC%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8.html"/>
      <url>/2019-06-14-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98JVM%E7%9A%84GC%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        垃圾收集 Garbage Collection 通常被称为“GC”，它诞生于1960年 MIT 的 Lisp 语言，经过半个多世纪，目前已经十分成熟了。</p><p>jvm 中，程序计数器、虚拟机栈、本地方法栈都是随线程而生随线程而灭，栈帧随着方法的进入和退出做入栈和出栈操作，实现了自动的内存清理，因此，我们的<strong>内存垃圾回收主要集中于 java 堆和方法区中</strong>，在程序运行期间，这部分内存的分配和使用都是动态的。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、垃圾收集器-garbage-collector-GC-是什么？"><a href="#一、垃圾收集器-garbage-collector-GC-是什么？" class="headerlink" title="一、垃圾收集器(garbage collector (GC)) 是什么？"></a>一、<strong>垃圾收集器(garbage collector (GC)) 是什么？</strong></h2><p>GC其实是一种自动的内存管理工具，其行为主要包括2步</p><ul><li>在Java堆中，为新<strong>创建</strong>的对象分配空间</li><li>在Java堆中，<strong>回收</strong>没用的对象占用的空间</li></ul><h2 id="二、为什么需要GC？"><a href="#二、为什么需要GC？" class="headerlink" title="二、为什么需要GC？**"></a>二、为什么需要GC？**</h2><p>释放开发人员的生产力</p><h2 id="三、为什么需要多种GC？"><a href="#三、为什么需要多种GC？" class="headerlink" title="三、为什么需要多种GC？**"></a>三、为什么需要多种GC？**</h2><p>首先，Java平台被部署在各种各样的硬件资源上，其次，在Java平台上部署和运行着各种各样的应用，并且用户对不同的应用的 <em>性能指标</em> (吞吐率和延迟) 预期也不同，为了满足不同应用的对内存管理的不同需求，JVM提供了多种GC以供选择</p><p><em>性能指标</em><br>最大停顿时长：垃圾回收导致的应用停顿时间的最大值<br>吞吐率：垃圾回收停顿时长和应用运行总时长的比例</p><p>不同的GC能满足不同应用不同的性能需求，现有的GC包括：</p><ul><li><ul><li>序列化GC(serial garbage collector)：适合占用内存少的应用</li><li>并行GC 或 吞吐率GC(parallel or throughput garbage collector)：适合占用内存较多，多CPU，追求高吞吐率的应用</li><li>并发GC：适合占用内存较多，多CPU的应用，对延迟有要求的应用</li></ul></li></ul><h2 id="四、对象存活的判断"><a href="#四、对象存活的判断" class="headerlink" title="四、对象存活的判断"></a>四、对象存活的判断</h2><p>判断对象是否存活一般有两种方式：</p><p><strong>引用计数</strong>：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，缺点是无法解决对象相互循环引用的问题。</p><p><strong>可达性分析（Reachability Analysis）</strong>：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。不可达对象。</p><blockquote><p>在Java语言中，GC Roots包括：</p><p>  虚拟机栈中引用的对象。</p><p>  方法区中类静态属性实体引用的对象。</p><p>  方法区中常量引用的对象。</p><p>  本地方法栈中JNI引用的对象。</p></blockquote><p>由于循环引用的问题，一般采用跟踪（<strong>可达性分析</strong>）方法</p><h2 id="五、垃圾回收算法"><a href="#五、垃圾回收算法" class="headerlink" title="五、垃圾回收算法"></a>五、垃圾回收算法</h2><h3 id="5-1-标记-清除算法"><a href="#5-1-标记-清除算法" class="headerlink" title="5.1　标记 -清除算法"></a>5.1　标记 -清除算法</h3><p>“标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。</p><p>它的主要缺点有两个：一个是效率问题，标记和清除过程的效率都不高；另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430153744337-319263441.png" alt="img"></p><h3 id="5-2-复制算法"><a href="#5-2-复制算法" class="headerlink" title="5.2　复制算法"></a>5.2　复制算法</h3><p>“复制”（Copying）的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。</p><p>这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，持续复制长生存期的对象则导致效率降低。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430153826268-1037960889.png" alt="img"></p><h3 id="5-3-标记-整理算法"><a href="#5-3-标记-整理算法" class="headerlink" title="5.3　标记-整理算法"></a>5.3　标记-整理算法</h3><p>复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。</p><p>根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430153949072-653785951.png" alt="img"></p><h3 id="5-4-分代收集算法"><a href="#5-4-分代收集算法" class="headerlink" title="5.4　分代收集算法"></a>5.4　分代收集算法</h3><p>GC分代的基本假设：绝大部分对象的生命周期都非常短暂，存活时间短。</p><p>“分代收集”（Generational Collection）算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或“标记-整理”算法来进行回收。</p><h2 id="六、垃圾收集器"><a href="#六、垃圾收集器" class="headerlink" title="六、垃圾收集器"></a>六、垃圾收集器</h2><p>如果说收集算法是内存回收的方法论，垃圾收集器就是内存回收的具体实现，不同厂商、不同版本的虚拟机实现差别很大，HotSpot中包含的收集器如下：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430154441316-1293727491.png" alt="img"></p><h3 id="6-1-Serial收集器"><a href="#6-1-Serial收集器" class="headerlink" title="6.1　Serial收集器"></a>6.1　Serial收集器</h3><p><strong>串行收集器</strong>是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。新生代、老年代使用串行回收；新生代复制算法、老年代标记-压缩；垃圾收集的过程中会Stop The World（服务暂停）</p><p>参数控制：-XX:+UseSerialGC  串行收集器</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430154734737-1197475351.png" alt="img"></p><h3 id="6-2-ParNew收集器"><a href="#6-2-ParNew收集器" class="headerlink" title="6.2　ParNew收集器"></a>6.2　ParNew收集器</h3><p>ParNew收集器其实就是Serial收集器的多线程版本。新生代并行，老年代串行；新生代复制算法、老年代标记-压缩</p><p>参数控制：-XX:+UseParNewGC  ParNew收集器</p><p>-XX:ParallelGCThreads 限制线程数量</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430155050197-460365340.png" alt="img"></p><h3 id="6-3-Parallel收集器"><a href="#6-3-Parallel收集器" class="headerlink" title="6.3　Parallel收集器"></a>6.3　Parallel收集器</h3><p>Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。可以通过参数来打开自适应调节策略，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或最大的吞吐量；也可以通过参数控制GC的时间不大于多少毫秒或者比例；新生代复制算法、老年代标记-压缩</p><p>参数控制：-XX:+UseParallelGC  使用Parallel收集器+ 老年代串行</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430155431788-84955319.png" alt="img"></p><h3 id="6-4-CMS收集器"><a href="#6-4-CMS收集器" class="headerlink" title="6.4　CMS收集器"></a>6.4　CMS收集器</h3><p>CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用都集中在互联网站或B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。</p><p>从名字（包含“Mark Sweep”）上就可以看出CMS收集器是基于“标记-清除”算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为4个步骤，包括： </p><p>初始标记（CMS initial mark）</p><p>并发标记（CMS concurrent mark）</p><p>重新标记（CMS remark）</p><p>并发清除（CMS concurrent sweep）</p><p> 其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。<br>      由于整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，所以总体上来说，CMS收集器的内存回收过程是与用户线程一起并发地执行。老年代收集器（新生代使用ParNew）</p><p>  优点:并发收集、低停顿 </p><p>   缺点：产生大量空间碎片、并发阶段会降低吞吐量</p><p>   参数控制：-XX:+UseConcMarkSweepGC  使用CMS收集器</p><p>​             -XX:+ UseCMSCompactAtFullCollection Full GC后，进行一次碎片整理；整理过程是独占的，会引起停顿时间变长</p><p>​            -XX:+CMSFullGCsBeforeCompaction  设置进行几次Full GC后，进行一次碎片整理</p><p>​            -XX:ParallelCMSThreads  设定CMS的线程数量（一般情况约等于可用CPU数量）</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430155615448-2073405176.png" alt="img"></p><h3 id="6-5-G1收集器"><a href="#6-5-G1收集器" class="headerlink" title="6.5　G1收集器"></a>6.5　G1收集器</h3><p>G1是目前技术发展的最前沿成果之一，HotSpot开发团队赋予它的使命是未来可以替换掉JDK1.5中发布的CMS收集器。与CMS收集器相比G1收集器有以下特点：</p><ol><li><p>空间整合，G1收集器采用标记整理算法，不会产生内存空间碎片。分配大对象时不会因为无法找到连续空间而提前触发下一次GC。</p></li><li><p>可预测停顿，这是G1的另一大优势，降低停顿时间是G1和CMS的共同关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为N毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了。</p></li></ol><p>上面提到的垃圾收集器，收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔阂了，它们都是一部分（可以不连续）Region的集合。</p><h4 id="G1对Heap的划分"><a href="#G1对Heap的划分" class="headerlink" title="G1对Heap的划分"></a>G1对Heap的划分</h4><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430155951152-2106136185.png" alt="img"></p><p>G1的新生代收集跟ParNew类似，当新生代占用达到一定比例的时候，开始出发收集。和CMS类似，G1收集器收集老年代对象会有短暂停顿。</p><h4 id="收集步骤"><a href="#收集步骤" class="headerlink" title="收集步骤"></a>收集步骤</h4><p>1、标记阶段，首先初始标记(Initial-Mark),这个阶段是停顿的(Stop the World Event)，并且会触发一次普通Mintor GC。对应GC log:GC pause (young) (inital-mark)</p><p>2、Root Region Scanning，程序运行过程中会回收survivor区(存活到老年代)，这一过程必须在young GC之前完成。</p><p>3、Concurrent Marking，在整个堆中进行并发标记(和应用程序并发执行)，此过程可能被young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那个这个区域会被立即回收(图中打X)。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430160141901-1411612026.png" alt="img"></p><p>4、Remark, 再标记，会有短暂停顿(STW)。再标记阶段是用来收集 并发标记阶段 产生新的垃圾(并发阶段和应用程序一同运行)；G1中采用了比CMS更快的初始快照算法:snapshot-at-the-beginning (SATB)。</p><p>5、Copy/Clean up，多线程清除失活对象，会有STW。G1将回收区域的存活对象拷贝到新区域，清除Remember Sets，并发清空回收区域并把它返回到空闲区域链表中。</p><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430160158306-1501053707.png" alt="img"></p><p>6、复制/清除过程后。回收区域的活性对象已经被集中回收到深蓝色和深绿色区域。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430160215565-69737884.png" alt="img"></p><h2 id="八、常用的收集器组合"><a href="#八、常用的收集器组合" class="headerlink" title="八、常用的收集器组合"></a>八、常用的收集器组合</h2><table><thead><tr><th></th><th>新生代GC策略</th><th>年老代GC策略</th><th>说明</th></tr></thead><tbody><tr><td>组合1</td><td>Serial</td><td>Serial Old</td><td>Serial和Serial Old都是单线程进行GC，特点就是GC时暂停所有应用线程。</td></tr><tr><td>组合2</td><td>Serial</td><td>CMS+Serial Old</td><td>CMS（Concurrent Mark Sweep）是并发GC，实现GC线程和应用线程并发工作，不需要暂停所有应用线程。另外，当CMS进行GC失败时，会自动使用Serial Old策略进行GC。</td></tr><tr><td>组合3</td><td>ParNew</td><td>CMS</td><td>使用-XX:+UseParNewGC选项来开启。ParNew是Serial的并行版本，可以指定GC线程数，默认GC线程数为CPU的数量。可以使用-XX:ParallelGCThreads选项指定GC的线程数。如果指定了选项-XX:+UseConcMarkSweepGC选项，则新生代默认使用ParNew GC策略。</td></tr><tr><td>组合4</td><td>ParNew</td><td>Serial Old</td><td>使用-XX:+UseParNewGC选项来开启。新生代使用ParNew GC策略，年老代默认使用Serial Old GC策略。</td></tr><tr><td>组合5</td><td>Parallel Scavenge</td><td>Serial Old</td><td>Parallel Scavenge策略主要是关注一个可控的吞吐量：应用程序运行时间 / (应用程序运行时间 + GC时间)，可见这会使得CPU的利用率尽可能的高，适用于后台持久运行的应用程序，而不适用于交互较多的应用程序。</td></tr><tr><td>组合6</td><td>Parallel Scavenge</td><td>Parallel Old</td><td>Parallel Old是Serial Old的并行版本</td></tr><tr><td>组合7</td><td>G1GC</td><td>G1GC</td><td>-XX:+UnlockExperimentalVMOptions -XX:+UseG1GC        #开启 -XX:MaxGCPauseMillis =50                  #暂停时间目标 -XX:GCPauseIntervalMillis =200          #暂停间隔目标 -XX:+G1YoungGenSize=512m            #年轻代大小 -XX:SurvivorRatio=6                            #幸存区比例</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构</title>
      <link href="/2019-06-13-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98JVM%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84.html"/>
      <url>/2019-06-13-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98JVM%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （一）Spark初识</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、JVM的结构图"><a href="#一、JVM的结构图" class="headerlink" title="一、JVM的结构图"></a>一、JVM的结构图</h2><h3 id="1-1-Java内存结构"><a href="#1-1-Java内存结构" class="headerlink" title="1.1　Java内存结构"></a>1.1　Java内存结构</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430151809602-1476931965.png" alt="img"></p><p>JVM内存结构主要有三大块：<strong>堆内存、方法区和栈</strong>。</p><p><strong>堆内存</strong>是JVM中最大的一块由年轻代和老年代组成，而年轻代内存又被分成三部分，Eden空间、From Survivor空间、To Survivor空间,默认情况下年轻代按照8:1:1的比例来分配；</p><p><strong>方法区</strong>存储类信息、常量、静态变量等数据，是线程共享的区域，为与Java堆区分，方法区还有一个别名Non-Heap(非堆)；</p><p><strong>栈</strong>又分为java虚拟机栈和本地方法栈主要用于方法的执行。</p><h3 id="1-2-如何通过参数来控制各区域的内存大小"><a href="#1-2-如何通过参数来控制各区域的内存大小" class="headerlink" title="1.2　如何通过参数来控制各区域的内存大小"></a>1.2　如何通过参数来控制各区域的内存大小</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430152017807-1294956408.png" alt="img"></p><h3 id="1-3-控制参数"><a href="#1-3-控制参数" class="headerlink" title="1.3　控制参数"></a>1.3　控制参数</h3><p>-Xms设置堆的最小空间大小。</p><p>-Xmx设置堆的最大空间大小。</p><p>-XX:NewSize设置新生代最小空间大小。</p><p>-XX:MaxNewSize设置新生代最大空间大小。</p><p>-XX:PermSize设置永久代最小空间大小。</p><p>-XX:MaxPermSize设置永久代最大空间大小。</p><p>-Xss设置每个线程的堆栈大小。</p><p>没有直接设置老年代的参数，但是可以设置堆空间大小和新生代空间大小两个参数来间接控制。</p><p>  <strong>老年代空间大小=堆空间大小-年轻代大空间大小</strong></p><h3 id="1-4-JVM和系统调用之间的关系"><a href="#1-4-JVM和系统调用之间的关系" class="headerlink" title="1.4　JVM和系统调用之间的关系"></a>1.4　JVM和系统调用之间的关系</h3><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430152138687-378200875.png" alt="img"></p><p><strong>方法区和堆是所有线程共享的内存区域；而java栈、本地方法栈和程序员计数器是运行是线程私有的内存区域。</strong></p><h2 id="二、JVM各区域的作用"><a href="#二、JVM各区域的作用" class="headerlink" title="二、JVM各区域的作用"></a>二、JVM各区域的作用</h2><h3 id="2-1-Java堆（Heap）"><a href="#2-1-Java堆（Heap）" class="headerlink" title="2.1　Java堆（Heap）"></a>2.1　Java堆（Heap）</h3><p>​    对于大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。</p><p>​     Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”。如果从内存回收的角度看，由于现在收集器基本都是采用的分代收集算法，所以Java堆中还可以细分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。</p><p>根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx和-Xms控制）。</p><p>如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。</p><h3 id="2-2-方法区（Method-Area）"><a href="#2-2-方法区（Method-Area）" class="headerlink" title="2.2　方法区（Method Area）"></a>2.2　方法区（Method Area）</h3><p>  方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。</p><p>对于习惯在HotSpot虚拟机上开发和部署程序的开发者来说，很多人愿意把方法区称为“永久代”（Permanent Generation），本质上两者并不等价，仅仅是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已。</p><p>Java虚拟机规范对这个区域的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是有必要的。</p><p>根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 </p><h3 id="2-3-程序计数器（Program-Counter-Register）"><a href="#2-3-程序计数器（Program-Counter-Register）" class="headerlink" title="2.3　程序计数器（Program Counter Register）"></a>2.3　程序计数器（Program Counter Register）</h3><p>程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。<br>由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。<br>      如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie方法，这个计数器值则为空（Undefined）。</p><p>此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。</p><h3 id="2-4-JVM栈（JVM-Stacks）"><a href="#2-4-JVM栈（JVM-Stacks）" class="headerlink" title="2.4　JVM栈（JVM Stacks）"></a>2.4　JVM栈（JVM Stacks）</h3><p>与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 </p><p>局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，根据不同的虚拟机实现，它可能是一个指向对象起始地址的引用指针，也可能指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。</p><p>其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。</p><p>在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。</p><h3 id="2-5-本地方法栈（Native-Method-Stacks）"><a href="#2-5-本地方法栈（Native-Method-Stacks）" class="headerlink" title="2.5　本地方法栈（Native Method Stacks）"></a>2.5　本地方法栈（Native Method Stacks）</h3><p>本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （十二）SparkCore的调优之资源调优</title>
      <link href="/2019-06-12-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98.html"/>
      <url>/2019-06-12-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （十二）SparkCore的调优之资源调优：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、Spark作业基本运行原理"><a href="#一、Spark作业基本运行原理" class="headerlink" title="一、Spark作业基本运行原理"></a>一、Spark作业基本运行原理</h2><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430150441732-1974371206.png" alt="img"></p><p>详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。</p><p>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。</p><p>Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。</p><p>当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。</p><p>因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。</p><p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</p><p>以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。</p><h2 id="二、资源参数调优"><a href="#二、资源参数调优" class="headerlink" title="二、资源参数调优"></a>二、资源参数调优</h2><p>了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p><h3 id="2-1-num-executors"><a href="#2-1-num-executors" class="headerlink" title="2.1　num-executors"></a>2.1　num-executors</h3><ul><li>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。<strong>这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</strong></li><li>参数调优建议：<strong>每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适</strong>，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li></ul><h3 id="2-2-executor-memory"><a href="#2-2-executor-memory" class="headerlink" title="2.2　executor-memory"></a>2.2　executor-memory</h3><ul><li>参数说明：<strong>该参数用于设置每个Executor进程的内存</strong>。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li><li>参数调优建议：<strong>每个Executor进程的内存设置4G~8G较为合适。</strong>但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li></ul><h3 id="2-3-executor-cores"><a href="#2-3-executor-cores" class="headerlink" title="2.3　executor-cores"></a>2.3　executor-cores</h3><ul><li>参数说明：<strong>该参数用于设置每个Executor进程的CPU core数量</strong>。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li><li>参数调优建议：Executor的CPU core数量设置为2<del>4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3</del>1/2左右比较合适，也是避免影响其他同学的作业运行。<strong>最好的应该就是一个cpu core对应两到三个task</strong></li></ul><h3 id="2-4-driver-memory"><a href="#2-4-driver-memory" class="headerlink" title="2.4　driver-memory"></a>2.4　driver-memory</h3><ul><li>参数说明：该参数用于设置Driver进程的内存。</li><li>参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</li></ul><h3 id="2-5-spark-default-parallelism"><a href="#2-5-spark-default-parallelism" class="headerlink" title="2.5　spark.default.parallelism"></a>2.5　spark.default.parallelism</h3><ul><li>参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。<strong>一个分区对应一个task，也就是这个参数其实就是设置task的数量</strong></li><li>参数调优建议：<strong>Spark作业的默认task数量为500~1000个较为合适。</strong>很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</li></ul><h3 id="2-6-spark-storage-memoryFraction"><a href="#2-6-spark-storage-memoryFraction" class="headerlink" title="2.6　spark.storage.memoryFraction"></a>2.6　spark.storage.memoryFraction</h3><ul><li>参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li><li>参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><h3 id="2-7-spark-shuffle-memoryFraction"><a href="#2-7-spark-shuffle-memoryFraction" class="headerlink" title="2.7　spark.shuffle.memoryFraction"></a>2.7　spark.shuffle.memoryFraction</h3><ul><li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li><li>参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><p>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （十一）SparkCore的调优之Spark内存模型</title>
      <link href="/2019-06-11-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8BSpark%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B.html"/>
      <url>/2019-06-11-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8BSpark%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （十一）SparkCore的调优之Spark内存模型：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark 作为一个基于内存的分布式计算引擎，其内存管理模块在整个系统中扮演着非常重要的角色。理解 Spark 内存管理的基本原理，有助于更好地开发 Spark 应用程序和进行性能调优。本文旨在梳理出 Spark 内存管理的脉络，抛砖引玉，引出读者对这个话题的深入探讨。本文中阐述的原理基于 Spark 2.1 版本，阅读本文需要读者有一定的 Spark 和 Java 基础，了解 RDD、Shuffle、JVM 等相关概念。</p><p>在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能[1]。由于 Driver 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-堆内和堆外内存规划"><a href="#1-堆内和堆外内存规划" class="headerlink" title="1. 堆内和堆外内存规划"></a>1. 堆内和堆外内存规划</h2><p>​        作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内（On-heap）空间进行了更为详细的分配，以充分利用内存。同时，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。</p><h5 id="图-1-堆内和堆外内存示意图"><a href="#图-1-堆内和堆外内存示意图" class="headerlink" title="图 1 . 堆内和堆外内存示意图"></a>图 1 . 堆内和堆外内存示意图</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image001.png" alt="img"></p><h3 id="1-1-堆内内存"><a href="#1-1-堆内内存" class="headerlink" title="1.1 堆内内存"></a>1.1 堆内内存</h3><p>堆内内存的大小，由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置。Executor 内运行的并发任务共享 JVM 堆内内存，这些任务在缓存 RDD 数据和广播（Broadcast）数据时占用的内存被规划为存储（Storage）内存，而这些任务在执行 Shuffle 时占用的内存被规划为执行（Execution）内存，剩余的部分不做特殊规划，那些 Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间。不同的管理模式下，这三部分占用的空间大小各不相同（下面第 2 小节会进行介绍）。</p><p>Spark 对堆内内存的管理是一种逻辑上的”规划式”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前<strong>记录</strong>这些内存，我们来看其具体流程：</p><ul><li><strong>申请内存</strong>：</li></ul><ol><li>Spark 在代码中 new 一个对象实例</li><li>JVM 从堆内内存分配空间，创建对象并返回对象引用</li><li>Spark 保存该对象的引用，记录该对象占用的内存</li></ol><ul><li><strong>释放内存</strong>：</li></ul><ol><li>Spark 记录该对象释放的内存，删除该对象的引用</li><li>等待 JVM 的垃圾回收机制释放该对象占用的堆内内存</li></ol><p>我们知道，JVM 的对象可以以序列化的方式存储，序列化的过程是将对象转换为二进制字节流，本质上可以理解为将非连续空间的链式存储转化为连续空间或块存储，在访问时则需要进行序列化的逆过程——反序列化，将字节流转化为对象，序列化的方式可以节省存储空间，但增加了存储和读取时候的计算开销。</p><p>对于 Spark 中序列化的对象，由于是字节流的形式，其占用的内存大小可直接计算，而对于非序列化的对象，其占用的内存是通过周期性地采样近似估算而得，即并不是每次新增的数据项都会计算一次占用的内存大小，这种方法降低了时间开销但是有可能误差较大，导致某一时刻的实际内存有可能远远超出预期[2]。此外，在被 Spark 标记为释放的对象实例，很有可能在实际上并没有被 JVM 回收，导致实际可用的内存小于 Spark 记录的可用内存。所以 Spark 并不能准确记录实际可用的堆内内存，从而也就无法完全避免内存溢出（OOM, Out of Memory）的异常。</p><p>虽然不能精准控制堆内内存的申请和释放，但 Spark 通过对存储内存和执行内存各自独立的规划管理，可以决定是否要在存储内存里缓存新的 RDD，以及是否为新的任务分配执行内存，在一定程度上可以提升内存的利用率，减少异常的出现。</p><h3 id="1-2-堆外内存"><a href="#1-2-堆外内存" class="headerlink" title="1.2 堆外内存"></a>1.2 堆外内存</h3><p>为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。利用 JDK Unsafe API（从 Spark 2.0 开始，在管理堆外的存储内存时不再基于 Tachyon，而是与堆外的执行内存一样，基于 JDK Unsafe API 实现[3]），Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。堆外内存可以被精确地申请和释放，而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。</p><p>在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存。</p><h3 id="1-3-内存管理接口"><a href="#1-3-内存管理接口" class="headerlink" title="1.3 内存管理接口"></a>1.3 内存管理接口</h3><p>Spark 为存储内存和执行内存的管理提供了统一的接口——MemoryManager，同一个 Executor 内的任务都调用这个接口的方法来申请或释放内存:</p><h4 id="清单-1-内存管理接口的主要方法"><a href="#清单-1-内存管理接口的主要方法" class="headerlink" title="清单 1 . 内存管理接口的主要方法"></a>清单 1 . 内存管理接口的主要方法</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//申请存储内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">acquireStorageMemory</span></span>(blockId: <span class="type">BlockId</span>, numBytes: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Boolean</span></span><br><span class="line"><span class="comment">//申请展开内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">acquireUnrollMemory</span></span>(blockId: <span class="type">BlockId</span>, numBytes: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Boolean</span></span><br><span class="line"><span class="comment">//申请执行内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">acquireExecutionMemory</span></span>(numBytes: <span class="type">Long</span>, taskAttemptId: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Long</span></span><br><span class="line"><span class="comment">//释放存储内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">releaseStorageMemory</span></span>(numBytes: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Unit</span></span><br><span class="line"><span class="comment">//释放执行内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">releaseExecutionMemory</span></span>(numBytes: <span class="type">Long</span>, taskAttemptId: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Unit</span></span><br><span class="line"><span class="comment">//释放展开内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">releaseUnrollMemory</span></span>(numBytes: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Unit</span></span><br></pre></td></tr></table></figure><p>我们看到，在调用这些方法时都需要指定其内存模式（MemoryMode），这个参数决定了是在堆内还是堆外完成这次操作。</p><p>MemoryManager 的具体实现上，Spark 1.6 之后默认为统一管理（<a href="https://github.com/apache/spark/blob/v2.1.0/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala" target="_blank" rel="noopener">Unified Memory Manager</a>）方式，1.6 之前采用的静态管理（<a href="https://github.com/apache/spark/blob/v2.1.0/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala" target="_blank" rel="noopener">Static Memory Manager</a>）方式仍被保留，可通过配置 spark.memory.useLegacyMode 参数启用。两种方式的区别在于对空间分配的方式，下面的第 2 小节会分别对这两种方式进行介绍。</p><h2 id="2-内存空间分配"><a href="#2-内存空间分配" class="headerlink" title="2 . 内存空间分配"></a>2 . 内存空间分配</h2><h3 id="2-1-静态内存管理"><a href="#2-1-静态内存管理" class="headerlink" title="2.1 静态内存管理"></a>2.1 静态内存管理</h3><p>在 Spark 最初采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置，堆内内存的分配如图 2 所示：</p><h5 id="图-2-静态内存管理图示——堆内"><a href="#图-2-静态内存管理图示——堆内" class="headerlink" title="图 2 . 静态内存管理图示——堆内"></a>图 2 . 静态内存管理图示——堆内</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image002.png" alt="img"></p><p>可以看到，可用的堆内内存的大小需要按照下面的方式计算：</p><h4 id="清单-2-可用堆内内存空间"><a href="#清单-2-可用堆内内存空间" class="headerlink" title="清单 2 . 可用堆内内存空间"></a>清单 2 . 可用堆内内存空间</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">可用的存储内存 = systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction</span><br><span class="line">可用的执行内存 = systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction</span><br></pre></td></tr></table></figure><p>其中 systemMaxMemory 取决于当前 JVM 堆内内存的大小，最后可用的执行内存或者存储内存要在此基础上与各自的 memoryFraction 参数和 safetyFraction 参数相乘得出。上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 1-safetyFraction 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险（上文提到，对于非序列化对象的内存采样估算会产生误差）。值得注意的是，这个预留的保险区域仅仅是一种逻辑上的规划，在具体使用时 Spark 并没有区别对待，和”其它内存”一样交给了 JVM 去管理。</p><p>堆外的空间分配较为简单，只有存储内存和执行内存，如图 3 所示。可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域。</p><h5 id="图-3-静态内存管理图示——堆外"><a href="#图-3-静态内存管理图示——堆外" class="headerlink" title="图 3 . 静态内存管理图示——堆外"></a>图 3 . 静态内存管理图示——堆外</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image003.png" alt="img"></p><p>静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成”一半海水，一半火焰”的局面，即存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。</p><h3 id="2-2-统一内存管理"><a href="#2-2-统一内存管理" class="headerlink" title="2.2 统一内存管理"></a>2.2 统一内存管理</h3><p>Spark 1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域，如图 4 和图 5 所示</p><h5 id="图-4-统一内存管理图示——堆内"><a href="#图-4-统一内存管理图示——堆内" class="headerlink" title="图 4 . 统一内存管理图示——堆内"></a>图 4 . 统一内存管理图示——堆内</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image004.png" alt="img"></p><h5 id="图-5-统一内存管理图示——堆外"><a href="#图-5-统一内存管理图示——堆外" class="headerlink" title="图 5 . 统一内存管理图示——堆外"></a>图 5 . 统一内存管理图示——堆外</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image005.png" alt="img"></p><p>其中最重要的优化在于动态占用机制，其规则如下：</p><ul><li>设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围</li><li>双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block）</li><li>执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间</li><li>存储内存的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂[4]</li></ul><h5 id="图-6-动态占用机制图示"><a href="#图-6-动态占用机制图示" class="headerlink" title="图 6 . 动态占用机制图示"></a>图 6 . 动态占用机制图示</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image006.png" alt="img"></p><p>凭借统一内存管理机制，Spark 在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护 Spark 内存的难度，但并不意味着开发者可以高枕无忧。譬如，所以如果存储内存的空间太大或者说缓存的数据过多，反而会导致频繁的全量垃圾回收，降低任务执行时的性能，因为缓存的 RDD 数据通常都是长期驻留内存的 [5] 。所以要想充分发挥 Spark 的性能，需要开发者进一步了解存储内存和执行内存各自的管理方式和实现原理。</p><h2 id="3-存储内存管理"><a href="#3-存储内存管理" class="headerlink" title="3. 存储内存管理"></a>3. 存储内存管理</h2><h3 id="3-1-RDD-的持久化机制"><a href="#3-1-RDD-的持久化机制" class="headerlink" title="3.1 RDD 的持久化机制"></a>3.1 RDD 的持久化机制</h3><p>弹性分布式数据集（RDD）作为 Spark 最根本的数据抽象，是只读的分区记录（Partition）的集合，只能基于在稳定物理存储中的数据集上创建，或者在其他已有的 RDD 上执行转换（Transformation）操作产生一个新的 RDD。转换后的 RDD 与原始的 RDD 之间产生的依赖关系，构成了血统（Lineage）。凭借血统，Spark 保证了每一个 RDD 都可以被重新恢复。但 RDD 的所有转换都是惰性的，即只有当一个返回结果给 Driver 的行动（Action）发生时，Spark 才会创建任务读取 RDD，然后真正触发转换的执行。<br>Task 在启动之初读取一个分区时，会先判断这个分区是否已经被持久化，如果没有则需要检查 Checkpoint 或按照血统重新计算。所以如果一个 RDD 上要执行多次行动，可以在第一次行动中使用 persist 或 cache 方法，在内存或磁盘中持久化或缓存这个 RDD，从而在后面的行动时提升计算速度。事实上，cache 方法是使用默认的 MEMORY_ONLY 的存储级别将 RDD 持久化到内存，故缓存是一种特殊的持久化。 <strong>堆内和堆外存储内存的设计，便可以对缓存</strong> <strong>RDD</strong> <strong>时使用的内存做统一的规划和管</strong> <strong>理</strong> （存储内存的其他应用场景，如缓存 broadcast 数据，暂时不在本文的讨论范围之内）。</p><p>RDD 的持久化由 Spark 的 Storage 模块 [7] 负责，实现了 RDD 与物理存储的解耦合。Storage 模块负责管理 Spark 在计算过程中产生的数据，将那些在内存或磁盘、在本地或远程存取数据的功能封装了起来。在具体实现时 Driver 端和 Executor 端的 Storage 模块构成了主从式的架构，即 Driver 端的 BlockManager 为 Master，Executor 端的 BlockManager 为 Slave。Storage 模块在逻辑上以 Block 为基本存储单位，RDD 的每个 Partition 经过处理后唯一对应一个 Block（BlockId 的格式为 rdd_RDD-ID_PARTITION-ID ）。Master 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 Slave 需要将 Block 的更新等状态上报到 Master，同时接收 Master 的命令，例如新增或删除一个 RDD。</p><h5 id="图-7-Storage-模块示意图"><a href="#图-7-Storage-模块示意图" class="headerlink" title="图 7 . Storage 模块示意图"></a>图 7 . Storage 模块示意图</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image007.png" alt="img"></p><p>在对 RDD 持久化时，Spark 规定了 MEMORY_ONLY、MEMORY_AND_DISK 等12 种不同的 <a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence" target="_blank" rel="noopener">存储级别 </a>，而存储级别是以下 5 个变量的组合：</p><h4 id="清单-3-存储级别"><a href="#清单-3-存储级别" class="headerlink" title="清单 3 . 存储级别"></a>清单 3 . 存储级别</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useDisk: <span class="type">Boolean</span>, //磁盘</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useMemory: <span class="type">Boolean</span>, //这里其实是指堆内内存</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useOffHeap: <span class="type">Boolean</span>, //堆外内存</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _deserialized: <span class="type">Boolean</span>, //是否为非序列化</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _replication: <span class="type">Int</span> = 1 //副本个数</span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br></pre></td></tr></table></figure><p>通过对数据结构的分析，可以看出存储级别从三个维度定义了 RDD 的 Partition（同时也就是 Block）的存储方式：</p><ul><li>存储位置：磁盘／堆内内存／堆外内存。如 MEMORY_AND_DISK 是同时在磁盘和堆内内存上存储，实现了冗余备份。OFF_HEAP 则是只在堆外内存存储，目前选择堆外内存时不能同时存储到其他位置。</li><li>存储形式：Block 缓存到存储内存后，是否为非序列化的形式。如 MEMORY_ONLY 是非序列化方式存储，OFF_HEAP 是序列化方式存储。</li><li>副本数量：大于 1 时需要远程冗余备份到其他节点。如 DISK_ONLY_2 需要远程备份 1 个副本。</li></ul><h3 id="3-2-RDD-缓存的过程"><a href="#3-2-RDD-缓存的过程" class="headerlink" title="3.2 RDD 缓存的过程"></a>3.2 RDD 缓存的过程</h3><p>RDD 在缓存到存储内存之前，Partition 中的数据一般以迭代器（<a href="http://www.scala-lang.org/docu/files/collections-api/collections_43.html" target="_blank" rel="noopener">Iterator</a>）的数据结构来访问，这是 Scala 语言中一种遍历数据集合的方法。通过 Iterator 可以获取分区中每一条序列化或者非序列化的数据项(Record)，这些 Record 的对象实例在逻辑上占用了 JVM 堆内内存的 other 部分的空间，同一 Partition 的不同 Record 的空间并不连续。</p><p>RDD 在缓存到存储内存之后，Partition 被转换成 Block，Record 在堆内或堆外存储内存中占用一块连续的空间。<strong>将Partition由不连续的存储空间转换为连续存储空间的过程，Spark称之为”展开”（Unroll）</strong>。Block 有序列化和非序列化两种存储格式，具体以哪种方式取决于该 RDD 的存储级别。非序列化的 Block 以一种 DeserializedMemoryEntry 的数据结构定义，用一个数组存储所有的对象实例，序列化的 Block 则以 SerializedMemoryEntry的数据结构定义，用字节缓冲区（ByteBuffer）来存储二进制数据。每个 Executor 的 Storage 模块用一个链式 Map 结构（LinkedHashMap）来管理堆内和堆外存储内存中所有的 Block 对象的实例[6]，对这个 LinkedHashMap 新增和删除间接记录了内存的申请和释放。</p><p>因为不能保证存储空间可以一次容纳 Iterator 中的所有数据，当前的计算任务在 Unroll 时要向 MemoryManager 申请足够的 Unroll 空间来临时占位，空间不足则 Unroll 失败，空间足够时可以继续进行。对于序列化的 Partition，其所需的 Unroll 空间可以直接累加计算，一次申请。而非序列化的 Partition 则要在遍历 Record 的过程中依次申请，即每读取一条 Record，采样估算其所需的 Unroll 空间并进行申请，空间不足时可以中断，释放已占用的 Unroll 空间。如果最终 Unroll 成功，当前 Partition 所占用的 Unroll 空间被转换为正常的缓存 RDD 的存储空间，如下图 8 所示。</p><h5 id="图-8-Spark-Unroll-示意图"><a href="#图-8-Spark-Unroll-示意图" class="headerlink" title="图 8. Spark Unroll 示意图"></a>图 8. Spark Unroll 示意图</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image008.png" alt="img"></p><p>在图 3 和图 5 中可以看到，在静态内存管理时，Spark 在存储内存中专门划分了一块 Unroll 空间，其大小是固定的，统一内存管理时则没有对 Unroll 空间进行特别区分，当存储空间不足时会根据动态占用机制进行处理。</p><h3 id="3-3-淘汰和落盘"><a href="#3-3-淘汰和落盘" class="headerlink" title="3.3 淘汰和落盘"></a>3.3 淘汰和落盘</h3><p>由于同一个 Executor 的所有的计算任务共享有限的存储内存空间，当有新的 Block 需要缓存但是剩余空间不足且无法动态占用时，就要对 LinkedHashMap 中的旧 Block 进行淘汰（Eviction），而被淘汰的 Block 如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘（Drop），否则直接删除该 Block。</p><p>存储内存的淘汰规则为：</p><ul><li>被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存</li><li>新旧 Block 不能属于同一个 RDD，避免循环淘汰</li><li>旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题</li><li>遍历 LinkedHashMap 中 Block，按照最近最少使用（LRU）的顺序淘汰，直到满足新 Block 所需的空间。其中 LRU 是 LinkedHashMap 的特性。</li></ul><p>落盘的流程则比较简单，如果其存储级别符合_useDisk 为 true 的条件，再根据其_deserialized 判断是否是非序列化的形式，若是则对其进行序列化，最后将数据存储到磁盘，在 Storage 模块中更新其信息。</p><h2 id="4-执行内存管理"><a href="#4-执行内存管理" class="headerlink" title="4. 执行内存管理"></a>4. 执行内存管理</h2><h3 id="4-1-多任务间内存分配"><a href="#4-1-多任务间内存分配" class="headerlink" title="4.1 多任务间内存分配"></a>4.1 多任务间内存分配</h3><p>Executor 内运行的任务同样共享执行内存，Spark 用一个 HashMap 结构保存了任务到内存耗费的映射。每个任务可占用的执行内存大小的范围为 1/2N ~ 1/N，其中 N 为当前 Executor 内正在运行的任务的个数。每个任务在启动之时，要向 MemoryManager 请求申请最少为 1/2N 的执行内存，如果不能被满足要求则该任务被阻塞，直到有其他任务释放了足够的执行内存，该任务才可以被唤醒。</p><h3 id="4-2-Shuffle-的内存占用"><a href="#4-2-Shuffle-的内存占用" class="headerlink" title="4.2 Shuffle 的内存占用"></a>4.2 Shuffle 的内存占用</h3><p>执行内存主要用来存储任务在执行 Shuffle 时占用的内存，Shuffle 是按照一定规则对 RDD 数据重新分区的过程，我们来看 Shuffle 的 Write 和 Read 两阶段对执行内存的使用：</p><ul><li>Shuffle Write</li></ul><ol><li>若在 map 端选择普通的排序方式，会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间。</li><li>若在 map 端选择 Tungsten 的排序方式，则采用 ShuffleExternalSorter 直接对以序列化形式存储的数据排序，在内存中存储数据时可以占用堆外或堆内执行空间，取决于用户是否开启了堆外内存以及堆外执行内存是否足够。</li></ol><ul><li>Shuffle Read</li></ul><ol><li>在对 reduce 端的数据进行聚合时，要将数据交给 Aggregator 处理，在内存中存储数据时占用堆内执行空间。</li><li>如果需要进行最终结果排序，则要将再次将数据交给 ExternalSorter 处理，占用堆内执行空间。</li></ol><p>在 ExternalSorter 和 Aggregator 中，Spark 会使用一种叫 AppendOnlyMap 的哈希表在堆内执行内存中存储数据，但在 Shuffle 过程中所有数据并不能都保存到该哈希表中，当这个哈希表占用的内存会进行周期性地采样估算，当其大到一定程度，无法再从 MemoryManager 申请到新的执行内存时，Spark 就会将其全部内容存储到磁盘文件中，这个过程被称为溢存(Spill)，溢存到磁盘的文件最后会被归并(Merge)。</p><p>Shuffle Write 阶段中用到的 Tungsten 是 Databricks 公司提出的对 Spark 优化内存和 CPU 使用的计划[9]，解决了一些 JVM 在性能上的限制和弊端。Spark 会根据 Shuffle 的情况来自动选择是否采用 Tungsten 排序。Tungsten 采用的页式内存管理机制建立在 MemoryManager 之上，即 Tungsten 对执行内存的使用进行了一步的抽象，这样在 Shuffle 过程中无需关心数据具体存储在堆内还是堆外。每个内存页用一个 MemoryBlock 来定义，并用 Object obj 和 long offset 这两个变量统一标识一个内存页在系统内存中的地址。堆内的 MemoryBlock 是以 long 型数组的形式分配的内存，其 obj 的值为是这个数组的对象引用，offset 是 long 型数组的在 JVM 中的初始偏移地址，两者配合使用可以定位这个数组在堆内的绝对地址；堆外的 MemoryBlock 是直接申请到的内存块，其 obj 为 null，offset 是这个内存块在系统内存中的 64 位绝对地址。Spark 用 MemoryBlock 巧妙地将堆内和堆外内存页统一抽象封装，并用页表(pageTable)管理每个 Task 申请到的内存页。</p><p>Tungsten 页式管理下的所有内存用 64 位的逻辑地址表示，由页号和页内偏移量组成：</p><ul><li>页号：占 13 位，唯一标识一个内存页，Spark 在申请内存页之前要先申请空闲页号。</li><li>页内偏移量：占 51 位，是在使用内存页存储数据时，数据在页内的偏移地址。</li></ul><p>有了统一的寻址方式，Spark 可以用 64 位逻辑地址的指针定位到堆内或堆外的内存，整个 Shuffle Write 排序的过程只需要对指针进行排序，并且无需反序列化，整个过程非常高效，对于内存访问效率和 CPU 使用效率带来了明显的提升[10]。</p><p>Spark 的存储内存和执行内存有着截然不同的管理方式：对于存储内存来说，Spark 用一个 LinkedHashMap 来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成；而对于执行内存，Spark 用 AppendOnlyMap 来存储 Shuffle 过程中的数据，在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制。</p><p>转自：<a href="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （十）SparkCore的调优之Shuffle调优</title>
      <link href="/2019-06-10-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8BShuffle%E8%B0%83%E4%BC%98.html"/>
      <url>/2019-06-10-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8BShuffle%E8%B0%83%E4%BC%98.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （十）SparkCore的调优之Shuffle调优：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、shuffle的定义"><a href="#一、shuffle的定义" class="headerlink" title="一、shuffle的定义"></a>一、shuffle的定义</h2><p>Spark的运行主要分为2部分：</p><p>　　一部分是驱动程序，其核心是SparkContext；</p><p>　　另一部分是Worker节点上Task,它是运行实际任务的。程序运行的时候，Driver和Executor进程相互交互：运行什么任务，即Driver会分配Task到Executor，Driver 跟 Executor 进行网络传输; 任务数据从哪儿获取，即Task要从 Driver 抓取其他上游的 Task 的数据结果，所以有这个过程中就不断的产生网络结果。其中，<strong>下一个 Stage 向上一个 Stage 要数据这个过程，我们就称之为 Shuffle</strong>。</p><h2 id="二、ShuffleManager发展概述"><a href="#二、ShuffleManager发展概述" class="headerlink" title="二、ShuffleManager发展概述"></a>二、ShuffleManager发展概述</h2><p>​        在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。</p><p>在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。</p><p>因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</p><p>下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。</p><h2 id="三、HashShuffleManager的运行原理"><a href="#三、HashShuffleManager的运行原理" class="headerlink" title="三、HashShuffleManager的运行原理"></a>三、HashShuffleManager的运行原理</h2><p>在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。</p><p>在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。</p><p>因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</p><p>下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。</p><h3 id="3-1-未经优化的HashShuffleManager"><a href="#3-1-未经优化的HashShuffleManager" class="headerlink" title="3.1　未经优化的HashShuffleManager"></a>3.1　未经优化的HashShuffleManager</h3><h4 id="图解说明"><a href="#图解说明" class="headerlink" title="图解说明"></a>图解说明</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180426185748765-1326220564.png" alt="img"></p><h4 id="文字说明"><a href="#文字说明" class="headerlink" title="文字说明"></a>文字说明</h4><p>上图说明了未经优化的HashShuffleManager的原理。这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。</p><p>我们先从shuffle write开始说起。shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。</p><p>那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢？很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。</p><p>接着我们来说说shuffle read。shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。</p><p>shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。</p><h3 id="3-2-优化后的HashShuffleManager"><a href="#3-2-优化后的HashShuffleManager" class="headerlink" title="3.2　优化后的HashShuffleManager"></a>3.2　优化后的HashShuffleManager</h3><h4 id="图解说明-1"><a href="#图解说明-1" class="headerlink" title="图解说明"></a>图解说明</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180426190133554-1433961373.png" alt="img"></p><h4 id="文字说明-1"><a href="#文字说明-1" class="headerlink" title="文字说明"></a>文字说明</h4><p>上图说明了优化后的HashShuffleManager的原理。这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p><p>开启consolidate机制之后，在shuffle write过程中，task就不是为下游stage的每个task创建一个磁盘文件了。此时会出现shuffleFileGroup的概念，每个shuffleFileGroup会对应一批磁盘文件，磁盘文件的数量与下游stage的task数量是相同的。一个Executor上有多少个CPU core，就可以并行执行多少个task。而第一批并行执行的每个task都会创建一个shuffleFileGroup，并将数据写入对应的磁盘文件内。</p><p>当Executor的CPU core执行完一批task，接着执行下一批task时，下一批task就会复用之前已有的shuffleFileGroup，包括其中的磁盘文件。也就是说，此时task会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。</p><p>假设第二个stage有100个task，第一个stage有50个task，总共还是有10个Executor，每个Executor执行5个task。那么原本使用未经优化的HashShuffleManager时，每个Executor会产生500个磁盘文件，所有Executor会产生5000个磁盘文件的。但是此时经过优化之后，每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。也就是说，每个Executor此时只会创建100个磁盘文件，所有Executor只会创建1000个磁盘文件。</p><h2 id="四、SortShuffleManager运行原理"><a href="#四、SortShuffleManager运行原理" class="headerlink" title="四、SortShuffleManager运行原理"></a>四、SortShuffleManager运行原理</h2><p>SortShuffleManager的运行机制主要分成两种，一种是普通运行机制，另一种是bypass运行机制。当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。</p><h3 id="4-1-普通运行机制"><a href="#4-1-普通运行机制" class="headerlink" title="4.1　普通运行机制"></a>4.1　普通运行机制</h3><h4 id="图解说明-2"><a href="#图解说明-2" class="headerlink" title="图解说明"></a>图解说明</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180426190605260-1242830112.png" alt="img"></p><h4 id="文字说明-2"><a href="#文字说明-2" class="headerlink" title="文字说明"></a>文字说明</h4><p>上图说明了普通的SortShuffleManager的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。</p><p>在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。<strong>默认的batch数量是10000条</strong>，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。</p><p>一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。</p><p>SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于每个task最终只有一个磁盘文件，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。</p><h3 id="4-2-bypass运行机制"><a href="#4-2-bypass运行机制" class="headerlink" title="4.2　bypass运行机制"></a>4.2　bypass运行机制</h3><h4 id="图解说明-3"><a href="#图解说明-3" class="headerlink" title="图解说明"></a>图解说明</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180426191008146-746530859.png" alt="img"></p><h4 id="文字说明-3"><a href="#文字说明-3" class="headerlink" title="文字说明"></a>文字说明</h4><p>上图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下：</p><blockquote><ul><li>shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。</li><li>不是聚合类的shuffle算子（比如reduceByKey）。</li></ul></blockquote><p>此时task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。</p><p>该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。</p><p>而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。</p><h2 id="五、shuffle相关参数调优"><a href="#五、shuffle相关参数调优" class="headerlink" title="五、shuffle相关参数调优"></a>五、shuffle相关参数调优</h2><p>以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。</p><p>Spark各个版本的参数默认值可能会有不同，具体使用请参考官方网站的说明：</p><p>（1）先选择对应的Spark版本：<a href="http://spark.apache.org/documentation.html" target="_blank" rel="noopener">http://spark.apache.org/documentation.html</a></p><p>（2）再查看对应的文档说明</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180426192151929-1075706486.png" alt="img"></p><h3 id="spark-shuffle-file-buffer"><a href="#spark-shuffle-file-buffer" class="headerlink" title="spark.shuffle.file.buffer"></a>spark.shuffle.file.buffer</h3><ul><li>默认值：32k</li><li>参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul><h3 id="spark-reducer-maxSizeInFlight"><a href="#spark-reducer-maxSizeInFlight" class="headerlink" title="spark.reducer.maxSizeInFlight"></a>spark.reducer.maxSizeInFlight</h3><ul><li>默认值：48m</li><li>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul><h3 id="spark-shuffle-io-maxRetries"><a href="#spark-shuffle-io-maxRetries" class="headerlink" title="spark.shuffle.io.maxRetries"></a>spark.shuffle.io.maxRetries</h3><ul><li>默认值：3</li><li>参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。</li><li>调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。</li></ul><h3 id="spark-shuffle-io-retryWait"><a href="#spark-shuffle-io-retryWait" class="headerlink" title="spark.shuffle.io.retryWait"></a>spark.shuffle.io.retryWait</h3><ul><li>默认值：5s</li><li>参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。</li><li>调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。</li></ul><h3 id="spark-shuffle-memoryFraction（已经弃用）"><a href="#spark-shuffle-memoryFraction（已经弃用）" class="headerlink" title="spark.shuffle.memoryFraction（已经弃用）"></a>spark.shuffle.memoryFraction（已经弃用）</h3><ul><li>默认值：0.2</li><li>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。</li><li>调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。</li></ul><h3 id="spark-shuffle-manager（已经弃用）"><a href="#spark-shuffle-manager（已经弃用）" class="headerlink" title="spark.shuffle.manager（已经弃用）"></a>spark.shuffle.manager（已经弃用）</h3><ul><li>默认值：sort</li><li>参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。</li><li>调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。</li></ul><h3 id="spark-shuffle-sort-bypassMergeThreshold"><a href="#spark-shuffle-sort-bypassMergeThreshold" class="headerlink" title="spark.shuffle.sort.bypassMergeThreshold"></a>spark.shuffle.sort.bypassMergeThreshold</h3><ul><li>默认值：200</li><li>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</li><li>调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</li></ul><h3 id="spark-shuffle-consolidateFiles（已经弃用）"><a href="#spark-shuffle-consolidateFiles（已经弃用）" class="headerlink" title="spark.shuffle.consolidateFiles（已经弃用）"></a>spark.shuffle.consolidateFiles（已经弃用）</h3><ul><li>默认值：false</li><li>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</li><li>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （九）SparkCore的调优之数据倾斜调优</title>
      <link href="/2019-06-09-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B9%9D%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E8%B0%83%E4%BC%98.html"/>
      <url>/2019-06-09-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B9%9D%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E8%B0%83%E4%BC%98.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （九）SparkCore的调优之数据倾斜调优：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="数据倾斜发生时的现象"><a href="#数据倾斜发生时的现象" class="headerlink" title="数据倾斜发生时的现象"></a>数据倾斜发生时的现象</h2><ul><li>绝大多数task执行得都非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。</li><li>原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。</li></ul><h2 id="数据倾斜发生的原理"><a href="#数据倾斜发生的原理" class="headerlink" title="数据倾斜发生的原理"></a>数据倾斜发生的原理</h2><p>数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。</p><p>因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。</p><p>下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425190936210-1283450149.png" alt="img"></p><h2 id="如何定位导致数据倾斜的代码"><a href="#如何定位导致数据倾斜的代码" class="headerlink" title="如何定位导致数据倾斜的代码"></a>如何定位导致数据倾斜的代码</h2><p>数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</p><h3 id="某个task执行特别慢的情况"><a href="#某个task执行特别慢的情况" class="headerlink" title="某个task执行特别慢的情况"></a>某个task执行特别慢的情况</h3><p>首先要看的，就是数据倾斜发生在第几个stage中。</p><p>如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。</p><p>比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425190955841-989714022.png" alt="img"></p><p>知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。</p><p>这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。</p><ul><li>stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。</li><li>stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">"hdfs://..."</span>)</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> pairs = words.map((_, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">wordCounts.collect().foreach(println(_))</span><br></pre></td></tr></table></figure><p>​        通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由reduceByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。</p><h3 id="某个task莫名其妙内存溢出的情况"><a href="#某个task莫名其妙内存溢出的情况" class="headerlink" title="某个task莫名其妙内存溢出的情况"></a>某个task莫名其妙内存溢出的情况</h3><p>​        这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。</p><p>但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。</p><h2 id="查看导致数据倾斜的key的数据分布情况"><a href="#查看导致数据倾斜的key的数据分布情况" class="headerlink" title="查看导致数据倾斜的key的数据分布情况"></a>查看导致数据倾斜的key的数据分布情况</h2><p>​        知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。</p><p>此时根据你执行操作的情况不同，可以有很多种查看key分布的方式：</p><ol><li>如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。</li><li>如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。</li></ol><p>举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sampledPairs = pairs.sample(<span class="literal">false</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">val</span> sampledWordCounts = sampledPairs.countByKey()</span><br><span class="line">sampledWordCounts.foreach(println(_))</span><br></pre></td></tr></table></figure><h2 id="数据倾斜的解决方案"><a href="#数据倾斜的解决方案" class="headerlink" title="数据倾斜的解决方案"></a>数据倾斜的解决方案</h2><h3 id="解决方案一：使用Hive-ETL预处理数据"><a href="#解决方案一：使用Hive-ETL预处理数据" class="headerlink" title="解决方案一：使用Hive ETL预处理数据"></a>解决方案一：使用Hive ETL预处理数据</h3><p><strong>方案适用场景：</strong>导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。</p><p><strong>方案实现思路：</strong>此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。</p><p><strong>方案实现原理：</strong>这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。</p><p><strong>方案优点：</strong>实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p><p><strong>方案缺点：</strong>治标不治本，Hive ETL中还是会发生数据倾斜。</p><p><strong>方案实践经验：</strong>在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p><p><strong>项目实践经验：</strong>在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</p><h3 id="解决方案二：过滤少数导致倾斜的key"><a href="#解决方案二：过滤少数导致倾斜的key" class="headerlink" title="解决方案二：过滤少数导致倾斜的key"></a>解决方案二：过滤少数导致倾斜的key</h3><p><strong>方案适用场景：</strong>如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p><p><strong>方案实现思路：</strong>如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</p><p><strong>方案实现原理：</strong>将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。</p><p><strong>方案优点：</strong>实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p><p><strong>方案缺点：</strong>适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p><p><strong>方案实践经验：</strong>在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p><h3 id="解决方案三：提高shuffle操作的并行度"><a href="#解决方案三：提高shuffle操作的并行度" class="headerlink" title="解决方案三：提高shuffle操作的并行度"></a>解决方案三：提高shuffle操作的并行度</h3><p><strong>方案适用场景：</strong>如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</p><p><strong>方案实现思路：</strong>在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p><p><strong>方案实现原理：</strong>增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。</p><p><strong>方案优点：</strong>实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p><p><strong>方案缺点：</strong>只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p><p><strong>方案实践经验：</strong>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425191209688-1729367236.png" alt="img"></p><h3 id="解决方案四：两阶段聚合（局部聚合-全局聚合）"><a href="#解决方案四：两阶段聚合（局部聚合-全局聚合）" class="headerlink" title="解决方案四：两阶段聚合（局部聚合+全局聚合）"></a>解决方案四：两阶段聚合（局部聚合+全局聚合）</h3><p><strong>方案适用场景：</strong>对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</p><p><strong>方案实现思路：</strong>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p><p><strong>方案实现原理：</strong>将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p><p><strong>方案优点：</strong>对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p><p><strong>方案缺点：</strong>仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425191231729-2107528275.png" alt="img"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 第一步，给RDD中的每个key都打上一个随机前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; randomPrefixRdd = rdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Long</span>&gt;, <span class="type">String</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">                int prefix = random.nextInt(<span class="number">10</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二步，对打上随机前缀的key进行局部聚合。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; localAggrRdd = randomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Long</span> call(<span class="type">Long</span> v1, <span class="type">Long</span> v2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第三步，去除RDD中每个key的随机前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>,<span class="type">Long</span>&gt;, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                long originalKey = <span class="type">Long</span>.valueOf(tuple._1.split(<span class="string">"_"</span>)[<span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt;(originalKey, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第四步，对去除了随机前缀的RDD进行全局聚合。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Long</span> call(<span class="type">Long</span> v1, <span class="type">Long</span> v2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><h3 id="解决方案五：将reduce-join转为map-join"><a href="#解决方案五：将reduce-join转为map-join" class="headerlink" title="解决方案五：将reduce join转为map join"></a>解决方案五：将reduce join转为map join</h3><p><strong>方案适用场景：</strong>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p><p><strong>方案实现思路：</strong>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p><p><strong>方案实现原理：</strong>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p><p><strong>方案优点：</strong>对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点：</strong>适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425191304938-280219887.png" alt="img"></p><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先将数据量比较小的RDD的数据，collect到Driver中来。</span></span><br><span class="line"><span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt;&gt; rdd1Data = rdd1.collect()</span><br><span class="line"><span class="comment">// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。</span></span><br><span class="line"><span class="comment">// 可以尽可能节省内存空间，并且减少网络传输性能开销。</span></span><br><span class="line"><span class="keyword">final</span> <span class="type">Broadcast</span>&lt;<span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对另外一个RDD执行map类操作，而不再是join类操作。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRdd = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="comment">// 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。</span></span><br><span class="line">                <span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt;&gt; rdd1Data = rdd1DataBroadcast.value();</span><br><span class="line">                <span class="comment">// 可以将rdd1的数据转换为一个Map，便于后面进行join操作。</span></span><br><span class="line">                <span class="type">Map</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; rdd1DataMap = <span class="keyword">new</span> <span class="type">HashMap</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt;();</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; data : rdd1Data) &#123;</span><br><span class="line">                    rdd1DataMap.put(data._1, data._2);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 获取当前RDD数据的key以及value。</span></span><br><span class="line">                <span class="type">String</span> key = tuple._1;</span><br><span class="line">                <span class="type">String</span> value = tuple._2;</span><br><span class="line">                <span class="comment">// 从rdd1数据Map中，根据key获取到可以join到的数据。</span></span><br><span class="line">                <span class="type">Row</span> rdd1Value = rdd1DataMap.get(key);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(key, <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;(value, rdd1Value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里得提示一下。</span></span><br><span class="line"><span class="comment">// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。</span></span><br><span class="line"><span class="comment">// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。</span></span><br><span class="line"><span class="comment">// rdd2中每条数据都可能会返回多条join后的数据。</span></span><br></pre></td></tr></table></figure><h3 id="解决方案六：采样倾斜key并分拆join操作"><a href="#解决方案六：采样倾斜key并分拆join操作" class="headerlink" title="解决方案六：采样倾斜key并分拆join操作"></a>解决方案六：采样倾斜key并分拆join操作</h3><p><strong>方案适用场景：</strong>两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</p><p><strong>方案实现思路：</strong></p><ul><li>对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。</li><li>然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。</li><li>接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。</li><li>再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。</li><li>而另外两个普通的RDD就照常join即可。</li><li>最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</li></ul><p><strong>方案实现原理：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。</p><p><strong>方案优点：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</p><p><strong>方案缺点：</strong>如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425191444321-17069054.png" alt="img"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; sampledRDD = rdd1.sample(<span class="literal">false</span>, <span class="number">0.1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。</span></span><br><span class="line"><span class="comment">// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。</span></span><br><span class="line"><span class="comment">// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; mappedSampledRDD = sampledRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt;(tuple._1, <span class="number">1</span>L);</span><br><span class="line">            &#125;     </span><br><span class="line">        &#125;);</span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; countedSampledRDD = mappedSampledRDD.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Long</span> call(<span class="type">Long</span> v1, <span class="type">Long</span> v2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; reversedSampledRDD = countedSampledRDD.mapToPair( </span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Long</span>&gt;, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="keyword">final</span> <span class="type">Long</span> skewedUserid = reversedSampledRDD.sortByKey(<span class="literal">false</span>).take(<span class="number">1</span>).get(<span class="number">0</span>)._2;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; skewedRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Function</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">Boolean</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Boolean</span> call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="comment">// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; commonRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Function</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">Boolean</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Boolean</span> call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> !tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125; </span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// rdd2，就是那个所有key的分布相对较为均匀的rdd。</span></span><br><span class="line"><span class="comment">// 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。</span></span><br><span class="line"><span class="comment">// 对扩容的每条数据，都打上0～100的前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt; skewedRdd2 = rdd2.filter(</span><br><span class="line">         <span class="keyword">new</span> <span class="type">Function</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Row</span>&gt;, <span class="type">Boolean</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Boolean</span> call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).flatMapToPair(<span class="keyword">new</span> <span class="type">PairFlatMapFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Row</span>&gt;, <span class="type">String</span>, <span class="type">Row</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Iterable</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; call(</span><br><span class="line">                    <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">                <span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; list = <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(int i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;(i + <span class="string">"_"</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。</span></span><br><span class="line"><span class="comment">// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD1 = skewedRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">String</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">                int prefix = random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .join(skewedUserid2infoRDD)</span><br><span class="line">        .mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>,<span class="type">Tuple2</span>&lt;<span class="type">String</span>,<span class="type">Row</span>&gt;&gt;, <span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;() &#123;</span><br><span class="line">                        <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; call(</span><br><span class="line">                            <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; tuple)</span><br><span class="line">                            <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                            long key = <span class="type">Long</span>.valueOf(tuple._1.split(<span class="string">"_"</span>)[<span class="number">1</span>]);</span><br><span class="line">                            <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;(key, tuple._2);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将倾斜key join后的结果与普通key join后的结果，uinon起来。</span></span><br><span class="line"><span class="comment">// 就是最终的join结果。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2);</span><br></pre></td></tr></table></figure><h3 id="解决方案七：使用随机前缀和扩容RDD进行join"><a href="#解决方案七：使用随机前缀和扩容RDD进行join" class="headerlink" title="解决方案七：使用随机前缀和扩容RDD进行join"></a>解决方案七：使用随机前缀和扩容RDD进行join</h3><p><strong>方案适用场景：</strong>如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。</p><p><strong>方案实现思路：</strong></p><ul><li>该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。</li><li>然后将该RDD的每条数据都打上一个n以内的随机前缀。</li><li>同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。</li><li>最后将两个处理后的RDD进行join即可。</li></ul><p><strong>方案实现原理：</strong>将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p><p><strong>方案优点：</strong>对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p><p><strong>方案缺点：</strong>该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p><p><strong>方案实践经验：</strong>曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt; expandedRDD = rdd1.flatMapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFlatMapFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Row</span>&gt;, <span class="type">String</span>, <span class="type">Row</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Iterable</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; list = <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(int i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;(<span class="number">0</span> + <span class="string">"_"</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; mappedRDD = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">String</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">                int prefix = random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将两个处理后的RDD进行join即可。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD);</span><br></pre></td></tr></table></figure><h3 id="解决方案八：多种方案组合使用"><a href="#解决方案八：多种方案组合使用" class="headerlink" title="解决方案八：多种方案组合使用"></a>解决方案八：多种方案组合使用</h3><p>​        在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，预处理一部分数据，并过滤一部分数据来缓解；其次可以对某些shuffle操作提升并行度，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cache和persist的区别</title>
      <link href="/2019-06-08-cache%E5%92%8Cpersist%E7%9A%84%E5%8C%BA%E5%88%AB.html"/>
      <url>/2019-06-08-cache%E5%92%8Cpersist%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
      
        <content type="html"><![CDATA[<p>** cache和persist的区别：** &lt;Excerpt in index | 首页摘要&gt;</p><p>cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h1 id="cache和persist的区别"><a href="#cache和persist的区别" class="headerlink" title="cache和persist的区别"></a>cache和persist的区别</h1><p>基于Spark 1.6.1 的源码，可以看到</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist()</span><br></pre></td></tr></table></figure><p>说明是cache()调用了persist(), 想要知道二者的不同还需要看一下persist函数：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br></pre></td></tr></table></figure><p>可以看到persist()内部调用了persist(StorageLevel.MEMORY_ONLY)，继续深入：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Set this RDD's storage level to persist its values across operations after the first time</span></span><br><span class="line"><span class="comment"> * it is computed. This can only be used to assign a new storage level if the RDD does not</span></span><br><span class="line"><span class="comment"> * have a storage level set yet..</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> Handle changes of StorageLevel</span></span><br><span class="line">  <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span> &amp;&amp; newLevel != storageLevel) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnsupportedOperationException</span>(</span><br><span class="line">      <span class="string">"Cannot change storage level of an RDD after it was already assigned a level"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  sc.persistRDD(<span class="keyword">this</span>)</span><br><span class="line">  <span class="comment">// Register the RDD with the ContextCleaner for automatic GC-based cleanup</span></span><br><span class="line">  sc.cleaner.foreach(_.registerRDDForCleanup(<span class="keyword">this</span>))</span><br><span class="line">  storageLevel = newLevel</span><br><span class="line">  <span class="keyword">this</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看出来persist有一个 StorageLevel 类型的参数，这个表示的是RDD的缓存级别。</p><p>至此便可得出cache和persist的区别了：cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。</p><h1 id="RDD的缓存级别"><a href="#RDD的缓存级别" class="headerlink" title="RDD的缓存级别"></a>RDD的缓存级别</h1><p>顺便看一下RDD都有哪些缓存级别，查看 StorageLevel 类的源码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>)</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到这里列出了12种缓存级别，但这些有什么区别呢？可以看到每个缓存级别后面都跟了一个StorageLevel的构造函数，里面包含了4个或5个参数，如下</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br></pre></td></tr></table></figure><p>查看其构造函数</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useDisk: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useMemory: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useOffHeap: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _deserialized: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _replication: <span class="type">Int</span> = 1</span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Externalizable</span> </span>&#123;</span><br><span class="line">  ......</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">useDisk</span></span>: <span class="type">Boolean</span> = _useDisk</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">useMemory</span></span>: <span class="type">Boolean</span> = _useMemory</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">useOffHeap</span></span>: <span class="type">Boolean</span> = _useOffHeap</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deserialized</span></span>: <span class="type">Boolean</span> = _deserialized</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">replication</span></span>: <span class="type">Int</span> = _replication</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到StorageLevel类的主构造器包含了5个参数：</p><ul><li>useDisk：使用硬盘（外存）</li><li>useMemory：使用内存</li><li>useOffHeap：使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。</li><li>deserialized：反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象</li><li>replication：备份数（在多个节点上备份）</li></ul><p>理解了这5个参数，StorageLevel 的12种缓存级别就不难理解了。</p><p>val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) 就表示使用这种缓存级别的RDD将存储在硬盘以及内存中，使用序列化（在硬盘中），并且在多个节点上备份2份（正常的RDD只有一份）</p><p>另外还注意到有一种特殊的缓存级别</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure><p>使用了堆外内存，StorageLevel 类的源码中有一段代码可以看出这个的特殊性，它不能和其它几个参数共存。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (useOffHeap) &#123;</span><br><span class="line">  require(!useDisk, <span class="string">"Off-heap storage level does not support using disk"</span>)</span><br><span class="line">  require(!useMemory, <span class="string">"Off-heap storage level does not support using heap memory"</span>)</span><br><span class="line">  require(!deserialized, <span class="string">"Off-heap storage level does not support deserialized storage"</span>)</span><br><span class="line">  require(replication == <span class="number">1</span>, <span class="string">"Off-heap storage level does not support multiple replication"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （八）SparkCore的调优之开发调优</title>
      <link href="/2019-06-08-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%85%AB%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E5%BC%80%E5%8F%91%E8%B0%83%E4%BC%98.html"/>
      <url>/2019-06-08-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%85%AB%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E5%BC%80%E5%8F%91%E8%B0%83%E4%BC%98.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （八）SparkCore的调优之开发调优：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。</p><p>​        然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>​        Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。</p><p>​        笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为<strong>开发调优、资源调优、数据倾斜调优、shuffle调优</strong>几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。</p><p>本文作为Spark性能优化指南的基础篇，主要讲解开发调优以及资源调优。</p><h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>​        Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。</p><h2 id="原则一：避免创建重复的RDD"><a href="#原则一：避免创建重复的RDD" class="headerlink" title="原则一：避免创建重复的RDD"></a>原则一：避免创建重复的RDD</h2><p>​        通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。</p><p>我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。</p><p>一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。</p><h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span></span><br><span class="line"><span class="comment">// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。</span></span><br><span class="line"><span class="comment">// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd2.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span></span><br><span class="line"><span class="comment">// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。</span></span><br><span class="line"><span class="comment">// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</span></span><br><span class="line"><span class="comment">// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><h2 id="原则二：尽可能复用同一个RDD"><a href="#原则二：尽可能复用同一个RDD" class="headerlink" title="原则二：尽可能复用同一个RDD"></a>原则二：尽可能复用同一个RDD</h2><p>​        除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。</p><h3 id="一个简单的例子-1"><a href="#一个简单的例子-1" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 错误的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。</span></span><br><span class="line"><span class="comment">// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line"><span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; rdd2 = rdd1.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分别对rdd1和rdd2执行了不同的算子操作。</span></span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd2.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</span></span><br><span class="line"><span class="comment">// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 其实在这种情况下完全可以复用同一个RDD。</span></span><br><span class="line"><span class="comment">// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。</span></span><br><span class="line"><span class="comment">// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd1.map(tuple._2...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</span></span><br><span class="line"><span class="comment">// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。</span></span><br><span class="line"><span class="comment">// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。</span></span><br></pre></td></tr></table></figure><h2 id="原则三：对多次使用的RDD进行持久化"><a href="#原则三：对多次使用的RDD进行持久化" class="headerlink" title="原则三：对多次使用的RDD进行持久化"></a>原则三：对多次使用的RDD进行持久化</h2><p>​        当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。</p><p>​        Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p><p>​        因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。</p><h3 id="对多次使用的RDD进行持久化的代码示例"><a href="#对多次使用的RDD进行持久化的代码示例" class="headerlink" title="对多次使用的RDD进行持久化的代码示例"></a>对多次使用的RDD进行持久化的代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"><span class="comment">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span></span><br><span class="line"><span class="comment">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span></span><br><span class="line"><span class="comment">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).cache()</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span></span><br><span class="line"><span class="comment">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。</span></span><br><span class="line"><span class="comment">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span></span><br><span class="line"><span class="comment">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><p>对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。</p><h3 id="Spark的持久化级别"><a href="#Spark的持久化级别" class="headerlink" title="Spark的持久化级别"></a>Spark的持久化级别</h3><table><thead><tr><th>持久化级别</th><th>含义解释</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。</td></tr><tr><td>MEMORY_AND_DISK</td><td>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</td></tr><tr><td>MEMORY_ONLY_SER</td><td>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>DISK_ONLY</td><td>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</td></tr><tr><td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.</td><td>对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。</td></tr></tbody></table><h3 id="如何选择一种最合适的持久化策略"><a href="#如何选择一种最合适的持久化策略" class="headerlink" title="如何选择一种最合适的持久化策略"></a>如何选择一种最合适的持久化策略</h3><ul><li>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</li><li>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li><li>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</li><li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li></ul><h2 id="原则四：尽量避免使用shuffle类算子"><a href="#原则四：尽量避免使用shuffle类算子" class="headerlink" title="原则四：尽量避免使用shuffle类算子"></a>原则四：尽量避免使用shuffle类算子</h2><p>​    如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。</p><p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</p><p>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p><h3 id="Broadcast与map进行join代码示例"><a href="#Broadcast与map进行join代码示例" class="headerlink" title="Broadcast与map进行join代码示例"></a>Broadcast与map进行join代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统的join操作会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Broadcast+map的join操作，不会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></span><br><span class="line"><span class="keyword">val</span> rdd2Data = rdd2.collect()</span><br><span class="line"><span class="keyword">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></span><br><span class="line"><span class="comment">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span></span><br><span class="line"><span class="comment">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></span><br><span class="line"><span class="comment">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></span><br></pre></td></tr></table></figure><h2 id="原则五：使用map-side预聚合的shuffle操作"><a href="#原则五：使用map-side预聚合的shuffle操作" class="headerlink" title="原则五：使用map-side预聚合的shuffle操作"></a>原则五：使用map-side预聚合的shuffle操作</h2><p><strong>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。</strong></p><p>​        所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p><p>比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425185853777-491379087.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425185910086-1609967670.png" alt="img"></p><h2 id="原则六：使用高性能的算子"><a href="#原则六：使用高性能的算子" class="headerlink" title="原则六：使用高性能的算子"></a>原则六：使用高性能的算子</h2><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p><h3 id="使用reduceByKey-aggregateByKey替代groupByKey"><a href="#使用reduceByKey-aggregateByKey替代groupByKey" class="headerlink" title="使用reduceByKey/aggregateByKey替代groupByKey"></a>使用reduceByKey/aggregateByKey替代groupByKey</h3><p>详情见“原则五：使用map-side预聚合的shuffle操作”。</p><h3 id="使用mapPartitions替代普通map"><a href="#使用mapPartitions替代普通map" class="headerlink" title="使用mapPartitions替代普通map"></a>使用mapPartitions替代普通map</h3><p>​        mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p><h3 id="使用foreachPartitions替代foreach"><a href="#使用foreachPartitions替代foreach" class="headerlink" title="使用foreachPartitions替代foreach"></a>使用foreachPartitions替代foreach</h3><p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p><h3 id="使用filter之后进行coalesce操作"><a href="#使用filter之后进行coalesce操作" class="headerlink" title="使用filter之后进行coalesce操作"></a>使用filter之后进行coalesce操作</h3><p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p><h3 id="使用repartitionAndSortWithinPartitions替代repartition与sort类操作"><a href="#使用repartitionAndSortWithinPartitions替代repartition与sort类操作" class="headerlink" title="使用repartitionAndSortWithinPartitions替代repartition与sort类操作"></a>使用repartitionAndSortWithinPartitions替代repartition与sort类操作</h3><p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p><h2 id="原则七：广播大变量"><a href="#原则七：广播大变量" class="headerlink" title="原则七：广播大变量"></a>原则七：广播大变量</h2><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p><p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p><p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p><h3 id="广播大变量的代码示例"><a href="#广播大变量的代码示例" class="headerlink" title="广播大变量的代码示例"></a>广播大变量的代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 以下代码在算子函数中，使用了外部的变量。</span></span><br><span class="line"><span class="comment">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line">rdd1.map(list1...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下代码将list1封装成了Broadcast类型的广播变量。</span></span><br><span class="line"><span class="comment">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span></span><br><span class="line"><span class="comment">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span></span><br><span class="line"><span class="comment">// 每个Executor内存中，就只会驻留一份广播变量副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line"><span class="keyword">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class="line">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure><h2 id="原则八：使用Kryo优化序列化性能"><a href="#原则八：使用Kryo优化序列化性能" class="headerlink" title="原则八：使用Kryo优化序列化性能"></a>原则八：使用Kryo优化序列化性能</h2><p>在Spark中，主要有三个地方涉及到了序列化：</p><ul><li>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。</li><li>将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。</li><li>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</li></ul><p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p><p>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建SparkConf对象。</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line"><span class="comment">// 设置序列化器为KryoSerializer。</span></span><br><span class="line">conf.set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line"><span class="comment">// 注册要序列化的自定义类型。</span></span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br></pre></td></tr></table></figure><h2 id="原则九：优化数据结构"><a href="#原则九：优化数据结构" class="headerlink" title="原则九：优化数据结构"></a>原则九：优化数据结构</h2><p>Java中，有三种类型比较耗费内存：</p><ul><li>对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。</li><li>字符串，每个字符串内部都有一个字符数组以及长度等额外信息。</li><li>集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</li></ul><p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p><p>但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。</p><h2 id="原则十：Data-Locality本地化级别"><a href="#原则十：Data-Locality本地化级别" class="headerlink" title="原则十：Data Locality本地化级别"></a>原则十：Data Locality本地化级别</h2><p><strong>PROCESS_LOCAL</strong>：进程本地化，代码和数据在同一个进程中，也就是在同一个executor中；计算数据的task由executor执行，数据在executor的BlockManager中；性能最好</p><p><strong>NODE_LOCAL</strong>：节点本地化，代码和数据在同一个节点中；比如说，数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行；或者是，数据和task在一个节点上的不同executor中；数据需要在进程间进行传输<br><strong>NO_PREF</strong>：对于task来说，数据从哪里获取都一样，没有好坏之分<br><strong>RACK_LOCAL</strong>：机架本地化，数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输<br><strong>ANY</strong>：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差</p><p>spark.locality.wait，默认是3s</p><p>Spark在Driver上，对Application的每一个stage的task，进行分配之前，都会计算出每个task要计算的是哪个分片数据，RDD的某个partition；Spark的task分配算法，优先，会希望每个task正好分配到它要计算的数据所在的节点，这样的话，就不用在网络间传输数据；</p><p>但是可能task没有机会分配到它的数据所在的节点，因为可能那个节点的计算资源和计算能力都满了；所以呢，这种时候，通常来说，Spark会等待一段时间，默认情况下是3s钟（不是绝对的，还有很多种情况，对不同的本地化级别，都会去等待），到最后，实在是等待不了了，就会选择一个比较差的本地化级别，比如说，将task分配到靠它要计算的数据所在节点，比较近的一个节点，然后进行计算。</p><p>但是对于第二种情况，通常来说，肯定是要发生数据传输，task会通过其所在节点的BlockManager来获取数据，BlockManager发现自己本地没有数据，会通过一个getRemote()方法，通过TransferService（网络数据传输组件）从数据所在节点的BlockManager中，获取数据，通过网络传输回task所在节点。</p><p>对于我们来说，当然不希望是类似于第二种情况的了。最好的，当然是task和数据在一个节点上，直接从本地executor的BlockManager中获取数据，纯内存，或者带一点磁盘IO；如果要通过网络传输数据的话，那么实在是，性能肯定会下降的，大量网络传输，以及磁盘IO，都是性能的杀手。</p><p><strong>什么时候要调节这个参数？</strong></p><p>观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。<br>日志里面会显示，starting task。。。，PROCESS LOCAL、NODE LOCAL，观察大部分task的数据本地化级别。</p><p>如果大多都是PROCESS_LOCAL，那就不用调节了<br>如果是发现，好多的级别都是NODE_LOCAL、ANY，那么最好就去调节一下数据本地化的等待时长<br>调节完，应该是要反复调节，每次调节完以后，再来运行，观察日志<br>看看大部分的task的本地化级别有没有提升；看看，整个spark作业的运行时间有没有缩短</p><p>但是注意别本末倒置，本地化级别倒是提升了，但是因为大量的等待时长，spark作业的运行时间反而增加了，那就还是不要调节了。</p><p>spark.locality.wait，默认是3s；可以改成6s，10s</p><p>默认情况下，下面3个的等待时长，都是跟上面那个是一样的，都是3s</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.locality.wait.process//建议60s</span><br><span class="line">spark.locality.wait.node//建议30s</span><br><span class="line">spark.locality.wait.rack//建议20s</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （七）Spark 运行流程</title>
      <link href="/2019-06-07-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%83%EF%BC%89Spark%20%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B.html"/>
      <url>/2019-06-07-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%83%EF%BC%89Spark%20%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （七）Spark 运行流程：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （七）Spark 运行流程</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、Spark中的基本概念"><a href="#一、Spark中的基本概念" class="headerlink" title="一、Spark中的基本概念"></a>一、Spark中的基本概念</h2><p>（1）Application：表示你的应用程序</p><p>（2）Driver：表示main()函数，创建SparkContext。由SparkContext负责与ClusterManager通信，进行资源的申请，任务的分配和监控等。程序执行完毕后关闭SparkContext</p><p>（3）Executor：某个Application运行在Worker节点上的一个进程，该进程负责运行某些task，并且负责将数据存在内存或者磁盘上。在Spark on Yarn模式下，其进程名称为 CoarseGrainedExecutor Backend，一个CoarseGrainedExecutor Backend进程有且仅有一个executor对象，它负责将Task包装成taskRunner，并从线程池中抽取出一个空闲线程运行Task，这样，每个CoarseGrainedExecutorBackend能并行运行Task的数据就取决于分配给它的CPU的个数。</p><p>（4）Worker：集群中可以运行Application代码的节点。在Standalone模式中指的是通过slave文件配置的worker节点，在Spark on Yarn模式中指的就是NodeManager节点。</p><p>（5）Task：在Executor进程中执行任务的工作单元，多个Task组成一个Stage</p><p>（6）Job：包含多个Task组成的并行计算，是由Action行为触发的</p><p>（7）Stage：每个Job会被拆分很多组Task，作为一个TaskSet，其名称为Stage</p><p>（8）DAGScheduler：根据Job构建基于Stage的DAG，并提交Stage给TaskScheduler，其划分Stage的依据是RDD之间的依赖关系</p><p>（9）TaskScheduler：将TaskSet提交给Worker（集群）运行，每个Executor运行什么Task就是在此处分配的。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425154512801-941033013.png" alt="img"></p><h2 id="二、Spark的运行流程"><a href="#二、Spark的运行流程" class="headerlink" title="二、Spark的运行流程"></a>二、Spark的运行流程</h2><h3 id="2-1-Spark的基本运行流程"><a href="#2-1-Spark的基本运行流程" class="headerlink" title="2.1　Spark的基本运行流程"></a>2.1　Spark的基本运行流程</h3><h4 id="1、说明"><a href="#1、说明" class="headerlink" title="1、说明"></a>1、说明</h4><blockquote><p>(1)构建Spark Application的运行环境（启动SparkContext），SparkContext向资源管理器（可以是Standalone、Mesos或YARN）注册并申请运行Executor资源；</p><p>(2)资源管理器分配Executor资源并启动StandaloneExecutorBackend，Executor运行情况将随着心跳发送到资源管理器上；</p><p>(3)SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler。Executor向SparkContext申请Task</p><p>(4)Task Scheduler将Task发放给Executor运行同时SparkContext将应用程序代码发放给Executor。</p><p>(5)Task在Executor上运行，运行完毕释放所有资源。</p></blockquote><h4 id="2、图解"><a href="#2、图解" class="headerlink" title="2、图解"></a>2、图解</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425172026316-1086206534.png" alt="img"></p><h4 id="3、Spark运行架构特点"><a href="#3、Spark运行架构特点" class="headerlink" title="3、Spark运行架构特点"></a>3、Spark运行架构特点</h4><blockquote><p>（1）每个Application获取专属的executor进程，该进程在Application期间一直驻留，并以多线程方式运行tasks。这种Application隔离机制有其优势的，无论是从调度角度看（每个Driver调度它自己的任务），还是从运行角度看（来自不同Application的Task运行在不同的JVM中）。当然，这也意味着Spark Application不能跨应用程序共享数据，除非将数据写入到外部存储系统。</p><p>（2）Spark与资源管理器无关，只要能够获取executor进程，并能保持相互通信就可以了。</p><p>（3）提交SparkContext的Client应该靠近Worker节点（运行Executor的节点)，最好是在同一个Rack里，因为Spark Application运行过程中SparkContext和Executor之间有大量的信息交换；如果想在远程集群中运行，最好使用RPC将SparkContext提交给集群，不要远离Worker运行SparkContext。</p><p>（4）Task采用了数据本地性和推测执行的优化机制。</p></blockquote><h4 id="4、DAGScheduler"><a href="#4、DAGScheduler" class="headerlink" title="4、DAGScheduler"></a>4、DAGScheduler</h4><p>Job=多个stage，Stage=多个同种task, Task分为ShuffleMapTask和ResultTask，Dependency分为ShuffleDependency和NarrowDependency</p><p>面向stage的切分，切分依据为宽依赖</p><p>维护waiting jobs和active jobs，维护waiting stages、active stages和failed stages，以及与jobs的映射关系</p><p><strong>主要职能：</strong></p><blockquote><p>1、接收提交Job的主入口，<code>submitJob(rdd, ...)</code>或<code>runJob(rdd, ...)</code>。在<code>SparkContext</code>里会调用这两个方法。 </p><ul><li><ul><li>生成一个Stage并提交，接着判断Stage是否有父Stage未完成，若有，提交并等待父Stage，以此类推。结果是：DAGScheduler里增加了一些waiting stage和一个running stage。</li><li>running stage提交后，分析stage里Task的类型，生成一个Task描述，即TaskSet。</li><li>调用<code>TaskScheduler.submitTask(taskSet, ...)</code>方法，把Task描述提交给TaskScheduler。TaskScheduler依据资源量和触发分配条件，会为这个TaskSet分配资源并触发执行。</li><li><code>DAGScheduler</code>提交job后，异步返回<code>JobWaiter</code>对象，能够返回job运行状态，能够cancel job，执行成功后会处理并返回结果</li></ul></li></ul><p>2、处理<code>TaskCompletionEvent</code> </p><ul><li><ul><li>如果task执行成功，对应的stage里减去这个task，做一些计数工作： <ul><li>如果task是ResultTask，计数器<code>Accumulator</code>加一，在job里为该task置true，job finish总数加一。加完后如果finish数目与partition数目相等，说明这个stage完成了，标记stage完成，从running stages里减去这个stage，做一些stage移除的清理工作</li><li>如果task是ShuffleMapTask，计数器<code>Accumulator</code>加一，在stage里加上一个output location，里面是一个<code>MapStatus</code>类。<code>MapStatus</code>是<code>ShuffleMapTask</code>执行完成的返回，包含location信息和block size(可以选择压缩或未压缩)。同时检查该stage完成，向<code>MapOutputTracker</code>注册本stage里的shuffleId和location信息。然后检查stage的output location里是否存在空，若存在空，说明一些task失败了，整个stage重新提交；否则，继续从waiting stages里提交下一个需要做的stage</li></ul></li><li>如果task是重提交，对应的stage里增加这个task</li><li>如果task是fetch失败，马上标记对应的stage完成，从running stages里减去。如果不允许retry，abort整个stage；否则，重新提交整个stage。另外，把这个fetch相关的location和map任务信息，从stage里剔除，从<code>MapOutputTracker</code>注销掉。最后，如果这次fetch的blockManagerId对象不为空，做一次<code>ExecutorLost</code>处理，下次shuffle会换在另一个executor上去执行。</li><li>其他task状态会由<code>TaskScheduler</code>处理，如Exception, TaskResultLost, commitDenied等。</li></ul></li></ul><p>3、其他与job相关的操作还包括：cancel job， cancel stage, resubmit failed stage等</p></blockquote><p>其他职能：</p><p> cacheLocations 和 preferLocation</p><h4 id="5、TaskScheduler"><a href="#5、TaskScheduler" class="headerlink" title="5、TaskScheduler"></a>5、TaskScheduler</h4><p>维护task和executor对应关系，executor和物理资源对应关系，在排队的task和正在跑的task。</p><p>内部维护一个任务队列，根据FIFO或Fair策略，调度任务。</p><p><code>TaskScheduler</code>本身是个接口，spark里只实现了一个<code>TaskSchedulerImpl</code>，理论上任务调度可以定制。</p><p>主要功能：</p><blockquote><p><code>1、submitTasks(taskSet)</code>，接收<code>DAGScheduler</code>提交来的tasks </p><ul><li><ul><li>为tasks创建一个<code>TaskSetManager</code>，添加到任务队列里。<code>TaskSetManager</code>跟踪每个task的执行状况，维护了task的许多具体信息。</li><li>触发一次资源的索要。 <ul><li>首先，<code>TaskScheduler</code>对照手头的可用资源和Task队列，进行executor分配(考虑优先级、本地化等策略)，符合条件的executor会被分配给<code>TaskSetManager</code>。</li><li>然后，得到的Task描述交给<code>SchedulerBackend</code>，调用<code>launchTask(tasks)</code>，触发executor上task的执行。task描述被序列化后发给executor，executor提取task信息，调用task的<code>run()</code>方法执行计算。</li></ul></li></ul></li></ul><p><code>2、cancelTasks(stageId)</code>，取消一个stage的tasks </p><ul><li><ul><li>调用<code>SchedulerBackend</code>的<code>killTask(taskId, executorId, ...)</code>方法。taskId和executorId在<code>TaskScheduler</code>里一直维护着。</li></ul></li></ul><p>3、resourceOffer(offers: Seq[Workers])，这是非常重要的一个方法，调用者是SchedulerBacnend，用途是底层资源SchedulerBackend把空余的workers资源交给TaskScheduler，让其根据调度策略为排队的任务分配合理的cpu和内存资源，然后把任务描述列表传回给SchedulerBackend</p><ul><li><ul><li>从worker offers里，搜集executor和host的对应关系、active executors、机架信息等等</li><li>worker offers资源列表进行随机洗牌，任务队列里的任务列表依据调度策略进行一次排序</li><li>遍历每个taskSet，按照进程本地化、worker本地化、机器本地化、机架本地化的优先级顺序，为每个taskSet提供可用的cpu核数，看是否满足 <ul><li>默认一个task需要一个cpu，设置参数为<code>&quot;spark.task.cpus=1&quot;</code></li><li>为taskSet分配资源，校验是否满足的逻辑，最终在<code>TaskSetManager</code>的<code>resourceOffer(execId, host, maxLocality)</code>方法里</li><li>满足的话，会生成最终的任务描述，并且调用<code>DAGScheduler</code>的<code>taskStarted(task, info)</code>方法，通知<code>DAGScheduler</code>，这时候每次会触发<code>DAGScheduler</code>做一次<code>submitMissingStage</code>的尝试，即stage的tasks都分配到了资源的话，马上会被提交执行</li></ul></li></ul></li></ul><p><code>4、statusUpdate(taskId, taskState, data)</code>,另一个非常重要的方法，调用者是<code>SchedulerBacnend</code>，用途是<code>SchedulerBacnend</code>会将task执行的状态汇报给<code>TaskScheduler</code>做一些决定 </p><ul><li><ul><li>若<code>TaskLost</code>，找到该task对应的executor，从active executor里移除，避免这个executor被分配到其他task继续失败下去。</li><li>task finish包括四种状态：finished, killed, failed, lost。只有finished是成功执行完成了。其他三种是失败。</li><li>task成功执行完，调用<code>TaskResultGetter.enqueueSuccessfulTask(taskSet, tid, data)</code>，否则调用<code>TaskResultGetter.enqueueFailedTask(taskSet, tid, state, data)</code>。<code>TaskResultGetter</code>内部维护了一个线程池，负责异步fetch task执行结果并反序列化。默认开四个线程做这件事，可配参数<code>&quot;spark.resultGetter.threads&quot;=4</code>。</li></ul></li></ul></blockquote><p> <strong>TaskResultGetter取task result的逻辑</strong></p><blockquote><p>1、对于success task，如果taskResult里的数据是直接结果数据，直接把data反序列出来得到结果；如果不是，会调用<code>blockManager.getRemoteBytes(blockId)</code>从远程获取。如果远程取回的数据是空的，那么会调用<code>TaskScheduler.handleFailedTask</code>，告诉它这个任务是完成了的但是数据是丢失的。否则，取到数据之后会通知<code>BlockManagerMaster</code>移除这个block信息，调用<code>TaskScheduler.handleSuccessfulTask</code>，告诉它这个任务是执行成功的，并且把result data传回去。</p><p>2、对于failed task，从data里解析出fail的理由，调用<code>TaskScheduler.handleFailedTask</code>，告诉它这个任务失败了，理由是什么。</p></blockquote><h4 id="6、SchedulerBackend"><a href="#6、SchedulerBackend" class="headerlink" title="6、SchedulerBackend"></a>6、SchedulerBackend</h4><p>在<code>TaskScheduler</code>下层，用于对接不同的资源管理系统，<code>SchedulerBackend</code>是个接口，需要实现的主要方法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def start(): Unit</span><br><span class="line">def stop(): Unit</span><br><span class="line">def reviveOffers(): Unit // 重要方法：SchedulerBackend把自己手头上的可用资源交给TaskScheduler，TaskScheduler根据调度策略分配给排队的任务吗，返回一批可执行的任务描述，SchedulerBackend负责launchTask，即最终把task塞到了executor模型上，executor里的线程池会执行task的run()</span><br><span class="line">def killTask(taskId: Long, executorId: String, interruptThread: Boolean): Unit =</span><br><span class="line">    throw new UnsupportedOperationException</span><br></pre></td></tr></table></figure><p>粗粒度：进程常驻的模式，典型代表是standalone模式，mesos粗粒度模式，yarn</p><p>细粒度：mesos细粒度模式</p><p>这里讨论粗粒度模式，更好理解：<code>CoarseGrainedSchedulerBackend</code>。</p><p>维护executor相关信息(包括executor的地址、通信端口、host、总核数，剩余核数)，手头上executor有多少被注册使用了，有多少剩余，总共还有多少核是空的等等。</p><p>主要职能</p><blockquote><p>1、Driver端主要通过actor监听和处理下面这些事件： </p><ul><li><ul><li><code>RegisterExecutor(executorId, hostPort, cores, logUrls)</code>。这是executor添加的来源，通常worker拉起、重启会触发executor的注册。<code>CoarseGrainedSchedulerBackend</code>把这些executor维护起来，更新内部的资源信息，比如总核数增加。最后调用一次<code>makeOffer()</code>，即把手头资源丢给<code>TaskScheduler</code>去分配一次，返回任务描述回来，把任务launch起来。这个<code>makeOffer()</code>的调用会出现在<em>任何与资源变化相关的事件</em>中，下面会看到。</li><li><code>StatusUpdate(executorId, taskId, state, data)</code>。task的状态回调。首先，调用<code>TaskScheduler.statusUpdate</code>上报上去。然后，判断这个task是否执行结束了，结束了的话把executor上的freeCore加回去，调用一次<code>makeOffer()</code>。</li><li><code>ReviveOffers</code>。这个事件就是别人直接向<code>SchedulerBackend</code>请求资源，直接调用<code>makeOffer()</code>。</li><li><code>KillTask(taskId, executorId, interruptThread)</code>。这个killTask的事件，会被发送给executor的actor，executor会处理<code>KillTask</code>这个事件。</li><li><code>StopExecutors</code>。通知每一个executor，处理<code>StopExecutor</code>事件。</li><li><code>RemoveExecutor(executorId, reason)</code>。从维护信息中，那这堆executor涉及的资源数减掉，然后调用<code>TaskScheduler.executorLost()</code>方法，通知上层我这边有一批资源不能用了，你处理下吧。<code>TaskScheduler</code>会继续把<code>executorLost</code>的事件上报给<code>DAGScheduler</code>，原因是<code>DAGScheduler</code>关心shuffle任务的output location。<code>DAGScheduler</code>会告诉<code>BlockManager</code>这个executor不可用了，移走它，然后把所有的stage的shuffleOutput信息都遍历一遍，移走这个executor，并且把更新后的shuffleOutput信息注册到<code>MapOutputTracker</code>上，最后清理下本地的<code>CachedLocations</code>Map。</li></ul></li></ul><p><code>2、reviveOffers()</code>方法的实现。直接调用了<code>makeOffers()</code>方法，得到一批可执行的任务描述，调用<code>launchTasks</code>。</p><p><code>3、launchTasks(tasks: Seq[Seq[TaskDescription]])</code>方法。 </p><ul><li><ul><li>遍历每个task描述，序列化成二进制，然后发送给每个对应的executor这个任务信息 <ul><li>如果这个二进制信息太大，超过了9.2M(默认的akkaFrameSize 10M 减去 默认 为akka留空的200K)，会出错，abort整个taskSet，并打印提醒增大akka frame size</li><li>如果二进制数据大小可接受，发送给executor的actor，处理<code>LaunchTask(serializedTask)</code>事件。</li></ul></li></ul></li></ul></blockquote><h4 id="7、Executor"><a href="#7、Executor" class="headerlink" title="7、Executor"></a>7、Executor</h4><p>Executor是spark里的进程模型，可以套用到不同的资源管理系统上，与<code>SchedulerBackend</code>配合使用。</p><p>内部有个线程池，有个running tasks map，有个actor，接收上面提到的由<code>SchedulerBackend</code>发来的事件。</p><p><strong>事件处理</strong></p><ol><li><code>launchTask</code>。根据task描述，生成一个<code>TaskRunner</code>线程，丢尽running tasks map里，用线程池执行这个<code>TaskRunner</code></li><li><code>killTask</code>。从running tasks map里拿出线程对象，调它的kill方法。</li></ol><h2 id="三、Spark在不同集群中的运行架构"><a href="#三、Spark在不同集群中的运行架构" class="headerlink" title="三、Spark在不同集群中的运行架构"></a>三、Spark在不同集群中的运行架构</h2><p>Spark注重建立良好的生态系统，它不仅支持多种外部文件存储系统，提供了多种多样的集群运行模式。部署在单台机器上时，既可以用本地（Local）模式运行，也可以使用伪分布式模式来运行；当以分布式集群部署的时候，可以根据自己集群的实际情况选择Standalone模式（Spark自带的模式）、YARN-Client模式或者YARN-Cluster模式。Spark的各种运行模式虽然在启动方式、运行位置、调度策略上各有不同，但它们的目的基本都是一致的，就是在合适的位置安全可靠的根据用户的配置和Job的需要运行和管理Task。</p><h3 id="3-1-Spark-on-Standalone运行过程"><a href="#3-1-Spark-on-Standalone运行过程" class="headerlink" title="3.1　Spark on Standalone运行过程"></a>3.1　Spark on Standalone运行过程</h3><p>Standalone模式是Spark实现的资源调度框架，其主要的节点有Client节点、Master节点和Worker节点。其中Driver既可以运行在Master节点上中，也可以运行在本地Client端。当用spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行；当使用spark-submit工具提交Job或者在Eclips、IDEA等开发平台上使用”new SparkConf().setMaster(“spark://master:7077”)”方式运行Spark任务时，Driver是运行在本地Client端上的。</p><p>运行过程文字说明</p><blockquote><p>1、我们提交一个任务，任务就叫Application<br>2、初始化程序的入口SparkContext，<br>　　2.1 初始化DAG Scheduler<br>　　2.2 初始化Task Scheduler<br>3、Task Scheduler向master去进行注册并申请资源（CPU Core和Memory）<br>4、Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上获取资源，然后启动StandaloneExecutorBackend；顺便初<br>      始化好了一个线程池<br>5、StandaloneExecutorBackend向Driver(SparkContext)注册,这样Driver就知道哪些Executor为他进行服务了。<br>　  到这个时候其实我们的初始化过程基本完成了，我们开始执行transformation的代码，但是代码并不会真正的运行，直到我们遇到一个action操作。生产一个job任务，进行stage的划分<br>6、SparkContext将Applicaiton代码发送给StandaloneExecutorBackend；并且SparkContext解析Applicaiton代码，构建DAG图，并提交给DAG Scheduler分解成Stage（当碰到Action操作        时，就会催生Job；每个Job中含有1个或多个Stage，Stage一般在获取外部数据和shuffle之前产生）。<br>7、将Stage（或者称为TaskSet）提交给Task Scheduler。Task Scheduler负责将Task分配到相应的Worker，最后提交给StandaloneExecutorBackend执行；<br>8、对task进行序列化，并根据task的分配算法，分配task<br>9、对接收过来的task进行反序列化，把task封装成一个线程<br>10、开始执行Task，并向SparkContext报告，直至Task完成。<br>11、资源注销</p></blockquote><p>运行过程图形说明</p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425170820868-121220770.png" alt="img" style="zoom:200%;"><h3 id="3-2-Spark-on-YARN运行过程"><a href="#3-2-Spark-on-YARN运行过程" class="headerlink" title="3.2　Spark on YARN运行过程"></a>3.2　Spark on YARN运行过程</h3><p>YARN是一种统一资源管理机制，在其上面可以运行多套计算框架。目前的大数据技术世界，大多数公司除了使用Spark来进行数据计算，由于历史原因或者单方面业务处理的性能考虑而使用着其他的计算框架，比如MapReduce、Storm等计算框架。Spark基于此种情况开发了Spark on YARN的运行模式，由于借助了YARN良好的弹性资源管理机制，不仅部署Application更加方便，而且用户在YARN集群中运行的服务和Application的资源也完全隔离，更具实践应用价值的是YARN可以通过队列的方式，管理同时运行在集群中的多个服务。</p><p>Spark on YARN模式根据Driver在集群中的位置分为两种模式：一种是YARN-Client模式，另一种是YARN-Cluster（或称为YARN-Standalone模式）。</p><h4 id="3-2-1-YARN框架流程"><a href="#3-2-1-YARN框架流程" class="headerlink" title="3.2.1　YARN框架流程"></a>3.2.1　YARN框架流程</h4><p>任何框架与YARN的结合，都必须遵循YARN的开发模式。在分析Spark on YARN的实现细节之前，有必要先分析一下YARN框架的一些基本原理。</p><p>参考：<a href="http://www.cnblogs.com/qingyunzong/p/8615096.html" target="_blank" rel="noopener">http://www.cnblogs.com/qingyunzong/p/8615096.html</a></p><h4 id="3-2-2-YARN-Client"><a href="#3-2-2-YARN-Client" class="headerlink" title="3.2.2　YARN-Client"></a>3.2.2　YARN-Client</h4><p>Yarn-Client模式中，Driver在客户端本地运行，这种模式可以使得Spark Application和客户端进行交互，因为Driver在客户端，所以可以通过webUI访问Driver的状态，默认是<a href="http://hadoop1:4040访问，而YARN通过http://" target="_blank" rel="noopener">http://hadoop1:4040访问，而YARN通过http://</a> hadoop1:8088访问。</p><p>YARN-client的工作流程分为以下几个步骤：</p><p>文字说明</p><blockquote><p>1.Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend；</p><p>2.ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派；</p><p>3.Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）；</p><p>4.一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task；</p><p>5.Client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；</p><p>6.应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。</p></blockquote><p>图片说明</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425184438832-531815073.png" alt="img"></p><h4 id="3-2-3-YARN-Cluster"><a href="#3-2-3-YARN-Cluster" class="headerlink" title="3.2.3　YARN-Cluster"></a>3.2.3　YARN-Cluster</h4><p>在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动；第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成。</p><p>YARN-cluster的工作流程分为以下几个步骤：</p><p>文字说明</p><blockquote><ol><li><p>Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等；</p></li><li><p>ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化；</p></li><li><p>ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束；</p></li><li><p>一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等；</p></li><li><p>ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；</p></li><li><p>应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己。</p></li></ol></blockquote><p> 图片说明</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425184600831-1537558526.png" alt="img"></p><h4 id="3-2-4-YARN-Client-与-YARN-Cluster-区别"><a href="#3-2-4-YARN-Client-与-YARN-Cluster-区别" class="headerlink" title="3.2.4　YARN-Client 与 YARN-Cluster 区别"></a>3.2.4　YARN-Client 与 YARN-Cluster 区别</h4><p>理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别。</p><blockquote><p>1、YARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业；</p><p>2、YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开。</p></blockquote><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425184834257-26846302.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425184850521-1989517165.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>map与flatMap的区别</title>
      <link href="/2019-06-06-map%E4%B8%8EflatMap%E7%9A%84%E5%8C%BA%E5%88%AB.html"/>
      <url>/2019-06-06-map%E4%B8%8EflatMap%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
      
        <content type="html"><![CDATA[<p>** map与flatMap的区别：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        map与flatMap的区别</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>spark的转换算子中map和flatMap都十分常见，要了解清楚它们的区别，我们必须弄懂每执行一次的数据结构是什么。</p><blockquote><p>we are superman</p><p>torrow is good</p><p>color green red</p></blockquote><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p>map操作结果：Array[Array[String]] = Array(Array(we, are, superman), Array(torrow, is, good), Array(color, green, red))</p><p>flatmap操作结果：Array[String] = Array(we, are, superman, torrow, is, good, color, green, red)</p><p>spark中map函数会对每一条输入进行指定操作，然后为每一条输入返回一个对象；</p><p>而flatmap函数则是两个操作的集合，最后将所有对象合并为一个对象。需要特别说明一下，flatmap适用于统计文件单词类的。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （五）Spark伪分布式安装</title>
      <link href="/2019-06-05-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%94%EF%BC%89Spark%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85.html"/>
      <url>/2019-06-05-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%94%EF%BC%89Spark%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （五）Spark伪分布式安装：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （五）Spark伪分布式安装</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p><strong>Hadoop部分建议参考hadoop伪分布部署</strong></p><h2 id="一、JDK的安装"><a href="#一、JDK的安装" class="headerlink" title="一、JDK的安装"></a>一、JDK的安装</h2><p>​    LINUX系统安装jdk（最好是1.8版本）</p><h3 id="1-1-上传安装包并解压"><a href="#1-1-上传安装包并解压" class="headerlink" title="1.1　上传安装包并解压"></a>1.1　上传安装包并解压</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 soft]# tar -zxvf jdk-8u73-linux-x64.tar.gz -C /usr/local/</span><br></pre></td></tr></table></figure><h3 id="1-2-配置环境变量"><a href="#1-2-配置环境变量" class="headerlink" title="1.2　配置环境变量"></a>1.2　配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 soft]# vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">JAVA</span></span><br><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_73</span><br><span class="line">export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib </span><br><span class="line">export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH:$HOME/bin</span><br></pre></td></tr></table></figure><h3 id="1-3-验证Java版本"><a href="#1-3-验证Java版本" class="headerlink" title="1.3　验证Java版本"></a>1.3　验证Java版本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 soft]# java -version</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421201645784-1699980464.png" alt="img"></p><h2 id="二、配置免密登陆"><a href="#二、配置免密登陆" class="headerlink" title="二、配置免密登陆"></a>二、配置免密登陆</h2><h3 id="2-1-检测"><a href="#2-1-检测" class="headerlink" title="2.1　检测"></a>2.1　检测</h3><p>正常情况下，本机通过ssh连接自己也是需要输入密码的</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421202047798-1815346135.png" alt="img"></p><h3 id="2-2-生成私钥和公钥秘钥对"><a href="#2-2-生成私钥和公钥秘钥对" class="headerlink" title="2.2　生成私钥和公钥秘钥对"></a>2.2　生成私钥和公钥秘钥对</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421202332319-461480990.png" alt="img"></p><h3 id="2-3-将公钥添加到authorized-keys"><a href="#2-3-将公钥添加到authorized-keys" class="headerlink" title="2.3　将公钥添加到authorized_keys"></a>2.3　将公钥添加到authorized_keys</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><h3 id="2-4-赋予authorized-keys文件600的权限"><a href="#2-4-赋予authorized-keys文件600的权限" class="headerlink" title="2.4　赋予authorized_keys文件600的权限"></a>2.4　赋予authorized_keys文件600的权限</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><h3 id="2-5-修改Linux映射文件-root用户"><a href="#2-5-修改Linux映射文件-root用户" class="headerlink" title="2.5　修改Linux映射文件(root用户)"></a>2.5　修改Linux映射文件(root用户)</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 ~]$ vi /etc/hosts</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421202658345-1846477390.png" alt="img"></p><h3 id="2-6-验证"><a href="#2-6-验证" class="headerlink" title="2.6　验证"></a>2.6　验证</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ ssh hadoop1</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421203427884-1155695129.png" alt="img"></p><p>此时不需要输入密码，免密登录设置成功。</p><h2 id="三、安装Hadoop-2-7-5"><a href="#三、安装Hadoop-2-7-5" class="headerlink" title="三、安装Hadoop-2.7.5"></a>三、安装Hadoop-2.7.5</h2><h3 id="3-1-上传解压缩"><a href="#3-1-上传解压缩" class="headerlink" title="3.1　上传解压缩"></a>3.1　上传解压缩</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ tar -zxvf hadoop-2.7.5-centos-6.7.tar.gz -C apps/</span><br></pre></td></tr></table></figure><h3 id="3-2-创建安装包对应的软连接"><a href="#3-2-创建安装包对应的软连接" class="headerlink" title="3.2　创建安装包对应的软连接"></a>3.2　创建安装包对应的软连接</h3><p>为解压的hadoop包创建软连接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop1 apps]$ ll</span><br><span class="line">总用量 4</span><br><span class="line">drwxr-xr-x. 9 hadoop hadoop 4096 12月 24 13:43 hadoop-2.7.5</span><br><span class="line">[hadoop@hadoop1 apps]$ ln -s hadoop-2.7.5/ hadoop</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421203914403-1198562712.png" alt="img"></p><h3 id="3-3-修改配置文件"><a href="#3-3-修改配置文件" class="headerlink" title="3.3　修改配置文件"></a>3.3　修改配置文件</h3><p>进入/home/hadoop/apps/hadoop/etc/hadoop/目录下修改配置文件</p><h4 id="（1）修改hadoop-env-sh"><a href="#（1）修改hadoop-env-sh" class="headerlink" title="（1）修改hadoop-env.sh"></a>（1）修改hadoop-env.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ vi hadoop-env.sh </span><br><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_73</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421204243611-778828987.png" alt="img"></p><h4 id="（2）修改core-site-xml"><a href="#（2）修改core-site-xml" class="headerlink" title="（2）修改core-site.xml"></a>（2）修改core-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ vi core-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/data/hadoopdata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421204517804-907096171.png" alt="img"></p><h4 id="（3）修改hdfs-site-xml"><a href="#（3）修改hdfs-site-xml" class="headerlink" title="（3）修改hdfs-site.xml"></a>（3）修改hdfs-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ vi hdfs-site.xml</span><br></pre></td></tr></table></figure><p>dfs的备份数目，单机用1份就行</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/data/hadoopdata/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>为了保证元数据的安全一般配置多个不同目录<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/data/hadoopdata/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>datanode 的数据存储目录<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>HDFS 的数据块的副本存储个数, 默认是3<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421204809515-38083445.png" alt="img"></p><h4 id="（4）修改mapred-site-xml"><a href="#（4）修改mapred-site-xml" class="headerlink" title="（4）修改mapred-site.xml"></a>（4）修改mapred-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ cp mapred-site.xml.template mapred-site.xml</span><br><span class="line">[hadoop@hadoop1 hadoop]$ vi mapred-site.xml</span><br></pre></td></tr></table></figure><p>mapreduce.framework.name：指定mr框架为yarn方式,Hadoop二代MP也基于资源管理系统Yarn来运行 。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421205034511-1378796109.png" alt="img"></p><h4 id="（5）修改yarn-site-xml"><a href="#（5）修改yarn-site-xml" class="headerlink" title="（5）修改yarn-site.xml"></a>（5）修改yarn-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ vi yarn-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">description</span>&gt;</span>YARN 集群为 MapReduce 程序提供的 shuffle 服务<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421205232938-938870356.png" alt="img"></p><h3 id="3-4-配置环境变量"><a href="#3-4-配置环境变量" class="headerlink" title="3.4　配置环境变量"></a>3.4　配置环境变量</h3><p>千万注意：</p><p>1、如果你使用root用户进行安装。 vi /etc/profile 即可 系统变量</p><p>2、如果你使用普通用户进行安装。 vi ~/.bashrc 用户变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ vi .bashrc</span><br><span class="line"><span class="meta">#</span><span class="bash">HADOOP_HOMEexport HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5</span></span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421205503165-1776868707.png" alt="img"></p><p>使环境变量生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 bin]$ source ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="3-5-查看hadoop版本"><a href="#3-5-查看hadoop版本" class="headerlink" title="3.5　查看hadoop版本"></a>3.5　查看hadoop版本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop version</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421205704500-162738228.png" alt="img"></p><h3 id="3-6-创建文件夹"><a href="#3-6-创建文件夹" class="headerlink" title="3.6　创建文件夹"></a>3.6　创建文件夹</h3><p>文件夹的路径参考配置文件hdfs-site.xml里面的路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ mkdir -p /home/hadoop/data/hadoopdata/name</span><br><span class="line">[hadoop@hadoop1 ~]$ mkdir -p /home/hadoop/data/hadoopdata/data</span><br></pre></td></tr></table></figure><h3 id="3-7-Hadoop的初始化"><a href="#3-7-Hadoop的初始化" class="headerlink" title="3.7　Hadoop的初始化"></a>3.7　Hadoop的初始化</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop namenode -format</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421210111098-1871019705.png" alt="img"></p><h3 id="3-8-启动HDFS和YARN"><a href="#3-8-启动HDFS和YARN" class="headerlink" title="3.8　启动HDFS和YARN"></a>3.8　启动HDFS和YARN</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ start-dfs.sh[hadoop@hadoop1 ~]$ start-yarn.sh</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180422113818864-1072637752.png" alt="img"></p><h3 id="3-9-检查WebUI"><a href="#3-9-检查WebUI" class="headerlink" title="3.9　检查WebUI"></a>3.9　检查WebUI</h3><p>浏览器打开端口50070：<a href="http://hadoop1:50070/" target="_blank" rel="noopener">http://hadoop1:50070</a></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180422113937564-1552905493.png" alt="img"></p><p>其他端口说明：<br>port 8088: cluster and all applications<br>port 50070: Hadoop NameNode<br>port 50090: Secondary NameNode<br>port 50075: DataNode </p><h2 id="四、Scala的安装（可选）"><a href="#四、Scala的安装（可选）" class="headerlink" title="四、Scala的安装（可选）"></a>四、Scala的安装（可选）</h2><p>使用root安装</p><h3 id="4-1-下载"><a href="#4-1-下载" class="headerlink" title="4.1　下载"></a>4.1　下载</h3><p>Scala下载地址<a href="http://www.scala-lang.org/download/all.html" target="_blank" rel="noopener">http://www.scala-lang.org/download/all.html</a></p><p>选择对应的版本，此处在Linux上安装，选择的版本是scala-2.11.8.tgz</p><h3 id="4-2-上传解压缩"><a href="#4-2-上传解压缩" class="headerlink" title="4.2　上传解压缩"></a>4.2　上传解压缩</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 hadoop]# tar -zxvf scala-2.11.8.tgz -C /usr/local/</span><br></pre></td></tr></table></figure><h3 id="4-3-配置环境变量"><a href="#4-3-配置环境变量" class="headerlink" title="4.3　配置环境变量"></a>4.3　配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 hadoop]# vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">Scala</span></span><br><span class="line">export SCALA_HOME=/usr/local/scala-2.11.8</span><br><span class="line">export PATH=$SCALA_HOME/bin:$PATH</span><br></pre></td></tr></table></figure><p>保存并使其立即生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 scala-2.11.8]# source /etc/profile</span><br></pre></td></tr></table></figure><h3 id="4-4-验证是否安装成功"><a href="#4-4-验证是否安装成功" class="headerlink" title="4.4　验证是否安装成功"></a>4.4　验证是否安装成功</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 ~]# scala -version</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180422115201465-370446833.png" alt="img"></p><h2 id="五、Spark的安装"><a href="#五、Spark的安装" class="headerlink" title="五、Spark的安装"></a>五、Spark的安装</h2><h3 id="5-1-下载安装包"><a href="#5-1-下载安装包" class="headerlink" title="5.1　下载安装包"></a>5.1　下载安装包</h3><p>下载地址：</p><p><a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">http://spark.apache.org/downloads.html</a></p><h3 id="5-2-上传解压缩"><a href="#5-2-上传解压缩" class="headerlink" title="5.2　上传解压缩"></a>5.2　上传解压缩</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz -C apps/</span><br></pre></td></tr></table></figure><h3 id="5-3-为解压包创建一个软连接"><a href="#5-3-为解压包创建一个软连接" class="headerlink" title="5.3　为解压包创建一个软连接"></a>5.3　为解压包创建一个软连接</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop1 apps]$ ls</span><br><span class="line">hadoop  hadoop-2.7.5  spark-2.3.0-bin-hadoop2.7</span><br><span class="line">[hadoop@hadoop1 apps]$ ln -s spark-2.3.0-bin-hadoop2.7/ spark</span><br></pre></td></tr></table></figure><h3 id="5-4-进入spark-conf修改配置文件"><a href="#5-4-进入spark-conf修改配置文件" class="headerlink" title="5.4　进入spark/conf修改配置文件"></a>5.4　进入spark/conf修改配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 apps]$ cd spark/conf/</span><br></pre></td></tr></table></figure><p> 复制spark-env.sh.template并重命名为spark-env.sh，并在文件最后添加配置内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ cp spark-env.sh.template spark-env.sh</span><br><span class="line">[hadoop@hadoop1 conf]$ vi spark-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_73</span><br><span class="line">export SCALA_HOME=/usr/share/scala-2.11.8</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop/apps/hadoop-2.7.5/etc/hadoop</span><br><span class="line">export SPARK_MASTER_IP=hadoop1</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure><h3 id="5-5-配置环境变量"><a href="#5-5-配置环境变量" class="headerlink" title="5.5　配置环境变量"></a>5.5　配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ vi ~/.bashrc </span><br><span class="line"><span class="meta">#</span><span class="bash">SPARK_HOME</span></span><br><span class="line">export SPARK_HOME=/home/hadoop/apps/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><p>保存使其立即生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ source ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="5-6-启动Spark"><a href="#5-6-启动Spark" class="headerlink" title="5.6　启动Spark"></a>5.6　启动Spark</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$  ~/apps/spark/sbin/start-all.sh</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180422120649667-993379882.png" alt="img"></p><h3 id="5-7-查看进程"><a href="#5-7-查看进程" class="headerlink" title="5.7　查看进程"></a>5.7　查看进程</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180422120759529-1681016835.png" alt="img"></p><h3 id="5-8-查看web界面"><a href="#5-8-查看web界面" class="headerlink" title="5.8　查看web界面"></a>5.8　查看web界面</h3><p><a href="http://hadoop1:8080/" target="_blank" rel="noopener">http://hadoop1:8080/</a></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180422120839429-839164601.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （四）Spark的广播变量和累加器</title>
      <link href="/2019-06-04-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%9B%9B%EF%BC%89Spark%E7%9A%84%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F%E5%92%8C%E7%B4%AF%E5%8A%A0%E5%99%A8.html"/>
      <url>/2019-06-04-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%9B%9B%EF%BC%89Spark%E7%9A%84%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F%E5%92%8C%E7%B4%AF%E5%8A%A0%E5%99%A8.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （四）Spark的广播变量和累加器：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        在spark程序中，当一个传递给Spark操作(例如map和reduce)的函数在远程节点上面运行时，Spark操作实际上操作的是这个函数所用变量的一个独立副本。这些变量会被复制到每台机器上，并且这些变量在远程机器上的所有更新都不会传递回驱动程序。通常跨任务的读写变量是低效的，但是，Spark还是为两种常见的使用模式提供了两种有限的共享变量：广播变（broadcast variable）和累加器（accumulator）</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、广播变量broadcast-variable"><a href="#一、广播变量broadcast-variable" class="headerlink" title="一、广播变量broadcast variable"></a>一、广播变量broadcast variable</h2><p>​        广播变量允许程序员在每台机器上保留一个只读变量，而不是随副本一起发送它的副本。例如，它们可用于以有效的方式为每个节点提供大输入数据集的副本。Spark还尝试使用有效的广播算法来分发广播变量，以降低通信成本。</p><p>​        Spark动作通过一组阶段执行，由分布式“shuffle”操作分隔。Spark自动广播每个阶段中任务所需的公共数据。以这种方式广播的数据以序列化形式缓存并在运行每个任务之前反序列化。这意味着显式创建广播变量仅在跨多个阶段的任务需要相同数据或以反序列化形式缓存数据很重要时才有用。</p><p>​        广播变量是<code>v</code>通过调用从变量创建的<code>SparkContext.broadcast(v)</code>。广播变量是一个包装器<code>v</code>，可以通过调用该<code>value</code> 方法来访问它的值。下面的代码显示了这个：</p><h3 id="1-1-为什么要将变量定义成广播变量？"><a href="#1-1-为什么要将变量定义成广播变量？" class="headerlink" title="1.1　为什么要将变量定义成广播变量？"></a>1.1　为什么要将变量定义成广播变量？</h3><p>如果我们要在分布式计算里面分发大对象，例如：字典，集合，黑白名单等，这个都会由Driver端进行分发，一般来讲，如果这个变量不是广播变量，那么每个task就会分发一份，这在<strong>task数目十分多的情况下Driver的带宽会成为系统的瓶颈，而且会大量消耗task服务器上的资源</strong>，如果将这个变量声明为广播变量，那么知识每个executor拥有一份，这个executor启动的task会共享这个变量，节省了通信的成本和服务器的资源。</p><h3 id="1-2-广播变量图解"><a href="#1-2-广播变量图解" class="headerlink" title="1.2　广播变量图解"></a>1.2　广播变量图解</h3><p>错误的，不使用广播变量</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421162057226-1988253385.png" alt="img"></p><p>正确的，使用广播变量的情况</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421162148572-1992224700.png" alt="img"></p><h3 id="2-3-如何定义一个广播变量？"><a href="#2-3-如何定义一个广播变量？" class="headerlink" title="2.3　如何定义一个广播变量？"></a>2.3　如何定义一个广播变量？</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> a = <span class="number">3</span></span><br><span class="line"><span class="keyword">val</span> broadcast = sc.broadcast(a)</span><br></pre></td></tr></table></figure><h3 id="2-4-如何还原一个广播变量？"><a href="#2-4-如何还原一个广播变量？" class="headerlink" title="2.4　如何还原一个广播变量？"></a>2.4　如何还原一个广播变量？</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> c = broadcast.value</span><br></pre></td></tr></table></figure><h3 id="2-5-定义广播变量需要的注意点？"><a href="#2-5-定义广播变量需要的注意点？" class="headerlink" title="2.5　定义广播变量需要的注意点？"></a>2.5　定义广播变量需要的注意点？</h3><p>变量一旦被定义为一个广播变量，那么这个变量只能读，不能修改</p><h3 id="2-6-注意事项"><a href="#2-6-注意事项" class="headerlink" title="2.6　注意事项"></a>2.6　<strong>注意事项</strong></h3><p>1、能不能将一个RDD使用广播变量广播出去？</p><p>​       不能，因为RDD是不存储数据的。<strong>可以将RDD的结果广播出去。</strong></p><p>2、 广播变量只能在Driver端定义，<strong>不能在Executor端定义。</strong></p><p>3、 在Driver端可以修改广播变量的值，<strong>在Executor端无法修改广播变量的值。</strong></p><p>4、如果executor端用到了Driver的变量，如果<strong>不使用广播变量在Executor有多少task就有多少Driver端的变量副本。</strong></p><p>5、如果Executor端用到了Driver的变量，如果<strong>使用广播变量在每个Executor中只有一份Driver端的变量副本。</strong></p><h2 id="二、累加器"><a href="#二、累加器" class="headerlink" title="二、累加器"></a>二、累加器</h2><p>​    累加器是仅通过关联和交换操作“添加”的变量，因此可以并行有效地支持。它们可用于实现计数器（如MapReduce）或总和。Spark本身支持数值类型的累加器，程序员可以添加对新类型的支持。</p><p>作为用户，您可以创建命名或未命名的累加器。如下图所示，命名累加器（在此实例中<code>counter</code>）将显示在Web UI中，用于修改该累加器的阶段。Spark显示“任务”表中任务修改的每个累加器的值。</p><p><img src="http://spark.apache.org/docs/latest/img/spark-webui-accumulators.png" alt="Spark UI中的累加器"></p><p>跟踪UI中的累加器对于理解运行阶段的进度非常有用（注意：Python中尚不支持）。</p><h3 id="2-1-为什么要将一个变量定义为一个累加器？"><a href="#2-1-为什么要将一个变量定义为一个累加器？" class="headerlink" title="2.1　为什么要将一个变量定义为一个累加器？"></a>2.1　为什么要将一个变量定义为一个累加器？</h3><p>​        在spark应用程序中，我们经常会有这样的需求，如异常监控，调试，记录符合某特性的数据的数目，这种需求都需要用到计数器，如果一个变量不被声明为一个累加器，那么它将在被改变时不会再driver端进行全局汇总，即在分布式运行时每个task运行的只是原始变量的一个副本，并不能改变原始变量的值，但是当这个变量被声明为累加器后，该变量就会有分布式计数的功能。</p><h3 id="2-2-图解累加器"><a href="#2-2-图解累加器" class="headerlink" title="2.2　图解累加器"></a>2.2　图解累加器</h3><p>错误的图解</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421164701390-9845184.png" alt="img"></p><p>正确的图解</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421165419534-240211041.png" alt="img"></p><h3 id="2-3-如何定义一个累加器？"><a href="#2-3-如何定义一个累加器？" class="headerlink" title="2.3　如何定义一个累加器？"></a>2.3　如何定义一个累加器？</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> a = sc.accumulator(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="2-4-如何还原一个累加器？"><a href="#2-4-如何还原一个累加器？" class="headerlink" title="2.4　如何还原一个累加器？"></a>2.4　如何还原一个累加器？</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> b = a.value</span><br></pre></td></tr></table></figure><h3 id="2-5-注意事项"><a href="#2-5-注意事项" class="headerlink" title="2.5　注意事项"></a>2.5　<strong>注意事项</strong></h3><p>1、 <strong>累加器在Driver端定义赋初始值，累加器只能在Driver端读取最后的值，在Excutor端更新。</strong></p><p>2、累加器不是一个调优的操作，因为如果不这样做，结果是错的</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （三）Spark之RDD</title>
      <link href="/2019-06-03-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%89%EF%BC%89Spark%E4%B9%8BRDD.html"/>
      <url>/2019-06-03-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%89%EF%BC%89Spark%E4%B9%8BRDD.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （三）Spark之RDD：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （三）Spark之RDD</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、RDD的概述"><a href="#一、RDD的概述" class="headerlink" title="一、RDD的概述"></a>一、RDD的概述</h2><h3 id="1-1-什么是RDD"><a href="#1-1-什么是RDD" class="headerlink" title="1.1　什么是RDD"></a>1.1　什么是RDD</h3><p>​        <strong>RDD</strong>（Resilient Distributed Dataset）叫做<strong>弹性分布式数据集</strong>，<strong>是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。</strong>RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。</p><h3 id="1-2-RDD的属性"><a href="#1-2-RDD的属性" class="headerlink" title="1.2　RDD的属性"></a>1.2　RDD的属性</h3><p><a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala</a></p><blockquote><p> A list of partitions<br> A function for computing each split<br> A list of dependencies on other RDDs<br> Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<br> Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</p></blockquote><p>（1）一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</p><p>（2）一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</p><p>（3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p><p>（4）一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p><p>（5）一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421133911520-1150689001.png" alt="img"></p><p>其中hello.txt</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421134031551-1670646166.png" alt="img"></p><h2 id="二、RDD的创建方式"><a href="#二、RDD的创建方式" class="headerlink" title="二、RDD的创建方式"></a>二、RDD的创建方式</h2><h3 id="2-1-通过读取文件生成的"><a href="#2-1-通过读取文件生成的" class="headerlink" title="2.1　通过读取文件生成的"></a>2.1　通过读取文件生成的</h3><p>由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val file = sc.textFile(&quot;/spark/hello.txt&quot;)</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421134515478-653027491.png" alt="img"></p><h3 id="2-2-通过并行化的方式创建RDD"><a href="#2-2-通过并行化的方式创建RDD" class="headerlink" title="2.2　通过并行化的方式创建RDD"></a>2.2　通过并行化的方式创建RDD</h3><p>由一个已经存在的Scala集合创建。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> array = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">array: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(array)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">27</span>] at parallelize at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421134820158-111255712.png" alt="img"></p><h3 id="2-3-其他方式"><a href="#2-3-其他方式" class="headerlink" title="2.3　其他方式"></a>2.3　其他方式</h3><p>读取数据库等等其他的操作。也可以生成RDD。RDD转换为ParallelCollectionRDD。</p><h2 id="三、RDD编程API"><a href="#三、RDD编程API" class="headerlink" title="三、RDD编程API"></a>三、RDD编程API</h2><p><strong>Spark支持两个类型（算子）操作：Transformation和Action</strong></p><h3 id="3-1-Transformation"><a href="#3-1-Transformation" class="headerlink" title="3.1　Transformation"></a>3.1　Transformation</h3><p>​    主要做的是就是将一个已有的RDD生成另外一个RDD。Transformation具有<strong>lazy**</strong>特性(延迟加载)**。Transformation算子的代码不会真正被执行。只有当我们的程序里面遇到一个action算子的时候，代码才会真正的被执行。这种设计让Spark更加有效率地运行。</p><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds</a></p><p><strong>常用的Transformation</strong>：</p><table><thead><tr><th><strong>转换</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td><strong>map</strong>(func)</td><td>返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成</td></tr><tr><td><strong>filter</strong>(func)</td><td>返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成</td></tr><tr><td><strong>flatMap</strong>(func)</td><td>类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）</td></tr><tr><td><strong>mapPartitions</strong>(func)</td><td>类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]</td></tr><tr><td><strong>mapPartitionsWithIndex</strong>(func)</td><td>类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]</td></tr><tr><td><strong>sample</strong>(withReplacement, fraction, seed)</td><td>根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子</td></tr><tr><td><strong>union</strong>(otherDataset)</td><td>对源RDD和参数RDD求并集后返回一个新的RDD</td></tr><tr><td><strong>intersection</strong>(otherDataset)</td><td>对源RDD和参数RDD求交集后返回一个新的RDD</td></tr><tr><td><strong>distinct</strong>([numTasks]))</td><td>对源RDD进行去重后返回一个新的RDD</td></tr><tr><td><strong>groupByKey</strong>([numTasks])</td><td>在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD</td></tr><tr><td><strong>reduceByKey</strong>(func, [numTasks])</td><td>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置</td></tr><tr><td><strong>aggregateByKey</strong>(zeroValue)(seqOp, combOp, [numTasks])</td><td>先按分区聚合 再总的聚合   每次要跟初始值交流 例如：aggregateByKey(0)(<em>+</em>,<em>+</em>) 对k/y的RDD进行操作</td></tr><tr><td><strong>sortByKey</strong>([ascending], [numTasks])</td><td>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</td></tr><tr><td><strong>sortBy</strong>(func,[ascending], [numTasks])</td><td>与sortByKey类似，但是更灵活 第一个参数是根据什么排序  第二个是怎么排序 false倒序   第三个排序后分区数  默认与原RDD一样</td></tr><tr><td><strong>join</strong>(otherDataset, [numTasks])</td><td>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD  相当于内连接（求交集）</td></tr><tr><td><strong>cogroup</strong>(otherDataset, [numTasks])</td><td>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<v>,Iterable<w>))类型的RDD</w></v></td></tr><tr><td><strong>cartesian</strong>(otherDataset)</td><td>两个RDD的笛卡尔积  的成很多个K/V</td></tr><tr><td><strong>pipe</strong>(command, [envVars])</td><td>调用外部程序</td></tr><tr><td><strong>coalesce</strong>(numPartitions<strong>)</strong></td><td>重新分区 第一个参数是要分多少区，第二个参数是否shuffle 默认false  少分区变多分区 true   多分区变少分区 false</td></tr><tr><td><strong>repartition</strong>(numPartitions)</td><td>重新分区 必须shuffle  参数是要分多少区  少变多</td></tr><tr><td><strong>repartitionAndSortWithinPartitions</strong>(partitioner)</td><td>重新分区+排序  比先分区再排序效率高  对K/V的RDD进行操作</td></tr><tr><td><strong>foldByKey</strong>(zeroValue)(seqOp)</td><td>该函数用于K/V做折叠，合并处理 ，与aggregate类似   第一个括号的参数应用于每个V值  第二括号函数是聚合例如：<em>+</em></td></tr><tr><td><strong>combineByKey</strong></td><td>合并相同的key的值 rdd1.combineByKey(x =&gt; x, (a: Int, b: Int) =&gt; a + b, (m: Int, n: Int) =&gt; m + n)</td></tr><tr><td><strong>partitionBy**</strong>（partitioner）**</td><td>对RDD进行分区  partitioner是分区器 例如new HashPartition(2</td></tr><tr><td><strong>cache</strong></td><td>RDD缓存，可以避免重复计算从而减少时间，区别：cache内部调用了persist算子，cache默认就一个缓存级别MEMORY-ONLY ，而persist则可以选择缓存级别</td></tr><tr><td><strong>persist</strong></td><td></td></tr><tr><td></td><td></td></tr><tr><td><strong>Subtract**</strong>（rdd）**</td><td>返回前rdd元素不在后rdd的rdd</td></tr><tr><td><strong>leftOuterJoin</strong></td><td>leftOuterJoin类似于SQL中的左外关联left outer join，返回结果以前面的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可。</td></tr><tr><td><strong>rightOuterJoin</strong></td><td>rightOuterJoin类似于SQL中的有外关联right outer join，返回结果以参数中的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可</td></tr><tr><td>subtractByKey</td><td>substractByKey和基本转换操作中的subtract类似只不过这里是针对K的，返回在主RDD中出现，并且不在otherRDD中出现的元素</td></tr></tbody></table><h3 id="3-2-Action"><a href="#3-2-Action" class="headerlink" title="3.2　Action"></a>3.2　Action</h3><p>触发代码的运行，我们一段spark代码里面至少需要有一个action操作。</p><p><strong>常用的Action</strong>:</p><table><thead><tr><th><strong>动作</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td><strong>reduce</strong>(<em>func</em>)</td><td>通过func函数聚集RDD中的所有元素，这个功能必须是课交换且可并联的</td></tr><tr><td><strong>collect</strong>()</td><td>在驱动程序中，以数组的形式返回数据集的所有元素</td></tr><tr><td><strong>count</strong>()</td><td>返回RDD的元素个数</td></tr><tr><td><strong>first</strong>()</td><td>返回RDD的第一个元素（类似于take(1)）</td></tr><tr><td><strong>take</strong>(<em>n</em>)</td><td>返回一个由数据集的前n个元素组成的数组</td></tr><tr><td><strong>takeSample</strong>(<em>withReplacement</em>,<em>num</em>, [<em>seed</em>])</td><td>返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子</td></tr><tr><td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td><td></td></tr><tr><td><strong>saveAsTextFile</strong>(<em>path</em>)</td><td>将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本</td></tr><tr><td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td><td>将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。</td></tr><tr><td><strong>saveAsObjectFile</strong>(<em>path</em>)</td><td></td></tr><tr><td><strong>countByKey</strong>()</td><td>针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</td></tr><tr><td><strong>foreach</strong>(<em>func</em>)</td><td>在数据集的每一个元素上，运行函数func进行更新。</td></tr><tr><td><strong>aggregate</strong></td><td>先对分区进行操作，在总体操作</td></tr><tr><td><strong>reduceByKeyLocally</strong></td><td></td></tr><tr><td><strong>lookup</strong></td><td></td></tr><tr><td><strong>top</strong></td><td></td></tr><tr><td><strong>fold</strong></td><td></td></tr><tr><td><strong>foreachPartition</strong></td><td></td></tr></tbody></table><h3 id="3-3-Spark-WordCount代码编写"><a href="#3-3-Spark-WordCount代码编写" class="headerlink" title="3.3　Spark WordCount代码编写"></a>3.3　Spark WordCount代码编写</h3><p>使用maven进行项目构建</p><h4 id="（1）使用scala进行编写"><a href="#（1）使用scala进行编写" class="headerlink" title="（1）使用scala进行编写"></a>（1）使用scala进行编写</h4><p>查看官方网站，需要导入2个依赖包</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421144526496-1152731884.png" alt="img"></p><p>详细代码</p><p>SparkWordCountWithScala.scala</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkWordCountWithScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 如果这个参数不设置，默认认为你运行的是集群模式</span></span><br><span class="line"><span class="comment">      * 如果设置成local代表运行的是local模式</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    conf.setMaster(<span class="string">"local"</span>)</span><br><span class="line">    <span class="comment">//设置任务名</span></span><br><span class="line">    conf.setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">    <span class="comment">//创建SparkCore的程序入口</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//读取文件 生成RDD</span></span><br><span class="line">    <span class="keyword">val</span> file: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"E:\\hello.txt"</span>)</span><br><span class="line">    <span class="comment">//把每一行数据按照，分割</span></span><br><span class="line">    <span class="keyword">val</span> word: <span class="type">RDD</span>[<span class="type">String</span>] = file.flatMap(_.split(<span class="string">","</span>))</span><br><span class="line">    <span class="comment">//让每一个单词都出现一次</span></span><br><span class="line">    <span class="keyword">val</span> wordOne: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = word.map((_,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">//单词计数</span></span><br><span class="line">    <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.reduceByKey(_+_)</span><br><span class="line">    <span class="comment">//按照单词出现的次数 降序排序</span></span><br><span class="line">    <span class="keyword">val</span> sortRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordCount.sortBy(tuple =&gt; tuple._2,<span class="literal">false</span>)</span><br><span class="line">    <span class="comment">//将最终的结果进行保存</span></span><br><span class="line">    sortRdd.saveAsTextFile(<span class="string">"E:\\result"</span>)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421151823241-1753369845.png" alt="img"></p><h4 id="（2）使用java-jdk7进行编写"><a href="#（2）使用java-jdk7进行编写" class="headerlink" title="（2）使用java jdk7进行编写"></a>（2）使用java jdk7进行编写</h4><p>SparkWordCountWithJava7.java</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaPairRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaSparkContext</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.<span class="type">FlatMapFunction</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.<span class="type">Function2</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.<span class="type">PairFunction</span>;</span><br><span class="line"><span class="keyword">import</span> scala.<span class="type">Tuple2</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Arrays</span>;</span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Iterator</span>;</span><br><span class="line"></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SparkWordCountWithJava7</span> </span>&#123;</span><br><span class="line">    public static void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line">        <span class="type">SparkConf</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>();</span><br><span class="line">        conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">        conf.setAppName(<span class="string">"WordCount"</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> sc = <span class="keyword">new</span> <span class="type">JavaSparkContext</span>(conf);</span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; fileRdd = sc.textFile(<span class="string">"E:\\hello.txt"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; wordRDD = fileRdd.flatMap(<span class="keyword">new</span> <span class="type">FlatMapFunction</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Iterator</span>&lt;<span class="type">String</span>&gt; call(<span class="type">String</span> line) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="type">Arrays</span>.asList(line.split(<span class="string">","</span>)).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordOneRDD = wordRDD.mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">String</span>, <span class="type">String</span>, <span class="type">Integer</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; call(<span class="type">String</span> word) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(word, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordCountRDD = wordOneRDD.reduceByKey(<span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Integer</span>, <span class="type">Integer</span>, <span class="type">Integer</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Integer</span> call(<span class="type">Integer</span> i1, <span class="type">Integer</span> i2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> i1 + i2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; count2WordRDD = wordCountRDD.mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt;, <span class="type">Integer</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; sortRDD = count2WordRDD.sortByKey(<span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; resultRDD = sortRDD.mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt;, <span class="type">String</span>, <span class="type">Integer</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        resultRDD.saveAsTextFile(<span class="string">"E:\\result7"</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（3）使用java-jdk8进行编写"><a href="#（3）使用java-jdk8进行编写" class="headerlink" title="（3）使用java jdk8进行编写"></a>（3）使用java jdk8进行编写</h4><p>lambda表达式</p><p>SparkWordCountWithJava8.java</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaPairRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaSparkContext</span>;</span><br><span class="line"><span class="keyword">import</span> scala.<span class="type">Tuple2</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Arrays</span>;</span><br><span class="line"></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SparkWordCountWithJava8</span> </span>&#123;</span><br><span class="line">    public static void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line">        <span class="type">SparkConf</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>();</span><br><span class="line">        conf.setAppName(<span class="string">"WortCount"</span>);</span><br><span class="line">        conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> sc = <span class="keyword">new</span> <span class="type">JavaSparkContext</span>(conf);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; fileRDD = sc.textFile(<span class="string">"E:\\hello.txt"</span>);</span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; wordRdd = fileRDD.flatMap(line -&gt; <span class="type">Arrays</span>.asList(line.split(<span class="string">","</span>)).iterator());</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordOneRDD = wordRdd.mapToPair(word -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordCountRDD = wordOneRDD.reduceByKey((x, y) -&gt; x + y);</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; count2WordRDD = wordCountRDD.mapToPair(tuple -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1));</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; sortRDD = count2WordRDD.sortByKey(<span class="literal">false</span>);</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; resultRDD = sortRDD.mapToPair(tuple -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1));</span><br><span class="line">        resultRDD.saveAsTextFile(<span class="string">"E:\\result8"</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425153140543-8294264.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425153515149-1269337605.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425153556469-238789142.png" alt="img"></p><h2 id="四、RDD的宽依赖和窄依赖"><a href="#四、RDD的宽依赖和窄依赖" class="headerlink" title="四、RDD的宽依赖和窄依赖"></a>四、RDD的宽依赖和窄依赖</h2><h3 id="4-1-RDD依赖关系的本质内幕"><a href="#4-1-RDD依赖关系的本质内幕" class="headerlink" title="4.1　RDD依赖关系的本质内幕"></a>4.1　<strong>RDD依赖关系的本质内幕</strong></h3><p>由于RDD是粗粒度的操作数据集，每个Transformation操作都会生成一个新的RDD，所以RDD之间就会形成类似流水线的前后依赖关系；RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。如图所示显示了RDD之间的依赖关系。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425151022105-1121308065.png" alt="img"></p><p>从图中可知：</p><p><strong>窄依赖：</strong>是指每个父RDD的一个Partition最多被子RDD的一个Partition所使用，例如map、filter、union等操作都会产生窄依赖；（独生子女）</p><p><strong>宽依赖：</strong>是指一个父RDD的Partition会被多个子RDD的Partition所使用，例如groupByKey、reduceByKey、sortByKey等操作都会产生宽依赖；（超生）</p><p>需要特别说明的是对join操作有两种情况：</p><p>（1）图中左半部分join：如果两个RDD在进行join操作时，一个RDD的partition仅仅和另一个RDD中已知个数的Partition进行join，那么这种类型的join操作就是窄依赖，例如图1中左半部分的join操作(join with inputs co-partitioned)；</p><p>（2）图中右半部分join：其它情况的join操作就是宽依赖,例如图1中右半部分的join操作(join with inputs not co-partitioned)，由于是需要父RDD的所有partition进行join的转换，这就涉及到了shuffle，因此这种类型的join操作也是宽依赖。</p><p>总结：</p><blockquote><p>在这里我们是从父RDD的partition被使用的个数来定义窄依赖和宽依赖，因此可以用一句话概括下：如果父RDD的一个Partition被子RDD的一个Partition所使用就是窄依赖，否则的话就是宽依赖。因为是确定的partition数量的依赖关系，所以RDD之间的依赖关系就是窄依赖；由此我们可以得出一个推论：即窄依赖不仅包含一对一的窄依赖，还包含一对固定个数的窄依赖。</p><p>一对固定个数的窄依赖的理解：即子RDD的partition对父RDD依赖的Partition的数量不会随着RDD数据规模的改变而改变；换句话说，无论是有100T的数据量还是1P的数据量，在窄依赖中，子RDD所依赖的父RDD的partition的个数是确定的，而宽依赖是shuffle级别的，数据量越大，那么子RDD所依赖的父RDD的个数就越多，从而子RDD所依赖的父RDD的partition的个数也会变得越来越多。</p></blockquote><h3 id="4-2-依赖关系下的数据流视图"><a href="#4-2-依赖关系下的数据流视图" class="headerlink" title="4.2　依赖关系下的数据流视图"></a>4.2　<strong>依赖关系下的数据流视图</strong></h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425151747136-185909749.png" alt="img"></p><p>在spark中，会根据RDD之间的依赖关系将DAG图（有向无环图）划分为不同的阶段，对于窄依赖，由于partition依赖关系的确定性，partition的转换处理就可以在同一个线程里完成，窄依赖就被spark划分到同一个stage中，而对于宽依赖，只能等父RDD shuffle处理完成后，下一个stage才能开始接下来的计算。</p><p><strong>因此spark划分stage的整体思路是</strong>：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。因此在图2中RDD C,RDD D,RDD E,RDDF被构建在一个stage中,RDD A被构建在一个单独的Stage中,而RDD B和RDD G又被构建在同一个stage中。</p><p>在spark中，Task的类型分为2种：<strong>ShuffleMapTask</strong>和<strong>ResultTask</strong>；</p><p>简单来说，DAG的最后一个阶段会为每个结果的partition生成一个ResultTask，即每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！而其余所有阶段都会生成ShuffleMapTask；之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中；也就是说上图中的stage1和stage2相当于mapreduce中的Mapper,而ResultTask所代表的stage3就相当于mapreduce中的reducer。</p><p>在之前动手操作了一个wordcount程序，因此可知，Hadoop中MapReduce操作中的Mapper和Reducer在spark中的基本等量算子是map和reduceByKey;不过区别在于：Hadoop中的MapReduce天生就是排序的；而reduceByKey只是根据Key进行reduce，但spark除了这两个算子还有其他的算子；因此从这个意义上来说，Spark比Hadoop的计算算子更为丰富。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler</title>
      <link href="/2019-06-02-Hadoop%20%E7%9A%84%E4%B8%89%E7%A7%8D%E8%B0%83%E5%BA%A6%E5%99%A8FIFO%E3%80%81Capacity%20Scheduler%E3%80%81Fair%20Scheduler.html"/>
      <url>/2019-06-02-Hadoop%20%E7%9A%84%E4%B8%89%E7%A7%8D%E8%B0%83%E5%BA%A6%E5%99%A8FIFO%E3%80%81Capacity%20Scheduler%E3%80%81Fair%20Scheduler.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p><strong>目前Hadoop有三种比较流行的资源调度器：FIFO 、Capacity Scheduler、Fair Scheduler。目前hadoop2.7默认使用的是Capacity Scheduler容量调度器。</strong></p><h3 id="一、FIFO（先入先出调度器）"><a href="#一、FIFO（先入先出调度器）" class="headerlink" title="一、FIFO（先入先出调度器）"></a>一、FIFO（先入先出调度器）</h3><p>hadoop1.x使用的默认调度器就是FIFO。FIFO采用队列方式将一个一个job任务按照时间先后顺序进行服务。比如排在最前面的job需要若干maptask和若干reducetask，当发现有空闲的服务器节点就分配给这个job，直到job执行完毕。</p><p><img src="https://img-blog.csdn.net/20180907181312127?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hmd2VhdGhlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h3 id="二、Capacity-Scheduler（容量调度器）"><a href="#二、Capacity-Scheduler（容量调度器）" class="headerlink" title="二、Capacity Scheduler（容量调度器）"></a>二、Capacity Scheduler（容量调度器）</h3><p>hadoop2.x使用的默认调度器是Capacity Scheduler。</p><p>1、支持多个队列，每个队列可配置一定量的资源，每个采用FIFO的方式调度。</p><p>2、为了防止同一个用户的job任务独占队列中的资源，调度器会对同一用户提交的job任务所占资源进行限制。</p><p>3、分配新的job任务时，首先计算每个队列中正在运行task个数与其队列应该分配的资源量做比值，然后选择比值最小的队列。比如如图队列A15个task，20%资源量，那么就是15%0.2=70，队列B是25%0.5=50 ，队列C是25%0.3=80.33 。所以选择最小值队列B。</p><p>4、其次，按照job任务的优先级和时间顺序，同时要考虑到用户的资源量和内存的限制，对队列中的job任务进行排序执行。</p><p>5、多个队列同时按照任务队列内的先后顺序一次执行。例如下图中job11、job21、job31分别在各自队列中顺序比较靠前，三个任务就同时执行。</p><p><img src="https://img-blog.csdn.net/20180907183145655?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hmd2VhdGhlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h3 id="三、Fair-Scheduler（公平调度器）"><a href="#三、Fair-Scheduler（公平调度器）" class="headerlink" title="三、Fair Scheduler（公平调度器）"></a>三、Fair Scheduler（公平调度器）</h3><p>1、支持多个队列，每个队列可以配置一定的资源，每个队列中的job任务公平共享其所在队列的所有资源。</p><p>2、队列中的job任务都是按照优先级分配资源，优先级越高分配的资源越多，但是为了确保公平每个job任务都会分配到资源。优先级是根据每个job任务的理想获取资源量减去实际获取资源量的差值决定的，差值越大优先级越高。</p><p><img src="https://img-blog.csdn.net/20180909212500326?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hmd2VhdGhlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>原文链接：<a href="https://blog.csdn.net/xiaomage510/article/details/82500067" target="_blank" rel="noopener">https://blog.csdn.net/xiaomage510/article/details/82500067</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark调度模式-FIFO和FAIR</title>
      <link href="/2019-06-02-Spark%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%BC%8F-FIFO%E5%92%8CFAIR.html"/>
      <url>/2019-06-02-Spark%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%BC%8F-FIFO%E5%92%8CFAIR.html</url>
      
        <content type="html"><![CDATA[<p>** Spark调度模式-FIFO和FAIR：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark调度模式-FIFO和FAIR</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>　　<strong>Spark中的调度模式主要有两种：FIFO和FAIR。</strong>默认情况下Spark的调度模式是FIFO（先进先出），谁先提交谁先执行，后面的任务需要等待前面的任务执行。而FAIR（公平调度）模式支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。对这两种调度模式的具体实现，接下来会根据spark-1.6.0的源码来进行详细的分析。使用哪种调度器由参数spark.scheduler.mode来设置，可选的参数有FAIR和FIFO，默认是FIFO。</p><h2 id="一、源码入口"><a href="#一、源码入口" class="headerlink" title="一、源码入口"></a>一、源码入口</h2><p>　　在Scheduler模块中，当Stage划分好，然后提交Task的过程中，会进入TaskSchedulerImpl#submitTasks方法。</p><blockquote><p>schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)   </p><p>//目前支持FIFO和FAIR两种调度策略。</p></blockquote><p>在上面代码中有一个schedulableBuilder对象，这个对象在TaskSchedulerImpl类中的定义及实现可以参考下面这段源代码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> schedulableBuilder: <span class="type">SchedulableBuilder</span> = <span class="literal">null</span></span><br><span class="line">...</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(backend: <span class="type">SchedulerBackend</span>) &#123;</span><br><span class="line">    <span class="keyword">this</span>.backend = backend</span><br><span class="line">    <span class="comment">// temporarily set rootPool name to empty</span></span><br><span class="line">    rootPool = <span class="keyword">new</span> <span class="type">Pool</span>(<span class="string">""</span>, schedulingMode, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    schedulableBuilder = &#123;</span><br><span class="line">      schedulingMode <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">SchedulingMode</span>.<span class="type">FIFO</span> =&gt;</span><br><span class="line">          <span class="keyword">new</span> <span class="type">FIFOSchedulableBuilder</span>(rootPool)  <span class="comment">//rootPool包含了一组TaskSetManager</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">SchedulingMode</span>.<span class="type">FAIR</span> =&gt;</span><br><span class="line">          <span class="keyword">new</span> <span class="type">FairSchedulableBuilder</span>(rootPool, conf)  <span class="comment">//rootPool包含了一组Pool树，这棵树的叶子节点都是TaskSetManager</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    schedulableBuilder.buildPools() <span class="comment">//在FIFO中的实现是空</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>　　根据用户配置的SchedulingMode决定是生成FIFOSchedulableBuilder还是生成FairSchedulableBuilder类型的schedulableBuilder对象。<br>　 <img src="https://img-blog.csdn.net/20160528143551855" alt="SchedulableBuilder继承关系" style="zoom:150%;"></p><p>　　在生成schedulableBuilder后，调用其buildPools方法生成调度池。 调度模式由配置参数spark.scheduler.mode（默认值为FIFO）来确定。 两种模式的调度逻辑图如下：<br>　<img src="https://img-blog.csdn.net/20160528181016610" alt="调度模式逻辑图" style="zoom:150%;"></p><h2 id="二、FIFOSchedulableBuilder"><a href="#二、FIFOSchedulableBuilder" class="headerlink" title="二、FIFOSchedulableBuilder"></a>二、FIFOSchedulableBuilder</h2><p>　　FIFO的rootPool包含一组TaskSetManager。从上面的类继承图中看出在FIFOSchedulableBuilder中有两个方法：</p><h3 id="1、buildPools"><a href="#1、buildPools" class="headerlink" title="1、buildPools"></a>1、buildPools</h3><p>实现为空:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildPools</span></span>() &#123;</span><br><span class="line">    <span class="comment">// nothing</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>所以，对于FIFO模式，获取到schedulableBuilder对象后，在调用buildPools方法后，不做任何操作。</p><h3 id="2、addTaskSetManager"><a href="#2、addTaskSetManager" class="headerlink" title="2、addTaskSetManager"></a>2、addTaskSetManager</h3><p>　　该方法将TaskSetManager装载到rootPool中。直接调用的方法是Pool#addSchedulable()。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">addTaskSetManager</span></span>(manager: <span class="type">Schedulable</span>, properties: <span class="type">Properties</span>) &#123;</span><br><span class="line">  rootPool.addSchedulable(manager)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Pool#addSchedulable()方法：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> schedulableQueue = <span class="keyword">new</span> <span class="type">ConcurrentLinkedQueue</span>[<span class="type">Schedulable</span>]</span><br><span class="line">...</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">addSchedulable</span></span>(schedulable: <span class="type">Schedulable</span>) &#123;</span><br><span class="line">    require(schedulable != <span class="literal">null</span>)</span><br><span class="line">    schedulableQueue.add(schedulable)</span><br><span class="line">    schedulableNameToSchedulable.put(schedulable.name, schedulable)</span><br><span class="line">    schedulable.parent = <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>将该TaskSetManager加入到调度队列schedulableQueue中。</p><h2 id="三、FairSchedulableBuilder"><a href="#三、FairSchedulableBuilder" class="headerlink" title="三、FairSchedulableBuilder"></a>三、FairSchedulableBuilder</h2><p>　　FAIR的rootPool中包含一组Pool，在Pool中包含了TaskSetManager。</p><h3 id="1、buildPools-1"><a href="#1、buildPools-1" class="headerlink" title="1、buildPools"></a>1、buildPools</h3><p>　　在该方法中，会读取配置文件，按照配置文件中的配置参数调用buildFairSchedulerPool生成配置的调度池，以及调用buildDefaultPool生成默认调度池。<br>　　默认情况下FAIR模式的配置文件是位于SPARK_HOME/conf/fairscheduler.xml文件，也可以通过参数spark.scheduler.allocation.file设置用户自定义配置文件。<br>spark中提供的fairscheduler.xml模板如下所示：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">allocations</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pool</span> <span class="attr">name</span>=<span class="string">"production"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">schedulingMode</span>&gt;</span>FAIR<span class="tag">&lt;/<span class="name">schedulingMode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">weight</span>&gt;</span>1<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minShare</span>&gt;</span>2<span class="tag">&lt;/<span class="name">minShare</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">pool</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pool</span> <span class="attr">name</span>=<span class="string">"test"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">schedulingMode</span>&gt;</span>FIFO<span class="tag">&lt;/<span class="name">schedulingMode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">weight</span>&gt;</span>2<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minShare</span>&gt;</span>3<span class="tag">&lt;/<span class="name">minShare</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">pool</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">allocations</span>&gt;</span></span><br></pre></td></tr></table></figure><p>参数含义：<br>（1）name: 该调度池的名称，可根据该参数使用指定pool，入sc.setLocalProperty(“spark.scheduler.pool”, “test”)<br>（2）weight: 该调度池的权重，各调度池根据该参数分配系统资源。每个调度池得到的资源数为weight / sum(weight)，weight为2的分配到的资源为weight为1的两倍。<br>（3）minShare: 该调度池需要的最小资源数（CPU核数）。fair调度器首先会尝试为每个调度池分配最少minShare资源，然后剩余资源才会按照weight大小继续分配。<br>（4）schedulingMode: 该调度池内的调度模式。</p><h3 id="2、buildFairSchedulerPool"><a href="#2、buildFairSchedulerPool" class="headerlink" title="2、buildFairSchedulerPool"></a>2、buildFairSchedulerPool</h3><p>　　从上面的配置文件可以看到，每一个调度池有一个name属性指定名字，然后在该pool中可以设置其schedulingMode(可为空，默认为FIFO), weight(可为空，默认值是1), 以及minShare(可为空，默认值是0)参数。然后使用这些参数生成一个Pool对象，把该pool对象放入rootPool中。入下所示：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pool = <span class="keyword">new</span> <span class="type">Pool</span>(poolName, schedulingMode, minShare, weight)</span><br><span class="line">rootPool.addSchedulable(pool)</span><br></pre></td></tr></table></figure><h3 id="3、buildDefaultPool"><a href="#3、buildDefaultPool" class="headerlink" title="3、buildDefaultPool"></a>3、buildDefaultPool</h3><p>　　如果如果配置文件中没有设置一个name为default的pool，系统才会自动生成一个使用默认参数生成的pool对象。各项参数的默认值在buildFairSchedulerPool中有提到。</p><h3 id="4、addTaskSetManager"><a href="#4、addTaskSetManager" class="headerlink" title="4、addTaskSetManager"></a>4、addTaskSetManager</h3><p>　　这一段逻辑中是把配置文件中的pool，或者default pool放入rootPool中，然后把TaskSetManager存入rootPool对应的子pool。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">addTaskSetManager</span></span>(manager: <span class="type">Schedulable</span>, properties: <span class="type">Properties</span>) &#123;</span><br><span class="line">   <span class="keyword">var</span> poolName = <span class="type">DEFAULT_POOL_NAME</span></span><br><span class="line">   <span class="keyword">var</span> parentPool = rootPool.getSchedulableByName(poolName)</span><br><span class="line">   <span class="keyword">if</span> (properties != <span class="literal">null</span>) &#123;</span><br><span class="line">     poolName = properties.getProperty(<span class="type">FAIR_SCHEDULER_PROPERTIES</span>, <span class="type">DEFAULT_POOL_NAME</span>)</span><br><span class="line">     parentPool = rootPool.getSchedulableByName(poolName)</span><br><span class="line">     <span class="keyword">if</span> (parentPool == <span class="literal">null</span>) &#123;</span><br><span class="line">       <span class="comment">// we will create a new pool that user has configured in app</span></span><br><span class="line">       <span class="comment">// instead of being defined in xml file</span></span><br><span class="line">       parentPool = <span class="keyword">new</span> <span class="type">Pool</span>(poolName, <span class="type">DEFAULT_SCHEDULING_MODE</span>,</span><br><span class="line">         <span class="type">DEFAULT_MINIMUM_SHARE</span>, <span class="type">DEFAULT_WEIGHT</span>)</span><br><span class="line">       rootPool.addSchedulable(parentPool)</span><br><span class="line">       logInfo(<span class="string">"Created pool %s, schedulingMode: %s, minShare: %d, weight: %d"</span>.format(</span><br><span class="line">         poolName, <span class="type">DEFAULT_SCHEDULING_MODE</span>, <span class="type">DEFAULT_MINIMUM_SHARE</span>, <span class="type">DEFAULT_WEIGHT</span>))</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   parentPool.addSchedulable(manager)</span><br><span class="line">   logInfo(<span class="string">"Added task set "</span> + manager.name + <span class="string">" tasks to pool "</span> + poolName)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="5、FAIR调度池使用方法"><a href="#5、FAIR调度池使用方法" class="headerlink" title="5、FAIR调度池使用方法"></a>5、FAIR调度池使用方法</h3><p>　　在Spark-1.6.1官方文档中写道：</p><blockquote><p>如果不加设置，jobs会提交到default调度池中。由于调度池的使用是Thread级别的，只能通过具体的SparkContext来设置local属性（即无法在配置文件中通过参数spark.scheduler.pool来设置，因为配置文件中的参数会被加载到SparkConf对象中）。所以需要使用指定调度池的话，需要在具体代码中通过SparkContext对象sc来按照如下方法进行设置：<br>sc.setLocalProperty(“spark.scheduler.pool”, “test”)<br>设置该参数后，在该thread中提交的所有job都会提交到test Pool中。<br>如果接下来不再需要使用到该test调度池，<br>sc.setLocalProperty(“spark.scheduler.pool”, null)</p></blockquote><h2 id="四、FIFO和FAIR的调度顺序"><a href="#四、FIFO和FAIR的调度顺序" class="headerlink" title="四、FIFO和FAIR的调度顺序"></a>四、FIFO和FAIR的调度顺序</h2><p>这里必须提到的一个类是上面提到的Pool，在这个类中实现了不同调度模式的调度算法。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> taskSetSchedulingAlgorithm: <span class="type">SchedulingAlgorithm</span> = &#123;</span><br><span class="line">  schedulingMode <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">SchedulingMode</span>.<span class="type">FAIR</span> =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">FairSchedulingAlgorithm</span>()</span><br><span class="line">    <span class="keyword">case</span> <span class="type">SchedulingMode</span>.<span class="type">FIFO</span> =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">FIFOSchedulingAlgorithm</span>()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>FIFO模式的算法类是FIFOSchedulingAlgorithm，FAIR模式的算法实现类是FairSchedulingAlgorithm。</p><p>　　接下来的两节中comparator方法传入参数Schedulable类型是一个trait，具体实现主要有两个：1，Pool；2，TaskSetManager。与最前面那个调度模式的逻辑图相对应。</p><h3 id="1、FIFO模式的调度算法FIFOSchedulingAlgorithm"><a href="#1、FIFO模式的调度算法FIFOSchedulingAlgorithm" class="headerlink" title="1、FIFO模式的调度算法FIFOSchedulingAlgorithm"></a>1、FIFO模式的调度算法FIFOSchedulingAlgorithm</h3><p>在这个类里面，主要逻辑是一个comparator方法。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">comparator</span></span>(s1: <span class="type">Schedulable</span>, s2: <span class="type">Schedulable</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> priority1 = s1.priority   <span class="comment">//实际上是Job ID</span></span><br><span class="line">  <span class="keyword">val</span> priority2 = s2.priority</span><br><span class="line">  <span class="keyword">var</span> res = math.signum(priority1 - priority2)</span><br><span class="line">  <span class="keyword">if</span> (res == <span class="number">0</span>) &#123; <span class="comment">//如果Job ID相同，就比较Stage ID</span></span><br><span class="line">    <span class="keyword">val</span> stageId1 = s1.stageId</span><br><span class="line">    <span class="keyword">val</span> stageId2 = s2.stageId</span><br><span class="line">    res = math.signum(stageId1 - stageId2)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (res &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果有两个调度任务s1和s2，首先获得两个任务的priority，在FIFO中该优先级实际上是Job ID。首先比较两个任务的Job ID，如果priority1比priority2小，那么返回true，表示s1的优先级比s2的高。我们知道Job ID是顺序生成的，先生成的Job ID比较小，所以先提交的job肯定比后提交的job先执行。但是如果是同一个job的不同任务，接下来就比较各自的Stage ID，类似于比较Job ID，Stage ID小的优先级高。</p><h3 id="2、FAIR模式的调度算法FairSchedulingAlgorithm"><a href="#2、FAIR模式的调度算法FairSchedulingAlgorithm" class="headerlink" title="2、FAIR模式的调度算法FairSchedulingAlgorithm"></a>2、FAIR模式的调度算法FairSchedulingAlgorithm</h3><p>　　这个类中的comparator方法源代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">comparator</span></span>(s1: <span class="type">Schedulable</span>, s2: <span class="type">Schedulable</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">   <span class="keyword">val</span> minShare1 = s1.minShare <span class="comment">//在这里share理解成份额，即每个调度池要求的最少cpu核数</span></span><br><span class="line">   <span class="keyword">val</span> minShare2 = s2.minShare</span><br><span class="line">   <span class="keyword">val</span> runningTasks1 = s1.runningTasks <span class="comment">// 该Pool或者TaskSetManager中正在运行的任务数</span></span><br><span class="line">   <span class="keyword">val</span> runningTasks2 = s2.runningTasks</span><br><span class="line">   <span class="keyword">val</span> s1Needy = runningTasks1 &lt; minShare1 <span class="comment">// 如果正在运行任务数比该调度池最小cpu核数要小</span></span><br><span class="line">   <span class="keyword">val</span> s2Needy = runningTasks2 &lt; minShare2</span><br><span class="line">   <span class="keyword">val</span> minShareRatio1 = runningTasks1.toDouble / math.max(minShare1, <span class="number">1.0</span>).toDouble</span><br><span class="line">   <span class="keyword">val</span> minShareRatio2 = runningTasks2.toDouble / math.max(minShare2, <span class="number">1.0</span>).toDouble</span><br><span class="line">   <span class="keyword">val</span> taskToWeightRatio1 = runningTasks1.toDouble / s1.weight.toDouble</span><br><span class="line">   <span class="keyword">val</span> taskToWeightRatio2 = runningTasks2.toDouble / s2.weight.toDouble</span><br><span class="line">   <span class="keyword">var</span> compare: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">   <span class="keyword">if</span> (s1Needy &amp;&amp; !s2Needy) &#123;</span><br><span class="line">     <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!s1Needy &amp;&amp; s2Needy) &#123;</span><br><span class="line">     <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (s1Needy &amp;&amp; s2Needy) &#123;</span><br><span class="line">     compare = minShareRatio1.compareTo(minShareRatio2)</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     compare = taskToWeightRatio1.compareTo(taskToWeightRatio2)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (compare &lt; <span class="number">0</span>) &#123;</span><br><span class="line">     <span class="literal">true</span></span><br><span class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (compare &gt; <span class="number">0</span>) &#123;</span><br><span class="line">     <span class="literal">false</span></span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     s1.name &lt; s2.name</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>　　minShare对应fairscheduler.xml配置文件中的minShare属性。<br>（1）如果s1所在Pool或者TaskSetManager中运行状态的task数量比minShare小，s2所在Pool或者TaskSetManager中运行状态的task数量比minShare大，那么s1会优先调度。反之，s2优先调度。<br>（2）如果s1和s2所在Pool或者TaskSetManager中运行状态的task数量都比各自minShare小，那么minShareRatio小的优先被调度。<br>minShareRatio是运行状态task数与minShare的比值，即相对来说minShare使用较少的先被调度。<br>（3）如果minShareRatio相同，那么最后比较各自Pool的名字。</p><p>原文链接：<a href="https://blog.csdn.net/dabokele/article/details/51526048" target="_blank" rel="noopener">https://blog.csdn.net/dabokele/article/details/51526048</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark中parallelize函数和makeRDD函数的区别</title>
      <link href="/2019-06-02-Spark%E4%B8%ADparallelize%E5%87%BD%E6%95%B0%E5%92%8CmakeRDD%E5%87%BD%E6%95%B0%E7%9A%84%E5%8C%BA%E5%88%AB.html"/>
      <url>/2019-06-02-Spark%E4%B8%ADparallelize%E5%87%BD%E6%95%B0%E5%92%8CmakeRDD%E5%87%BD%E6%95%B0%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
      
        <content type="html"><![CDATA[<p>** Spark中parallelize函数和makeRDD函数的区别：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark中parallelize函数和makeRDD函数的区别</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>我们知道，在<a href="https://www.iteblog.com/archives/tag/spark/" target="_blank" rel="noopener">Spark</a>中创建RDD的创建方式大概可以分为三种：</p><p>（1）、从集合中创建RDD；</p><p>（2）、从外部存储创建RDD；</p><p>（3）、从其他RDD创建。</p><p>　　而从集合中创建RDD，<a href="https://www.iteblog.com/archives/tag/spark/" target="_blank" rel="noopener">Spark</a>主要提供了两中函数：parallelize和makeRDD。我们可以先看看这两个函数的声明：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p>　　我们可以从上面看出makeRDD有两种实现，而且第一个makeRDD函数接收的参数和parallelize完全一致。其实第一种makeRDD函数实现是依赖了parallelize函数的实现，来看看Spark中是怎么实现这个makeRDD函数的：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  parallelize(seq, numSlices)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​        我们可以看出，这个makeRDD函数完全和parallelize函数一致。但是我们得看看第二种makeRDD函数函数实现了，它接收的参数类型是Seq[(T, Seq[String])]，Spark文档的说明是</p><blockquote><p>Distribute a local Scala collection to form an RDD, with one or more location preferences (hostnames of Spark nodes) for each object. Create a new partition for each collection item.</p><p>分发本地scala集合以形成RDD，每个对象具有一个或多个位置首选项（spark节点的主机名）。为每个集合项创建一个新分区。</p></blockquote><p>原来，这个函数还为数据提供了位置信息，来看看我们怎么使用：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> iteblog1 = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">iteblog1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> iteblog2 = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">iteblog2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">11</span>] at makeRDD at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> seq = <span class="type">List</span>((<span class="number">1</span>, <span class="type">List</span>(<span class="string">"iteblog.com"</span>, <span class="string">"sparkhost1.com"</span>, <span class="string">"sparkhost2.com"</span>)),</span><br><span class="line">     | (<span class="number">2</span>, <span class="type">List</span>(<span class="string">"iteblog.com"</span>, <span class="string">"sparkhost2.com"</span>)))</span><br><span class="line">seq: <span class="type">List</span>[(<span class="type">Int</span>, <span class="type">List</span>[<span class="type">String</span>])] = <span class="type">List</span>((<span class="number">1</span>,<span class="type">List</span>(iteblog.com, sparkhost1.com, sparkhost2.com)),</span><br><span class="line"> (<span class="number">2</span>,<span class="type">List</span>(iteblog.com, sparkhost2.com)))</span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> iteblog3 = sc.makeRDD(seq)</span><br><span class="line">iteblog3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at makeRDD at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"> </span><br><span class="line">scala&gt; iteblog3.preferredLocations(iteblog3.partitions(<span class="number">1</span>))</span><br><span class="line">res26: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>(iteblog.com, sparkhost2.com)</span><br><span class="line"> </span><br><span class="line">scala&gt; iteblog3.preferredLocations(iteblog3.partitions(<span class="number">0</span>))</span><br><span class="line">res27: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>(iteblog.com, sparkhost1.com, sparkhost2.com)</span><br><span class="line"> </span><br><span class="line">scala&gt; iteblog1.preferredLocations(iteblog1.partitions(<span class="number">0</span>))</span><br><span class="line">res28: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>()</span><br></pre></td></tr></table></figure><p>我们可以看到，makeRDD函数有两种实现，第一种实现其实完全和parallelize一致；而第二种实现可以为数据提供位置信息，而除此之外的实现和parallelize函数也是一致的，如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq, numSlices, <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">String</span>]]())</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">val</span> indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq.map(_._1), seq.size, indexToPrefs)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>都是返回ParallelCollectionRDD，而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。</p><p><strong>转载自过往记忆（<a href="https://www.iteblog.com/）" target="_blank" rel="noopener">https://www.iteblog.com/）</a></strong></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （二）Spark2.3 HA集群的分布式安装</title>
      <link href="/2019-06-02-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%EF%BC%89Spark2.3%20HA%E9%9B%86%E7%BE%A4%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85.html"/>
      <url>/2019-06-02-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%EF%BC%89Spark2.3%20HA%E9%9B%86%E7%BE%A4%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （二）Spark2.3 HA集群的分布式安装：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （二）Spark2.3 HA集群的分布式安装</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、下载Spark安装包"><a href="#一、下载Spark安装包" class="headerlink" title="一、下载Spark安装包"></a>一、下载Spark安装包</h2><h3 id="1、从官网下载"><a href="#1、从官网下载" class="headerlink" title="1、从官网下载"></a>1、从官网下载</h3><p><a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">http://spark.apache.org/downloads.html</a></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420084850299-679933183.png" alt="img"></p><h2 id="二、安装基础"><a href="#二、安装基础" class="headerlink" title="二、安装基础"></a>二、安装基础</h2><p>1、Java8安装成功</p><p>2、Zookeeper安装成功</p><p>3、hadoop2.7.5 HA安装成功</p><p>4、Scala安装成功（不安装进程也可以启动）</p><h2 id="三、Spark安装过程"><a href="#三、Spark安装过程" class="headerlink" title="三、Spark安装过程"></a>三、Spark安装过程</h2><h3 id="1、上传并解压缩"><a href="#1、上传并解压缩" class="headerlink" title="1、上传并解压缩"></a>1、上传并解压缩</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ ls</span><br><span class="line">apps     data      exam        inithive.conf  movie     spark-2.3.0-bin-hadoop2.7.tgz  udf.jar</span><br><span class="line">cookies  data.txt  executions  json.txt       projects  student                        zookeeper.out</span><br><span class="line">course   emp       hive.sql    log            sougou    temp</span><br><span class="line">[hadoop@hadoop1 ~]$ tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz -C apps/</span><br></pre></td></tr></table></figure><h3 id="2、为安装包创建一个软连接"><a href="#2、为安装包创建一个软连接" class="headerlink" title="2、为安装包创建一个软连接"></a>2、为安装包创建一个软连接</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop1 apps]$ ls</span><br><span class="line">hadoop-2.7.5  hbase-1.2.6  spark-2.3.0-bin-hadoop2.7  zookeeper-3.4.10  zookeeper.out</span><br><span class="line">[hadoop@hadoop1 apps]$ ln -s spark-2.3.0-bin-hadoop2.7/ spark</span><br><span class="line">[hadoop@hadoop1 apps]$ ll</span><br><span class="line">总用量 36</span><br><span class="line">drwxr-xr-x. 10 hadoop hadoop  4096 3月  23 20:29 hadoop-2.7.5</span><br><span class="line">drwxrwxr-x.  7 hadoop hadoop  4096 3月  29 13:15 hbase-1.2.6</span><br><span class="line">lrwxrwxrwx.  1 hadoop hadoop    26 4月  20 13:48 spark -&gt; spark-2.3.0-bin-hadoop2.7/</span><br><span class="line">drwxr-xr-x. 13 hadoop hadoop  4096 2月  23 03:42 spark-2.3.0-bin-hadoop2.7</span><br><span class="line">drwxr-xr-x. 10 hadoop hadoop  4096 3月  23 2017 zookeeper-3.4.10</span><br><span class="line">-rw-rw-r--.  1 hadoop hadoop 17559 3月  29 13:37 zookeeper.out</span><br><span class="line">[hadoop@hadoop1 apps]$</span><br></pre></td></tr></table></figure><h3 id="3、进入spark-conf修改配置文件"><a href="#3、进入spark-conf修改配置文件" class="headerlink" title="3、进入spark/conf修改配置文件"></a>3、进入spark/conf修改配置文件</h3><p>（1）进入配置文件所在目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/spark/conf/</span><br><span class="line">[hadoop@hadoop1 conf]$ ll</span><br><span class="line">总用量 36</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  996 2月  23 03:42 docker.properties.template</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 1105 2月  23 03:42 fairscheduler.xml.template</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 2025 2月  23 03:42 log4j.properties.template</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 7801 2月  23 03:42 metrics.properties.template</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  865 2月  23 03:42 slaves.template</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 1292 2月  23 03:42 spark-defaults.conf.template</span><br><span class="line">-rwxr-xr-x. 1 hadoop hadoop 4221 2月  23 03:42 spark-env.sh.template</span><br><span class="line">[hadoop@hadoop1 conf]$</span><br></pre></td></tr></table></figure><p>（2）复制spark-env.sh.template并重命名为spark-env.sh，并在文件最后添加配置内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ cp spark-env.sh.template spark-env.sh</span><br><span class="line">[hadoop@hadoop1 conf]$ vi spark-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_73</span><br><span class="line">#export SCALA_HOME=/usr/share/scala</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop/apps/hadoop-2.7.5/etc/hadoop</span><br><span class="line">export SPARK_WORKER_MEMORY=500m</span><br><span class="line">export SPARK_WORKER_CORES=1</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop1:2181,hadoop2:2181,hadoop3:2181,hadoop4:2181 -Dspark.deploy.zookeeper.dir=/spark"</span><br></pre></td></tr></table></figure><blockquote><p>注：</p><p>#export SPARK_MASTER_IP=hadoop1  这个配置要注释掉。<br>集群搭建时配置的spark参数可能和现在的不一样，主要是考虑个人电脑配置问题，如果memory配置太大，机器运行很慢。<br>说明：<br>-Dspark.deploy.recoveryMode=ZOOKEEPER    #说明整个集群状态是通过zookeeper来维护的，整个集群状态的恢复也是通过zookeeper来维护的。就是说用zookeeper做了spark的HA配置，Master(Active)挂掉的话，Master(standby)要想变成Master（Active）的话，Master(Standby)就要像zookeeper读取整个集群状态信息，然后进行恢复所有Worker和Driver的状态信息，和所有的Application状态信息；<br>-Dspark.deploy.zookeeper.url=hadoop1:2181,hadoop2:2181,hadoop3:2181,hadoop4:2181#将所有配置了zookeeper，并且在这台机器上有可能做master(Active)的机器都配置进来；（我用了4台，就配置了4台） </p><p>-Dspark.deploy.zookeeper.dir=/spark<br>这里的dir和zookeeper配置文件zoo.cfg中的dataDir的区别？？？<br>-Dspark.deploy.zookeeper.dir是保存spark的元数据，保存了spark的作业运行状态；<br>zookeeper会保存spark集群的所有的状态信息，包括所有的Workers信息，所有的Applactions信息，所有的Driver信息,如果集群 </p></blockquote><p>（3）复制slaves.template成slaves</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ cp slaves.template slaves</span><br><span class="line">[hadoop@hadoop1 conf]$ vi slaves</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop1</span><br><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br><span class="line">hadoop4</span><br></pre></td></tr></table></figure><p>（4）将安装包分发给其他节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop1 apps]$ scp -r spark-2.3.0-bin-hadoop2.7/ hadoop2:$PWD</span><br><span class="line">[hadoop@hadoop1 apps]$ scp -r spark-2.3.0-bin-hadoop2.7/ hadoop3:$PWD</span><br><span class="line">[hadoop@hadoop1 apps]$ scp -r spark-2.3.0-bin-hadoop2.7/ hadoop4:$PWD</span><br></pre></td></tr></table></figure><p>创建软连接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop2 apps]$ ls</span><br><span class="line">hadoop-2.7.5  hbase-1.2.6  spark-2.3.0-bin-hadoop2.7  zookeeper-3.4.10</span><br><span class="line">[hadoop@hadoop2 apps]$ ln -s spark-2.3.0-bin-hadoop2.7/ spark</span><br><span class="line">[hadoop@hadoop2 apps]$ ll</span><br><span class="line">总用量 16</span><br><span class="line">drwxr-xr-x 10 hadoop hadoop 4096 3月  23 20:29 hadoop-2.7.5</span><br><span class="line">drwxrwxr-x  7 hadoop hadoop 4096 3月  29 13:15 hbase-1.2.6</span><br><span class="line">lrwxrwxrwx  1 hadoop hadoop   26 4月  20 19:26 spark -&gt; spark-2.3.0-bin-hadoop2.7/</span><br><span class="line">drwxr-xr-x 13 hadoop hadoop 4096 4月  20 19:24 spark-2.3.0-bin-hadoop2.7</span><br><span class="line">drwxr-xr-x 10 hadoop hadoop 4096 3月  21 19:31 zookeeper-3.4.10</span><br><span class="line">[hadoop@hadoop2 apps]$</span><br></pre></td></tr></table></figure><h3 id="4、配置环境变量"><a href="#4、配置环境变量" class="headerlink" title="4、配置环境变量"></a>4、配置环境变量</h3><p>所有节点均要配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 spark]$ vi ~/.bashrc </span><br><span class="line"><span class="meta">#</span><span class="bash">Spark</span></span><br><span class="line">export SPARK_HOME=/home/hadoop/apps/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><p>保存并使其立即生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 spark]$ source ~/.bashrc</span><br></pre></td></tr></table></figure><h2 id="四、启动"><a href="#四、启动" class="headerlink" title="四、启动"></a>四、启动</h2><h3 id="1、先启动zookeeper集群"><a href="#1、先启动zookeeper集群" class="headerlink" title="1、先启动zookeeper集群"></a>1、先启动zookeeper集群</h3><p>所有节点均要执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /home/hadoop/apps/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">[hadoop@hadoop1 ~]$ zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /home/hadoop/apps/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><h3 id="2、在启动HDFS集群"><a href="#2、在启动HDFS集群" class="headerlink" title="2、在启动HDFS集群"></a>2、在启动HDFS集群</h3><p>任意一个节点执行即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ start-dfs.sh</span><br></pre></td></tr></table></figure><h3 id="3、在启动Spark集群"><a href="#3、在启动Spark集群" class="headerlink" title="3、在启动Spark集群"></a>3、在启动Spark集群</h3><p>在一个节点上执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/spark/sbin/</span><br><span class="line">[hadoop@hadoop1 sbin]$ start-all.sh</span><br></pre></td></tr></table></figure><h3 id="4、查看进程"><a href="#4、查看进程" class="headerlink" title="4、查看进程"></a>4、查看进程</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420201419188-161981735.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420201445760-578558622.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420201503589-1845421183.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420201519992-501293445.png" alt="img"></p><h3 id="5、问题"><a href="#5、问题" class="headerlink" title="5、问题"></a>5、问题</h3><p>查看进程发现spark集群只有hadoop1成功启动了Master进程，其他3个节点均没有启动成功，需要手动启动，进入到/home/hadoop/apps/spark/sbin目录下执行以下命令，3个节点都要执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~]$ cd ~/apps/spark/sbin/</span><br><span class="line">[hadoop@hadoop2 sbin]$ start-master.sh</span><br></pre></td></tr></table></figure><h3 id="6、执行之后再次查看进程"><a href="#6、执行之后再次查看进程" class="headerlink" title="6、执行之后再次查看进程"></a>6、执行之后再次查看进程</h3><p>Master进程和Worker进程都以启动成功</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420201942655-1416446127.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202004959-38463551.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202024084-1987751130.png" alt="img"></p><h2 id="五、验证"><a href="#五、验证" class="headerlink" title="五、验证"></a>五、验证</h2><h3 id="1、查看Web界面Master状态"><a href="#1、查看Web界面Master状态" class="headerlink" title="1、查看Web界面Master状态"></a>1、查看Web界面Master状态</h3><p>hadoop1是ALIVE状态，hadoop2、hadoop3和hadoop4均是STANDBY状态</p><p><strong>hadoop1节点</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202404904-1216450970.png" alt="img"></p><p><strong>hadoop2节点</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202505998-2125653978.png" alt="img"></p><p><strong>hadoop3</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202714859-448500440.png" alt="img"></p><p><strong>hadoop4</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202843592-757375998.png" alt="img"></p><h3 id="2、验证HA的高可用"><a href="#2、验证HA的高可用" class="headerlink" title="2、验证HA的高可用"></a>2、验证HA的高可用</h3><p>手动干掉hadoop1上面的Master进程，观察是否会自动进行切换</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420203250642-1623659016.png" alt="img"></p><p>干掉hadoop1上的Master进程之后，再次查看web界面</p><p><strong>hadoo1节点</strong>，由于Master进程被干掉，所以界面无法访问</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420203421981-757224448.png" alt="img"></p><p><strong>hadoop2节点</strong>，Master被干掉之后，hadoop2节点上的Master成功篡位成功，成为ALIVE状态</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420203548607-1978973082.png" alt="img"></p><p><strong>hadoop3节点</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420203751677-1108153453.png" alt="img"></p><p><strong>hadoop4节点</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420203824105-812085082.png" alt="img"></p><h3 id="1、执行第一个Spark程序"><a href="#1、执行第一个Spark程序" class="headerlink" title="1、执行第一个Spark程序"></a>1、执行第一个Spark程序</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ /home/hadoop/apps/spark/bin/spark-submit \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --class org.apache.spark.examples.SparkPi \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --master spark://hadoop1:7077 \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --total-executor-cores 1 \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> /home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 100</span></span><br></pre></td></tr></table></figure><p>其中的spark://hadoop1:7077是下图中的地址</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421115203993-483927862.png" alt="img"></p><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421115300933-1021926480.png" alt="img"></p><h3 id="2、启动spark-shell"><a href="#2、启动spark-shell" class="headerlink" title="2、启动spark shell"></a>2、启动spark shell</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ /home/hadoop/apps/spark/bin/spark-shell \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --master spark://hadoop1:7077 \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --total-executor-cores 1</span></span><br></pre></td></tr></table></figure><p>参数说明：</p><blockquote><p><strong>–master spark://hadoop1:</strong>7077 指定Master的地址</p><p><strong>–executor-memory 500m:</strong>指定每个worker可用内存为500m</p><p><strong>–total-executor-cores 1:</strong> 指定整个集群使用的cup核数为1个</p></blockquote><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421115600861-294197793.png" alt="img"></p><p>注意：</p><p>如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。</p><p>Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可</p><p>Spark Shell中已经默认将SparkSQl类初始化为对象spark。用户代码如果需要用到，则直接应用spark即可</p><h3 id="3、-在spark-shell中编写WordCount程序"><a href="#3、-在spark-shell中编写WordCount程序" class="headerlink" title="3、 在spark shell中编写WordCount程序"></a>3、 在spark shell中编写WordCount程序</h3><p>（1）编写一个hello.txt文件并上传到HDFS上的spark目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ vi hello.txt</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -mkdir -p /spark</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -put hello.txt /spark</span><br></pre></td></tr></table></figure><p>hello.txt的内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">you,jump</span><br><span class="line">i,jump</span><br><span class="line">you,jump</span><br><span class="line">i,jump</span><br><span class="line">jump</span><br></pre></td></tr></table></figure><p>（2）在spark shell中用scala语言编写spark程序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.textFile(&quot;/spark/hello.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;/spark/out&quot;)</span><br></pre></td></tr></table></figure><p>说明：</p><blockquote><p>sc是SparkContext对象，该对象是提交spark程序的入口</p><p>textFile(“/spark/hello.txt”)是hdfs中读取数据</p><p>flatMap(_.split(“ “))先map再压平</p><p>map((_,1))将单词和1构成元组</p><p>reduceByKey(<em>+</em>)按照key进行reduce，并将value累加</p><p>saveAsTextFile(“/spark/out”)将结果写入到hdfs中</p></blockquote><p>（3）使用hdfs命令查看结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~]$ hadoop fs -cat /spark/out/p*</span><br><span class="line">(jump,5)</span><br><span class="line">(you,2)</span><br><span class="line">(i,2)</span><br><span class="line">[hadoop@hadoop2 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421131948389-32143897.png" alt="img"></p><h2 id="七、-执行Spark程序on-YARN"><a href="#七、-执行Spark程序on-YARN" class="headerlink" title="七、 执行Spark程序on YARN"></a>七、 执行Spark程序on YARN</h2><h3 id="1、前提"><a href="#1、前提" class="headerlink" title="1、前提"></a>1、前提</h3><p>成功启动zookeeper集群、HDFS集群、YARN集群</p><h3 id="2、启动Spark-on-YARN"><a href="#2、启动Spark-on-YARN" class="headerlink" title="2、启动Spark on YARN"></a>2、启动Spark on YARN</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 bin]$ spark-shell --master yarn --deploy-mode client</span><br></pre></td></tr></table></figure><p>报错如下：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421172825817-1328970589.png" alt="img"></p><p><strong>报错原因：内存资源给的过小，yarn直接kill掉进程，则报rpc连接失败、ClosedChannelException等错误。</strong></p><p><strong>解决方法：</strong></p><p><strong>先停止YARN服务，然后修改yarn-site.xml，增加如下内容</strong></p><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Whether virtual memory limits will be enforced for containers&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Ratio between virtual memory to physical memory when setting memory limits for containers&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><p>将新的yarn-site.xml文件分发到其他Hadoop节点对应的目录下，最后在重新启动YARN。 </p><p>重新执行以下命令启动spark on yarn</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ spark-shell --master yarn --deploy-mode client</span><br></pre></td></tr></table></figure><p>启动成功</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421173924763-194192730.png" alt="img"></p><h3 id="3、打开YARN的web界面"><a href="#3、打开YARN的web界面" class="headerlink" title="3、打开YARN的web界面"></a>3、打开YARN的web界面</h3><p>打开YARN WEB页面：<a href="http://hadoop4:8088" target="_blank" rel="noopener">http://hadoop4:8088</a><br>可以看到Spark shell应用程序正在运行</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421174140672-405904964.png" alt="img"></p><p> 单击ID号链接，可以看到该应用程序的详细信息</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421174305638-985452065.png" alt="img"></p><p>单击“ApplicationMaster”链接</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421174408279-1529306592.png" alt="img"></p><h3 id="4、运行程序"><a href="#4、运行程序" class="headerlink" title="4、运行程序"></a>4、运行程序</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> array = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">array: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(array)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">res0: <span class="type">Long</span> = <span class="number">5</span>                                                                  </span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421174609129-1153563069.png" alt="img"></p><p>再次查看YARN的web界面</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421174716496-1159210602.png" alt="img"></p><p> 查看executors</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421175002608-557719051.png" alt="img"></p><h3 id="5、执行Spark自带的示例程序PI"><a href="#5、执行Spark自带的示例程序PI" class="headerlink" title="5、执行Spark自带的示例程序PI"></a>5、执行Spark自带的示例程序PI</h3><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --master yarn \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --deploy-mode cluster \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --driver-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-cores 1 \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> /home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 10</span></span><br></pre></td></tr></table></figure><p>执行过程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --master yarn \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --deploy-mode cluster \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --driver-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-cores 1 \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> /home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 10</span></span><br><span class="line">2018-04-21 17:57:32 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2018-04-21 17:57:34 INFO  ConfiguredRMFailoverProxyProvider:100 - Failing over to rm2</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Requesting a new application from cluster with 4 NodeManagers</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Will allocate AM container, with 884 MB memory including 384 MB overhead</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Setting up container launch context for our AM</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Setting up the launch environment for our AM container</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Preparing resources for our AM container</span><br><span class="line">2018-04-21 17:57:36 WARN  Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line">2018-04-21 17:57:39 INFO  Client:54 - Uploading resource file:/tmp/spark-93bd68c9-85de-482e-bbd7-cd2cee60e720/__spark_libs__8262081479435245591.zip -&gt; hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005/__spark_libs__8262081479435245591.zip</span><br><span class="line">2018-04-21 17:57:44 INFO  Client:54 - Uploading resource file:/home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar -&gt; hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005/spark-examples_2.11-2.3.0.jar</span><br><span class="line">2018-04-21 17:57:44 INFO  Client:54 - Uploading resource file:/tmp/spark-93bd68c9-85de-482e-bbd7-cd2cee60e720/__spark_conf__2498510663663992254.zip -&gt; hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005/__spark_conf__.zip</span><br><span class="line">2018-04-21 17:57:44 INFO  SecurityManager:54 - Changing view acls to: hadoop</span><br><span class="line">2018-04-21 17:57:44 INFO  SecurityManager:54 - Changing modify acls to: hadoop</span><br><span class="line">2018-04-21 17:57:44 INFO  SecurityManager:54 - Changing view acls groups to: </span><br><span class="line">2018-04-21 17:57:44 INFO  SecurityManager:54 - Changing modify acls groups to: </span><br><span class="line">2018-04-21 17:57:44 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()</span><br><span class="line">2018-04-21 17:57:44 INFO  Client:54 - Submitting application application_1524303370510_0005 to ResourceManager</span><br><span class="line">2018-04-21 17:57:44 INFO  YarnClientImpl:273 - Submitted application application_1524303370510_0005</span><br><span class="line">2018-04-21 17:57:45 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:45 INFO  Client:54 - </span><br><span class="line">     client token: N/A</span><br><span class="line">     diagnostics: N/A</span><br><span class="line">     ApplicationMaster host: N/A</span><br><span class="line">     ApplicationMaster RPC port: -1</span><br><span class="line">     queue: default</span><br><span class="line">     start time: 1524304664749</span><br><span class="line">     final status: UNDEFINED</span><br><span class="line">     tracking URL: http://hadoop4:8088/proxy/application_1524303370510_0005/</span><br><span class="line">     user: hadoop</span><br><span class="line">2018-04-21 17:57:46 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:47 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:48 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:49 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:50 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:51 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:52 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:53 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:54 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:57:54 INFO  Client:54 - </span><br><span class="line">     client token: N/A</span><br><span class="line">     diagnostics: N/A</span><br><span class="line">     ApplicationMaster host: 192.168.123.104</span><br><span class="line">     ApplicationMaster RPC port: 0</span><br><span class="line">     queue: default</span><br><span class="line">     start time: 1524304664749</span><br><span class="line">     final status: UNDEFINED</span><br><span class="line">     tracking URL: http://hadoop4:8088/proxy/application_1524303370510_0005/</span><br><span class="line">     user: hadoop</span><br><span class="line">2018-04-21 17:57:55 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:57:56 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:57:57 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:57:58 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:57:59 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:00 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:01 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:02 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:03 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:04 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:05 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:06 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:07 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:08 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:09 INFO  Client:54 - Application report for application_1524303370510_0005 (state: FINISHED)</span><br><span class="line">2018-04-21 17:58:09 INFO  Client:54 - </span><br><span class="line">     client token: N/A</span><br><span class="line">     diagnostics: N/A</span><br><span class="line">     ApplicationMaster host: 192.168.123.104</span><br><span class="line">     ApplicationMaster RPC port: 0</span><br><span class="line">     queue: default</span><br><span class="line">     start time: 1524304664749</span><br><span class="line">     final status: SUCCEEDED</span><br><span class="line">     tracking URL: http://hadoop4:8088/proxy/application_1524303370510_0005/</span><br><span class="line">     user: hadoop</span><br><span class="line">2018-04-21 17:58:09 INFO  Client:54 - Deleted staging directory hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005</span><br><span class="line">2018-04-21 17:58:09 INFO  ShutdownHookManager:54 - Shutdown hook called</span><br><span class="line">2018-04-21 17:58:09 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-93bd68c9-85de-482e-bbd7-cd2cee60e720</span><br><span class="line">2018-04-21 17:58:09 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-06de6905-8067-4f1e-a0a0-bc8a51daf535</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习之路 （一）Spark初识</title>
      <link href="/2019-06-01-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%80%EF%BC%89Spark%E5%88%9D%E8%AF%86.html"/>
      <url>/2019-06-01-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%80%EF%BC%89Spark%E5%88%9D%E8%AF%86.html</url>
      
        <content type="html"><![CDATA[<p>** Spark学习之路 （一）Spark初识：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （一）Spark初识</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1官网"><a href="#1-1官网" class="headerlink" title="1.1官网"></a>1.1官网</h2><p>官网地址：<a href="http://spark.apache.org" target="_blank" rel="noopener">http://spark.apache.org</a></p><h3 id="1、什么是Spark"><a href="#1、什么是Spark" class="headerlink" title="1、什么是Spark"></a>1、什么是Spark</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419210341840-1023204495.png" alt="img"></p><p><strong>Apache Spark™</strong>是用于大规模数据处理的统一分析引擎。</p><p>spark是一个实现快速通用的集群计算平台。它是由加州大学伯克利分校AMP实验室 开发的通用内存并行计算框架，用来构建大型的、低延迟的数据分析应用程序。它扩展了广泛使用的MapReduce计算</p><p>模型。高效的支撑更多计算模式，包括交互式查询和流处理。spark的一个主要特点是能够在内存中进行计算，及时依赖磁盘进行复杂的运算，Spark依然比MapReduce更加高效。</p><h3 id="2、为什么要学Spark"><a href="#2、为什么要学Spark" class="headerlink" title="2、为什么要学Spark"></a>2、为什么要学Spark</h3><p><strong>中间结果输出</strong>：基于MapReduce的计算引擎通常会将中间结果输出到磁盘上，进行存储和容错。出于任务管道承接的，考虑，当一些查询翻译到MapReduce任务时，往往会产生多个Stage，而这些串联的Stage又依赖于底层文件系统（如HDFS）来存储每一个Stage的输出结果。</p><p><strong>Spark是MapReduce的替代方案，而且兼容HDFS、Hive，可融入Hadoop的生态系统，以弥补MapReduce的不足。</strong></p><h2 id="二、Spark的四大特性"><a href="#二、Spark的四大特性" class="headerlink" title="二、Spark的四大特性"></a>二、Spark的四大特性</h2><h3 id="1、高效性"><a href="#1、高效性" class="headerlink" title="1、高效性"></a>1、高效性</h3><p>运行速度提高100倍。</p><p>​        Apache Spark使用最先进的DAG调度程序，查询优化程序和物理执行引擎，实现批量和流式数据的高性能。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419210923327-1692975321.png" alt="img"></p><h3 id="2、易用性"><a href="#2、易用性" class="headerlink" title="2、易用性"></a>2、易用性</h3><p>​        Spark提供80多个高级算法，可以轻松构建并行应用程序。您可以 从Scala，Python，R和SQL shell中以<em>交互</em>方式使用它。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419211144845-2020753937.png" alt="img"></p><h3 id="3、通用性"><a href="#3、通用性" class="headerlink" title="3、通用性"></a>3、通用性</h3><p>​        Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419211451611-367631364.png" alt="img"></p><p>Spark组成(BDAS)：全称伯克利数据分析栈，通过大规模集成算法、机器、人之间展现大数据应用的一个平台。也是处理大数据、云计算、通信的技术解决方案。</p><p>它的主要组件有：</p><p><strong>SparkCore</strong>：将分布式数据抽象为弹性分布式数据集（RDD），实现了应用任务调度、RPC、序列化和压缩，并为运行在其上的上层组件提供API。</p><p><strong>SparkSQL</strong>：Spark Sql 是Spark来操作结构化数据的程序包，可以让我使用SQL语句的方式来查询数据，Spark支持 多种数据源，包含Hive表，parquest以及JSON等内容。</p><p><strong>SparkStreaming</strong>： 是Spark提供的实时数据进行流式计算的组件。</p><p><strong>MLlib</strong>：提供常用机器学习算法的实现库。</p><p><strong>GraphX</strong>：提供一个分布式图计算框架，能高效进行图计算。</p><h3 id="4、兼容性"><a href="#4、兼容性" class="headerlink" title="4、兼容性"></a>4、兼容性</h3><p>​        Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419211657058-248260284.png" alt="img"></p><p><a href="https://mesos.apache.org/" target="_blank" rel="noopener">Mesos</a>：Spark可以运行在Mesos里面（Mesos 类似于yarn的一个资源调度框架）</p><p><a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener">standalone</a>：Spark自己可以给自己分配资源（master，worker）</p><p><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">YARN</a>：Spark可以运行在yarn上面</p><p> <a href="https://kubernetes.io/" target="_blank" rel="noopener">Kubernetes</a>：Spark接收 <a href="https://kubernetes.io/" target="_blank" rel="noopener">Kubernetes</a>的资源调度</p><h2 id="三、应用场景"><a href="#三、应用场景" class="headerlink" title="三、应用场景"></a>三、应用场景</h2><p>Yahoo将Spark用在Audience Expansion中的应用，进行点击预测和即席查询等</p><p>淘宝技术团队使用了Spark来解决多次迭代的机器学习算法、高计算复杂度的算法等。应用于内容推荐、社区发现等<br>腾讯大数据精准推荐借助Spark快速迭代的优势，实现了在“数据实时采集、算法实时训练、系统实时预测”的全流程实时并行高维算法，最终成功应用于广点通pCTR投放系统上。<br>优酷土豆将Spark应用于视频推荐(图计算)、广告业务，主要实现机器学习、图计算等迭代计算。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop学习之路</title>
      <link href="/2019-04-22-Sqoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF.html"/>
      <url>/2019-04-22-Sqoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF.html</url>
      
        <content type="html"><![CDATA[<p>** Sqoop学习之路：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Sqoop学习之路 （一）</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>sqoop 是 apache 旗下一款“Hadoop 和关系数据库服务器之间传送数据”的工具。</p><p>核心的功能有两个：</p><p>导入、迁入</p><p>导出、迁出</p><p><strong>导入数据</strong>：MySQL，Oracle 导入数据到 Hadoop 的 HDFS、HIVE、HBASE 等数据存储系统</p><p><strong>导出数据</strong>：从 Hadoop 的文件系统中导出数据到关系数据库 mysql 等 Sqoop 的本质还是一个命令行工具，和 HDFS，Hive 相比，并没有什么高深的理论。</p><p>sqoop：</p><p>工具：本质就是迁移数据， 迁移的方式：就是把sqoop的迁移命令转换成MR程序</p><p>hive</p><p>工具，本质就是执行计算，依赖于HDFS存储数据，把SQL转换成MR程序</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412130640231-449939615.png" alt="img"></p><h2 id="二、工作机制"><a href="#二、工作机制" class="headerlink" title="二、工作机制"></a>二、工作机制</h2><p>将导入或导出命令翻译成 MapReduce 程序来实现 在翻译出的 MapReduce 中主要是对 InputFormat 和 OutputFormat 进行定制</p><h2 id="三、安装"><a href="#三、安装" class="headerlink" title="三、安装"></a>三、安装</h2><h3 id="1、前提概述"><a href="#1、前提概述" class="headerlink" title="1、前提概述"></a>1、前提概述</h3><p>将来sqoop在使用的时候有可能会跟那些系统或者组件打交道？</p><p>HDFS， MapReduce， YARN， ZooKeeper， Hive， HBase， MySQL</p><p><strong>sqoop就是一个工具， 只需要在一个节点上进行安装即可。</strong></p><blockquote><p>补充一点： 如果你的sqoop工具将来要进行hive或者hbase等等的系统和MySQL之间的交互</p><p>你安装的SQOOP软件的节点一定要包含以上你要使用的集群或者软件系统的安装包</p><p>补充一点： 将来要使用的azakban这个软件 除了会调度 hadoop的任务或者hbase或者hive的任务之外， 还会调度sqoop的任务</p><p>azkaban这个软件的安装节点也必须包含以上这些软件系统的客户端/2、</p></blockquote><h3 id="2、软件下载"><a href="#2、软件下载" class="headerlink" title="2、软件下载"></a>2、软件下载</h3><p>下载地址<a href="http://mirrors.hust.edu.cn/apache/" target="_blank" rel="noopener">http://mirrors.hust.edu.cn/apache/</a></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412131040413-312918279.png" alt="img"></p><p><strong>sqoop版本说明</strong></p><p>绝大部分企业所使用的sqoop的版本都是 sqoop1</p><p>sqoop-1.4.6 或者 sqoop-1.4.7 它是 sqoop1</p><p>sqoop-1.99.4—-都是 sqoop2</p><p>此处使用sqoop-1.4.6版本sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz</p><h3 id="3、安装步骤"><a href="#3、安装步骤" class="headerlink" title="3、安装步骤"></a>3、安装步骤</h3><h4 id="（1）上传解压缩安装包到指定目录"><a href="#（1）上传解压缩安装包到指定目录" class="headerlink" title="（1）上传解压缩安装包到指定目录"></a>（1）上传解压缩安装包到指定目录</h4><p>因为之前hive只是安装在hadoop3机器上，所以sqoop也同样安装在hadoop3机器上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ tar -zxvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C apps/</span><br></pre></td></tr></table></figure><h4 id="（2）进入到-conf-文件夹，找到-sqoop-env-template-sh，修改其名称为-sqoop-env-sh-cd-conf"><a href="#（2）进入到-conf-文件夹，找到-sqoop-env-template-sh，修改其名称为-sqoop-env-sh-cd-conf" class="headerlink" title="（2）进入到 conf 文件夹，找到 sqoop-env-template.sh，修改其名称为 sqoop-env.sh cd conf"></a>（2）进入到 conf 文件夹，找到 sqoop-env-template.sh，修改其名称为 sqoop-env.sh cd conf</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop3 apps]$ ls</span><br><span class="line">apache-hive-2.3.3-bin  hadoop-2.7.5  hbase-1.2.6  sqoop-1.4.6.bin__hadoop-2.0.4-alpha  zookeeper-3.4.10</span><br><span class="line">[hadoop@hadoop3 apps]$ mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ sqoop-1.4.6</span><br><span class="line">[hadoop@hadoop3 apps]$ cd sqoop-1.4.6/conf/</span><br><span class="line">[hadoop@hadoop3 conf]$ ls</span><br><span class="line">oraoop-site-template.xml  sqoop-env-template.sh    sqoop-site.xml</span><br><span class="line">sqoop-env-template.cmd    sqoop-site-template.xml</span><br><span class="line">[hadoop@hadoop3 conf]$ mv sqoop-env-template.sh sqoop-env.sh</span><br></pre></td></tr></table></figure><h4 id="（3）修改-sqoop-env-sh"><a href="#（3）修改-sqoop-env-sh" class="headerlink" title="（3）修改 sqoop-env.sh"></a>（3）修改 sqoop-env.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 conf]$ vi sqoop-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_COMMON_HOME=/home/hadoop/apps/hadoop-2.7.5</span><br><span class="line"></span><br><span class="line">#Set path to where hadoop-*-core.jar is available</span><br><span class="line">export HADOOP_MAPRED_HOME=/home/hadoop/apps/hadoop-2.7.5</span><br><span class="line"></span><br><span class="line">#set the path to where bin/hbase is available</span><br><span class="line">export HBASE_HOME=/home/hadoop/apps/hbase-1.2.6</span><br><span class="line"></span><br><span class="line">#Set the path to where bin/hive is available</span><br><span class="line">export HIVE_HOME=/home/hadoop/apps/apache-hive-2.3.3-bin</span><br><span class="line"></span><br><span class="line">#Set the path for where zookeper config dir is</span><br><span class="line">export ZOOCFGDIR=/home/hadoop/apps/zookeeper-3.4.10/conf</span><br></pre></td></tr></table></figure><p>为什么在sqoop-env.sh 文件中会要求分别进行 common和mapreduce的配置呢？？？</p><blockquote><p>在apache的hadoop的安装中；四大组件都是安装在同一个hadoop_home中的</p><p>但是在CDH, HDP中， 这些组件都是可选的。</p><p>在安装hadoop的时候，可以选择性的只安装HDFS或者YARN，</p><p>CDH,HDP在安装hadoop的时候，会把HDFS和MapReduce有可能分别安装在不同的地方。</p></blockquote><h4 id="（4）加入-mysql-驱动包到-sqoop1-4-6-lib-目录下"><a href="#（4）加入-mysql-驱动包到-sqoop1-4-6-lib-目录下" class="headerlink" title="（4）加入 mysql 驱动包到 sqoop1.4.6/lib 目录下"></a>（4）加入 mysql 驱动包到 sqoop1.4.6/lib 目录下</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ cp mysql-connector-java-5.1.40-bin.jar apps/sqoop-1.4.6/lib/</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412133204656-1647317834.png" alt="img"></p><h4 id="（5）配置系统环境变量"><a href="#（5）配置系统环境变量" class="headerlink" title="（5）配置系统环境变量"></a>（5）配置系统环境变量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ vi .bashrc </span><br><span class="line">#Sqoop</span><br><span class="line">export SQOOP_HOME=/home/hadoop/apps/sqoop-1.4.6</span><br><span class="line">export PATH=$PATH:$SQOOP_HOME/bin</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412133442486-246649117.png" alt="img"></p><p>保存退出使其立即生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ source .bashrc</span><br></pre></td></tr></table></figure><h4 id="（6）验证安装是否成功"><a href="#（6）验证安装是否成功" class="headerlink" title="（6）验证安装是否成功"></a>（6）验证安装是否成功</h4><p> <strong>sqoop-version</strong> 或者 <strong>sqoop version</strong></p><p><strong><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412133617239-1494099736.png" alt="img"></strong></p><h2 id="四、Sqoop的基本命令"><a href="#四、Sqoop的基本命令" class="headerlink" title="四、Sqoop的基本命令"></a>四、Sqoop的基本命令</h2><h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><p>首先，我们可以使用 sqoop help 来查看，sqoop 支持哪些命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop help</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">18/04/12 13:37:19 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">usage: sqoop COMMAND [ARGS]</span><br><span class="line"></span><br><span class="line">Available commands:</span><br><span class="line">  codegen            Generate code to interact with database records</span><br><span class="line">  create-hive-table  Import a table definition into Hive</span><br><span class="line">  eval               Evaluate a SQL statement and display the results</span><br><span class="line">  export             Export an HDFS directory to a database table</span><br><span class="line">  help               List available commands</span><br><span class="line">  import             Import a table from a database to HDFS</span><br><span class="line">  import-all-tables  Import tables from a database to HDFS</span><br><span class="line">  import-mainframe   Import datasets from a mainframe server to HDFS</span><br><span class="line">  job                Work with saved jobs</span><br><span class="line">  list-databases     List available databases on a server</span><br><span class="line">  list-tables        List available tables in a database</span><br><span class="line">  merge              Merge results of incremental imports</span><br><span class="line">  metastore          Run a standalone Sqoop metastore</span><br><span class="line">  version            Display version information</span><br><span class="line"></span><br><span class="line">See 'sqoop help COMMAND' for information on a specific command.</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure><p>然后得到这些支持了的命令之后，如果不知道使用方式，可以使用 sqoop command 的方式 来查看某条具体命令的使用方式，比如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop help import</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">18/04/12 13:38:29 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">usage: sqoop import [GENERIC-ARGS] [TOOL-ARGS]</span><br><span class="line"></span><br><span class="line">Common arguments:</span><br><span class="line">   --connect &lt;jdbc-uri&gt;                         Specify JDBC connect</span><br><span class="line">                                                string</span><br><span class="line">   --connection-manager &lt;class-name&gt;            Specify connection manager</span><br><span class="line">                                                class name</span><br><span class="line">   --connection-param-file &lt;properties-file&gt;    Specify connection</span><br><span class="line">                                                parameters file</span><br><span class="line">   --driver &lt;class-name&gt;                        Manually specify JDBC</span><br><span class="line">                                                driver class to use</span><br><span class="line">   --hadoop-home &lt;hdir&gt;                         Override</span><br><span class="line">                                                $HADOOP_MAPRED_HOME_ARG</span><br><span class="line">   --hadoop-mapred-home &lt;dir&gt;                   Override</span><br><span class="line">                                                $HADOOP_MAPRED_HOME_ARG</span><br><span class="line">   --help                                       Print usage instructions</span><br><span class="line">-P                                              Read password from console</span><br><span class="line">   --password &lt;password&gt;                        Set authentication</span><br><span class="line">                                                password</span><br><span class="line">   --password-alias &lt;password-alias&gt;            Credential provider</span><br><span class="line">                                                password alias</span><br><span class="line">   --password-file &lt;password-file&gt;              Set authentication</span><br><span class="line">                                                password file path</span><br><span class="line">   --relaxed-isolation                          Use read-uncommitted</span><br><span class="line">                                                isolation for imports</span><br><span class="line">   --skip-dist-cache                            Skip copying jars to</span><br><span class="line">                                                distributed cache</span><br><span class="line">   --username &lt;username&gt;                        Set authentication</span><br><span class="line">                                                username</span><br><span class="line">   --verbose                                    Print more information</span><br><span class="line">                                                while working</span><br><span class="line"></span><br><span class="line">Import control arguments:</span><br><span class="line">   --append                                                   Imports data</span><br><span class="line">                                                              in append</span><br><span class="line">                                                              mode</span><br><span class="line">   --as-avrodatafile                                          Imports data</span><br><span class="line">                                                              to Avro data</span><br><span class="line">                                                              files</span><br><span class="line">   --as-parquetfile                                           Imports data</span><br><span class="line">                                                              to Parquet</span><br><span class="line">                                                              files</span><br><span class="line">   --as-sequencefile                                          Imports data</span><br><span class="line">                                                              to</span><br><span class="line">                                                              SequenceFile</span><br><span class="line">                                                              s</span><br><span class="line">   --as-textfile                                              Imports data</span><br><span class="line">                                                              as plain</span><br><span class="line">                                                              text</span><br><span class="line">                                                              (default)</span><br><span class="line">   --autoreset-to-one-mapper                                  Reset the</span><br><span class="line">                                                              number of</span><br><span class="line">                                                              mappers to</span><br><span class="line">                                                              one mapper</span><br><span class="line">                                                              if no split</span><br><span class="line">                                                              key</span><br><span class="line">                                                              available</span><br><span class="line">   --boundary-query &lt;statement&gt;                               Set boundary</span><br><span class="line">                                                              query for</span><br><span class="line">                                                              retrieving</span><br><span class="line">                                                              max and min</span><br><span class="line">                                                              value of the</span><br><span class="line">                                                              primary key</span><br><span class="line">   --columns &lt;col,col,col...&gt;                                 Columns to</span><br><span class="line">                                                              import from</span><br><span class="line">                                                              table</span><br><span class="line">   --compression-codec &lt;codec&gt;                                Compression</span><br><span class="line">                                                              codec to use</span><br><span class="line">                                                              for import</span><br><span class="line">   --delete-target-dir                                        Imports data</span><br><span class="line">                                                              in delete</span><br><span class="line">                                                              mode</span><br><span class="line">   --direct                                                   Use direct</span><br><span class="line">                                                              import fast</span><br><span class="line">                                                              path</span><br><span class="line">   --direct-split-size &lt;n&gt;                                    Split the</span><br><span class="line">                                                              input stream</span><br><span class="line">                                                              every 'n'</span><br><span class="line">                                                              bytes when</span><br><span class="line">                                                              importing in</span><br><span class="line">                                                              direct mode</span><br><span class="line">-e,--query &lt;statement&gt;                                        Import</span><br><span class="line">                                                              results of</span><br><span class="line">                                                              SQL</span><br><span class="line">                                                              'statement'</span><br><span class="line">   --fetch-size &lt;n&gt;                                           Set number</span><br><span class="line">                                                              'n' of rows</span><br><span class="line">                                                              to fetch</span><br><span class="line">                                                              from the</span><br><span class="line">                                                              database</span><br><span class="line">                                                              when more</span><br><span class="line">                                                              rows are</span><br><span class="line">                                                              needed</span><br><span class="line">   --inline-lob-limit &lt;n&gt;                                     Set the</span><br><span class="line">                                                              maximum size</span><br><span class="line">                                                              for an</span><br><span class="line">                                                              inline LOB</span><br><span class="line">-m,--num-mappers &lt;n&gt;                                          Use 'n' map</span><br><span class="line">                                                              tasks to</span><br><span class="line">                                                              import in</span><br><span class="line">                                                              parallel</span><br><span class="line">   --mapreduce-job-name &lt;name&gt;                                Set name for</span><br><span class="line">                                                              generated</span><br><span class="line">                                                              mapreduce</span><br><span class="line">                                                              job</span><br><span class="line">   --merge-key &lt;column&gt;                                       Key column</span><br><span class="line">                                                              to use to</span><br><span class="line">                                                              join results</span><br><span class="line">   --split-by &lt;column-name&gt;                                   Column of</span><br><span class="line">                                                              the table</span><br><span class="line">                                                              used to</span><br><span class="line">                                                              split work</span><br><span class="line">                                                              units</span><br><span class="line">   --table &lt;table-name&gt;                                       Table to</span><br><span class="line">                                                              read</span><br><span class="line">   --target-dir &lt;dir&gt;                                         HDFS plain</span><br><span class="line">                                                              table</span><br><span class="line">                                                              destination</span><br><span class="line">   --validate                                                 Validate the</span><br><span class="line">                                                              copy using</span><br><span class="line">                                                              the</span><br><span class="line">                                                              configured</span><br><span class="line">                                                              validator</span><br><span class="line">   --validation-failurehandler &lt;validation-failurehandler&gt;    Fully</span><br><span class="line">                                                              qualified</span><br><span class="line">                                                              class name</span><br><span class="line">                                                              for</span><br><span class="line">                                                              ValidationFa</span><br><span class="line">                                                              ilureHandler</span><br><span class="line">   --validation-threshold &lt;validation-threshold&gt;              Fully</span><br><span class="line">                                                              qualified</span><br><span class="line">                                                              class name</span><br><span class="line">                                                              for</span><br><span class="line">                                                              ValidationTh</span><br><span class="line">                                                              reshold</span><br><span class="line">   --validator &lt;validator&gt;                                    Fully</span><br><span class="line">                                                              qualified</span><br><span class="line">                                                              class name</span><br><span class="line">                                                              for the</span><br><span class="line">                                                              Validator</span><br><span class="line">   --warehouse-dir &lt;dir&gt;                                      HDFS parent</span><br><span class="line">                                                              for table</span><br><span class="line">                                                              destination</span><br><span class="line">   --where &lt;where clause&gt;                                     WHERE clause</span><br><span class="line">                                                              to use</span><br><span class="line">                                                              during</span><br><span class="line">                                                              import</span><br><span class="line">-z,--compress                                                 Enable</span><br><span class="line">                                                              compression</span><br><span class="line"></span><br><span class="line">Incremental import arguments:</span><br><span class="line">   --check-column &lt;column&gt;        Source column to check for incremental</span><br><span class="line">                                  change</span><br><span class="line">   --incremental &lt;import-type&gt;    Define an incremental import of type</span><br><span class="line">                                  'append' or 'lastmodified'</span><br><span class="line">   --last-value &lt;value&gt;           Last imported value in the incremental</span><br><span class="line">                                  check column</span><br><span class="line"></span><br><span class="line">Output line formatting arguments:</span><br><span class="line">   --enclosed-by &lt;char&gt;               Sets a required field enclosing</span><br><span class="line">                                      character</span><br><span class="line">   --escaped-by &lt;char&gt;                Sets the escape character</span><br><span class="line">   --fields-terminated-by &lt;char&gt;      Sets the field separator character</span><br><span class="line">   --lines-terminated-by &lt;char&gt;       Sets the end-of-line character</span><br><span class="line">   --mysql-delimiters                 Uses MySQL's default delimiter set:</span><br><span class="line">                                      fields: ,  lines: \n  escaped-by: \</span><br><span class="line">                                      optionally-enclosed-by: '</span><br><span class="line">   --optionally-enclosed-by &lt;char&gt;    Sets a field enclosing character</span><br><span class="line"></span><br><span class="line">Input parsing arguments:</span><br><span class="line">   --input-enclosed-by &lt;char&gt;               Sets a required field encloser</span><br><span class="line">   --input-escaped-by &lt;char&gt;                Sets the input escape</span><br><span class="line">                                            character</span><br><span class="line">   --input-fields-terminated-by &lt;char&gt;      Sets the input field separator</span><br><span class="line">   --input-lines-terminated-by &lt;char&gt;       Sets the input end-of-line</span><br><span class="line">                                            char</span><br><span class="line">   --input-optionally-enclosed-by &lt;char&gt;    Sets a field enclosing</span><br><span class="line">                                            character</span><br><span class="line"></span><br><span class="line">Hive arguments:</span><br><span class="line">   --create-hive-table                         Fail if the target hive</span><br><span class="line">                                               table exists</span><br><span class="line">   --hive-database &lt;database-name&gt;             Sets the database name to</span><br><span class="line">                                               use when importing to hive</span><br><span class="line">   --hive-delims-replacement &lt;arg&gt;             Replace Hive record \0x01</span><br><span class="line">                                               and row delimiters (\n\r)</span><br><span class="line">                                               from imported string fields</span><br><span class="line">                                               with user-defined string</span><br><span class="line">   --hive-drop-import-delims                   Drop Hive record \0x01 and</span><br><span class="line">                                               row delimiters (\n\r) from</span><br><span class="line">                                               imported string fields</span><br><span class="line">   --hive-home &lt;dir&gt;                           Override $HIVE_HOME</span><br><span class="line">   --hive-import                               Import tables into Hive</span><br><span class="line">                                               (Uses Hive's default</span><br><span class="line">                                               delimiters if none are</span><br><span class="line">                                               set.)</span><br><span class="line">   --hive-overwrite                            Overwrite existing data in</span><br><span class="line">                                               the Hive table</span><br><span class="line">   --hive-partition-key &lt;partition-key&gt;        Sets the partition key to</span><br><span class="line">                                               use when importing to hive</span><br><span class="line">   --hive-partition-value &lt;partition-value&gt;    Sets the partition value to</span><br><span class="line">                                               use when importing to hive</span><br><span class="line">   --hive-table &lt;table-name&gt;                   Sets the table name to use</span><br><span class="line">                                               when importing to hive</span><br><span class="line">   --map-column-hive &lt;arg&gt;                     Override mapping for</span><br><span class="line">                                               specific column to hive</span><br><span class="line">                                               types.</span><br><span class="line"></span><br><span class="line">HBase arguments:</span><br><span class="line">   --column-family &lt;family&gt;    Sets the target column family for the</span><br><span class="line">                               import</span><br><span class="line">   --hbase-bulkload            Enables HBase bulk loading</span><br><span class="line">   --hbase-create-table        If specified, create missing HBase tables</span><br><span class="line">   --hbase-row-key &lt;col&gt;       Specifies which input column to use as the</span><br><span class="line">                               row key</span><br><span class="line">   --hbase-table &lt;table&gt;       Import to &lt;table&gt; in HBase</span><br><span class="line"></span><br><span class="line">HCatalog arguments:</span><br><span class="line">   --hcatalog-database &lt;arg&gt;                        HCatalog database name</span><br><span class="line">   --hcatalog-home &lt;hdir&gt;                           Override $HCAT_HOME</span><br><span class="line">   --hcatalog-partition-keys &lt;partition-key&gt;        Sets the partition</span><br><span class="line">                                                    keys to use when</span><br><span class="line">                                                    importing to hive</span><br><span class="line">   --hcatalog-partition-values &lt;partition-value&gt;    Sets the partition</span><br><span class="line">                                                    values to use when</span><br><span class="line">                                                    importing to hive</span><br><span class="line">   --hcatalog-table &lt;arg&gt;                           HCatalog table name</span><br><span class="line">   --hive-home &lt;dir&gt;                                Override $HIVE_HOME</span><br><span class="line">   --hive-partition-key &lt;partition-key&gt;             Sets the partition key</span><br><span class="line">                                                    to use when importing</span><br><span class="line">                                                    to hive</span><br><span class="line">   --hive-partition-value &lt;partition-value&gt;         Sets the partition</span><br><span class="line">                                                    value to use when</span><br><span class="line">                                                    importing to hive</span><br><span class="line">   --map-column-hive &lt;arg&gt;                          Override mapping for</span><br><span class="line">                                                    specific column to</span><br><span class="line">                                                    hive types.</span><br><span class="line"></span><br><span class="line">HCatalog import specific options:</span><br><span class="line">   --create-hcatalog-table            Create HCatalog before import</span><br><span class="line">   --hcatalog-storage-stanza &lt;arg&gt;    HCatalog storage stanza for table</span><br><span class="line">                                      creation</span><br><span class="line"></span><br><span class="line">Accumulo arguments:</span><br><span class="line">   --accumulo-batch-size &lt;size&gt;          Batch size in bytes</span><br><span class="line">   --accumulo-column-family &lt;family&gt;     Sets the target column family for</span><br><span class="line">                                         the import</span><br><span class="line">   --accumulo-create-table               If specified, create missing</span><br><span class="line">                                         Accumulo tables</span><br><span class="line">   --accumulo-instance &lt;instance&gt;        Accumulo instance name.</span><br><span class="line">   --accumulo-max-latency &lt;latency&gt;      Max write latency in milliseconds</span><br><span class="line">   --accumulo-password &lt;password&gt;        Accumulo password.</span><br><span class="line">   --accumulo-row-key &lt;col&gt;              Specifies which input column to</span><br><span class="line">                                         use as the row key</span><br><span class="line">   --accumulo-table &lt;table&gt;              Import to &lt;table&gt; in Accumulo</span><br><span class="line">   --accumulo-user &lt;user&gt;                Accumulo user name.</span><br><span class="line">   --accumulo-visibility &lt;vis&gt;           Visibility token to be applied to</span><br><span class="line">                                         all rows imported</span><br><span class="line">   --accumulo-zookeepers &lt;zookeepers&gt;    Comma-separated list of</span><br><span class="line">                                         zookeepers (host:port)</span><br><span class="line"></span><br><span class="line">Code generation arguments:</span><br><span class="line">   --bindir &lt;dir&gt;                        Output directory for compiled</span><br><span class="line">                                         objects</span><br><span class="line">   --class-name &lt;name&gt;                   Sets the generated class name.</span><br><span class="line">                                         This overrides --package-name.</span><br><span class="line">                                         When combined with --jar-file,</span><br><span class="line">                                         sets the input class.</span><br><span class="line">   --input-null-non-string &lt;null-str&gt;    Input null non-string</span><br><span class="line">                                         representation</span><br><span class="line">   --input-null-string &lt;null-str&gt;        Input null string representation</span><br><span class="line">   --jar-file &lt;file&gt;                     Disable code generation; use</span><br><span class="line">                                         specified jar</span><br><span class="line">   --map-column-java &lt;arg&gt;               Override mapping for specific</span><br><span class="line">                                         columns to java types</span><br><span class="line">   --null-non-string &lt;null-str&gt;          Null non-string representation</span><br><span class="line">   --null-string &lt;null-str&gt;              Null string representation</span><br><span class="line">   --outdir &lt;dir&gt;                        Output directory for generated</span><br><span class="line">                                         code</span><br><span class="line">   --package-name &lt;name&gt;                 Put auto-generated classes in</span><br><span class="line">                                         this package</span><br><span class="line"></span><br><span class="line">Generic Hadoop command-line arguments:</span><br><span class="line">(must preceed any tool-specific arguments)</span><br><span class="line">Generic options supported are</span><br><span class="line">-conf &lt;configuration file&gt;     specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;            use value for given property</span><br><span class="line">-fs &lt;local|namenode:port&gt;      specify a namenode</span><br><span class="line">-jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager</span><br><span class="line">-files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include in the classpath.</span><br><span class="line">-archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.</span><br><span class="line"></span><br><span class="line">The general command line syntax is</span><br><span class="line">bin/hadoop command [genericOptions] [commandOptions]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">At minimum, you must specify --connect and --table</span><br><span class="line">Arguments to mysqldump and other subprograms may be supplied</span><br><span class="line">after a '--' on the command line.</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><h4 id="列出MySQL数据有哪些数据库"><a href="#列出MySQL数据有哪些数据库" class="headerlink" title="列出MySQL数据有哪些数据库"></a>列出MySQL数据有哪些数据库</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop list-databases \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --connect jdbc:mysql://hadoop1:3306/ \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --username root \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --password root</span></span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">18/04/12 13:43:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">18/04/12 13:43:51 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">18/04/12 13:43:51 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">information_schema</span><br><span class="line">hivedb</span><br><span class="line">mysql</span><br><span class="line">performance_schema</span><br><span class="line">test</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412134538467-1445351095.png" alt="img"></p><h4 id="列出MySQL中的某个数据库有哪些数据表："><a href="#列出MySQL中的某个数据库有哪些数据表：" class="headerlink" title="列出MySQL中的某个数据库有哪些数据表："></a>列出MySQL中的某个数据库有哪些数据表：</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop list-tables **</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --connect jdbc:mysql://hadoop1:3306/mysql **</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --username root **</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --password root**</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop list-tables \</span><br><span class="line">&gt; --connect jdbc:mysql://hadoop1:3306/mysql \</span><br><span class="line">&gt; --username root \</span><br><span class="line">&gt; --password root</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">18/04/12 13:46:21 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">18/04/12 13:46:21 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">18/04/12 13:46:21 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">columns_priv</span><br><span class="line">db</span><br><span class="line">event</span><br><span class="line">func</span><br><span class="line">general_log</span><br><span class="line">help_category</span><br><span class="line">help_keyword</span><br><span class="line">help_relation</span><br><span class="line">help_topic</span><br><span class="line">innodb_index_stats</span><br><span class="line">innodb_table_stats</span><br><span class="line">ndb_binlog_index</span><br><span class="line">plugin</span><br><span class="line">proc</span><br><span class="line">procs_priv</span><br><span class="line">proxies_priv</span><br><span class="line">servers</span><br><span class="line">slave_master_info</span><br><span class="line">slave_relay_log_info</span><br><span class="line">slave_worker_info</span><br><span class="line">slow_log</span><br><span class="line">tables_priv</span><br><span class="line">time_zone</span><br><span class="line">time_zone_leap_second</span><br><span class="line">time_zone_name</span><br><span class="line">time_zone_transition</span><br><span class="line">time_zone_transition_type</span><br><span class="line">user</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure><h4 id="创建一张跟mysql中的help-keyword表一样的hive表hk："><a href="#创建一张跟mysql中的help-keyword表一样的hive表hk：" class="headerlink" title="创建一张跟mysql中的help_keyword表一样的hive表hk："></a>创建一张跟mysql中的help_keyword表一样的hive表hk：</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sqoop create-hive-table \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table help_keyword \</span><br><span class="line">--hive-table hk</span><br></pre></td></tr></table></figure> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop create-hive-table \</span><br><span class="line">&gt; --connect jdbc:mysql://hadoop1:3306/mysql \</span><br><span class="line">&gt; --username root \</span><br><span class="line">&gt; --password root \</span><br><span class="line">&gt; --table help_keyword \</span><br><span class="line">&gt; --hive-table hk</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">18/04/12 13:50:20 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">18/04/12 13:50:20 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">18/04/12 13:50:20 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override</span><br><span class="line">18/04/12 13:50:20 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.</span><br><span class="line">18/04/12 13:50:20 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">18/04/12 13:50:21 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 1</span><br><span class="line">18/04/12 13:50:21 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 1</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">18/04/12 13:50:23 INFO hive.HiveImport: Loading uploaded data into Hive</span><br><span class="line">18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/home/hadoop/apps/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">18/04/12 13:50:34 INFO hive.HiveImport: SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">18/04/12 13:50:36 INFO hive.HiveImport: </span><br><span class="line">18/04/12 13:50:36 INFO hive.HiveImport: Logging initialized using configuration in jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/hive-common-2.3.3.jar!/hive-log4j2.properties Async: true</span><br><span class="line">18/04/12 13:50:50 INFO hive.HiveImport: OK</span><br><span class="line">18/04/12 13:50:50 INFO hive.HiveImport: Time taken: 11.651 seconds</span><br><span class="line">18/04/12 13:50:51 INFO hive.HiveImport: Hive import complete.</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure><h2 id="五、Sqoop的数据导入"><a href="#五、Sqoop的数据导入" class="headerlink" title="五、Sqoop的数据导入"></a>五、Sqoop的数据导入</h2><p>“导入工具”导入单个表从 RDBMS 到 HDFS。表中的每一行被视为 HDFS 的记录。所有记录 都存储为文本文件的文本数据（或者 Avro、sequence 文件等二进制数据） </p><h3 id="1、从RDBMS导入到HDFS中"><a href="#1、从RDBMS导入到HDFS中" class="headerlink" title="1、从RDBMS导入到HDFS中"></a>1、从RDBMS导入到HDFS中</h3><h4 id="语法格式"><a href="#语法格式" class="headerlink" title="语法格式"></a>语法格式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop import (generic-args) (import-args)</span><br></pre></td></tr></table></figure><p>常用参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">--connect &lt;jdbc-uri&gt; jdbc 连接地址</span><br><span class="line">--connection-manager &lt;class-name&gt; 连接管理者</span><br><span class="line">--driver &lt;class-name&gt; 驱动类</span><br><span class="line">--hadoop-mapred-home &lt;dir&gt; $HADOOP_MAPRED_HOME</span><br><span class="line">--help help 信息</span><br><span class="line">-P 从命令行输入密码</span><br><span class="line">--password &lt;password&gt; 密码</span><br><span class="line">--username &lt;username&gt; 账号</span><br><span class="line">--verbose 打印流程信息</span><br><span class="line">--connection-param-file &lt;filename&gt; 可选参数</span><br></pre></td></tr></table></figure><h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h4><p><strong>普通导入：导入mysql库中的help_keyword的数据到HDFS上</strong></p><p><strong>导入的默认路径：/user/hadoop/help_keyword</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sqoop import   \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql   \</span><br><span class="line">--username root  \</span><br><span class="line">--password root   \</span><br><span class="line">--table help_keyword   \</span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop import   \</span><br><span class="line">&gt; --connect jdbc:mysql://hadoop1:3306/mysql   \</span><br><span class="line">&gt; --username root  \</span><br><span class="line">&gt; --password root   \</span><br><span class="line">&gt; --table help_keyword   \</span><br><span class="line">&gt; -m 1</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">18/04/12 13:53:48 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">18/04/12 13:53:48 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">18/04/12 13:53:48 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">18/04/12 13:53:48 INFO tool.CodeGenTool: Beginning code generation</span><br><span class="line">18/04/12 13:53:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 1</span><br><span class="line">18/04/12 13:53:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 1</span><br><span class="line">18/04/12 13:53:49 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /home/hadoop/apps/hadoop-2.7.5</span><br><span class="line">注: /tmp/sqoop-hadoop/compile/979d87b9521d0a09ee6620060a112d60/help_keyword.java使用或覆盖了已过时的 API。</span><br><span class="line">注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。</span><br><span class="line">18/04/12 13:53:51 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/979d87b9521d0a09ee6620060a112d60/help_keyword.jar</span><br><span class="line">18/04/12 13:53:51 WARN manager.MySQLManager: It looks like you are importing from mysql.</span><br><span class="line">18/04/12 13:53:51 WARN manager.MySQLManager: This transfer can be faster! Use the --direct</span><br><span class="line">18/04/12 13:53:51 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.</span><br><span class="line">18/04/12 13:53:51 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)</span><br><span class="line">18/04/12 13:53:51 INFO mapreduce.ImportJobBase: Beginning import of help_keyword</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">18/04/12 13:53:52 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar</span><br><span class="line">18/04/12 13:53:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps</span><br><span class="line">18/04/12 13:53:58 INFO db.DBInputFormat: Using read commited transaction isolation</span><br><span class="line">18/04/12 13:53:58 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">18/04/12 13:53:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1523510178850_0001</span><br><span class="line">18/04/12 13:54:00 INFO impl.YarnClientImpl: Submitted application application_1523510178850_0001</span><br><span class="line">18/04/12 13:54:00 INFO mapreduce.Job: The url to track the job: http://hadoop3:8088/proxy/application_1523510178850_0001/</span><br><span class="line">18/04/12 13:54:00 INFO mapreduce.Job: Running job: job_1523510178850_0001</span><br><span class="line">18/04/12 13:54:17 INFO mapreduce.Job: Job job_1523510178850_0001 running in uber mode : false</span><br><span class="line">18/04/12 13:54:17 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">18/04/12 13:54:33 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">18/04/12 13:54:34 INFO mapreduce.Job: Job job_1523510178850_0001 completed successfully</span><br><span class="line">18/04/12 13:54:35 INFO mapreduce.Job: Counters: 30</span><br><span class="line">    File System Counters</span><br><span class="line">        FILE: Number of bytes read=0</span><br><span class="line">        FILE: Number of bytes written=142965</span><br><span class="line">        FILE: Number of read operations=0</span><br><span class="line">        FILE: Number of large read operations=0</span><br><span class="line">        FILE: Number of write operations=0</span><br><span class="line">        HDFS: Number of bytes read=87</span><br><span class="line">        HDFS: Number of bytes written=8264</span><br><span class="line">        HDFS: Number of read operations=4</span><br><span class="line">        HDFS: Number of large read operations=0</span><br><span class="line">        HDFS: Number of write operations=2</span><br><span class="line">    Job Counters </span><br><span class="line">        Launched map tasks=1</span><br><span class="line">        Other local map tasks=1</span><br><span class="line">        Total time spent by all maps in occupied slots (ms)=12142</span><br><span class="line">        Total time spent by all reduces in occupied slots (ms)=0</span><br><span class="line">        Total time spent by all map tasks (ms)=12142</span><br><span class="line">        Total vcore-milliseconds taken by all map tasks=12142</span><br><span class="line">        Total megabyte-milliseconds taken by all map tasks=12433408</span><br><span class="line">    Map-Reduce Framework</span><br><span class="line">        Map input records=619</span><br><span class="line">        Map output records=619</span><br><span class="line">        Input split bytes=87</span><br><span class="line">        Spilled Records=0</span><br><span class="line">        Failed Shuffles=0</span><br><span class="line">        Merged Map outputs=0</span><br><span class="line">        GC time elapsed (ms)=123</span><br><span class="line">        CPU time spent (ms)=1310</span><br><span class="line">        Physical memory (bytes) snapshot=93212672</span><br><span class="line">        Virtual memory (bytes) snapshot=2068234240</span><br><span class="line">        Total committed heap usage (bytes)=17567744</span><br><span class="line">    File Input Format Counters </span><br><span class="line">        Bytes Read=0</span><br><span class="line">    File Output Format Counters </span><br><span class="line">        Bytes Written=8264</span><br><span class="line">18/04/12 13:54:35 INFO mapreduce.ImportJobBase: Transferred 8.0703 KB in 41.8111 seconds (197.6507 bytes/sec)</span><br><span class="line">18/04/12 13:54:35 INFO mapreduce.ImportJobBase: Retrieved 619 records.</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412135733601-512350541.png" alt="img"></p><p>查看导入的文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop4 ~]$ hadoop fs -cat /user/hadoop/help_keyword/part-m-00000</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412140014258-1069061594.png" alt="img"></p><p><strong>导入： 指定分隔符和导入路径</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop import   \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql   \</span><br><span class="line">--username root  \</span><br><span class="line">--password root   \</span><br><span class="line">--table help_keyword   \</span><br><span class="line">--target-dir /user/hadoop11/my_help_keyword1  \</span><br><span class="line">--fields-terminated-by &apos;\t&apos;  \</span><br><span class="line">-m 2</span><br></pre></td></tr></table></figure><p><strong>导入数据：带where条件</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop import   \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql   \</span><br><span class="line">--username root  \</span><br><span class="line">--password root   \</span><br><span class="line">--where &quot;name=&apos;STRING&apos; &quot; \</span><br><span class="line">--table help_keyword   \</span><br><span class="line">--target-dir /sqoop/hadoop11/myoutport1  \</span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure><p><strong>查询指定列</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import   \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql   \</span><br><span class="line">--username root  \</span><br><span class="line">--password root   \</span><br><span class="line">--columns &quot;name&quot; \</span><br><span class="line">--where &quot;name=&apos;STRING&apos; &quot; \</span><br><span class="line">--table help_keyword  \</span><br><span class="line">--target-dir /sqoop/hadoop11/myoutport22  \</span><br><span class="line">-m 1</span><br><span class="line">selct name from help_keyword where name = &quot;string&quot;</span><br></pre></td></tr></table></figure><p><strong>导入：指定自定义查询SQL</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sqoop import   \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/  \</span><br><span class="line">--username root  \</span><br><span class="line">--password root   \</span><br><span class="line">--target-dir /user/hadoop/myimport33_1  \</span><br><span class="line">--query &apos;select help_keyword_id,name from mysql.help_keyword where $CONDITIONS and name = &quot;STRING&quot;&apos; \</span><br><span class="line">--split-by  help_keyword_id \</span><br><span class="line">--fields-terminated-by &apos;\t&apos;  \</span><br><span class="line">-m 4</span><br></pre></td></tr></table></figure><p>在以上需要按照自定义SQL语句导出数据到HDFS的情况下：<br>1、引号问题，要么外层使用单引号，内层使用双引号，$CONDITIONS的$符号不用转义， 要么外层使用双引号，那么内层使用单引号，然后$CONDITIONS的$符号需要转义<br>2、自定义的SQL语句中必须带有WHERE $CONDITIONS</p><h3 id="2、把MySQL数据库中的表数据导入到Hive中"><a href="#2、把MySQL数据库中的表数据导入到Hive中" class="headerlink" title="2、把MySQL数据库中的表数据导入到Hive中"></a>2、把MySQL数据库中的表数据导入到Hive中</h3><p><strong>Sqoop 导入关系型数据到 hive 的过程是先导入到 hdfs，然后再 load 进入 hive</strong></p><p><strong>普通导入：数据存储在默认的default hive库中，表名就是对应的mysql的表名：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop import   \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql   \</span><br><span class="line">--username root  \</span><br><span class="line">--password root   \</span><br><span class="line">--table help_keyword   \</span><br><span class="line">--hive-import \</span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure><p>导入过程</p><blockquote><p>第一步：导入mysql.help_keyword的数据到hdfs的默认路径<br>第二步：自动仿造mysql.help_keyword去创建一张hive表, 创建在默认的default库中<br>第三步：把临时目录中的数据导入到hive表中</p></blockquote><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412214708733-1227478125.png" alt="img"></p><p>查看数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ hadoop fs -cat /user/hive/warehouse/help_keyword/part-m-00000</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412214921384-1664338223.png" alt="img"></p><p><strong>指定行分隔符和列分隔符，指定hive-import，指定覆盖导入，指定自动创建hive表，指定表名，指定删除中间结果数据目录</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sqoop import  \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql  \</span><br><span class="line">--username root  \</span><br><span class="line">--password root  \</span><br><span class="line">--table help_keyword  \</span><br><span class="line">--fields-terminated-by "\t"  \</span><br><span class="line">--lines-terminated-by "\n"  \</span><br><span class="line">--hive-import  \</span><br><span class="line">--hive-overwrite  \</span><br><span class="line">--create-hive-table  \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--hive-database  mydb_test \</span><br><span class="line">--hive-table new_help_keyword</span><br></pre></td></tr></table></figure><p> 报错原因是hive-import 当前这个导入命令。 sqoop会自动给创建hive的表。 但是不会自动创建不存在的库</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412215258496-1424370804.png" alt="img"></p><p>手动创建mydb_test数据块</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create database mydb_test;</span><br><span class="line">OK</span><br><span class="line">Time taken: 6.147 seconds</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p>之后再执行上面的语句没有报错</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412215621342-788714829.png" alt="img"></p><p>查询一下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from new_help_keyword limit 10;</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412215929162-1575352315.png" alt="img"></p><p>上面的导入语句等价于</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sqoop import  \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://hadoop1:3306/mysql  \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password root  \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t"  \</span></span><br><span class="line"><span class="comment">--lines-terminated-by "\n"  \</span></span><br><span class="line"><span class="comment">--hive-import  \</span></span><br><span class="line"><span class="comment">--hive-overwrite  \</span></span><br><span class="line"><span class="comment">--create-hive-table  \ </span></span><br><span class="line"><span class="comment">--hive-table  mydb_test.new_help_keyword  \</span></span><br><span class="line"><span class="comment">--delete-target-dir</span></span><br></pre></td></tr></table></figure><p><strong>增量导入</strong></p><p>执行增量导入之前，先清空hive数据库中的help_keyword表中的数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">truncate table help_keyword;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import   \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://hadoop1:3306/mysql   \</span></span><br><span class="line"><span class="comment">--username root  \</span></span><br><span class="line"><span class="comment">--password root   \</span></span><br><span class="line"><span class="comment">--table help_keyword  \</span></span><br><span class="line"><span class="comment">--target-dir /user/hadoop/myimport_add  \</span></span><br><span class="line"><span class="comment">--incremental  append  \</span></span><br><span class="line"><span class="comment">--check-column  help_keyword_id \</span></span><br><span class="line"><span class="comment">--last-value 500  \</span></span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure><p>语句执行成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ sqoop import   \</span><br><span class="line">&gt; --connect jdbc:mysql://hadoop1:3306/mysql   \</span><br><span class="line">&gt; --username root  \</span><br><span class="line">&gt; --password root   \</span><br><span class="line">&gt; --table help_keyword  \</span><br><span class="line">&gt; --target-dir /user/hadoop/myimport_add  \</span><br><span class="line">&gt; --incremental  append  \</span><br><span class="line">&gt; --check-column  help_keyword_id \</span><br><span class="line">&gt; --last-value 500  \</span><br><span class="line">&gt; -m 1</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.</span><br><span class="line">Please set $HCAT_HOME to the root of your HCatalog installation.</span><br><span class="line">Warning: /home/hadoop/apps/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ACCUMULO_HOME to the root of your Accumulo installation.</span><br><span class="line">18/04/12 22:01:07 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">18/04/12 22:01:08 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">18/04/12 22:01:08 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">18/04/12 22:01:08 INFO tool.CodeGenTool: Beginning code generation</span><br><span class="line">18/04/12 22:01:08 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 1</span><br><span class="line">18/04/12 22:01:08 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `help_keyword` AS t LIMIT 1</span><br><span class="line">18/04/12 22:01:08 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /home/hadoop/apps/hadoop-2.7.5</span><br><span class="line">注: /tmp/sqoop-hadoop/compile/a51619d1ef8c6e4b112a209326ed9e0f/help_keyword.java使用或覆盖了已过时的 API。</span><br><span class="line">注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。</span><br><span class="line">18/04/12 22:01:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/a51619d1ef8c6e4b112a209326ed9e0f/help_keyword.jar</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">18/04/12 22:01:12 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(`help_keyword_id`) FROM `help_keyword`</span><br><span class="line">18/04/12 22:01:12 INFO tool.ImportTool: Incremental import based on column `help_keyword_id`</span><br><span class="line">18/04/12 22:01:12 INFO tool.ImportTool: Lower bound value: 500</span><br><span class="line">18/04/12 22:01:12 INFO tool.ImportTool: Upper bound value: 618</span><br><span class="line">18/04/12 22:01:12 WARN manager.MySQLManager: It looks like you are importing from mysql.</span><br><span class="line">18/04/12 22:01:12 WARN manager.MySQLManager: This transfer can be faster! Use the --direct</span><br><span class="line">18/04/12 22:01:12 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.</span><br><span class="line">18/04/12 22:01:12 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)</span><br><span class="line">18/04/12 22:01:12 INFO mapreduce.ImportJobBase: Beginning import of help_keyword</span><br><span class="line">18/04/12 22:01:12 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar</span><br><span class="line">18/04/12 22:01:12 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps</span><br><span class="line">18/04/12 22:01:17 INFO db.DBInputFormat: Using read commited transaction isolation</span><br><span class="line">18/04/12 22:01:17 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">18/04/12 22:01:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1523510178850_0010</span><br><span class="line">18/04/12 22:01:19 INFO impl.YarnClientImpl: Submitted application application_1523510178850_0010</span><br><span class="line">18/04/12 22:01:19 INFO mapreduce.Job: The url to track the job: http://hadoop3:8088/proxy/application_1523510178850_0010/</span><br><span class="line">18/04/12 22:01:19 INFO mapreduce.Job: Running job: job_1523510178850_0010</span><br><span class="line">18/04/12 22:01:30 INFO mapreduce.Job: Job job_1523510178850_0010 running in uber mode : false</span><br><span class="line">18/04/12 22:01:30 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">18/04/12 22:01:40 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">18/04/12 22:01:40 INFO mapreduce.Job: Job job_1523510178850_0010 completed successfully</span><br><span class="line">18/04/12 22:01:41 INFO mapreduce.Job: Counters: 30</span><br><span class="line">    File System Counters</span><br><span class="line">        FILE: Number of bytes read=0</span><br><span class="line">        FILE: Number of bytes written=143200</span><br><span class="line">        FILE: Number of read operations=0</span><br><span class="line">        FILE: Number of large read operations=0</span><br><span class="line">        FILE: Number of write operations=0</span><br><span class="line">        HDFS: Number of bytes read=87</span><br><span class="line">        HDFS: Number of bytes written=1576</span><br><span class="line">        HDFS: Number of read operations=4</span><br><span class="line">        HDFS: Number of large read operations=0</span><br><span class="line">        HDFS: Number of write operations=2</span><br><span class="line">    Job Counters </span><br><span class="line">        Launched map tasks=1</span><br><span class="line">        Other local map tasks=1</span><br><span class="line">        Total time spent by all maps in occupied slots (ms)=7188</span><br><span class="line">        Total time spent by all reduces in occupied slots (ms)=0</span><br><span class="line">        Total time spent by all map tasks (ms)=7188</span><br><span class="line">        Total vcore-milliseconds taken by all map tasks=7188</span><br><span class="line">        Total megabyte-milliseconds taken by all map tasks=7360512</span><br><span class="line">    Map-Reduce Framework</span><br><span class="line">        Map input records=118</span><br><span class="line">        Map output records=118</span><br><span class="line">        Input split bytes=87</span><br><span class="line">        Spilled Records=0</span><br><span class="line">        Failed Shuffles=0</span><br><span class="line">        Merged Map outputs=0</span><br><span class="line">        GC time elapsed (ms)=86</span><br><span class="line">        CPU time spent (ms)=870</span><br><span class="line">        Physical memory (bytes) snapshot=95576064</span><br><span class="line">        Virtual memory (bytes) snapshot=2068234240</span><br><span class="line">        Total committed heap usage (bytes)=18608128</span><br><span class="line">    File Input Format Counters </span><br><span class="line">        Bytes Read=0</span><br><span class="line">    File Output Format Counters </span><br><span class="line">        Bytes Written=1576</span><br><span class="line">18/04/12 22:01:41 INFO mapreduce.ImportJobBase: Transferred 1.5391 KB in 28.3008 seconds (55.6875 bytes/sec)</span><br><span class="line">18/04/12 22:01:41 INFO mapreduce.ImportJobBase: Retrieved 118 records.</span><br><span class="line">18/04/12 22:01:41 INFO util.AppendUtils: Creating missing output directory - myimport_add</span><br><span class="line">18/04/12 22:01:41 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:</span><br><span class="line">18/04/12 22:01:41 INFO tool.ImportTool:  --incremental append</span><br><span class="line">18/04/12 22:01:41 INFO tool.ImportTool:   --check-column help_keyword_id</span><br><span class="line">18/04/12 22:01:41 INFO tool.ImportTool:   --last-value 618</span><br><span class="line">18/04/12 22:01:41 INFO tool.ImportTool: (Consider saving this with &apos;sqoop job --create&apos;)</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure><p> 查看结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180412221000267-1226635235.png" alt="img"></p><h3 id="3、把MySQL数据库中的表数据导入到hbase"><a href="#3、把MySQL数据库中的表数据导入到hbase" class="headerlink" title="3、把MySQL数据库中的表数据导入到hbase"></a>3、把MySQL数据库中的表数据导入到hbase</h3><p> 普通导入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop1:3306/mysql \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table help_keyword \</span><br><span class="line">--hbase-table new_help_keyword \</span><br><span class="line">--column-family person \</span><br><span class="line">--hbase-row-key help_keyword_id</span><br></pre></td></tr></table></figure><p>此时会报错，因为需要先创建Hbase里面的表，再执行导入的语句</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; create 'new_help_keyword', 'base_info'</span><br><span class="line">0 row(s) in 3.6280 seconds</span><br><span class="line"></span><br><span class="line">=&gt; Hbase::Table - new_help_keyword</span><br><span class="line">hbase(main):002:0&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala学习之路 （十）Scala的Actor</title>
      <link href="/2019-05-10-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%EF%BC%89Scala%E7%9A%84Actor.html"/>
      <url>/2019-05-10-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%EF%BC%89Scala%E7%9A%84Actor.html</url>
      
        <content type="html"><![CDATA[<p>** Scala学习之路 （十）Scala的Actor：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Scala学习之路 （十）Scala的Actor</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、Scala中的并发编程"><a href="#一、Scala中的并发编程" class="headerlink" title="一、Scala中的并发编程"></a>一、Scala中的并发编程</h2><h3 id="1、Java中的并发编程"><a href="#1、Java中的并发编程" class="headerlink" title="1、Java中的并发编程"></a>1、Java中的并发编程</h3><p>①Java中的并发编程基本上满足了事件之间相互独立，但是事件能够同时发生的场景的需要。 </p><p>②Java中的并发编程是基于共享数据和加锁的一种机制，即会有一个共享的数据，然后有若干个线程去访问这个共享的数据(主要是对这个共享的数据进行修改)，同时Java利用加锁的机制(即synchronized)来确保同一时间只有一个线程对我们的共享数据进行访问,进而保证共享数据的一致性。 </p><p>③Java中的并发编程存在资源争夺和死锁等多种问题，因此程序越大问题越麻烦。 </p><h3 id="2、Scala中的并发编程"><a href="#2、Scala中的并发编程" class="headerlink" title="2、Scala中的并发编程"></a>2、Scala中的并发编程</h3><p>①Scala中的并发编程思想与Java中的并发编程思想完全不一样，Scala中的Actor是一种不共享数据，依赖于消息传递的一种并发编程模式， 避免了死锁、资源争夺等情况。在具体实现的过程中，Scala中的Actor会不断的循环自己的邮箱，并通过receive偏函数进行消息的模式匹配并进行相应的处理。 </p><p>②如果Actor A和 Actor B要相互沟通的话，首先A要给B传递一个消息，B会有一个收件箱，然后B会不断的循环自己的收件箱， 若看见A发过来的消息，B就会解析A的消息并执行，处理完之后就有可能将处理的结果通过邮件的方式发送给A。</p><h2 id="二、Scala中的Actor"><a href="#二、Scala中的Actor" class="headerlink" title="二、Scala中的Actor"></a>二、Scala中的Actor</h2><h3 id="1、什么是Actor"><a href="#1、什么是Actor" class="headerlink" title="1、什么是Actor"></a>1、什么是Actor</h3><p>一个actor是一个容器，它包含 <strong>状态， 行为，信箱，子Actor 和 监管策略</strong>，所有这些包含在一个<strong>ActorReference（Actor引用）</strong>里。一个actor需要与外界隔离才能从actor模型中获益，所以<strong>actor是以actor引用的形式展现给外界的</strong>。</p><h3 id="2、ActorSystem的层次结构"><a href="#2、ActorSystem的层次结构" class="headerlink" title="2、ActorSystem的层次结构"></a>2、ActorSystem的层次结构</h3><p>如果一个Actor中的业务逻辑非常复杂，为了降低代码的复杂度，可以将其拆分成多个子任务（在一个actor的内部可以创建一个或多个actor，actor的创建者也是该actor的监控者） </p><p>一个ActorSystem应该被正确规划，例如哪一个Actor负责监控，监控什么等等：</p><ul><li><ul><li>负责分发的actor管理接受任务的actor</li><li>拥有重要数据的actor，找出所有可能丢失数据的子actor，并且处理他们的错误。</li></ul></li></ul><h3 id="3、ActorPath"><a href="#3、ActorPath" class="headerlink" title="3、ActorPath"></a>3、ActorPath</h3><p>ActorPath是通过字符串描述Actor的层级关系，并唯一标识一个Actor的方法。</p><p>ActorPath包含协议，位置 和 Actor层级关系</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//本地path</span><br><span class="line">&quot;akka://my-sys/user/service-a/worker1&quot;   </span><br><span class="line"></span><br><span class="line">//远程path　　akka.tcp://（ActorSystem的名称）@（远程地址的IP）：（远程地址的端口）/user/（Actor的名称）</span><br><span class="line">&quot;akka.tcp://my-sys@host.example.com:5678/user/service-b&quot; </span><br><span class="line"></span><br><span class="line">//akka集群</span><br><span class="line">&quot;cluster://my-cluster/service-c&quot;</span><br></pre></td></tr></table></figure><p> 远程地址不清楚是多少的话，可以在远程的服务启动的时候查看</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419201539137-507126956.png" alt="img"></p><h3 id="4、获取Actor-Reference"><a href="#4、获取Actor-Reference" class="headerlink" title="4、获取Actor Reference"></a>4、获取Actor Reference</h3><p>获取Actor引用的方式有两种：创建 和 查找。 </p><p>要创建Actor，可以调用ActorSystem.actorOf(..)，它创建的actor在guardian actor之下，接着可以调用ActorContext的actorOf(…) 在刚才创建的Actor内生成一个actor树。这些方法会返回新创建的actor的引用，每一个actor都可以通过访问ActorContext来获得自己（self），子Actor（children，child）和父actor（parent）。</p><p>要查找Actor Reference，可以调用ActorSystem或ActorContext的actorSelection(“path”)，在查找ActorRef时，可以使用相对路径或绝对路径，如果是相对路径，可以用 .. 来表示parent actor。</p><h4 id="actorOf-actorSelection-actorFor的区别"><a href="#actorOf-actorSelection-actorFor的区别" class="headerlink" title="actorOf / actorSelection / actorFor的区别"></a>actorOf / actorSelection / actorFor的区别</h4><blockquote><ul><li>actorOf 创建一个新的actor，创建的actor为调用该方法所属的context的直接子actor。</li><li>actorSelection 查找现有actor，并不会创建新的actor。</li><li>actorFor 查找现有actor，不创建新的actor，已过时。</li></ul></blockquote><h3 id="5、Actor和ActorSystem"><a href="#5、Actor和ActorSystem" class="headerlink" title="5、Actor和ActorSystem"></a>5、Actor和ActorSystem</h3><p>Actor：<br>就是用来做消息传递的<br>用来接收和发送消息的，一个actor就相当于是一个老师或者是学生。<br>如果我们想要多个老师，或者学生，就需要创建多个actor实例。<br>ActorSystem:<br>用来创建和管理actor，并且还需要监控Actor。ActorSystem是单例的（object）<br>在同一个进程里面，只需要一个ActorSystem就可以了</p><h2 id="三、Actor的示例"><a href="#三、Actor的示例" class="headerlink" title="三、Actor的示例"></a>三、Actor的示例</h2><h3 id="1、示例说明"><a href="#1、示例说明" class="headerlink" title="1、示例说明"></a>1、示例说明</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419202008412-1842122010.png" alt="img"></p><h3 id="2、代码实现"><a href="#2、代码实现" class="headerlink" title="2、代码实现"></a>2、代码实现</h3><p>MyResourceManager.scala（服务端）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rpc</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> akka.actor._</span><br><span class="line"><span class="keyword">import</span> com.typesafe.config.&#123;<span class="type">Config</span>, <span class="type">ConfigFactory</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyResourceManager</span>(<span class="params">var resourceManagerHostName:<span class="type">String</span>, var resourceManagerPort:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Actor</span> </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 定义一个Map,接受MyNodeManager的注册信息，key是主机名，</span></span><br><span class="line"><span class="comment">    * value是NodeManagerInfo对象，里面存储主机名、CPU和内存信息</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">  <span class="keyword">var</span> registerMap = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">String</span>,<span class="type">NodeManagerInfo</span>]()</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 定义一个Set,接受MyNodeManager的注册信息，key是主机名，</span></span><br><span class="line"><span class="comment">    * value是NodeManagerInfo对象，里面存储主机名、CPU和内存信息</span></span><br><span class="line"><span class="comment">    * 实际上和上面的Map里面存档内容一样，容易变历，可以不用写，主要是模仿后面Spark里面的内容</span></span><br><span class="line"><span class="comment">    * 方便到时理解Spark源码</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">  <span class="keyword">var</span> registerSet = <span class="keyword">new</span> mutable.<span class="type">HashSet</span>[<span class="type">NodeManagerInfo</span>]()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">preStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">import</span> scala.concurrent.duration._</span><br><span class="line">    <span class="keyword">import</span> context.dispatcher</span><br><span class="line">    context.system.scheduler.schedule(<span class="number">0</span> millis, <span class="number">5000</span> millis, self,<span class="type">CheckTimeOut</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//对MyNodeManager传过来的信息进行匹配</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="comment">//匹配到NodeManager的注册信息进行对应处理</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">NodeManagerRegisterMsg</span>(nodeManagerID,cpu,memory) =&gt; &#123;</span><br><span class="line">      <span class="comment">//将注册信息实例化为一个NodeManagerInfo对象</span></span><br><span class="line">      <span class="keyword">val</span> registerMsg = <span class="keyword">new</span> <span class="type">NodeManagerInfo</span>(nodeManagerID,cpu,memory)</span><br><span class="line">      <span class="comment">//将注册信息存储到registerMap和registerSet里面，key是主机名，value是NodeManagerInfo对象</span></span><br><span class="line">      registerMap.put(nodeManagerID,registerMsg)</span><br><span class="line">      registerSet += registerMsg</span><br><span class="line">      <span class="comment">//注册成功之后，反馈个MyNodeManager一个成功的信息</span></span><br><span class="line">      sender() ! <span class="keyword">new</span> <span class="type">RegisterFeedbackMsg</span>(<span class="string">"注册成功!"</span> + resourceManagerHostName+<span class="string">":"</span>+resourceManagerPort)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//匹配到心跳信息做相应处理</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">HeartBeat</span>(nodeManagerID) =&gt; &#123;</span><br><span class="line">      <span class="comment">//获取当前时间</span></span><br><span class="line">      <span class="keyword">val</span> time:<span class="type">Long</span> = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">      <span class="comment">//根据nodeManagerID获取NodeManagerInfo对象</span></span><br><span class="line">      <span class="keyword">val</span> info = registerMap(nodeManagerID)</span><br><span class="line">      info.lastHeartBeatTime = time</span><br><span class="line">      <span class="comment">//更新registerMap和registerSet里面nodeManagerID对应的NodeManagerInfo对象信息(最后一次心跳时间)</span></span><br><span class="line">      registerMap(nodeManagerID) = info</span><br><span class="line">      registerSet += info</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//检测超时，对超时的数据从集合中删除</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">CheckTimeOut</span> =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> time = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">      registerSet</span><br><span class="line">        .filter( nm =&gt; time - nm.lastHeartBeatTime &gt; <span class="number">10000</span>)</span><br><span class="line">        .foreach(deadnm =&gt; &#123;</span><br><span class="line">          registerSet -= deadnm</span><br><span class="line">          registerMap.remove(deadnm.nodeManagerID)</span><br><span class="line">        &#125;)</span><br><span class="line">      println(<span class="string">"当前注册成功的节点数："</span> + registerMap.size)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyResourceManager</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 传参：</span></span><br><span class="line"><span class="comment">      *   ResourceManager的主机地址、端口号</span></span><br><span class="line"><span class="comment">      * */</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">RM_HOSTNAME</span> = args(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> <span class="type">RM_PORT</span> = args(<span class="number">1</span>).toInt</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> str:<span class="type">String</span> =</span><br><span class="line">      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        |akka.actor.provider = "</span>akka.remote.<span class="type">RemoteActorRefProvider</span><span class="string">"</span></span><br><span class="line"><span class="string">        |akka.remote.netty.tcp.hostname =localhost</span></span><br><span class="line"><span class="string">        |akka.remote.netty.tcp.port=19888</span></span><br><span class="line"><span class="string">      "</span><span class="string">""</span>.stripMargin</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">Config</span> = <span class="type">ConfigFactory</span>.parseString(str)</span><br><span class="line">    <span class="keyword">val</span> actorSystem = <span class="type">ActorSystem</span>(<span class="type">Conf</span>.<span class="type">RMAS</span>,conf)</span><br><span class="line">    actorSystem.actorOf(<span class="type">Props</span>(<span class="keyword">new</span> <span class="type">MyResourceManager</span>(<span class="type">RM_HOSTNAME</span>,<span class="type">RM_PORT</span>)),<span class="type">Conf</span>.<span class="type">RMA</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>MyNodeManager.scala（客户端）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rpc</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">UUID</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> akka.actor._</span><br><span class="line"><span class="keyword">import</span> com.typesafe.config.&#123;<span class="type">Config</span>, <span class="type">ConfigFactory</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyNodeManager</span>(<span class="params">resourceManagerHostName:<span class="type">String</span>,resourceManagerPort:<span class="type">Int</span>,cpu:<span class="type">Int</span>,memory:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Actor</span></span>&#123;</span><br><span class="line">  <span class="comment">//MyNodeManager的UUID</span></span><br><span class="line">  <span class="keyword">var</span> nodeManagerID:<span class="type">String</span> = _</span><br><span class="line">  <span class="keyword">var</span> rmref:<span class="type">ActorSelection</span> = _</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">preStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//获取MyResourceManager的Actor的引用</span></span><br><span class="line">    rmref = context.actorSelection(<span class="string">s"akka.tcp://<span class="subst">$&#123;Conf.RMAS&#125;</span>@<span class="subst">$&#123;resourceManagerHostName&#125;</span>:<span class="subst">$&#123;resourceManagerPort&#125;</span>/user/<span class="subst">$&#123;Conf.RMA&#125;</span>"</span>)</span><br><span class="line">    <span class="comment">//生成随机的UUID</span></span><br><span class="line">    nodeManagerID = <span class="type">UUID</span>.randomUUID().toString</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 向MyResourceManager发送注册信息</span></span><br><span class="line"><span class="comment">      * */</span></span><br><span class="line">    rmref ! <span class="type">NodeManagerRegisterMsg</span>(nodeManagerID,cpu,memory)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//进行信息匹配</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="comment">//匹配到注册成功之后MyResourceManager反馈回的信息，进行相应处理</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">RegisterFeedbackMsg</span>(feedbackMsg) =&gt; &#123;</span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * initialDelay: FiniteDuration, 多久以后开始执行</span></span><br><span class="line"><span class="comment">        * interval:     FiniteDuration, 每隔多长时间执行一次</span></span><br><span class="line"><span class="comment">        * receiver:     ActorRef, 给谁发送这个消息</span></span><br><span class="line"><span class="comment">        * message:      Any  发送的消息是啥</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">      <span class="comment">//定时任务需要导入的工具包</span></span><br><span class="line">      <span class="keyword">import</span> scala.concurrent.duration._</span><br><span class="line">      <span class="keyword">import</span> context.dispatcher</span><br><span class="line">      <span class="comment">//定时向自己发送信息</span></span><br><span class="line">      context.system.scheduler.schedule(<span class="number">0</span> millis, <span class="number">3000</span> millis, self, <span class="type">SendMessage</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//匹配到SendMessage信息之后做相应处理</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">SendMessage</span> =&gt; &#123;</span><br><span class="line">      <span class="comment">//向MyResourceManager发送心跳信息</span></span><br><span class="line">      rmref ! <span class="type">HeartBeat</span>(nodeManagerID)</span><br><span class="line">      println(<span class="type">Thread</span>.currentThread().getId + <span class="string">":"</span> + <span class="type">System</span>.currentTimeMillis())</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyNodeManager</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 传参：</span></span><br><span class="line"><span class="comment">      *   NodeManager的主机地址、端口号、CPU、内存</span></span><br><span class="line"><span class="comment">      *   ResourceManager的主机地址、端口号</span></span><br><span class="line"><span class="comment">      * */</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">NM_HOSTNAME</span> = args(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> <span class="type">NM_PORT</span> = args(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> <span class="type">NM_CPU</span>:<span class="type">Int</span> = args(<span class="number">2</span>).toInt</span><br><span class="line">    <span class="keyword">val</span> <span class="type">NM_MEMORY</span>:<span class="type">Int</span> = args(<span class="number">3</span>).toInt</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> <span class="type">RM_HOSTNAME</span> = args(<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">val</span> <span class="type">RM_PORT</span> = args(<span class="number">5</span>).toInt</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> str:<span class="type">String</span> =</span><br><span class="line">      <span class="string">s""</span><span class="string">"</span></span><br><span class="line"><span class="string">        |akka.actor.provider = "</span>akka.remote.<span class="type">RemoteActorRefProvider</span><span class="string">"</span></span><br><span class="line"><span class="string">        |akka.remote.netty.tcp.hostname = $&#123;NM_HOSTNAME&#125;</span></span><br><span class="line"><span class="string">        |akka.remote.netty.tcp.port = $&#123;NM_PORT&#125;</span></span><br><span class="line"><span class="string">      "</span><span class="string">""</span>.stripMargin</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">Config</span> = <span class="type">ConfigFactory</span>.parseString(str)</span><br><span class="line">    <span class="keyword">val</span> actorSystem = <span class="type">ActorSystem</span>(<span class="type">Conf</span>.<span class="type">NMAS</span>,conf)</span><br><span class="line">    actorSystem.actorOf(<span class="type">Props</span>(<span class="keyword">new</span> <span class="type">MyNodeManager</span>(<span class="type">RM_HOSTNAME</span>,<span class="type">RM_PORT</span>,<span class="type">NM_CPU</span>,<span class="type">NM_MEMORY</span>)),<span class="type">Conf</span>.<span class="type">NMA</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Conf.scala（配置文件）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rpc</span><br><span class="line"></span><br><span class="line"><span class="comment">//避免硬编码</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Conf</span> </span>&#123;</span><br><span class="line">  <span class="comment">//ResourceManagerActorSystem</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">RMAS</span> = <span class="string">"MyRMActorSystem"</span></span><br><span class="line">  <span class="comment">//ResourceManagerActor</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">RMA</span> = <span class="string">"MyRMActor"</span></span><br><span class="line">  <span class="comment">//NodeManagerActorSystem</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">NMAS</span> = <span class="string">"MyNMActorSystem"</span></span><br><span class="line">  <span class="comment">//NodeManagerActor</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">NMA</span> = <span class="string">"MyNMactor"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Message.scala</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rpc</span><br><span class="line"><span class="comment">//NodeManager注册信息</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">NodeManagerRegisterMsg</span>(<span class="params">val nodeManagerID:<span class="type">String</span>, var cpu:<span class="type">Int</span>, var memory:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">//ResourceManager接收到注册信息成功之后的返回信息</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">RegisterFeedbackMsg</span>(<span class="params">val feedbackMsg: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">//NodeManager的心跳信息</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">HeartBeat</span>(<span class="params">val nodeManagerID:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">//NodeManager注册信息</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">NodeManagerInfo</span>(<span class="params">val nodeManagerID:<span class="type">String</span>, var cpu:<span class="type">Int</span>, var memory:<span class="type">Int</span></span>)</span>&#123;</span><br><span class="line">  <span class="comment">//定义一个属性，存储上一次的心跳时间</span></span><br><span class="line">  <span class="keyword">var</span> lastHeartBeatTime:<span class="type">Long</span> = _</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">SendMessage</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">object</span> <span class="title">CheckTimeOut</span></span></span><br></pre></td></tr></table></figure><h3 id="3、运行"><a href="#3、运行" class="headerlink" title="3、运行"></a>3、运行</h3><h4 id="（1）运行MyResourceManager"><a href="#（1）运行MyResourceManager" class="headerlink" title="（1）运行MyResourceManager"></a>（1）运行MyResourceManager</h4><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419202501325-1299088212.png" alt="img"></p><p>发现报错数组越界，原因是在启动时需要传入<strong>2个参数</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419202655884-1641690964.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419203056909-706519667.png" alt="img"></p><p>重新启动，启动成功</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419203128231-1041039527.png" alt="img"></p><h4 id="2、运行MyNodeManager"><a href="#2、运行MyNodeManager" class="headerlink" title="2、运行MyNodeManager"></a>2、运行MyNodeManager</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419203254725-2061083589.png" alt="img"></p><p>报相同的错误，不过此处需要传入6个参数</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419203345875-1492099787.png" alt="img"></p><p>重新启动，启动成功</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419203424400-1724482315.png" alt="img"></p><h4 id="3、观察MyResourceManager"><a href="#3、观察MyResourceManager" class="headerlink" title="3、观察MyResourceManager"></a>3、观察MyResourceManager</h4><p>发现有一个节点连接成功</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419203522147-1867579592.png" alt="img"></p><h4 id="4、再启动一个MyNodeManager观察情况"><a href="#4、再启动一个MyNodeManager观察情况" class="headerlink" title="4、再启动一个MyNodeManager观察情况"></a>4、再启动一个MyNodeManager观察情况</h4><p>先修改MyNodeManager配置里面的端口</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419204356269-1589754138.png" alt="img"></p><p>再启动</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419204438691-19651477.png" alt="img"></p><p>启动成功之后观察MyResourceManager，此时有2个节点连接成功</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419204557625-765575853.png" alt="img"></p><h4 id="5、关闭一个节点，观察情况"><a href="#5、关闭一个节点，观察情况" class="headerlink" title="5、关闭一个节点，观察情况"></a>5、关闭一个节点，观察情况</h4><p>集合中连接超时的成功删除</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419204715553-280254975.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala学习之路 （九）Scala的上界和下届</title>
      <link href="/2019-05-09-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B9%9D%EF%BC%89Scala%E7%9A%84%E4%B8%8A%E7%95%8C%E5%92%8C%E4%B8%8B%E5%B1%8A.html"/>
      <url>/2019-05-09-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B9%9D%EF%BC%89Scala%E7%9A%84%E4%B8%8A%E7%95%8C%E5%92%8C%E4%B8%8B%E5%B1%8A.html</url>
      
        <content type="html"><![CDATA[<p>** Scala学习之路 （九）Scala的上界和下届：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Scala学习之路 （九）Scala的上界和下届</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、泛型"><a href="#一、泛型" class="headerlink" title="一、泛型"></a>一、泛型</h2><h3 id="1、泛型的介绍"><a href="#1、泛型的介绍" class="headerlink" title="1、泛型的介绍"></a>1、泛型的介绍</h3><p>泛型用于指定方法或类可以接受任意类型参数，参数在实际使用时才被确定，泛型可以有效地增强程序的适用性，使用泛型可以使得类或方法具有更强的通用性。泛型的典型应用场景是集合及集合中的方法参数，可以说同java一样，scala中泛型无处不在，具体可以查看scala的api。 </p><h3 id="2、泛型类、泛型方法"><a href="#2、泛型类、泛型方法" class="headerlink" title="2、泛型类、泛型方法"></a>2、泛型类、泛型方法</h3><p><strong>泛型类</strong>：指定类可以接受任意类型参数。 </p><p><strong>泛型方法</strong>：指定方法可以接受任意类型参数。</p><h3 id="3、示例"><a href="#3、示例" class="headerlink" title="3、示例"></a>3、示例</h3><h4 id="1、定义泛型类"><a href="#1、定义泛型类" class="headerlink" title="1、定义泛型类"></a>1、定义泛型类</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 下面的意思就是表示只要是Comparable就可以传递,下面是类上定义的泛型</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GenericTest1</span>[<span class="type">T</span> &lt;: <span class="type">Comparable</span>[<span class="type">T</span>]] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">choose</span></span>(one:<span class="type">T</span>,two:<span class="type">T</span>): <span class="type">T</span> =&#123;</span><br><span class="line">    <span class="comment">//定义一个选择的方法</span></span><br><span class="line">    <span class="keyword">if</span>(one.compareTo(two) &gt; <span class="number">0</span>) one <span class="keyword">else</span> two</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Boy</span>(<span class="params">val name:<span class="type">String</span>,var age:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Comparable</span>[<span class="type">Boy</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compareTo</span></span>(o: <span class="type">Boy</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.age - o.age</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GenericTestOne</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> gt = <span class="keyword">new</span> <span class="type">GenericTest1</span>[<span class="type">Boy</span>]</span><br><span class="line">    <span class="keyword">val</span> huangbo = <span class="keyword">new</span> <span class="type">Boy</span>(<span class="string">"huangbo"</span>,<span class="number">60</span>)</span><br><span class="line">    <span class="keyword">val</span> xuzheng = <span class="keyword">new</span> <span class="type">Boy</span>(<span class="string">"xuzheng"</span>,<span class="number">66</span>)</span><br><span class="line">    <span class="keyword">val</span> boy = gt.choose(huangbo,xuzheng)</span><br><span class="line">    println(boy.name)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418200418474-179801305.png" alt="img"></p><h4 id="2、定义泛型方法"><a href="#2、定义泛型方法" class="headerlink" title="2、定义泛型方法"></a>2、定义泛型方法</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GenericTest2</span></span>&#123;</span><br><span class="line">  <span class="comment">//在方法上定义泛型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">choose</span></span>[<span class="type">T</span> &lt;: <span class="type">Comparable</span>[<span class="type">T</span>]](one:<span class="type">T</span>,two:<span class="type">T</span>): <span class="type">T</span> =&#123;</span><br><span class="line">    <span class="keyword">if</span>(one.compareTo(two) &gt; <span class="number">0</span>) one <span class="keyword">else</span> two</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Boy</span>(<span class="params">val name:<span class="type">String</span>,var age:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Comparable</span>[<span class="type">Boy</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compareTo</span></span>(o: <span class="type">Boy</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.age - o.age</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GenericTestTwo</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> gt = <span class="keyword">new</span> <span class="type">GenericTest2</span></span><br><span class="line">    <span class="keyword">val</span> huangbo = <span class="keyword">new</span> <span class="type">Boy</span>(<span class="string">"huangbo"</span>,<span class="number">60</span>)</span><br><span class="line">    <span class="keyword">val</span> xuzheng = <span class="keyword">new</span> <span class="type">Boy</span>(<span class="string">"xuzheng"</span>,<span class="number">66</span>)</span><br><span class="line">    <span class="keyword">val</span> boy = gt.choose(huangbo,xuzheng)</span><br><span class="line">    println(boy)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418201015731-1539624697.png" alt="img"></p><h2 id="二、上界和下届"><a href="#二、上界和下届" class="headerlink" title="二、上界和下届"></a>二、上界和下届</h2><h3 id="1、介绍"><a href="#1、介绍" class="headerlink" title="1、介绍"></a>1、介绍</h3><p>在指定泛型类型时，有时需要界定泛型类型的范围，而不是接收任意类型。比如，要求某个泛型类型，必须是某个类的子类，这样在程序中就可以放心的调用父类的方法，程序才能正常的使用与运行。此时，就可以使用上下边界Bounds的特性；<br>Scala的上下边界特性允许泛型类型是某个类的子类，或者是某个类的父类；</p><p><strong>(1) U &gt;: T</strong></p><p>这是类型下界的定义，也就是U必须是类型T的父类(或本身，自己也可以认为是自己的父类)。</p><p><strong>(2) S &lt;: T</strong></p><p>这是类型上界的定义，也就是S必须是类型T的子类（或本身，自己也可以认为是自己的子类)。</p><h3 id="2、示例"><a href="#2、示例" class="headerlink" title="2、示例"></a>2、示例</h3><h4 id="（1）上界示例"><a href="#（1）上界示例" class="headerlink" title="（1）上界示例"></a>（1）上界示例</h4><p>参照上面的泛型方法</p><h4 id="（2）下界示例"><a href="#（2）下界示例" class="headerlink" title="（2）下界示例"></a>（2）下界示例</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GranderFather</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Father</span> <span class="keyword">extends</span> <span class="title">GranderFather</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Son</span> <span class="keyword">extends</span> <span class="title">Father</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Tongxue</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">Card</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getIDCard</span></span>[<span class="type">T</span> &gt;: <span class="type">Son</span>](person:<span class="type">T</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">    println(<span class="string">"OK,交给你了"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    getIDCard[<span class="type">GranderFather</span>](<span class="keyword">new</span> <span class="type">Father</span>)</span><br><span class="line">    getIDCard[<span class="type">GranderFather</span>](<span class="keyword">new</span> <span class="type">GranderFather</span>)</span><br><span class="line">    getIDCard[<span class="type">GranderFather</span>](<span class="keyword">new</span> <span class="type">Son</span>)</span><br><span class="line">    <span class="comment">//getIDCard[GranderFather](new Tongxue)//报错，所以注释</span></span><br><span class="line">    </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418202223435-1916688729.png" alt="img"></p><h2 id="三、协变和逆变"><a href="#三、协变和逆变" class="headerlink" title="三、协变和逆变"></a>三、协变和逆变</h2><p>对于一个带类型参数的类型，比如 <code>List[T]：</code></p><p><strong>如果对A及其子类型B，满足 List[B]也符合 List[A]的子类型，那么就称为covariance(协变)；</strong></p><p><strong>如果 List[A]是 List[B]的子类型，即与原来的父子关系正相反，则称为contravariance(逆变)。</strong></p><p>协变</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">　____            　　_____________ </span><br><span class="line">|     |             |             |</span><br><span class="line">|  A  |             |  List[ A ]  |</span><br><span class="line">|_____|             |_____________|</span><br><span class="line">   ^                       ^ </span><br><span class="line">   |                       | </span><br><span class="line"> _____               _____________ </span><br><span class="line">|     |             |             |</span><br><span class="line">|  B  |             |  List[ B ]  |</span><br><span class="line">|_____|             |_____________|</span><br></pre></td></tr></table></figure><p>逆变</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">　____            　　_____________ </span><br><span class="line">|     |             |             |</span><br><span class="line">|  A  |             |  List[ B ]  |</span><br><span class="line">|_____|             |_____________|</span><br><span class="line">   ^                       ^ </span><br><span class="line">   |                       | </span><br><span class="line"> _____               _____________ </span><br><span class="line">|     |             |             |</span><br><span class="line">|  B  |             |  List[ A ]  |</span><br><span class="line">|_____|             |_____________|</span><br></pre></td></tr></table></figure><p>在声明Scala的泛型类型时，“+”表示协变，而“-”表示逆变。</p><blockquote><ul><li>C[+T]：如果A是B的子类，那么C[A]是C[B]的子类。</li><li>C[-T]：如果A是B的子类，那么C[B]是C[A]的子类。</li><li>C[T]：无论A和B是什么关系，C[A]和C[B]没有从属关系。</li></ul></blockquote><p>根据Liskov替换原则，如果A是B的子类，那么能适用于B的所有操作，都适用于A。让我们看看这边Function1的定义，是否满足这样的条件。假设Bird是Animal的子类，那么看看下面两个函数之间是什么关系：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f1</span></span>(x: <span class="type">Bird</span>): <span class="type">Animal</span> <span class="comment">// instance of Function1[Bird, Animal]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f2</span></span>(x: <span class="type">Animal</span>): <span class="type">Bird</span> <span class="comment">// instance of Function1[Animal, Bird]</span></span><br></pre></td></tr></table></figure><p>在这里f2的类型是f1的类型的子类。为什么？</p><p>我们先看一下参数类型，根据Liskov替换原则，f1能够接受的参数，f2也能接受。在这里f1接受的Bird类型，f2显然可以接受，因为Bird对象可以被当做其父类Animal的对象来使用。</p><p>再看返回类型，f1的返回值可以被当做Animal的实例使用，f2的返回值可以被当做Bird的实例使用，当然也可以被当做Animal的实例使用。</p><p>所以我们说，函数的参数类型是逆变的，而函数的返回类型是协变的。</p><p>那么我们在定义Scala类的时候，是不是可以随便指定泛型类型为协变或者逆变呢？答案是否定的。通过上面的例子可以看出，如果将Function1的参数类型定义为协变，或者返回类型定义为逆变，都会违反Liskov替换原则，因此，Scala规定，协变类型只能作为方法的返回类型，而逆变类型只能作为方法的参数类型。类比函数的行为，结合Liskov替换原则，就能发现这样的规定是非常合理的。</p><p><strong>总结：参数是逆变的或者不变的，返回值是协变的或者不变的。</strong></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala学习之路 （八）Scala的隐式转换和隐式参数</title>
      <link href="/2019-05-08-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%85%AB%EF%BC%89Scala%E7%9A%84%E9%9A%90%E5%BC%8F%E8%BD%AC%E6%8D%A2%E5%92%8C%E9%9A%90%E5%BC%8F%E5%8F%82%E6%95%B0.html"/>
      <url>/2019-05-08-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%85%AB%EF%BC%89Scala%E7%9A%84%E9%9A%90%E5%BC%8F%E8%BD%AC%E6%8D%A2%E5%92%8C%E9%9A%90%E5%BC%8F%E5%8F%82%E6%95%B0.html</url>
      
        <content type="html"><![CDATA[<p>** Scala学习之路 （八）Scala的隐式转换和隐式参数：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Scala学习之路 （八）Scala的隐式转换和隐式参数</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、概念"><a href="#一、概念" class="headerlink" title="一、概念"></a>一、概念</h2><p>Scala 2.10引入了一种叫做隐式类的新特性。隐式类指的是用<strong>implicit</strong>关键字修饰的类。在对应的作用域内，带有这个关键字的类的主构造函数可用于隐式转换。</p><p>隐式转换和隐式参数是Scala中两个非常强大的功能，利用隐式转换和隐式参数，你可以提供优雅的类库，对类库的使用者隐匿掉那些枯燥乏味的细节。 </p><h2 id="二、作用"><a href="#二、作用" class="headerlink" title="二、作用"></a>二、作用</h2><p>隐式的对类的方法进行增强，丰富现有类库的功能</p><h2 id="三、隐式参数"><a href="#三、隐式参数" class="headerlink" title="三、隐式参数"></a>三、隐式参数</h2><p>1）关键字：implicat<br>2）隐士的东西只能在object里面才能使用<br>3）作用域</p><h2 id="四、隐式转换函数"><a href="#四、隐式转换函数" class="headerlink" title="四、隐式转换函数"></a>四、隐式转换函数</h2><p>是指那种以implicit关键字声明的带有单个参数的函数。</p><p>可以通过：<strong>:implicit –v</strong>这个命令显示所有做隐式转换的类。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418181811202-1672606236.png" alt="img"></p><h2 id="四、隐士转换的发生的时机"><a href="#四、隐士转换的发生的时机" class="headerlink" title="四、隐士转换的发生的时机"></a>四、隐士转换的发生的时机</h2><h3 id="1、当一个对象去调用某个方法，但是这个对象并不具备这个方法"><a href="#1、当一个对象去调用某个方法，但是这个对象并不具备这个方法" class="headerlink" title="1、当一个对象去调用某个方法，但是这个对象并不具备这个方法"></a>1、当一个对象去调用某个方法，但是这个对象并不具备这个方法</h3><h4 id="（1）scala源码示例"><a href="#（1）scala源码示例" class="headerlink" title="（1）scala源码示例"></a>（1）scala源码示例</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418182007034-441832477.png" alt="img"></p><p>1是Int类型，从to方法看，Int应该有to方法</p><p>打开Int类的源码查看，并没有Int本身并没有to方法，发现Int继承了AnyVal类</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418182449725-2029241513.png" alt="img"></p><p>查看AnyVal类，发现AnyVal类同样没有to方法，而AnyVal类继承了Any类</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418182638386-61166817.png" alt="img"></p><p>Any类里面没有to方法，而在RichInt里面有to方法</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418182844620-1248295484.png" alt="img"></p><p>而在上面查看scala自动导入隐式转换函数时可以看到有Predef类的intWrapper方法，传入的参数是Int，返回的结果类型是RichInt</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418183040866-1563441844.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418183336731-2130776890.png" alt="img"></p><h4 id="（2）快学Scala示例值File和RichFile示例"><a href="#（2）快学Scala示例值File和RichFile示例" class="headerlink" title="（2）快学Scala示例值File和RichFile示例"></a>（2）快学Scala示例值File和RichFile示例</h4><p>不使用隐式转换时，使用装饰模式进行读取</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.io.<span class="type">Source</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RichFile</span>(<span class="params">val file : <span class="type">File</span></span>) </span>&#123;</span><br><span class="line">  <span class="comment">//定义一个read方法，返回String类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>():<span class="type">String</span> = <span class="type">Source</span>.fromFile(file.getPath).mkString</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RichFile</span></span>&#123;</span><br><span class="line">  <span class="comment">//隐式转换方法（将原有的File类型转成了file类型，在用的时候需要导入相应的包）</span></span><br><span class="line">  <span class="comment">//implicit def file2RichFile(file:File) = new RichFile(file)</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MainApp</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> file = <span class="keyword">new</span> <span class="type">File</span>(<span class="string">"D:\\student.txt"</span>)</span><br><span class="line">    <span class="comment">//装饰模式，显示的增强(本来想实现：val contents = file.read()，但是却使用RichFile的方式，所以是显示的增强)</span></span><br><span class="line">    <span class="keyword">val</span> rf = <span class="keyword">new</span> <span class="type">RichFile</span>(file)</span><br><span class="line">    <span class="keyword">val</span> str = rf.read()</span><br><span class="line">    print(str)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418184635294-1094510438.png" alt="img"></p><p>使用隐式转换方式</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.io.<span class="type">Source</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RichFile</span>(<span class="params">val file : <span class="type">File</span></span>) </span>&#123;</span><br><span class="line">  <span class="comment">//定义一个read方法，返回String类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>():<span class="type">String</span> = <span class="type">Source</span>.fromFile(file.getPath).mkString</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RichFile</span></span>&#123;</span><br><span class="line">  <span class="comment">//隐式转换方法（将原有的File类型转成了file类型，在用的时候需要导入相应的包）</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">file2RichFile</span></span>(file:<span class="type">File</span>) = <span class="keyword">new</span> <span class="type">RichFile</span>(file)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MainApp</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//目的是使用File的时候不知不觉的时候直接使用file.read()方法，所以这里就要做隐式转换</span></span><br><span class="line">    <span class="keyword">val</span> file = <span class="keyword">new</span> <span class="type">File</span>(<span class="string">"D:\\student.txt"</span>)</span><br><span class="line">    <span class="comment">//导入隐式转换，._将它下满的所有的方法都导入进去了。</span></span><br><span class="line">    <span class="keyword">import</span> <span class="type">RichFile</span>._</span><br><span class="line">    <span class="comment">//这里没有的read()方法的时候，它就到上面的这一行中的找带有implicit的定义方法</span></span><br><span class="line">    <span class="keyword">val</span> str = file.read()</span><br><span class="line">    <span class="comment">//打印读取的内容</span></span><br><span class="line">    println(str)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418185407742-265441113.png" alt="img"></p><h4 id="（3）超人示例"><a href="#（3）超人示例" class="headerlink" title="（3）超人示例"></a>（3）超人示例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class Man(val name:String)</span><br><span class="line">class SuperMan &#123;</span><br><span class="line">  def fly(): Unit =&#123;</span><br><span class="line">    println(&quot;我要上天&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">object SuperMan&#123;</span><br><span class="line">  //隐式转换，将Man转换为SuperMan</span><br><span class="line">  implicit def man2SuperMan(man:Man)=new SuperMan</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    new Man(&quot;灰太狼&quot;).fly</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418190329173-1982617902.png" alt="img"></p><h3 id="2、调用某个方法的时候，这个方法确实也存在，存入的参数类型不匹配"><a href="#2、调用某个方法的时候，这个方法确实也存在，存入的参数类型不匹配" class="headerlink" title="2、调用某个方法的时候，这个方法确实也存在，存入的参数类型不匹配"></a>2、调用某个方法的时候，这个方法确实也存在，存入的参数类型不匹配</h3><h4 id="售票厅卖票"><a href="#售票厅卖票" class="headerlink" title="售票厅卖票"></a>售票厅卖票</h4><p>老人和小孩是特殊人群，有单独的买票窗口</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//特殊人群（儿童和老人）</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpecialPerson</span>(<span class="params">var name:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">//儿童</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Children</span>(<span class="params">var name:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">//老人</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Older</span>(<span class="params">var name:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">//青年工作者</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Worker</span>(<span class="params">var name:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//特殊人群买票窗口</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">TicketHouse</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">buyTicket</span></span>(p:<span class="type">SpecialPerson</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">    println(p.name + <span class="string">"买到票了"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyPredef</span></span>&#123;</span><br><span class="line">  <span class="comment">//隐式转换，将儿童转换为特殊人群</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">children2SpecialPerson</span></span>(c:<span class="type">Children</span>)=<span class="keyword">new</span> <span class="type">SpecialPerson</span>(c.name)</span><br><span class="line">  <span class="comment">//隐式转换，将老人转换为特殊人群</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">older2SpecialPerson</span></span>(o:<span class="type">Older</span>)=<span class="keyword">new</span> <span class="type">SpecialPerson</span>(o.name)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestBuyTicket</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//导入MyPredef类中的所有隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> <span class="type">MyPredef</span>._</span><br><span class="line">    <span class="keyword">val</span> house = <span class="keyword">new</span> <span class="type">TicketHouse</span></span><br><span class="line">    <span class="comment">//测试儿童买票</span></span><br><span class="line">    <span class="keyword">val</span> children = <span class="keyword">new</span> <span class="type">Children</span>(<span class="string">"wangbaoqiang"</span>)</span><br><span class="line">    house.buyTicket(children)</span><br><span class="line">    <span class="comment">//测试老人买票</span></span><br><span class="line">    <span class="keyword">val</span> older = <span class="keyword">new</span> <span class="type">Older</span>(<span class="string">"xuzheng"</span>)</span><br><span class="line">    house.buyTicket(older)</span><br><span class="line">    <span class="comment">//测试青年工作者买票</span></span><br><span class="line">    <span class="keyword">val</span> worker = <span class="keyword">new</span> <span class="type">Worker</span>(<span class="string">"huangbo"</span>)</span><br><span class="line">    <span class="comment">//house.buyTicket(worker)//放开的话会报错</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418192109454-2078331583.png" alt="img"></p><h3 id="3、视图边界"><a href="#3、视图边界" class="headerlink" title="3、视图边界"></a>3、视图边界</h3><p> 人狗之恋</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name : <span class="type">String</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sayHello</span></span>: <span class="type">Unit</span> =&#123;</span><br><span class="line">    println(<span class="string">"Hello, my name is "</span> + name)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//2个人交朋友</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">mkFridens</span></span>(p:<span class="type">Person</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">    sayHello</span><br><span class="line">    p.sayHello</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">name : <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Person</span>(<span class="params">name</span>)</span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Dog</span>(<span class="params">val name : <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">//聚会时2个人交朋友</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Party</span>[<span class="type">T</span> &lt;% <span class="type">Person</span>](<span class="params">p1:<span class="type">Person</span>,p2:<span class="type">Person</span></span>)</span>&#123;</span><br><span class="line">  p1.mkFridens(p2)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span></span>&#123;</span><br><span class="line">  <span class="comment">//隐式转换，将狗转换成人</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">dog2Person</span></span>(dog:<span class="type">Dog</span>):<span class="type">Person</span>=&#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Person</span>(dog.name)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> huangxiaoming = <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"huangxiaoming"</span>)</span><br><span class="line">    <span class="keyword">val</span> angelababy = <span class="keyword">new</span> <span class="type">Student</span>(<span class="string">"angelababy"</span>)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Party</span>[<span class="type">Person</span>](huangxiaoming,angelababy)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"------------------------------------------------"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> erlangshen = <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"erlangshen"</span>)</span><br><span class="line">    <span class="keyword">val</span> xiaotianquan = <span class="keyword">new</span> <span class="type">Dog</span>(<span class="string">"xiaotianquan"</span>)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Party</span>[<span class="type">Person</span>](erlangshen,xiaotianquan)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418193958192-1529150253.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala学习之路 （七）Scala的柯里化及其应用</title>
      <link href="/2019-05-07-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%83%EF%BC%89Scala%E7%9A%84%E6%9F%AF%E9%87%8C%E5%8C%96%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8.html"/>
      <url>/2019-05-07-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%83%EF%BC%89Scala%E7%9A%84%E6%9F%AF%E9%87%8C%E5%8C%96%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8.html</url>
      
        <content type="html"><![CDATA[<p>** Scala学习之路 （七）Scala的柯里化及其应用：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Scala学习之路 （七）Scala的柯里化及其应用</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、概念"><a href="#一、概念" class="headerlink" title="一、概念"></a>一、概念</h2><p>柯里化(currying, 以逻辑学家Haskell Brooks Curry的名字命名)指的是将原来接受两个参数的函数变成新的接受一个参数的函数的过程。新的函数返回一个以原有第二个参数作为参数的函数。 在Scala中方法和函数有细微的差别，通常编译器会自动完成方法到函数的转换。</p><h2 id="二、Scala中柯里化的形式"><a href="#二、Scala中柯里化的形式" class="headerlink" title="二、Scala中柯里化的形式"></a>二、Scala中柯里化的形式</h2><p>Scala中柯里化方法的定义形式和普通方法类似，区别在于柯里化方法拥有多组参数列表，每组参数用圆括号括起来，例如：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418171633335-1567337108.png" alt="img"></p><p>mysum方法拥有两组参数，分别是(x: Int)和(y: Int)。<br>mysum方法对应的柯里化函数类型是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Int =&gt; Int = &gt;Int</span><br></pre></td></tr></table></figure><p>柯里化函数的类型声明是右结合的，即上面的类型等价于：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Int =&gt; (Int = &gt;Int)</span><br></pre></td></tr></table></figure><p>表明该函数若只接受一个Int参数，则返回一个Int =&gt; Int类型的函数，这也和柯里化的过程相吻合。</p><h2 id="三、示例"><a href="#三、示例" class="headerlink" title="三、示例"></a>三、示例</h2><p>上面的代码定义了一个柯里化方法，在Scala中可以直接操纵函数，但是不能直接操纵方法，所以在使用柯里化方法前，需要将其转换成柯里化函数。最简单的方式是使用编译器提供的语法糖：</p><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418171843650-1259028052.png" alt="img"></p><p>使用Scala中的部分应用函数(partially applied function)技巧也可以实现转换，但是请注意转后后得到的并不是柯里化函数，而是一个接受两个（而不是两组）参数的普通函数</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418172018766-140842735.png" alt="img"></p><p>传入一个参数</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418172124628-622084319.png" alt="img"></p><p>即一个接受一个Int参数返回Int类型的函数。 继续传入第2个参数：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418172210348-230331372.png" alt="img"></p><p>两组参数都已经传入，返回一个Int类型结果</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala学习之路 （六）Scala的类、对象、继承、特质</title>
      <link href="/2019-05-06-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%85%AD%EF%BC%89Scala%E7%9A%84%E7%B1%BB%E3%80%81%E5%AF%B9%E8%B1%A1%E3%80%81%E7%BB%A7%E6%89%BF%E3%80%81%E7%89%B9%E8%B4%A8.html"/>
      <url>/2019-05-06-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%85%AD%EF%BC%89Scala%E7%9A%84%E7%B1%BB%E3%80%81%E5%AF%B9%E8%B1%A1%E3%80%81%E7%BB%A7%E6%89%BF%E3%80%81%E7%89%B9%E8%B4%A8.html</url>
      
        <content type="html"><![CDATA[<p>** Scala学习之路 （六）Scala的类、对象、继承、特质：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Scala学习之路 （六）Scala的类、对象、继承、特质</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、类"><a href="#一、类" class="headerlink" title="一、类"></a>一、类</h2><h3 id="1、类的定义"><a href="#1、类的定义" class="headerlink" title="1、类的定义"></a>1、类的定义</h3><p>scala语言中没有static成员存在，但是scala允许以某种方式去使用static成员<br>这个就是伴生机制，所谓伴生，就是在语言层面上，把static成员和非static成员用不同的表达方式，class和object，<br>但双方具有相同的package和name，但是最终编译器会把他们编译到一起，这是纯粹从语法层面上的约定。通过javap可以反编译看到。<br>另外一个小魔法就是单例，单例本质上是通过伴生机制完成的，直接由编译器生成一个class对象，这样至少在底层能够统一。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//在Scala中，类并不用声明为public。</span></span><br><span class="line"><span class="comment">//Scala源文件中可以包含多个类，所有这些类都具有公有可见性。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassDemo</span> </span>&#123;</span><br><span class="line">  <span class="comment">//用val修饰的变量是只读属性，有getter但没有setter</span></span><br><span class="line">  <span class="comment">//（相当与Java中用final修饰的变量）</span></span><br><span class="line">  <span class="keyword">val</span> id = <span class="number">666</span></span><br><span class="line">  <span class="comment">//用var修饰的变量既有getter又有setter</span></span><br><span class="line">  <span class="keyword">var</span> name = <span class="string">"huangbo"</span></span><br><span class="line">  <span class="comment">//类私有字段,只能在类的内部使用</span></span><br><span class="line">  <span class="keyword">var</span> age = <span class="number">24</span></span><br><span class="line">  <span class="comment">//对象私有字段,访问权限更加严格的，ClassDemo类的方法只能访问到当前对象的字段</span></span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> address = <span class="string">"三里屯"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180417202859571-1591711363.png" alt="img"></p><h3 id="2、构造器"><a href="#2、构造器" class="headerlink" title="2、构造器"></a>2、构造器</h3><p><strong>注意：主构造器会执行类定义中的所有语句</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *每个类都有主构造器，主构造器的参数直接放置类名后面，与类交织在一起</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name:<span class="type">String</span>,val age:<span class="type">Int</span></span>) </span>&#123;</span><br><span class="line">  <span class="comment">//主构造器会执行类定义中的所有语句</span></span><br><span class="line">  println(<span class="string">"Hello Spark"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> x = <span class="number">1</span></span><br><span class="line">  <span class="keyword">if</span>(x &gt; <span class="number">1</span>)&#123;</span><br><span class="line">    println(<span class="string">"666"</span>)</span><br><span class="line">  &#125;<span class="keyword">else</span> <span class="keyword">if</span>(x &lt; <span class="number">1</span>)&#123;</span><br><span class="line">    println(<span class="string">"哈哈。。。"</span>)</span><br><span class="line">  &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">    println(<span class="string">"呵呵。。。"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> address = <span class="string">"BJ"</span></span><br><span class="line">  <span class="comment">//用this关键字定义辅助构造器</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(name:<span class="type">String</span>,age:<span class="type">Int</span>,address:<span class="type">String</span>)&#123;</span><br><span class="line">    <span class="comment">//每个辅助构造器必须以主构造器或其他的辅助构造器的调用开始</span></span><br><span class="line">    <span class="keyword">this</span>(name,age)</span><br><span class="line">    println(<span class="string">"执行辅助构造器"</span>)</span><br><span class="line">    <span class="keyword">this</span>.address = address</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> p = <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"dengchao"</span>,<span class="number">33</span>,<span class="string">"SH"</span>)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180417204349322-1543801547.png" alt="img"></p><p><strong>总结</strong>：</p><p>主构造方法：</p><p>   1）与类名交织在一起</p><p>   2）主构造方法运行，导致类名后面的大括号里面的代码都会运行</p><p>辅助构造方法：</p><p>   1）必须名字叫this</p><p>   2) 必须以调用主构造方法或者是其他辅助构造方法开始。</p><p>   3）里面的属性不能写修饰符</p><h2 id="对象"><a href="#对象" class="headerlink" title="对象"></a>对象</h2><h3 id="单例对象"><a href="#单例对象" class="headerlink" title="单例对象"></a>单例对象</h3><p>在Scala中没有静态方法和静态字段，但是<strong>可以使用object这个语法结构来达到同样的目的</strong></p><p>1.存放工具方法和常量</p><p>2.高效共享单个不可变的实例=单例模式</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SingletonDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> session = <span class="type">SessionFactory</span>.getSession()</span><br><span class="line">    println(session)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Session</span></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SessionFactory</span></span>&#123;</span><br><span class="line">  <span class="comment">//该部分相当于java中的静态块</span></span><br><span class="line">  <span class="keyword">var</span> counts = <span class="number">5</span></span><br><span class="line">  <span class="keyword">val</span> sessions = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Session</span>]()</span><br><span class="line">  <span class="keyword">while</span> (counts &gt; <span class="number">0</span>)&#123;</span><br><span class="line">    sessions += <span class="keyword">new</span> <span class="type">Session</span></span><br><span class="line">    counts -= <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//在object中的方法相当于java中的静态方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getSession</span></span>(): <span class="type">Session</span> =&#123;</span><br><span class="line">    sessions.remove(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180417205413203-1184131098.png" alt="img"></p><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><p>1）object里面的方法都是静态方法</p><p>2）Object里面的字段都是静态字段</p><p>3）它本身就是一个单例，(因为不需要去new)</p><h3 id="伴生对象"><a href="#伴生对象" class="headerlink" title="伴生对象"></a>伴生对象</h3><p>在同一个文件中，在Scala的类中，与类名相同的对象叫做伴生对象，类和伴生对象之间可以相互访问私有的方法和属性</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class Dog &#123;</span><br><span class="line">  val id = 666</span><br><span class="line">  private var name = &quot;道哥&quot;</span><br><span class="line">  def printName(): Unit =&#123;</span><br><span class="line">    //在Dog类中可以访问伴生对象Dog的私有属性</span><br><span class="line">    println(Dog.CONSTANT + name)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * 伴生对象</span><br><span class="line">  */</span><br><span class="line">object Dog&#123;</span><br><span class="line">  //伴生对象中的私有属性</span><br><span class="line">  private var CONSTANT = &quot;汪汪汪。。。&quot;</span><br><span class="line">  //主方法</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val dog = new Dog</span><br><span class="line">    //访问私有的字段name</span><br><span class="line">    dog.name = &quot;道哥666&quot;</span><br><span class="line">    dog.printName()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180417210411310-1209515989.png" alt="img"></p><h4 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a>总结：</h4><p>伴生对象和伴生类可以互相访问私有属性和私有方法。</p><h3 id="apply方法"><a href="#apply方法" class="headerlink" title="apply方法"></a>apply方法</h3><p>通常我们会在类的伴生对象中定义apply方法，当遇到类名(参数1,…参数n)时apply方法会被调用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">object ApplyDemo &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    //调用Array伴生对象的apply方法</span><br><span class="line">    val array = Array(1,2,3,4,5)</span><br><span class="line">    println(array.toBuffer)</span><br><span class="line">    //new了一个长度为9的数组，数组里面包含了9个null</span><br><span class="line">    var arr = new Array(9)</span><br><span class="line">    println(arr)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180417211347588-668807863.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180417211415170-873920824.png" alt="img"></p><h2 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h2><p>Scala中，让子类继承父类，与Java一样，也是使用<strong>extends</strong>关键字<br>继承就代表，子类可以从父类继承父类的field和method；然后子类可以在自己内部放入父类所没有，子类特有的field和method；使用继承可以有效复用代码<br>子类可以覆盖父类的field和method；但是如果父类用final修饰，field和method用final修饰，则该类是无法被继承的，field和method是无法被覆盖的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class People &#123;</span><br><span class="line">  private var name = &quot;始皇帝&quot;</span><br><span class="line">  def getName = name</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Student extends People&#123;</span><br><span class="line">  private var score = 59</span><br><span class="line">  def getScore = score</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">object Test&#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val student = new Student</span><br><span class="line">    println(student.getName)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180417212226725-1907362660.png" alt="img"></p><p>Scala中，<strong>如果子类要覆盖一个父类中的非抽象方法，则必须使用override关键字</strong><br>override关键字可以帮助我们尽早地发现代码里的错误，比如：override修饰的父类方法的方法名我们拼写错了；比如要覆盖的父类方法的参数我们写错了；等等<br>此外，<strong>在子类覆盖父类方法之后，如果我们在子类中就是要调用父类的被覆盖的方法</strong>呢？那就可以使用<strong>super关键字</strong>，显式地指定要调用父类的方法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">People</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> name = <span class="string">"始皇帝"</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getName</span> </span>= name</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span> <span class="keyword">extends</span> <span class="title">People</span></span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> score = <span class="number">59</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getScore</span> </span>= score</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getName</span></span>: <span class="type">String</span> = <span class="keyword">super</span>.getName + <span class="string">":嬴政"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> student = <span class="keyword">new</span> <span class="type">Student</span></span><br><span class="line">    println(student.getName)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180417212436218-478948462.png" alt="img"></p><h3 id="抽象类"><a href="#抽象类" class="headerlink" title="抽象类"></a>抽象类</h3><p>如果在父类中，有某些方法无法立即实现，而需要依赖不同的子来来覆盖，重写实现自己不同的方法实现。此时可以将父类中的这些方法不给出具体的实现，只有方法签名，这种方法就是抽象方法。</p><p>而一个类中如果有一个抽象方法，那么类就必须用abstract来声明为抽象类，此时抽象类是不可以实例化的</p><p><strong>在子类中覆盖抽象类的抽象方法时，不需要使用override**</strong>关键字**</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AbstractDemo</span>(<span class="params">name:<span class="type">String</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sayHello</span></span>:<span class="type">Unit</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StudentDemo</span>(<span class="params">name:<span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">AbstractDemo</span>(<span class="params">name</span>)</span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sayHello</span></span>: <span class="type">Unit</span> = println(<span class="string">"Hello "</span> + name)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StudentDemo</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> li = <span class="keyword">new</span> <span class="type">StudentDemo</span>(<span class="string">"Li"</span>)</span><br><span class="line">    li.sayHello</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180418091555689-911244317.png" alt="img"></p><h3 id="扩展类"><a href="#扩展类" class="headerlink" title="扩展类"></a>扩展类</h3><p>在Scala中扩展类的方式和Java一样都是使用extends关键字</p><h3 id="重写方法"><a href="#重写方法" class="headerlink" title="重写方法"></a>重写方法</h3><p>在Scala中重写一个非抽象的方法必须使用override修饰符</p><h2 id="特质-trait"><a href="#特质-trait" class="headerlink" title="特质(trait)"></a>特质(trait)</h2><h3 id="1将特质作为接口使用"><a href="#1将特质作为接口使用" class="headerlink" title="1将特质作为接口使用"></a>1将特质作为接口使用</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * // Scala中的Triat是一种特殊的概念</span></span><br><span class="line"><span class="comment">// 首先我们可以将Trait作为接口来使用，此时的Triat就与Java中的接口非常类似</span></span><br><span class="line"><span class="comment">// 在triat中可以定义抽象方法，就与抽象类中的抽象方法一样，只要不给出方法的具体实现即可</span></span><br><span class="line"><span class="comment">// 类可以使用extends关键字继承trait，注意，这里不是implement，而是extends，在scala中没有implement的概念，无论继承类还是trait，统一都是extends</span></span><br><span class="line"><span class="comment">// 类继承trait后，必须实现其中的抽象方法，实现时不需要使用override关键字</span></span><br><span class="line"><span class="comment">// scala不支持对类进行多继承，但是支持多重继承trait，使用with关键字即可</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">HelloTrait</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sayHello</span></span>(name:<span class="type">String</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">MakeFriendsTrait</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">makeFriends</span></span>(w:<span class="type">Worker</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Worker</span>(<span class="params">var name:<span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">HelloTrait</span> <span class="keyword">with</span> <span class="title">MakeFriendsTrait</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sayHello</span></span>(name:<span class="type">String</span>) = println(<span class="string">"hello ,"</span>+name)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">makeFriends</span></span>(w:<span class="type">Worker</span>)=println(<span class="string">"hello, my name is"</span>+name+<span class="string">" you name is"</span>+w.name)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> p1=<span class="keyword">new</span> <span class="type">Worker</span>(<span class="string">"xiaoma"</span>);</span><br><span class="line">    <span class="keyword">val</span> p2=<span class="keyword">new</span> <span class="type">Worker</span>(<span class="string">"linghuchong"</span>)</span><br><span class="line">    p1.sayHello(<span class="string">"lihuchong"</span>)</span><br><span class="line">    p1.makeFriends(p2)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2、在trait中定义具体方法"><a href="#2、在trait中定义具体方法" class="headerlink" title="2、在trait中定义具体方法"></a>2、在trait中定义具体方法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> // Scala中的Triat可以不是只定义抽象方法，还可以定义具体方法，此时trait更像是包含了通用工具方法的东西</span></span><br><span class="line"><span class="comment">// 有一个专有的名词来形容这种情况，就是说trait的功能混入了类</span></span><br><span class="line"><span class="comment">// 举例来说，trait中可以包含一些很多类都通用的功能方法，比如打印日志等等，spark中就使用了trait来定义了通用的日志打印方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Logger</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">log</span></span>(message:<span class="type">String</span>) = println(message)</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name:<span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Logger</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">makeFridends</span></span>(p:<span class="type">Person</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">    println(<span class="string">"I'm"</span>+name+<span class="string">" i'm glade to make friends with you"</span>+p.name);</span><br><span class="line">    log(<span class="string">"makeFridends method invoked!!"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> p1=<span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"linpingzhi"</span>)</span><br><span class="line">    <span class="keyword">val</span> p2=<span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"yuelingshan"</span>);</span><br><span class="line">    p1.makeFridends(p2)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3、在trait中定义具体字段"><a href="#3、在trait中定义具体字段" class="headerlink" title="3、在trait中定义具体字段"></a>3、在trait中定义具体字段</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * // Scala中的Triat可以定义具体field，此时继承trait的类就自动获得了trait中定义的field</span></span><br><span class="line"><span class="comment">// 但是这种获取field的方式与继承class是不同的：如果是继承class获取的field，实际是定义在父类中的；</span></span><br><span class="line"><span class="comment">  而继承trait获取的field，就直接被添加到了类中</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> eyeNum:<span class="type">Int</span>=<span class="number">2</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">val name:<span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sayHello</span></span>()=println(<span class="string">"Hi,I'm "</span>+name +<span class="string">"I have "</span>+eyeNum+<span class="string">"eyes !"</span> )</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> s=<span class="keyword">new</span> <span class="type">Student</span>(<span class="string">"zhangsanfeng"</span>)</span><br><span class="line">    s.sayHello();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4、-在trait中定义抽象字段"><a href="#4、-在trait中定义抽象字段" class="headerlink" title="4、 在trait中定义抽象字段"></a>4、 在trait中定义抽象字段</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * // Scala中的Triat可以定义抽象field，而trait中的具体方法则可以基于抽象field来编写</span></span><br><span class="line"><span class="comment">// 但是继承trait的类，则必须覆盖抽象field，提供具体的值</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">sayHello</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> msg:<span class="type">String</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sayHello</span></span>(name:<span class="type">String</span>)=println(msg +<span class="string">" , "</span>+name)</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name:<span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">sayHello</span></span>&#123;</span><br><span class="line">  <span class="keyword">val</span> msg:<span class="type">String</span> = <span class="string">"hello"</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">makeFriends</span></span>(p:<span class="type">Person</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">    sayHello(p.name)</span><br><span class="line">    println(<span class="string">"I'm"</span>+name +<span class="string">" I want to make frieds with you"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> p1=<span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"zhangwuji"</span>)</span><br><span class="line">    <span class="keyword">val</span> p2=<span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"zhangsanfeng"</span>)</span><br><span class="line">    p1.makeFriends(p2)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5、为实例对象混入trait"><a href="#5、为实例对象混入trait" class="headerlink" title="5、为实例对象混入trait"></a>5、为实例对象混入trait</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * // 有时我们可以在创建类的对象时，指定该对象混入某个trait，这样，就只有这个对象混入该trait的方法，而类的其他对象则没有</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Logged</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">log</span></span>(msg:<span class="type">String</span>)&#123;&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">AMyLogger</span> <span class="keyword">extends</span> <span class="title">Logged</span></span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">log</span></span>(msg:<span class="type">String</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">    println(<span class="string">"test:"</span>+msg)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">BMyLogger</span> <span class="keyword">extends</span> <span class="title">Logged</span></span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">log</span></span>(msg:<span class="type">String</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">    println(<span class="string">"log:"</span>+msg)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name:<span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">AMyLogger</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sayHello</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">    println(<span class="string">"Hi ,i'm name"</span>)</span><br><span class="line">    log(<span class="string">"sayHello is invoked!"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span>  <span class="title">Test</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> p1=<span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"liudehua"</span>)</span><br><span class="line">    p1.sayHello() </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> p2=<span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"zhangxueyou"</span>) <span class="keyword">with</span> <span class="type">BMyLogger</span></span><br><span class="line">    p2.sayHello()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="6、trait调用链"><a href="#6、trait调用链" class="headerlink" title="6、trait调用链"></a>6、trait调用链</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * // Scala中支持让类继承多个trait后，依次调用多个trait中的同一个方法，只要让多个trait的同一个方法中，在最后都执行super.方法 即可</span></span><br><span class="line"><span class="comment">// 类中调用多个trait中都有的这个方法时，首先会从最右边的trait的方法开始执行，然后依次往左执行，形成一个调用链条</span></span><br><span class="line"><span class="comment">// 这种特性非常强大，其实就相当于设计模式中的责任链模式的一种具体实现依赖</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Handler</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">handler</span></span>(data:<span class="type">String</span>)&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">DataValidHandler</span> <span class="keyword">extends</span> <span class="title">Handler</span></span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handler</span></span>(data:<span class="type">String</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">    println(<span class="string">"check data:"</span>+data)</span><br><span class="line">    <span class="keyword">super</span>.handler(data)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">SignatureValidHandler</span> <span class="keyword">extends</span> <span class="title">Handler</span></span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handler</span></span>(data:<span class="type">String</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">    println(<span class="string">"check signatrue:"</span>+data)</span><br><span class="line">    <span class="keyword">super</span>.handler(data)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span>  <span class="title">Person</span>(<span class="params">val name:<span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">SignatureValidHandler</span> <span class="keyword">with</span> <span class="title">DataValidHandler</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sayHello=</span></span>&#123;</span><br><span class="line">    println(<span class="string">"Hello "</span>+name)</span><br><span class="line">    handler(name)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> p=<span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"lixiaolong"</span>);</span><br><span class="line">    p.sayHello</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a>模式匹配</h2><p>Scala有一个十分强大的模式匹配机制，可以应用到很多场合：如switch语句、类型检查等。</p><p>并且Scala还提供了样例类，对模式匹配进行了优化，可以快速进行匹配</p><h3 id="1、匹配字符串"><a href="#1、匹配字符串" class="headerlink" title="1、匹配字符串"></a>1、匹配字符串</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CaseDemo01</span> <span class="keyword">extends</span> <span class="title">App</span></span>&#123;</span><br><span class="line">  <span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="string">"Hadoop"</span>, <span class="string">"HBase"</span>, <span class="string">"Spark"</span>)</span><br><span class="line">  <span class="keyword">val</span> name = arr(<span class="type">Random</span>.nextInt(arr.length))</span><br><span class="line">  name <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"Hadoop"</span> =&gt; println(<span class="string">"哈肚普..."</span>)</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"HBase"</span> =&gt; println(<span class="string">"H贝斯..."</span>)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; println(<span class="string">"真不知道你们在说什么..."</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2、匹配类型"><a href="#2、匹配类型" class="headerlink" title="2、匹配类型"></a>2、匹配类型</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CaseDemo01</span> <span class="keyword">extends</span> <span class="title">App</span></span>&#123;</span><br><span class="line">  <span class="comment">//val v = if(x &gt;= 5) 1 else if(x &lt; 2) 2.0 else "hello"</span></span><br><span class="line">  <span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="string">"hello"</span>, <span class="number">1</span>, <span class="number">2.0</span>, <span class="type">CaseDemo01</span>)</span><br><span class="line">  <span class="keyword">val</span> v = arr(<span class="type">Random</span>.nextInt(<span class="number">4</span>))</span><br><span class="line">  println(v)</span><br><span class="line">  v <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> x: <span class="type">Int</span> =&gt; println(<span class="string">"Int "</span> + x)</span><br><span class="line">    <span class="keyword">case</span> y: <span class="type">Double</span> <span class="keyword">if</span>(y &gt;= <span class="number">0</span>) =&gt; println(<span class="string">"Double "</span>+ y)</span><br><span class="line">    <span class="keyword">case</span> z: <span class="type">String</span> =&gt; println(<span class="string">"String "</span> + z)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">"not match exception"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>注意：</strong>case y: Double<strong>if(y &gt;= 0)</strong> =&gt; …</p><p>模式匹配的时候还可以添加守卫条件。如不符合守卫条件，<strong>将掉入case _中</strong></p><h3 id="3、匹配数组、元组、集合"><a href="#3、匹配数组、元组、集合" class="headerlink" title="3、匹配数组、元组、集合"></a>3、匹配数组、元组、集合</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CaseDemo03</span> <span class="keyword">extends</span> <span class="title">App</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">  arr <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Array</span>(<span class="number">1</span>, x, y) =&gt; println(x + <span class="string">" "</span> + y)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Array</span>(<span class="number">0</span>) =&gt; println(<span class="string">"only 0"</span>)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Array</span>(<span class="number">0</span>, _*) =&gt; println(<span class="string">"0 ..."</span>)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; println(<span class="string">"something else"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> lst = <span class="type">List</span>(<span class="number">3</span>, <span class="number">-1</span>)</span><br><span class="line">  lst <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">0</span> :: <span class="type">Nil</span> =&gt; println(<span class="string">"only 0"</span>)</span><br><span class="line">    <span class="keyword">case</span> x :: y :: <span class="type">Nil</span> =&gt; println(<span class="string">s"x: <span class="subst">$x</span> y: <span class="subst">$y</span>"</span>)</span><br><span class="line">    <span class="keyword">case</span> <span class="number">0</span> :: tail =&gt; println(<span class="string">"0 ..."</span>)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; println(<span class="string">"something else"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> tup = (<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">  tup <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> (<span class="number">2</span>, x, y) =&gt; println(<span class="string">s"1, <span class="subst">$x</span> , <span class="subst">$y</span>"</span>)</span><br><span class="line">    <span class="keyword">case</span> (_, z, <span class="number">5</span>) =&gt; println(z)</span><br><span class="line">    <span class="keyword">case</span>  _ =&gt; println(<span class="string">"else"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：在Scala中列表要么为空（Nil表示空列表）要么是一个head元素加上一个tail列表。</p><p>9 :: List(5, 2)  :: 操作符是将给定的头和尾创建一个新的列表</p><p>注意：:: 操作符是右结合的，如9 :: 5 :: 2 :: Nil相当于 9 :: (5 :: (2 :: Nil))</p><h3 id="4、样例类"><a href="#4、样例类" class="headerlink" title="4、样例类"></a>4、样例类</h3><p>在Scala中样例类是一中特殊的类，可用于模式匹配。case class是多例的，后面要跟构造参数，case object是单例的</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">SubmitTask</span>(<span class="params">id: <span class="type">String</span>, name: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">HeartBeat</span>(<span class="params">time: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">object</span> <span class="title">CheckTimeOutTask</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">CaseDemo04</span> <span class="keyword">extends</span> <span class="title">App</span></span>&#123;</span><br><span class="line">  <span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="type">CheckTimeOutTask</span>, <span class="type">HeartBeat</span>(<span class="number">12333</span>), <span class="type">SubmitTask</span>(<span class="string">"0001"</span>, <span class="string">"task-0001"</span>))</span><br><span class="line"></span><br><span class="line">  arr(<span class="type">Random</span>.nextInt(arr.length)) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">SubmitTask</span>(id, name) =&gt; &#123;</span><br><span class="line">      println(<span class="string">s"<span class="subst">$id</span>, <span class="subst">$name</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">HeartBeat</span>(time) =&gt; &#123;</span><br><span class="line">      println(time)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">CheckTimeOutTask</span> =&gt; &#123;</span><br><span class="line">      println(<span class="string">"check"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; 本质上来讲，<span class="class"><span class="keyword">class</span>  <span class="title">case</span> <span class="title">class用起来就是一样的：</span></span></span><br><span class="line"><span class="class"><span class="title">&gt;</span>    <span class="title">最不一样的一个东西：如果我们scala要做模式匹配，去匹配类型的话，建议使用</span></span></span><br><span class="line"><span class="class"><span class="title">&gt;</span> <span class="title">case</span> <span class="title">case</span> <span class="title">因为scala的底层对它做了优化，匹配起来性能较好。</span></span></span><br><span class="line"><span class="class"><span class="title">&gt;</span> <span class="title">*</span> 1</span>:<span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">自动生成伴生对象，自动实现了apply方法</span></span></span><br><span class="line"><span class="class"><span class="title">&gt;</span> <span class="title">*</span> 2</span>:<span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">用于做匹配，性能较好（scala的底层做过优化）</span></span></span><br><span class="line"><span class="class"><span class="title">&gt;</span> <span class="title">*</span> 3<span class="title">：case</span> <span class="title">class</span> <span class="title">默认实现了序列化</span> <span class="title">Serializable</span></span></span><br><span class="line"><span class="class"><span class="title">&gt;</span> <span class="title">*</span> 4</span>: <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">默认实现了toString</span> <span class="title">equals等方法</span></span></span><br><span class="line"><span class="class"><span class="title">&gt;</span> <span class="title">*</span> 5<span class="title">：case</span> <span class="title">class</span> <span class="title">主构造函数</span> <span class="title">里面没有修饰符，默认的是val</span></span></span><br><span class="line"><span class="class"><span class="title">&gt;</span></span></span><br></pre></td></tr></table></figure></blockquote><h2 id="偏函数"><a href="#偏函数" class="headerlink" title="偏函数"></a>偏函数</h2><p>被包在花括号内没有match的一组case语句是一个偏函数，它是PartialFunction[A, B]的一个实例，A代表参数类型，B代表返回类型，常用作输入模式匹配</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PartialFuncDemo</span>  </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">func1</span></span>: <span class="type">PartialFunction</span>[<span class="type">String</span>, <span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"one"</span> =&gt; <span class="number">1</span></span><br><span class="line">    <span class="keyword">case</span> <span class="string">"two"</span> =&gt; <span class="number">2</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="number">-1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">func2</span></span>(num: <span class="type">String</span>) : <span class="type">Int</span> = num <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"one"</span> =&gt; <span class="number">1</span></span><br><span class="line">    <span class="keyword">case</span> <span class="string">"two"</span> =&gt; <span class="number">2</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="number">-1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    println(func1(<span class="string">"one"</span>))</span><br><span class="line">    println(func2(<span class="string">"one"</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>总结：</p><p>偏函数就是用来做模式匹配的。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala学习之路 （五）Scala的关键字Lazy</title>
      <link href="/2019-05-05-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%94%EF%BC%89Scala%E7%9A%84%E5%85%B3%E9%94%AE%E5%AD%97Lazy.html"/>
      <url>/2019-05-05-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%94%EF%BC%89Scala%E7%9A%84%E5%85%B3%E9%94%AE%E5%AD%97Lazy.html</url>
      
        <content type="html"><![CDATA[<p>** Scala学习之路 （五）Scala的关键字Lazy：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Scala学习之路 （五）Scala的关键字Lazy</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>Scala中使用关键字lazy来定义惰性变量，实现延迟加载(懒加载)。<br>惰性变量只能是不可变变量，并且只有在调用惰性变量时，才会去实例化这个变量。</p><p>在Java中，要实现延迟加载(懒加载)，需要自己手动实现。一般的做法是这样的:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">JavaLazyDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">String</span> name;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//初始化姓名为huangbo</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">String</span> initName()&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"huangbo"</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public <span class="type">String</span> getName()&#123;</span><br><span class="line">        <span class="comment">//如果name为空，进行初始化</span></span><br><span class="line">        <span class="keyword">if</span>(name == <span class="literal">null</span>)&#123;</span><br><span class="line">            name = initName();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span>  name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Scala中对延迟加载这一特性提供了语法级别的支持:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lazy val name = initName()</span><br></pre></td></tr></table></figure><p>使用lazy关键字修饰变量后，只有在使用该变量时，才会调用其实例化方法。也就是说在定义property=initProperty()时并不会调用initProperty()方法，只有在后面的代码中使用变量property时才会调用initProperty()方法。</p><p>如果<strong>不使用lazy关键字对变量修饰</strong>，那么变量property是立即实例化的:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaLazyDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">init</span></span>():<span class="type">String</span> = &#123;</span><br><span class="line">    println(<span class="string">"huangbo 666"</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"huangbo"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> name = init();</span><br><span class="line">    println(<span class="string">"666"</span>)</span><br><span class="line">    println(name)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180417200825446-205644124.png" alt="img"></p><p>上面的property没有使用lazy关键字进行修饰，所以property是立即实例化的，调用了initName()方法进行实例化。</p><p><strong>使用Lazy进行修饰</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaLazyDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">init</span></span>():<span class="type">String</span> = &#123;</span><br><span class="line">    println(<span class="string">"huangbo 666"</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"huangbo"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> name = init();</span><br><span class="line">    println(<span class="string">"666"</span>)</span><br><span class="line">    println(name)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180417201020721-1174508623.png" alt="img"></p><p>在声明name时，并没有立即调用实例化方法initName(),而是在使用name时，才会调用实例化方法,并且无论缩少次调用，实例化方法只会执行一次。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala学习之路 （三）Scala的基本使用</title>
      <link href="/2019-05-03-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%89%EF%BC%89Scala%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.html"/>
      <url>/2019-05-03-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%89%EF%BC%89Scala%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.html</url>
      
        <content type="html"><![CDATA[<p>** Scala学习之路 （三）Scala的基本使用：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Scala学习之路 （三）Scala的基本使用</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、Scala概述"><a href="#一、Scala概述" class="headerlink" title="一、Scala概述"></a>一、Scala概述</h2><p>scala是一门多范式编程语言，集成了面向对象编程和函数式编程等多种特性。<br>scala运行在虚拟机上，并兼容现有的Java程序。<br>Scala源代码被编译成java字节码，所以运行在JVM上，并可以调用现有的Java类库。</p><h2 id="二、第一个Scala程序"><a href="#二、第一个Scala程序" class="headerlink" title="二、第一个Scala程序"></a>二、第一个Scala程序</h2><p>Scala语句末尾的分号可写可不写</p><p>HelloSpark.scala</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">object HelloSpark&#123;</span><br><span class="line">    def main(args:Array[String]):Unit = &#123;</span><br><span class="line">        println(&quot;Hello Spark!&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行过程需要先进行编译</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416185905543-159273955.png" alt="img"></p><p>编译之后生成2个文件</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416185943973-2067609845.png" alt="img"></p><p>运行HelloSpark.class</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416190028678-121914205.png" alt="img"></p><p>输出结果Hello Spark</p><h2 id="三、Scala的基本语法"><a href="#三、Scala的基本语法" class="headerlink" title="三、Scala的基本语法"></a>三、Scala的基本语法</h2><h3 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Scala基本语法:</span></span><br><span class="line"><span class="comment">  * 区分大小写</span></span><br><span class="line"><span class="comment">  * 类名首字母大写（MyFirstScalaClass）</span></span><br><span class="line"><span class="comment">  * 方法名称第一个字母小写（myMethodName()）</span></span><br><span class="line"><span class="comment">  * 程序文件名应该与对象名称完全匹配</span></span><br><span class="line"><span class="comment">  * def main(args:Array[String]):scala程序从main方法开始处理，程序的入口。</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * Scala注释：分为多行/**/</span>和单行<span class="comment">//</span></span><br><span class="line">  *</span><br><span class="line">  * 换行符：<span class="type">Scala</span>是面向行的语言，语句可以用分号（；）结束或换行符（println()）</span><br><span class="line">  *</span><br><span class="line">  * 定义包有两种方法：</span><br><span class="line">  *   <span class="number">1</span>、<span class="keyword">package</span> com.ahu</span><br><span class="line">  *      <span class="class"><span class="keyword">class</span> <span class="title">HelloScala</span></span></span><br><span class="line"><span class="class">  <span class="title">*</span>   2<span class="title">、package</span> <span class="title">com</span>.<span class="title">ahu</span></span>&#123;</span><br><span class="line">  *       <span class="class"><span class="keyword">class</span> <span class="title">HelloScala</span></span></span><br><span class="line"><span class="class">  <span class="title">*</span>     &#125;</span></span><br><span class="line"><span class="class">  <span class="title">*</span></span></span><br><span class="line"><span class="class">  <span class="title">*</span> <span class="title">引用：import</span> <span class="title">java</span>.<span class="title">awt</span>.<span class="title">Color</span></span></span><br><span class="line"><span class="class">  <span class="title">*</span> <span class="title">如果想要引入包中的几个成员，可以用selector（选取器）</span></span>:</span><br><span class="line">  *   <span class="keyword">import</span> java.awt.&#123;<span class="type">Color</span>,<span class="type">Font</span>&#125;</span><br><span class="line">  *   <span class="comment">// 重命名成员</span></span><br><span class="line">  *   <span class="keyword">import</span> java.util.&#123;<span class="type">HashMap</span> =&gt; <span class="type">JavaHashMap</span>&#125;</span><br><span class="line">  *   <span class="comment">// 隐藏成员 默认情况下，Scala 总会引入 java.lang._ 、 scala._ 和 Predef._，所以在使用时都是省去scala.的</span></span><br><span class="line">  *   <span class="keyword">import</span> java.util.&#123;<span class="type">HashMap</span> =&gt; _, _&#125; <span class="comment">//引入了util包所有成员，但HashMap被隐藏了</span></span><br><span class="line">  */</span><br></pre></td></tr></table></figure><h3 id="2、Scala的数据类型"><a href="#2、Scala的数据类型" class="headerlink" title="2、Scala的数据类型"></a>2、Scala的数据类型</h3><p>Scala 与 Java有着相同的数据类型，下表列出了 Scala 支持的数据类型：</p><table><thead><tr><th>数据类型</th><th>描述</th></tr></thead><tbody><tr><td>Byte</td><td>8位有符号补码整数。数值区间为 -128 到 127</td></tr><tr><td>Short</td><td>16位有符号补码整数。数值区间为 -32768 到 32767</td></tr><tr><td>Int</td><td>32位有符号补码整数。数值区间为 -2147483648 到 2147483647</td></tr><tr><td>Long</td><td>64位有符号补码整数。数值区间为 -9223372036854775808 到 9223372036854775807</td></tr><tr><td>Float</td><td>32位IEEE754单精度浮点数</td></tr><tr><td>Double</td><td>64位IEEE754单精度浮点数</td></tr><tr><td>Char</td><td>16位无符号Unicode字符, 区间值为 U+0000 到 U+FFFF</td></tr><tr><td>String</td><td>字符序列</td></tr><tr><td>Boolean</td><td>true或false</td></tr><tr><td>Unit</td><td>表示无值，和其他语言中void等同。用作不返回任何结果的方法的结果类型。Unit只有一个实例值，写成()。</td></tr><tr><td>Null</td><td>null 或空引用</td></tr><tr><td>Nothing</td><td>Nothing类型在Scala的类层级的最低端；它是任何其他类型的子类型。</td></tr><tr><td>Any</td><td>Any是所有其他类的超类</td></tr><tr><td>AnyRef</td><td>AnyRef类是Scala里所有引用类(reference class)的基类</td></tr></tbody></table><p> Scala多行字符串的表示方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">var str =</span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">        |第一行</span><br><span class="line">        |第二行</span><br><span class="line">        |第三行</span><br><span class="line">      &quot;&quot;&quot;.stripMargin</span><br><span class="line">    println(str)</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416190629959-647412258.png" alt="img"></p><h3 id="3、Scala的变量"><a href="#3、Scala的变量" class="headerlink" title="3、Scala的变量"></a>3、Scala的变量</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">VariableTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">//使用val定义的变量值是不可变的，相当于java里用final修饰的变量</span></span><br><span class="line">    <span class="keyword">val</span> i = <span class="number">1</span></span><br><span class="line">    <span class="comment">//使用var定义的变量是可变的，在Scala中鼓励使用val</span></span><br><span class="line">    <span class="keyword">var</span> s = <span class="string">"hello"</span></span><br><span class="line">    <span class="comment">//Scala编译器会自动推断变量的类型，必要的时候可以指定类型</span></span><br><span class="line">    <span class="comment">//变量名在前，类型在后</span></span><br><span class="line">    <span class="keyword">val</span> str: <span class="type">String</span> = <span class="string">"world"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>总结：</p><p>1）数据类型可以指定，也可以不指定，如果不指定，那么就会进行数据类型的推断。</p><p>2）如果指定数据类型，数据类型的执行 方式是 在变量名后面写一个冒号，然后写上数据类型。</p><p>3）我们的scala里面变量的修饰符一共有两个，一个是var 一个是val，如果是var修饰的变量，那么这个变量的值是可以修改的。如果是val修饰的变量，那么这个变量的值是不可以修改的。</p><h3 id="4、Scala访问修饰符"><a href="#4、Scala访问修饰符" class="headerlink" title="4、Scala访问修饰符"></a>4、Scala访问修饰符</h3><p>Scala访问修饰符和Java基本一样，分别有private、protected、public。</p><p>默认情况下，Scala对象的访问级别是public。</p><h4 id="（1）私有成员"><a href="#（1）私有成员" class="headerlink" title="（1）私有成员"></a>（1）私有成员</h4><p>用private关键字修饰的成员仅在包含了成员定义的类或对象内部可见。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Outer</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Inner</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() = println(<span class="string">"start"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">end</span></span>() = println(<span class="string">"end"</span>)</span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">pause</span></span>() = println(<span class="string">"pause"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">Inner</span>().start()</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Inner</span>().end()</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Inner</span>().pause()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在上面的代码里start和end两个方法被定义为public类型，可以通过任意Inner实例访问；pause被显示定义为private，这样就不能在Inner类外部访问它。执行这段代码，就会如注释处声明的一样，会在该处报错：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416192036236-1929410080.png" alt="img"></p><h4 id="（2）protected"><a href="#（2）protected" class="headerlink" title="（2）protected"></a>（2）protected</h4><p>和私有成员类似,Scala的访问控制比Java来说也是稍显严格些。在 Scala中,由protected定义的成员只能由定义该成员和其派生类型访问。而在 Java中,由protected定义的成员可以由同一个包中的其它类型访问。在Scala中,可以通过其它方式来实现这种功能。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> p</span><br><span class="line">&#123;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Super</span></span></span><br><span class="line"><span class="class">    </span>&#123;</span><br><span class="line">        <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">f</span></span>()&#123;println(<span class="string">"f"</span>)&#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Sub</span> <span class="keyword">extends</span> <span class="title">Super</span></span></span><br><span class="line"><span class="class">    </span>&#123;</span><br><span class="line">        f() <span class="comment">//OK</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Other</span></span></span><br><span class="line"><span class="class">    </span>&#123;</span><br><span class="line">        (<span class="keyword">new</span> <span class="type">Super</span>).f() <span class="comment">//Error:f不可访问</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（3）public"><a href="#（3）public" class="headerlink" title="（3）public"></a>（3）public</h4><p>public访问控制为Scala定义的缺省方式,所有没有使用private和 protected修饰的成员(定义的类和方法)都是“公开的”,这样的成员可以在任何地方被访问。Scala不需要使用public来指定“公开访问”修饰符。</p><p>注意：Scala中定义的类和方法默认都是public的，但在类中声明的属性默认是private的。</p><h4 id="（4）作用保护域"><a href="#（4）作用保护域" class="headerlink" title="（4）作用保护域"></a>（4）作用保护域</h4><p>作用域保护：Scala中，访问修饰符可以通过使用限定词强调。<br>private[x] 或者 protected[x]<br>private[x]：这个成员除了对[…]中的类或[…]中的包中的类及他们的伴生对象可见外，对其他的类都是private。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bobsrockets</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">package</span> navigation</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//如果为private class Navigator,则类Navigator只会对当前包navigation中所有类型可见。</span></span><br><span class="line">        <span class="comment">//即private默认省略了[X],X为当前包或者当前类或者当前单例对象。</span></span><br><span class="line">        <span class="comment">//private[bobsrockets]则表示将类Navigator从当前包扩展到对bobsrockets包中的所有类型可见。</span></span><br><span class="line">        <span class="keyword">private</span>[bobsrockets] <span class="class"><span class="keyword">class</span> <span class="title">Navigator</span></span></span><br><span class="line"><span class="class">        </span>&#123;</span><br><span class="line">            <span class="keyword">protected</span>[navigation] <span class="function"><span class="keyword">def</span> <span class="title">useStarChart</span></span>() &#123;&#125;</span><br><span class="line">            <span class="class"><span class="keyword">class</span> <span class="title">LegOfJourney</span></span></span><br><span class="line"><span class="class">            </span>&#123;</span><br><span class="line">                <span class="keyword">private</span>[<span class="type">Navigator</span>] <span class="keyword">val</span> distance = <span class="number">100</span></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> speed = <span class="number">200</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">package</span> launch</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">import</span> navigation._</span><br><span class="line">        <span class="class"><span class="keyword">object</span> <span class="title">Vehicle</span></span></span><br><span class="line"><span class="class">        </span>&#123;</span><br><span class="line">            <span class="comment">//private val guide：表示guide默认被当前单例对象可见。</span></span><br><span class="line">            <span class="comment">//private[launch] val guide：表示guide由默认对当前单例对象可见扩展到对launch包中的所有类型可见。</span></span><br><span class="line">            <span class="keyword">private</span>[launch] <span class="keyword">val</span> guide = <span class="keyword">new</span> <span class="type">Navigator</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这个例子中,类Navigator使用 private[bobsrockets] 来修饰,这表示这个类可以被bobsrockets包中所有类型访问,比如通常情况下 Vehicle无法访问私有类型Navigator,但使用包作用域之后,Vechile 中可以访问Navigator。</p><h3 id="5、Scala运算符"><a href="#5、Scala运算符" class="headerlink" title="5、Scala运算符"></a>5、Scala运算符</h3><p>与Java一样，不过Scala的运算符实际上是一种方法。</p><h3 id="6、条件表达式"><a href="#6、条件表达式" class="headerlink" title="6、条件表达式"></a>6、条件表达式</h3><p>Scala的的条件表达式比较简洁，例如：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> x = <span class="number">1</span></span><br><span class="line">    <span class="comment">//判断x的值，将结果赋给y</span></span><br><span class="line">    <span class="keyword">val</span> y = <span class="keyword">if</span> (x &gt; <span class="number">0</span>) <span class="number">1</span> <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">    <span class="comment">//打印y的值</span></span><br><span class="line">    println(<span class="string">"y="</span> + y)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//支持混合类型表达式</span></span><br><span class="line">    <span class="keyword">val</span> z = <span class="keyword">if</span> (x &gt; <span class="number">1</span>) <span class="number">1</span> <span class="keyword">else</span> <span class="string">"error"</span></span><br><span class="line">    <span class="comment">//打印z的值</span></span><br><span class="line">    println(<span class="string">"z="</span> + z)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//如果缺失else，相当于if (x &gt; 2) 1 else ()</span></span><br><span class="line">    <span class="keyword">val</span> m = <span class="keyword">if</span> (x &gt; <span class="number">2</span>) <span class="number">1</span></span><br><span class="line">    println(<span class="string">"m="</span> + m)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//在scala中每个表达式都有值，scala中有个Unit类，写做(),相当于Java中的void</span></span><br><span class="line">    <span class="keyword">val</span> n = <span class="keyword">if</span> (x &gt; <span class="number">2</span>) <span class="number">1</span> <span class="keyword">else</span> ()</span><br><span class="line">    println(<span class="string">"n="</span> + n)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//if和else if</span></span><br><span class="line">    <span class="keyword">val</span> k = <span class="keyword">if</span> (x &lt; <span class="number">0</span>) <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (x &gt;= <span class="number">1</span>) <span class="number">1</span> <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">    println(<span class="string">"k="</span> + k)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416193950234-1187815365.png" alt="img"></p><p>总结：</p><p><strong>1）**</strong>if<strong>**条件表达式它是有返回值的</strong></p><p>2）返回值会根据条件表达式的情况会进行自动的数据类型的推断。</p><h3 id="7、块表达式"><a href="#7、块表达式" class="headerlink" title="7、块表达式"></a>7、块表达式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="keyword">val</span> x = <span class="number">0</span></span><br><span class="line">   <span class="keyword">val</span> result = &#123;</span><br><span class="line">     <span class="keyword">if</span>(x &lt; <span class="number">0</span>)</span><br><span class="line">       <span class="number">1</span></span><br><span class="line">     <span class="keyword">else</span> <span class="keyword">if</span>(x &gt;= <span class="number">1</span>)</span><br><span class="line">       <span class="number">-1</span></span><br><span class="line">     <span class="keyword">else</span></span><br><span class="line">       <span class="string">"error"</span></span><br><span class="line">   &#125;</span><br><span class="line">   println(result)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416194407503-20425550.png" alt="img"></p><h3 id="8、循环"><a href="#8、循环" class="headerlink" title="8、循环"></a>8、循环</h3><h4 id="（1）while循环"><a href="#（1）while循环" class="headerlink" title="（1）while循环"></a>（1）while循环</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> n = <span class="number">10</span></span><br><span class="line">  <span class="keyword">while</span> ( &#123;</span><br><span class="line">    n &gt; <span class="number">0</span></span><br><span class="line">  &#125;) &#123;</span><br><span class="line">    println(n)</span><br><span class="line">    n -= <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>总结：</p><p>1）while使用跟java一模一样</p><p><strong>2）**</strong>注意点：在scala<strong>**里面不支持 i++  i–</strong> <strong>等操作</strong></p><p><strong>统一写成 i-=1</strong></p><h4 id="（2）for循环"><a href="#（2）for循环" class="headerlink" title="（2）for循环"></a>（2）for循环</h4><p>在scala中有for循环和while循环，用for循环比较多</p><p>for循环语法结构：</p><blockquote><p>for (i &lt;- 表达式/数组/集合)</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//for(i &lt;- 表达式),表达式1 to 10返回一个Range（区间）</span></span><br><span class="line">    <span class="comment">//每次循环将区间中的一个值赋给i</span></span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">      print(i+<span class="string">"\t"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//for(i &lt;- 数组)</span></span><br><span class="line">    println()</span><br><span class="line">    <span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>)</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- arr)</span><br><span class="line">      println(i)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//高级for循环</span></span><br><span class="line">    <span class="comment">//每个生成器都可以带一个条件，注意：if前面没有分号</span></span><br><span class="line">    <span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span>; j &lt;- <span class="number">1</span> to <span class="number">3</span> <span class="keyword">if</span> i != j)</span><br><span class="line">      print((<span class="number">10</span> * i + j) + <span class="string">" "</span>)</span><br><span class="line">    println()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//for推导式：如果for循环的循环体以yield开始，则该循环会构建出一个集合</span></span><br><span class="line">    <span class="comment">//每次迭代生成集合中的一个值</span></span><br><span class="line">    <span class="keyword">val</span> v = <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">10</span>) <span class="keyword">yield</span> i * <span class="number">10</span></span><br><span class="line">    println(v)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416195117549-87715630.png" alt="img"></p><p><strong>总结：</strong></p><p>　　1）在scala里面没有运算符，都有的符号其实都是方法。</p><p>　　2）在scala里面没有++  – 的用法</p><p>　　3）for( i  &lt;-  表达式/数组/集合)</p><p>　　4）在for循环里面我们是可以添加if表达式</p><p>　　5）有两个特殊表达式需要了解：</p><p> 　　  To  1 to 3   1 2 3</p><p> 　　  Until  1 until 3  12</p><p>　　6）如果在使用for循环的时候，for循环的时候我们需要获取，我们可以是使用yield关键字。</p><h3 id="9、方法和函数"><a href="#9、方法和函数" class="headerlink" title="9、方法和函数"></a>9、方法和函数</h3><h4 id="（1）定义方法"><a href="#（1）定义方法" class="headerlink" title="（1）定义方法"></a>（1）定义方法</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416195612825-1699240837.png" alt="img"></p><p>方法的返回值类型可以不写，编译器可以自动推断出来，但是<strong>对于递归函数，必须指定返回类型。</strong></p><p><strong>如果不写等号，代表没有返回值。</strong></p><h4 id="（2）定义函数"><a href="#（2）定义函数" class="headerlink" title="（2）定义函数"></a>（2）定义函数</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416195815736-789276526.png" alt="img"></p><h4 id="（3）方法和函数的区别"><a href="#（3）方法和函数的区别" class="headerlink" title="（3）方法和函数的区别"></a>（3）方法和函数的区别</h4><p>在函数式编程语言中，函数是“头等公民”，<strong>它可以像任何其他数据类型一样被传递和操作</strong></p><p>案例：首先定义一个方法，再定义一个函数，然后将函数传递到方法里面</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416195925972-1852464643.png" alt="img"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestScala</span> </span>&#123;</span><br><span class="line">  <span class="comment">//定义一个方法</span></span><br><span class="line">  <span class="comment">//方法m2参数要求是一个函数，函数的参数必须是两个Int类型</span></span><br><span class="line">  <span class="comment">//返回值类型也是Int类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">m1</span></span>(f:(<span class="type">Int</span>,<span class="type">Int</span>) =&gt; <span class="type">Int</span>) : <span class="type">Int</span> = &#123;</span><br><span class="line">    f(<span class="number">2</span>,<span class="number">6</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//定义一个函数f1，参数是两个Int类型，返回值是一个Int类型</span></span><br><span class="line">  <span class="keyword">val</span> f1 = (x:<span class="type">Int</span>,y:<span class="type">Int</span>) =&gt; x+y</span><br><span class="line">  <span class="comment">//再定义一个函数f2</span></span><br><span class="line">  <span class="keyword">val</span> f2 = (m:<span class="type">Int</span>,n:<span class="type">Int</span>) =&gt; m*n</span><br><span class="line">  <span class="comment">//main方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//调用m1方法，并传入f1函数</span></span><br><span class="line">    <span class="keyword">val</span> r1 = m1(f1)</span><br><span class="line">    println(<span class="string">"r1="</span>+r1)</span><br><span class="line">    <span class="comment">//调用m1方法，并传入f2函数</span></span><br><span class="line">    <span class="keyword">val</span> r2 = m1(f2)</span><br><span class="line">    println(<span class="string">"r2="</span>+r2)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（4）将方法转换成函数"><a href="#（4）将方法转换成函数" class="headerlink" title="（4）将方法转换成函数"></a>（4）将方法转换成函数</h4><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416200600439-1372602783.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala学习之路 （二）使用IDEA开发Scala</title>
      <link href="/2019-05-02-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%EF%BC%89%E4%BD%BF%E7%94%A8IDEA%E5%BC%80%E5%8F%91Scala.html"/>
      <url>/2019-05-02-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%EF%BC%89%E4%BD%BF%E7%94%A8IDEA%E5%BC%80%E5%8F%91Scala.html</url>
      
        <content type="html"><![CDATA[<p>** Scala学习之路 （二）使用IDEA开发Scala：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Scala学习之路 （二）使用IDEA开发Scala</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>目前Scala的开发工具主要有两种：Eclipse和IDEA，这两个开发工具都有相应的Scala插件，如果使用Eclipse，直接到Scala官网下载即可<a href="http://scala-ide.org/download/sdk.html。" target="_blank" rel="noopener">http://scala-ide.org/download/sdk.html。</a></p><p>由于IDEA的Scala插件更优秀，大多数Scala程序员都选择IDEA，可以到<a href="http://www.jetbrains.com/idea/download/下载，点击下一步安装即可，安装时如果有网络可以选择在线安装Scala插件。这里我们使用离线安装Scala插件：" target="_blank" rel="noopener">http://www.jetbrains.com/idea/download/下载，点击下一步安装即可，安装时如果有网络可以选择在线安装Scala插件。这里我们使用离线安装Scala插件：</a></p><p>1.安装IDEA，点击下一步即可。由于我们离线安装插件，所以点击Skip All and Set Defaul</p><p>2.下载IEDA的scala插件，地址<a href="http://plugins.jetbrains.com/?idea_ce" target="_blank" rel="noopener">http://plugins.jetbrains.com/?idea_ce</a></p><p>3.安装Scala插件：Configure -&gt; Plugins -&gt; Install plugin from disk -&gt; 选择Scala插件 -&gt; OK -&gt; 重启IDEA</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416112718976-208731375.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416112842026-373280438.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416114900488-1809718544.png" alt="img"></p><p>4、创建项目</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416125747183-1061495531.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416125816148-1757331299.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416130049895-889965146.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala学习之路 （一）Scala的安装</title>
      <link href="/2019-05-01-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%80%EF%BC%89Scala%E7%9A%84%E5%AE%89%E8%A3%85.html"/>
      <url>/2019-05-01-Scala%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%80%EF%BC%89Scala%E7%9A%84%E5%AE%89%E8%A3%85.html</url>
      
        <content type="html"><![CDATA[<p>** Scala学习之路 （一）Scala的安装：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Scala学习之路 （一）Scala的安装</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1、Scala下载"><a href="#1、Scala下载" class="headerlink" title="1、Scala下载"></a>1、Scala下载</h2><p>版本选择，看spark官网</p><p><a href="http://spark.apache.org/docs/latest/" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/</a></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416103535733-1032730391.png" alt="img"></p><p>spark2.3.0版本是用2.11版本的Scala进行开发的，所以此处下载Scala2.11的版本</p><p>Scala下载地址<a href="http://www.scala-lang.org/download/all.html" target="_blank" rel="noopener">http://www.scala-lang.org/download/all.html</a></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416104024785-1920253100.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416104048628-1177363989.png" alt="img"></p><h2 id="2、双击安装即可"><a href="#2、双击安装即可" class="headerlink" title="2、双击安装即可"></a>2、双击安装即可</h2><p>默认安装即可</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416104243620-1334770005.png" alt="img"></p><h2 id="3、检查Scala版本"><a href="#3、检查Scala版本" class="headerlink" title="3、检查Scala版本"></a>3、检查Scala版本</h2><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180416104419220-1888956213.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （二十一）Hive 优化策略</title>
      <link href="/2019-04-21-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%80%EF%BC%89Hive%20%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5.html"/>
      <url>/2019-04-21-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%80%EF%BC%89Hive%20%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （二十一）Hive 优化策略：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （二十一）Hive 优化策略</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、Hadoop-框架计算特性"><a href="#一、Hadoop-框架计算特性" class="headerlink" title="一、Hadoop 框架计算特性"></a>一、Hadoop 框架计算特性</h2><p>1、数据量大不是问题，数据倾斜是个问题</p><p>2、jobs 数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联多次 汇总，产生十几个 jobs，耗时很长。原因是 map reduce 作业初始化的时间是比较长的</p><p>3、sum,count,max,min 等 UDAF，不怕数据倾斜问题，hadoop 在 map 端的汇总合并优化，使 数据倾斜不成问题</p><p>4、count(distinct userid)，在数据量大的情况下，效率较低，如果是多 count(distinct userid,month)效率更低，因为 count(distinct)是按 group by 字段分组，按 distinct 字段排序， 一般这种分布方式是很</p><p>倾斜的，比如 PV 数据，淘宝一天 30 亿的 pv，如果按性别分组，分 配 2 个 reduce，每个 reduce 期望处理 15 亿数据，但现实必定是男少女多</p><h2 id="二、优化常用手段"><a href="#二、优化常用手段" class="headerlink" title="二、优化常用手段"></a>二、优化常用手段</h2><p>1、好的模型设计事半功倍</p><p>2、解决数据倾斜问题</p><p>3、减少 job 数</p><p>4、设置合理的 MapReduce 的 task 数，能有效提升性能。(比如，10w+级别的计算，用 160个 reduce，那是相当的浪费，1 个足够)</p><p>5、了解数据分布，自己动手解决数据倾斜问题是个不错的选择。这是通用的算法优化，但 算法优化有时不能适应特定业务背景，开发人员了解业务，了解数据，可以通过业务逻辑精 确有效的解决数据倾斜问题</p><p>6、数据量较大的情况下，慎用 count(distinct)，group by 容易产生倾斜问题</p><p>7、对小文件进行合并，是行之有效的提高调度效率的方法，假如所有的作业设置合理的文 件数，对云梯的整体调度效率也会产生积极的正向影响</p><p>8、优化时把握整体，单个作业最优不如整体最优</p><h2 id="三、排序选择"><a href="#三、排序选择" class="headerlink" title="三、排序选择"></a>三、排序选择</h2><p><strong>cluster by</strong>：对同一字段分桶并排序，不能和 sort by 连用</p><p><strong>distribute by + sort by</strong>：分桶，保证同一字段值只存在一个结果文件当中，结合 sort by 保证 每个 reduceTask 结果有序</p><p><strong>sort by</strong>：单机排序，单个 reduce 结果有序</p><p><strong>order by</strong>：全局排序，缺陷是只能使用一个 reduce</p><p><strong>一定要区分这四种排序的使用方式和适用场景</strong></p><h2 id="四、怎样做笛卡尔积"><a href="#四、怎样做笛卡尔积" class="headerlink" title="四、怎样做笛卡尔积"></a>四、怎样做笛卡尔积</h2><p>当 Hive 设定为严格模式（hive.mapred.mode=strict）时，不允许在 HQL 语句中出现笛卡尔积， 这实际说明了 Hive 对笛卡尔积支持较弱。因为找不到 Join key，Hive 只能使用 1 个 reducer 来完成笛卡尔积。</p><p>当然也可以使用 limit 的办法来减少某个表参与 join 的数据量，但对于需要笛卡尔积语义的 需求来说，经常是一个大表和一个小表的 Join 操作，结果仍然很大（以至于无法用单机处 理），这时 MapJoin才是最好的解决办法。MapJoin，顾名思义，会在 Map 端完成 Join 操作。 这需要将 Join 操作的一个或多个表完全读入内存。</p><p>PS：MapJoin 在子查询中可能出现未知 BUG。在大表和小表做笛卡尔积时，规避笛卡尔积的 方法是，给 Join 添加一个 Join key，<strong>原理很简单：将小表扩充一列 join key，并将小表的条 目复制数倍，join</strong> <strong>key 各不相同；将大表扩充一列 join key 为随机数。</strong></p><p><strong>精髓就在于复制几倍，最后就有几个 reduce 来做，而且大表的数据是前面小表扩张 key 值 范围里面随机出来的，所以复制了几倍 n，就相当于这个随机范围就有多大 n，那么相应的， 大表的数据就被随机的分为了 n 份。并且最后处理所用的 reduce 数量也是 n，而且也不会 出现数据倾斜。</strong></p><h2 id="五、怎样写-in-exists-语句"><a href="#五、怎样写-in-exists-语句" class="headerlink" title="五、怎样写 in/exists 语句"></a>五、怎样写 in/exists 语句</h2><p>虽然经过测验，hive1.2.1 也支持 in/exists 操作，但还是推荐使用 hive 的一个高效替代方案：<strong>left semi join</strong></p><p>比如说：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> a.id <span class="keyword">in</span> (<span class="keyword">select</span> b.id <span class="keyword">from</span> b);</span><br><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> <span class="keyword">exists</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> b <span class="keyword">where</span> a.id = b.id);</span><br></pre></td></tr></table></figure><p>应该转换成：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">semi</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id = b.id;</span><br></pre></td></tr></table></figure><h2 id="六、设置合理的-maptask-数量"><a href="#六、设置合理的-maptask-数量" class="headerlink" title="六、设置合理的 maptask 数量"></a>六、设置合理的 maptask 数量</h2><p>Map 数过大</p><p>　　Map 阶段输出文件太小，产生大量小文件</p><p>　　初始化和创建 Map 的开销很大</p><p>Map 数太小</p><p>　　文件处理或查询并发度小，Job 执行时间过长</p><p>　　大量作业时，容易堵塞集群 </p><p>在 MapReduce 的编程案例中，我们得知，一个MR Job的 MapTask 数量是由输入分片 InputSplit 决定的。而输入分片是由 FileInputFormat.getSplit()决定的。一个输入分片对应一个 MapTask， 而输入分片是由三个参数决定的：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415151017302-1064661736.png" alt="img"></p><p>输入分片大小的计算是这么计算出来的：</p><p><strong>long splitSize = Math.max(minSize, Math.min(maxSize, blockSize))</strong></p><p>默认情况下，输入分片大小和 HDFS 集群默认数据块大小一致，也就是默认一个数据块，启 用一个 MapTask 进行处理，这样做的好处是避免了服务器节点之间的数据传输，提高 job 处 理效率</p><p>两种经典的控制 MapTask 的个数方案：减少 MapTask 数或者增加 MapTask 数</p><blockquote><p><strong>1、 减少 MapTask 数是通过合并小文件来实现，这一点主要是针对数据源</strong></p><p><strong>2、 增加 MapTask 数可以通过控制上一个 job 的 reduceTask 个数</strong> </p></blockquote><p>因为 Hive 语句最终要转换为一系列的 MapReduce Job 的，而每一个 MapReduce Job 是由一 系列的 MapTask 和 ReduceTask 组成的，默认情况下， MapReduce 中一个 MapTask 或者一个 ReduceTask 就会启动一个 JVM 进程，一个 Task 执行完毕后， JVM 进程就退出。这样如果任 务花费时间很短，又要多次启动 JVM 的情况下，JVM 的启动时间会变成一个比较大的消耗， 这个时候，就可以通过重用 JVM 来解决：</p><blockquote><p> <strong>set mapred.job.reuse.jvm.num.tasks=5</strong> </p></blockquote><h2 id="七、小文件合并"><a href="#七、小文件合并" class="headerlink" title="七、小文件合并"></a>七、小文件合并</h2><p>文件数目过多，会给 HDFS 带来压力，并且会影响处理效率，可以通过合并 Map 和 Reduce 的 结果文件来消除这样的影响：</p><blockquote><p><strong>set hive.merge.mapfiles = true ##在 map only 的任务结束时合并小文件</strong></p><p><strong>set hive.merge.mapredfiles = false ## true 时在 MapReduce 的任务结束时合并小文件</strong></p><p><strong>set hive.merge.size.per.task = 256*1000*1000 ##合并文件的大小</strong></p><p><strong>set mapred.max.split.size=256000000; ##每个 Map 最大分割大小</strong></p><p><strong>set mapred.min.split.size.per.node=1; ##一个节点上 split 的最少值</strong></p><p><strong>set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; ##执行 Map 前进行小文件合并</strong></p></blockquote><h2 id="八、设置合理的-reduceTask-的数量"><a href="#八、设置合理的-reduceTask-的数量" class="headerlink" title="八、设置合理的 reduceTask 的数量"></a>八、设置合理的 reduceTask 的数量</h2><p>Hadoop MapReduce 程序中，reducer 个数的设定极大影响执行效率，这使得 Hive 怎样决定 reducer 个数成为一个关键问题。遗憾的是 Hive 的估计机制很弱，不指定 reducer 个数的情 况下，Hive 会猜测确定一个 reducer 个数，基于以下两个设定：</p><blockquote><p>1、hive.exec.reducers.bytes.per.reducer（默认为 256000000）</p><p>2、hive.exec.reducers.max（默认为 1009）</p><p>3、mapreduce.job.reduces=-1（设置一个常量 reducetask 数量）</p></blockquote><p>计算 reducer 数的公式很简单： N=min(参数 2，总输入数据量/参数 1) 通常情况下，有必要手动指定 reducer 个数。考虑到 map 阶段的输出数据量通常会比输入有 大幅减少，因此即使不设定 reducer 个数，重设参数 2 还是必要的。</p><p><strong>依据 Hadoop 的经验，可以将参数 2 设定为 0.95*(集群中 datanode 个数)。</strong> </p><h2 id="九、合并-MapReduce-操作"><a href="#九、合并-MapReduce-操作" class="headerlink" title="九、合并 MapReduce 操作"></a>九、合并 MapReduce 操作</h2><p>Multi-group by 是 Hive 的一个非常好的特性，它使得 Hive 中利用中间结果变得非常方便。 例如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">FROM (<span class="keyword">SELECT</span> a.status, b.school, b.gender <span class="keyword">FROM</span> status_updates a <span class="keyword">JOIN</span> <span class="keyword">profiles</span> b <span class="keyword">ON</span> (a.userid =</span><br><span class="line">b.userid <span class="keyword">and</span> a.ds=<span class="string">'2009-03-20'</span> ) ) subq1</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> gender_summary <span class="keyword">PARTITION</span>(ds=<span class="string">'2009-03-20'</span>)</span><br><span class="line"><span class="keyword">SELECT</span> subq1.gender, <span class="keyword">COUNT</span>(<span class="number">1</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> subq1.gender</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> school_summary <span class="keyword">PARTITION</span>(ds=<span class="string">'2009-03-20'</span>)</span><br><span class="line"><span class="keyword">SELECT</span> subq1.school, <span class="keyword">COUNT</span>(<span class="number">1</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> subq1.school</span><br></pre></td></tr></table></figure><p>上述查询语句使用了 multi-group by 特性连续 group by 了 2 次数据，使用不同的 group by key。 这一特性可以减少一次 MapReduce 操作</p><h2 id="十、合理利用分桶：Bucketing-和-Sampling"><a href="#十、合理利用分桶：Bucketing-和-Sampling" class="headerlink" title="十、合理利用分桶：Bucketing 和 Sampling"></a>十、合理利用分桶：Bucketing 和 Sampling</h2><p>Bucket 是指将数据以指定列的值为 key 进行 hash，hash 到指定数目的桶中。这样就可以支 持高效采样了。如下例就是以 userid 这一列为 bucket 的依据，共设置 32 个 buckets</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>,</span><br><span class="line"> page_url <span class="keyword">STRING</span>, referrer_url <span class="keyword">STRING</span>,</span><br><span class="line"> ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>)</span><br><span class="line"> <span class="keyword">COMMENT</span> <span class="string">'This is the page view table'</span></span><br><span class="line"> PARTITIONED <span class="keyword">BY</span>(dt <span class="keyword">STRING</span>, country <span class="keyword">STRING</span>)</span><br><span class="line"> CLUSTERED <span class="keyword">BY</span>(userid) SORTED <span class="keyword">BY</span>(viewTime) <span class="keyword">INTO</span> <span class="number">32</span> BUCKETS</span><br><span class="line"> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line"> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'1'</span></span><br><span class="line"> COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'2'</span></span><br><span class="line"> <span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'3'</span></span><br><span class="line"> <span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure><p>通常情况下，Sampling 在全体数据上进行采样，这样效率自然就低，它要去访问所有数据。 而如果一个表已经对某一列制作了 bucket，就可以采样所有桶中指定序号的某个桶，这就 减少了访问量。</p><p>如下例所示就是采样了 page_view 中 32 个桶中的第三个桶的全部数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> page_view <span class="keyword">TABLESAMPLE</span>(<span class="keyword">BUCKET</span> <span class="number">3</span> <span class="keyword">OUT</span> <span class="keyword">OF</span> <span class="number">32</span>);</span><br></pre></td></tr></table></figure><p>如下例所示就是采样了 page_view 中 32 个桶中的第三个桶的一半数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> page_view <span class="keyword">TABLESAMPLE</span>(<span class="keyword">BUCKET</span> <span class="number">3</span> <span class="keyword">OUT</span> <span class="keyword">OF</span> <span class="number">64</span>);</span><br></pre></td></tr></table></figure><h2 id="十一、合理利用分区：Partition"><a href="#十一、合理利用分区：Partition" class="headerlink" title="十一、合理利用分区：Partition"></a>十一、合理利用分区：Partition</h2><p> Partition 就是分区。分区通过在创建表时启用 partitioned by 实现，用来 partition 的维度并不 是实际数据的某一列，具体分区的标志是由插入内容时给定的。当要查询某一分区的内容时 可以采用 where 语句，形似 where tablename.partition_column = a 来实现。</p><p>创建含分区的表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>,</span><br><span class="line"> page_url <span class="keyword">STRING</span>, referrer_url <span class="keyword">STRING</span>,</span><br><span class="line"> ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span>(<span class="built_in">date</span> <span class="keyword">STRING</span>, country <span class="keyword">STRING</span>)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'1'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFILE;</span><br></pre></td></tr></table></figure><p>载入内容，并指定分区标志</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/pv_2008-06-08_us.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> page_view</span><br><span class="line"><span class="keyword">partition</span>(<span class="built_in">date</span>=<span class="string">'2008-06-08'</span>, country=<span class="string">'US'</span>);</span><br></pre></td></tr></table></figure><p>查询指定标志的分区内容</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> page_views.* <span class="keyword">FROM</span> page_views</span><br><span class="line"> <span class="keyword">WHERE</span> page_views.date &gt;= <span class="string">'2008-03-01'</span> <span class="keyword">AND</span> page_views.date &lt;= <span class="string">'2008-03-31'</span> <span class="keyword">AND</span></span><br><span class="line">page_views.referrer_url <span class="keyword">like</span> <span class="string">'%xyz.com'</span>;</span><br></pre></td></tr></table></figure><h2 id="十二、Join-优化"><a href="#十二、Join-优化" class="headerlink" title="十二、Join 优化"></a>十二、Join 优化</h2><p>总体原则：</p><p>　　1、 优先过滤后再进行 Join 操作，最大限度的减少参与 join 的数据量</p><p>　　2、 小表 join 大表，最好启动 mapjoin</p><p>　　3、 Join on 的条件相同的话，最好放入同一个 job，并且 join 表的排列顺序从小到大 </p><p><strong>在使用写有 Join 操作的查询语句时有一条原则：应该将条目少的表/子查询放在 Join 操作 符的左边。</strong>原因是在 Join 操作的 Reduce 阶段，位于 Join 操作符左边的表的内容会被加 载进内存，将条目少的表放在左边，可以有效减少发生 OOM 错误的几率。对于一条语句 中有多个 Join 的情况，如果 Join 的条件相同，比如查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> pv_users</span><br><span class="line"><span class="keyword">SELECT</span> pv.pageid, u.age <span class="keyword">FROM</span> page_view p</span><br><span class="line"><span class="keyword">JOIN</span> <span class="keyword">user</span> u <span class="keyword">ON</span> (pv.userid = u.userid)</span><br><span class="line"><span class="keyword">JOIN</span> newuser x <span class="keyword">ON</span> (u.userid = x.userid);</span><br></pre></td></tr></table></figure><p>如果 Join 的 key 相同，不管有多少个表，都会则会合并为一个 Map-Reduce 任务，而不 是”n”个，在做 OUTER JOIN 的时候也是一样</p><p>如果 join 的条件不相同，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE pv_users</span><br><span class="line"> SELECT pv.pageid, u.age FROM page_view p</span><br><span class="line"> JOIN user u ON (pv.userid = u.userid)</span><br><span class="line"> JOIN newuser x on (u.age = x.age);</span><br></pre></td></tr></table></figure><p>Map-Reduce 的任务数目和 Join 操作的数目是对应的，上述查询和以下查询是等价的</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--先 page_view 表和 user 表做链接</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tmptable</span><br><span class="line"> <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> page_view p <span class="keyword">JOIN</span> <span class="keyword">user</span> u <span class="keyword">ON</span> (pv.userid = u.userid);</span><br><span class="line"><span class="comment">-- 然后结果表 temptable 和 newuser 表做链接</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> pv_users</span><br><span class="line"> <span class="keyword">SELECT</span> x.pageid, x.age <span class="keyword">FROM</span> tmptable x <span class="keyword">JOIN</span> newuser y <span class="keyword">ON</span> (x.age = y.age);</span><br></pre></td></tr></table></figure><p>在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置：</p><blockquote><p><strong>set hive.skewjoin.key=100000; // 这个是 join 的键对应的记录条数超过这个值则会进行 分拆，值根据具体数据量设置</strong></p><p><strong>set hive.optimize.skewjoin=true; // 如果是 join 过程出现倾斜应该设置为 true</strong> </p></blockquote><h2 id="十三、Group-By-优化"><a href="#十三、Group-By-优化" class="headerlink" title="十三、Group By 优化"></a>十三、Group By 优化</h2><h3 id="1、Map-端部分聚合"><a href="#1、Map-端部分聚合" class="headerlink" title="1、Map 端部分聚合"></a>1、Map 端部分聚合</h3><p>并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进 行部分聚合，最后在 Reduce 端得出最终结果。</p><p>MapReduce 的 combiner 组件参数包括：</p><blockquote><p><strong>set hive.map.aggr = true 是否在 Map 端进行聚合，默认为 True</strong></p><p><strong>set hive.groupby.mapaggr.checkinterval = 100000 在 Map 端进行聚合操作的条目数目</strong></p></blockquote><h3 id="2、使用-Group-By-有数据倾斜的时候进行负载均衡"><a href="#2、使用-Group-By-有数据倾斜的时候进行负载均衡" class="headerlink" title="2、使用 Group By 有数据倾斜的时候进行负载均衡"></a>2、使用 Group By 有数据倾斜的时候进行负载均衡</h3><blockquote><p> <strong>set hive.groupby.skewindata = true</strong></p></blockquote><p>当 sql 语句使用 groupby 时数据出现倾斜时，如果该变量设置为 true，那么 Hive 会自动进行 负载均衡。<strong>策略就是把 MR 任务拆分成两个：第一个先做预汇总，第二个再做最终汇总</strong></p><p>在 MR 的第一个阶段中，Map 的输出结果集合会缓存到 maptaks 中，每个 Reduce 做部分聚 合操作，并输出结果，这样处理的结果是相同 Group By Key 有可能被分发到不同的 Reduce 中， 从而达到负载均衡的目的；第二个阶段 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成 最终的聚合操作。</p><h2 id="十四、合理利用文件存储格式"><a href="#十四、合理利用文件存储格式" class="headerlink" title="十四、合理利用文件存储格式"></a>十四、合理利用文件存储格式</h2><p>创建表时，尽量使用 orc、parquet 这些列式存储格式，因为列式存储的表，每一列的数据在 物理上是存储在一起的，Hive 查询时会只遍历需要列数据，大大减少处理的数据量。</p><h2 id="十五、本地模式执行-MapReduce"><a href="#十五、本地模式执行-MapReduce" class="headerlink" title="十五、本地模式执行 MapReduce"></a>十五、本地模式执行 MapReduce</h2><p>Hive 在集群上查询时，默认是在集群上 N 台机器上运行， 需要多个机器进行协调运行，这 个方式很好地解决了大数据量的查询问题。但是当 Hive 查询处理的数据量比较小时，其实 没有必要启动分布式模式去执行，因为以分布式方式执行就涉及到跨网络传输、多节点协调 等，并且消耗资源。这个时间可以只使用本地模式来执行 mapreduce job，只在一台机器上 执行，速度会很快。启动本地模式涉及到三个参数：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415152400015-1874477482.png" alt="img"></p><p>set hive.exec.mode.local.auto=true 是打开 hive 自动判断是否启动本地模式的开关，但是只 是打开这个参数并不能保证启动本地模式，要当 map 任务数不超过</p><p>hive.exec.mode.local.auto.input.files.max 的个数并且 map 输入文件大小不超过</p><p>hive.exec.mode.local.auto.inputbytes.max 所指定的大小时，才能启动本地模式。</p><h2 id="十六、并行化处理"><a href="#十六、并行化处理" class="headerlink" title="十六、并行化处理"></a>十六、并行化处理</h2><p>一个 hive sql 语句可能会转为多个 mapreduce Job，每一个 job 就是一个 stage，这些 job 顺序 执行，这个在 cli 的运行日志中也可以看到。但是有时候这些任务之间并不是是相互依赖的， 如果集群资源允许的话，可以让多个并不相互依赖 stage 并发执行，这样就节约了时间，提 高了执行速度，但是如果集群资源匮乏时，启用并行化反倒是会导致各个 job 相互抢占资源 而导致整体执行性能的下降。启用并行化：</p><blockquote><p><strong>set hive.exec.parallel=true;</strong></p><p><strong>set hive.exec.parallel.thread.number=8; //同一个 sql 允许并行任务的最大线程数</strong></p></blockquote><h2 id="十七、设置压缩存储"><a href="#十七、设置压缩存储" class="headerlink" title="十七、设置压缩存储"></a>十七、设置压缩存储</h2><h3 id="1、压缩的原因"><a href="#1、压缩的原因" class="headerlink" title="1、压缩的原因"></a>1、压缩的原因</h3><p>Hive 最终是转为 MapReduce 程序来执行的，而 MapReduce 的性能瓶颈在于网络 IO 和 磁盘 IO，要解决性能瓶颈，最主要的是减少数据量，对数据进行压缩是个好的方式。压缩 虽然是减少了数据量，但是压缩过程要消耗 CPU 的，但是在 Hadoop 中， 往往性能瓶颈不 在于 CPU，CPU 压力并不大，所以压缩充分利用了比较空闲的 CPU</p><h3 id="2、常用压缩方法对比"><a href="#2、常用压缩方法对比" class="headerlink" title="2、常用压缩方法对比"></a>2、常用压缩方法对比</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415152559984-1857141726.png" alt="img"></p><p>各个压缩方式所对应的 Class 类：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415152617806-1546480457.png" alt="img"></p><h3 id="3、压缩方式的选择"><a href="#3、压缩方式的选择" class="headerlink" title="3、压缩方式的选择"></a>3、压缩方式的选择</h3><blockquote><p><strong>压缩比率</strong></p><p><strong>压缩解压缩速度</strong></p><p><strong>是否支持 Split</strong></p></blockquote><h3 id="4、压缩使用"><a href="#4、压缩使用" class="headerlink" title="4、压缩使用"></a>4、压缩使用</h3><p>Job 输出文件按照 block 以 GZip 的方式进行压缩：</p><blockquote><p><strong>set mapreduce.output.fileoutputformat.compress=true // 默认值是 false</strong></p><p><strong>set mapreduce.output.fileoutputformat.compress.type=BLOCK // 默认值是 Record</strong></p><p><strong>set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec // 默认值是 org.apache.hadoop.io.compress.DefaultCodec</strong></p></blockquote><p>Map 输出结果也以 Gzip 进行压缩：</p><blockquote><p><strong>set mapred.map.output.compress=true</strong></p><p><strong>set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.GzipCodec // 默认值是 org.apache.hadoop.io.compress.DefaultCodec</strong> </p></blockquote><p>对 Hive 输出结果和中间都进行压缩：</p><blockquote><p><strong>set hive.exec.compress.output=true // 默认值是 false，不压缩</strong></p><p><strong>set hive.exec.compress.intermediate=true // 默认值是 false，为 true 时 MR 设置的压缩才启用</strong></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （二十）Hive 执行过程实例分析</title>
      <link href="/2019-04-20-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%E5%8D%81%EF%BC%89Hive%20%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90.html"/>
      <url>/2019-04-20-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%E5%8D%81%EF%BC%89Hive%20%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （二十）Hive 执行过程实例分析：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （二十）Hive 执行过程实例分析</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、Hive-执行过程概述"><a href="#一、Hive-执行过程概述" class="headerlink" title="一、Hive 执行过程概述"></a>一、Hive 执行过程概述</h2><h3 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h3><p>（1） Hive 将 HQL 转换成一组操作符（Operator），比如 GroupByOperator, JoinOperator 等</p><p>（2）操作符 Operator 是 Hive 的最小处理单元</p><p>（3）每个操作符代表一个 HDFS 操作或者 MapReduce 作业</p><p>（4）Hive 通过 ExecMapper 和 ExecReducer 执行 MapReduce 程序，执行模式有本地模式和分 布式两种模式</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415144838165-35453427.png" alt="img"></p><h3 id="2、Hive-操作符列表"><a href="#2、Hive-操作符列表" class="headerlink" title="2、Hive 操作符列表"></a>2、Hive 操作符列表</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415144858715-1662976456.png" alt="img"></p><h3 id="3、Hive-编译器的工作职责"><a href="#3、Hive-编译器的工作职责" class="headerlink" title="3、Hive 编译器的工作职责"></a>3、Hive 编译器的工作职责</h3><p>（1）Parser：将 HQL 语句转换成抽象语法树（AST：Abstract Syntax Tree）</p><p>（2）Semantic Analyzer：将抽象语法树转换成查询块</p><p>（3）Logic Plan Generator：将查询块转换成逻辑查询计划</p><p>（4）Logic Optimizer：重写逻辑查询计划，优化逻辑执行计划</p><p>（5）Physical Plan Gernerator：将逻辑计划转化成物理计划（MapReduce Jobs）</p><p>（6）Physical Optimizer：选择最佳的 Join 策略，优化物理执行计划</p><h3 id="4、优化器类型"><a href="#4、优化器类型" class="headerlink" title="4、优化器类型"></a>4、优化器类型</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415145116276-990825971.png" alt="img"></p><p>上表中带①符号的，优化目的都是尽量将任务合并到一个 Job 中，以减少 Job 数量，带②的 优化目的是尽量减少 shuffle 数据量</p><h2 id="二、join"><a href="#二、join" class="headerlink" title="二、join"></a>二、join</h2><h3 id="1、对于-join-操作"><a href="#1、对于-join-操作" class="headerlink" title="1、对于 join 操作"></a>1、对于 join 操作</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> pv.pageid, u.age <span class="keyword">FROM</span> page_view pv <span class="keyword">JOIN</span> <span class="keyword">user</span> u <span class="keyword">ON</span> pv.userid = u.userid;</span><br></pre></td></tr></table></figure><h3 id="2、实现过程"><a href="#2、实现过程" class="headerlink" title="2、实现过程"></a>2、实现过程</h3><blockquote><p> <strong>Map</strong>：</p><p>　　1、以 JOIN ON 条件中的列作为 Key，如果有多个列，则 Key 是这些列的组合</p><p>　　2、以 JOIN 之后所关心的列作为 Value，当有多个列时，Value 是这些列的组合。在 Value 中还会包含表的 Tag 信息，用于标明此 Value 对应于哪个表</p><p>　　3、按照 Key 进行排序</p><p><strong>Shuffle</strong>：</p><p>　　1、根据 Key 的值进行 Hash，并将 Key/Value 对按照 Hash 值推至不同对 Reduce 中</p><p><strong>Reduce</strong>：</p><p>　　1、 Reducer 根据 Key 值进行 Join 操作，并且通过 Tag 来识别不同的表中的数据</p></blockquote><h3 id="3、具体实现过程"><a href="#3、具体实现过程" class="headerlink" title="3、具体实现过程"></a>3、具体实现过程</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415145350148-1004247692.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415145404099-741231817.png" alt="img"></p><h2 id="三、Group-By"><a href="#三、Group-By" class="headerlink" title="三、Group By"></a>三、Group By</h2><h3 id="1、对于-group-by操作"><a href="#1、对于-group-by操作" class="headerlink" title="1、对于 group by操作"></a>1、对于 group by操作</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> pageid, age, <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">FROM</span> pv_users <span class="keyword">GROUP</span> <span class="keyword">BY</span> pageid, age;</span><br></pre></td></tr></table></figure><h3 id="2、实现过程-1"><a href="#2、实现过程-1" class="headerlink" title="2、实现过程"></a>2、实现过程</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415145530734-1261579451.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415145542017-1372728782.png" alt="img"></p><h2 id="四、Distinct"><a href="#四、Distinct" class="headerlink" title="四、Distinct"></a>四、Distinct</h2><h3 id="1、对于-distinct的操作"><a href="#1、对于-distinct的操作" class="headerlink" title="1、对于 distinct的操作"></a>1、对于 distinct的操作</h3><p>按照 age 分组，然后统计每个分组里面的不重复的 pageid 有多少个</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> age, <span class="keyword">count</span>(<span class="keyword">distinct</span> pageid) <span class="keyword">FROM</span> pv_users <span class="keyword">GROUP</span> <span class="keyword">BY</span> age;</span><br></pre></td></tr></table></figure><h3 id="2、实现过程-2"><a href="#2、实现过程-2" class="headerlink" title="2、实现过程"></a>2、实现过程</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415145718259-315447256.png" alt="img"></p><h3 id="3、详细过程解释"><a href="#3、详细过程解释" class="headerlink" title="3、详细过程解释"></a>3、详细过程解释</h3><p>该 SQL 语句会按照 age 和 pageid 预先分组，进行 distinct 操作。然后会再按 照 age 进行分组，再进行一次 distinct 操作</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （十九）Hive的数据倾斜</title>
      <link href="/2019-04-19-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89Hive%E7%9A%84%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C.html"/>
      <url>/2019-04-19-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89Hive%E7%9A%84%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （十九）Hive的数据倾斜：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （十九）Hive的数据倾斜</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1、什么是数据倾斜？"><a href="#1、什么是数据倾斜？" class="headerlink" title="1、什么是数据倾斜？"></a>1、什么是数据倾斜？</h2><p>由于数据分布不均匀，造成数据大量的集中到一点，造成数据热点</p><h2 id="2、Hadoop-框架的特性"><a href="#2、Hadoop-框架的特性" class="headerlink" title="2、Hadoop 框架的特性"></a>2、Hadoop 框架的特性</h2><p>　　A、不怕数据大，怕数据倾斜</p><p>　　B、Jobs 数比较多的作业运行效率相对比较低，如子查询比较多</p><p>　　C、 sum,count,max,min 等聚集函数，通常不会有数据倾斜问题</p><h2 id="3、主要表现"><a href="#3、主要表现" class="headerlink" title="3、主要表现"></a>3、主要表现</h2><p>任务进度长时间维持在 99%或者 100%的附近，查看任务监控页面，发现只有少量 reduce 子任务未完成，因为其处理的数据量和其他的 reduce 差异过大。 单一 reduce 处理的记录数和平均记录数相差太大，通常达到好几倍之多，最长时间远大 于平均时长。</p><h2 id="4、容易数据倾斜情况"><a href="#4、容易数据倾斜情况" class="headerlink" title="4、容易数据倾斜情况"></a>4、容易数据倾斜情况</h2><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415143533654-450330742.png" alt="img"></p><p>　　A、group by 不和聚集函数搭配使用的时候</p><p>　　B、count(distinct)，在数据量大的情况下，容易数据倾斜，因为 count(distinct)是按 group by 字段分组，按 distinct 字段排序</p><p>　　C、 小表关联超大表 join</p><h2 id="5、产生数据倾斜的原因"><a href="#5、产生数据倾斜的原因" class="headerlink" title="5、产生数据倾斜的原因"></a>5、产生数据倾斜的原因</h2><p>　　A：key 分布不均匀</p><p>　　B：业务数据本身的特性</p><p>　　C：建表考虑不周全</p><p>　　D：某些 HQL 语句本身就存在数据倾斜</p><h2 id="6、业务场景"><a href="#6、业务场景" class="headerlink" title="6、业务场景"></a>6、业务场景</h2><h3 id="（1）空值产生的数据倾斜"><a href="#（1）空值产生的数据倾斜" class="headerlink" title="（1）空值产生的数据倾斜"></a>（1）空值产生的数据倾斜</h3><h4 id="场景说明"><a href="#场景说明" class="headerlink" title="场景说明"></a>场景说明</h4><p>在日志中，常会有信息丢失的问题，比如日志中的 user_id，如果取其中的 user_id 和用户表中的 user_id 相关联，就会碰到数据倾斜的问题。</p><h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>解决方案 1：user_id 为空的不参与关联</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select * from log a join user b on a.user_id is not null and a.user_id = b.user_id</span><br><span class="line">union all</span><br><span class="line">select * from log c where c.user_id is null;</span><br></pre></td></tr></table></figure><p>解决方案 2：赋予空值新的 key 值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select * from log a left outer join user b on</span><br><span class="line">case when a.user_id is null then concat(&apos;hive&apos;,rand()) else a.user_id end = b.user_id</span><br></pre></td></tr></table></figure><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>方法 2 比方法 1 效率更好，不但 IO 少了，而且作业数也少了，方案 1 中，log 表 读了两次，jobs 肯定是 2，而方案 2 是 1。这个优化适合无效 id（比如-99，’’，null）产 生的数据倾斜，<strong>把空值的 key 变</strong></p><p><strong>成一个字符串加上一个随机数</strong>，就能把造成数据倾斜的 数据分到不同的 reduce 上解决数据倾斜的问题。</p><p>改变之处：使本身为 null 的所有记录不会拥挤在同一个 reduceTask 了，会由于有替代的 随机字符串值，而分散到了多个 reduceTask 中了，由于 null 值关联不上，处理后并不影响最终结果。</p><h3 id="（2）不同数据类型关联产生数据倾斜"><a href="#（2）不同数据类型关联产生数据倾斜" class="headerlink" title="（2）不同数据类型关联产生数据倾斜"></a>（2）不同数据类型关联产生数据倾斜</h3><h4 id="场景说明-1"><a href="#场景说明-1" class="headerlink" title="场景说明"></a>场景说明</h4><p>用户表中 user_id 字段为 int，log 表中 user_id 为既有 string 也有 int 的类型， 当按照两个表的 user_id 进行 join 操作的时候，默认的 hash 操作会按照 int 类型的 id 进 行分配，这样就会导致所有的 string 类型的 id 就被分到同一个 reducer 当中</p><h4 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h4><p>把数字类型 id 转换成 string 类型的 id</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">user</span> a <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> <span class="keyword">log</span> b <span class="keyword">on</span> b.user_id = <span class="keyword">cast</span>(a.user_id <span class="keyword">as</span> <span class="keyword">string</span>)</span><br></pre></td></tr></table></figure><h3 id="（3）大小表关联查询产生数据倾斜"><a href="#（3）大小表关联查询产生数据倾斜" class="headerlink" title="（3）大小表关联查询产生数据倾斜"></a>（3）大小表关联查询产生数据倾斜</h3><p> 注意：使用map join解决小表关联大表造成的数据倾斜问题。这个方法使用的频率很高。</p><blockquote><p><strong>map join 概念：将其中做连接的小表（全量数据）分发到所有 MapTask 端进行 Join，从 而避免了 reduceTask，前提要求是内存足以装下该全量数据</strong></p></blockquote><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415144152040-870536898.png" alt="img"></p><p>以大表 a 和小表 b 为例，所有的 maptask 节点都装载小表 b 的所有数据，然后大表 a 的 一个数据块数据比如说是 a1 去跟 b 全量数据做链接，就省去了 reduce 做汇总的过程。 所以相对来说，在内存允许的条件下使用 map join 比直接使用 MapReduce 效率还高些， 当然这只限于做 join 查询的时候。</p><p>在 hive 中，直接提供了能够在 HQL 语句指定该次查询使用 map join，map join 的用法是 在查询/子查询的SELECT关键字后面添加/*+ MAPJOIN(tablelist) */提示优化器转化为map join（早期的 Hive 版本的优化器是不能自动优化 map join 的）。其中 tablelist 可以是一个 表，或以逗号连接的表的列表。tablelist 中的表将会读入内存，通常应该是将小表写在 这里。</p><p>MapJoin 具体用法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/* +mapjoin(a) */</span> a.id aid, <span class="keyword">name</span>, age <span class="keyword">from</span> a <span class="keyword">join</span> b <span class="keyword">on</span> a.id = b.id;</span><br><span class="line"><span class="keyword">select</span> <span class="comment">/* +mapjoin(movies) */</span> a.title, b.rating <span class="keyword">from</span> movies a <span class="keyword">join</span> ratings b <span class="keyword">on</span> a.movieid =</span><br><span class="line">b.movieid;</span><br></pre></td></tr></table></figure><p>在 hive0.11 版本以后会自动开启 map join 优化，由两个参数控制：</p><blockquote><p><strong>set hive.auto.convert.join=true; //设置 MapJoin 优化自动开启</strong></p><p><strong>set hive.mapjoin.smalltable.filesize=25000000 //设置小表不超过多大时开启 mapjoin 优化</strong></p></blockquote><p>如果是大大表关联呢？那就大事化小，小事化了。<strong>把大表切分成小表，然后分别 map join</strong></p><p>那么如果小表不大不小，那该如何处理呢？？？</p><p>使用 map join 解决小表(记录数少)关联大表的数据倾斜问题，这个方法使用的频率非常 高，但如果小表很大，大到 map join 会出现 bug 或异常，这时就需要特别的处理</p><p>举一例：日志表和用户表做链接</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> a <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> <span class="keyword">users</span> b <span class="keyword">on</span> a.user_id = b.user_id;</span><br></pre></td></tr></table></figure><p>users 表有 600w+的记录，把 users 分发到所有的 map 上也是个不小的开销，而且 map join 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。</p><p>改进方案：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+mapjoin(x)*/</span>* <span class="keyword">from</span> <span class="keyword">log</span> a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> (</span><br><span class="line"> <span class="keyword">select</span> <span class="comment">/*+mapjoin(c)*/</span> d.*</span><br><span class="line"> <span class="keyword">from</span> ( <span class="keyword">select</span> <span class="keyword">distinct</span> user_id <span class="keyword">from</span> <span class="keyword">log</span> ) c <span class="keyword">join</span> <span class="keyword">users</span> d <span class="keyword">on</span> c.user_id = d.user_id</span><br><span class="line">) x</span><br><span class="line"><span class="keyword">on</span> a.user_id = x.user_id;</span><br></pre></td></tr></table></figure><p>假如，log 里 user_id 有上百万个，这就又回到原来 map join 问题。所幸，每日的会员 uv 不会太多，有交易的会员不会太多，有点击的会员不会太多，有佣金的会员不会太多等 等。所以这个方法能解决很多场景下的数据倾斜问题</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （十八）Hive的Shell操作</title>
      <link href="/2019-04-18-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89Hive%E7%9A%84Shell%E6%93%8D%E4%BD%9C.html"/>
      <url>/2019-04-18-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89Hive%E7%9A%84Shell%E6%93%8D%E4%BD%9C.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （十八）Hive的Shell操作：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （十八）Hive的Shell操作</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、Hive的命令行"><a href="#一、Hive的命令行" class="headerlink" title="一、Hive的命令行"></a>一、Hive的命令行</h2><h3 id="1、Hive支持的一些命令"><a href="#1、Hive支持的一些命令" class="headerlink" title="1、Hive支持的一些命令"></a>1、Hive支持的一些命令</h3><blockquote><p>Command Description</p><p><strong>quit</strong> Use quit or exit to leave the interactive shell.</p><p><strong>set key=value</strong> Use this to set value of particular configuration variable. One thing to note here is that if you misspell the variable name, cli will not show an error.</p><p><strong>set</strong> This will print a list of configuration variables that are overridden by user or hive.</p><p><strong>set -v</strong> This will print all hadoop and hive configuration variables.</p><p><strong>add FILE [file] [file]*</strong> Adds a file to the list of resources</p><p><strong>add jar jarname</strong></p><p><strong>list FILE</strong> list all the files added to the distributed cache</p><p><strong>list FILE [file]*</strong> Check if given resources are already added to distributed cache</p><p><strong>! [cmd]</strong> Executes a shell command from the hive shell</p><p><strong>dfs [dfs cmd]</strong> Executes a dfs command from the hive shell</p><p><strong>[query]</strong> Executes a hive query and prints results to standard out</p><p><strong>source FILE</strong> Used to execute a script file inside the CLI.</p></blockquote><h3 id="2、语法结构"><a href="#2、语法结构" class="headerlink" title="2、语法结构"></a>2、语法结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive [-hiveconf x=y]* [&lt;-i filename&gt;]* [&lt;-f filename&gt;|&lt;-e query-string&gt;] [-S]</span><br></pre></td></tr></table></figure><p>说明：</p><blockquote><p>1、-i 从文件初始化 HQL</p><p>2、-e 从命令行执行指定的 HQL</p><p>3、-f 执行 HQL 脚本</p><p>4、-v 输出执行的 HQL 语句到控制台</p><p>5、-p connect to Hive Server on port number</p><p>6、-hiveconf x=y（Use this to set hive/hadoop configuration variables）</p><p>7、-S：表示以不打印日志的形式执行命名操作</p></blockquote><h3 id="3、示例"><a href="#3、示例" class="headerlink" title="3、示例"></a>3、示例</h3><h4 id="（1）运行一个查询"><a href="#（1）运行一个查询" class="headerlink" title="（1）运行一个查询"></a>（1）运行一个查询</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ hive -e &quot;select * from cookie.cookie1;&quot;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415141508766-1214413094.png" alt="img"></p><h4 id="（2）运行一个文件"><a href="#（2）运行一个文件" class="headerlink" title="（2）运行一个文件"></a>（2）运行一个文件</h4><p> 编写hive.sql文件</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415141653695-2137330979.png" alt="img"></p><p>运行编写的文件</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415141814477-1348619144.png" alt="img"></p><h4 id="（3）运行参数文件"><a href="#（3）运行参数文件" class="headerlink" title="（3）运行参数文件"></a>（3）运行参数文件</h4><p>从配置文件启动 hive，并加载配置文件当中的配置参数</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415142106103-736436497.png" alt="img"></p><h2 id="二、Hive的参数配置方式"><a href="#二、Hive的参数配置方式" class="headerlink" title="二、Hive的参数配置方式"></a>二、Hive的参数配置方式</h2><h3 id="1、Hive的参数配置大全"><a href="#1、Hive的参数配置大全" class="headerlink" title="1、Hive的参数配置大全"></a>1、Hive的参数配置大全</h3><p><a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties</a></p><h3 id="2、Hive的参数设置方式"><a href="#2、Hive的参数设置方式" class="headerlink" title="2、Hive的参数设置方式"></a>2、Hive的参数设置方式</h3><p>开发 Hive 应用时，不可避免地需要设定 Hive 的参数。设定 Hive 的参数可以调优 HQL 代码 的执行效率，或帮助定位问题。然而实践中经常遇到的一个问题是，为什么设定的参数没有 起作用？这通常是错误的设定方式导致的</p><p>对于一般参数，有以下三种设定方式：</p><blockquote><p><strong>1、配置文件 （全局有效）</strong></p><p><strong>2、命令行参数（对 hive 启动实例有效）</strong></p><p><strong>3、参数声明 （对 hive 的连接 session 有效）</strong></p></blockquote><h4 id="（1）配置文件"><a href="#（1）配置文件" class="headerlink" title="（1）配置文件"></a>（1）配置文件</h4><p>Hive 的配置文件包括：</p><p>　　A.　用户自定义配置文件：$HIVE_CONF_DIR/hive-site.xml</p><p>　　B.　默认配置文件：$HIVE_CONF_DIR/hive-default.xml</p><p>用户自定义配置会覆盖默认配置。</p><p>另外，Hive 也会读入 Hadoop 的配置，因为 Hive 是作为 Hadoop 的客户端启动的，Hive 的配 置会覆盖 Hadoop 的配置。</p><p>配置文件的设定对本机启动的所有 Hive 进程都有效。</p><h4 id="（2）命令行参数"><a href="#（2）命令行参数" class="headerlink" title="（2）命令行参数"></a>（2）命令行参数</h4><p>启动 Hive（客户端或 Server 方式）时，可以在命令行添加-hiveconf param=value 来设定参数，例如：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415142748613-2031600308.png" alt="img"></p><p>这一设定对本次启动的 session（对于 server 方式启动，则是所有请求的 session）有效。</p><h4 id="（3）参数声明"><a href="#（3）参数声明" class="headerlink" title="（3）参数声明"></a>（3）参数声明</h4><p>可以在 HQL 中使用 SET 关键字设定参数，例如：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180415143012000-1554548601.png" alt="img"></p><p>这一设定的作用域也是 session 级的。</p><blockquote><p><strong>set hive.exec.reducers.bytes.per.reducer=</strong> 每个 reduce task 的平均负载数据量 Hive 会估算总数据量，然后用该值除以上述参数值，就能得出需要运行的 reduceTask 数</p><p><strong>set hive.exec.reducers.max=</strong> 设置 reduce task 数量的上限</p><p><strong>set mapreduce.job.reduces=</strong> 指定固定的 reduce task 数量</p></blockquote><p>但是，这个参数在必要时&lt;业务逻辑决定只能用一个 reduce task&gt; hive 会忽略，比如在设置 了 set mapreduce.job.reduces = 3，但是 HQL 语句当中使用了 order by 的话，那么就会忽略该参数的设置。</p><p>上述三种设定方式的优先级依次递增。即<strong>参数声明覆盖命令行参数，命令行参数覆盖配置 文件设定</strong>。注意某些系统级的参数，例如 log4j 相关的设定，必须用前两种方式设定，因为 那些参数的读取在 session 建立以前已经完成了。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （十七）Hive分析窗口函数(五) GROUPING SETS、GROUPING__ID、CUBE和ROLLUP</title>
      <link href="/2019-04-17-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89Hive%E5%88%86%E6%9E%90%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0(%E4%BA%94)%20GROUPING%20SETS%E3%80%81GROUPING__ID%E3%80%81CUBE%E5%92%8CROLLUP.html"/>
      <url>/2019-04-17-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89Hive%E5%88%86%E6%9E%90%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0(%E4%BA%94)%20GROUPING%20SETS%E3%80%81GROUPING__ID%E3%80%81CUBE%E5%92%8CROLLUP.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （十七）Hive分析窗口函数(五) GROUPING SETS、GROUPING__ID、CUBE和ROLLUP：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        GROUPING SETS,GROUPING__ID,CUBE,ROLLUP</p><p>这几个分析函数通常用于OLAP中，不能累加，而且需要根据不同维度上钻和下钻的指标统计，比如，分小时、天、月的UV数。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><h3 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">2015-03,2015-03-10,cookie1</span><br><span class="line">2015-03,2015-03-10,cookie5</span><br><span class="line">2015-03,2015-03-12,cookie7</span><br><span class="line">2015-04,2015-04-12,cookie3</span><br><span class="line">2015-04,2015-04-13,cookie2</span><br><span class="line">2015-04,2015-04-13,cookie4</span><br><span class="line">2015-04,2015-04-16,cookie4</span><br><span class="line">2015-03,2015-03-10,cookie2</span><br><span class="line">2015-03,2015-03-10,cookie3</span><br><span class="line">2015-04,2015-04-12,cookie5</span><br><span class="line">2015-04,2015-04-13,cookie6</span><br><span class="line">2015-04,2015-04-15,cookie3</span><br><span class="line">2015-04,2015-04-15,cookie2</span><br><span class="line">2015-04,2015-04-16,cookie1</span><br></pre></td></tr></table></figure><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> cookie;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> cookie5;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> cookie5(<span class="keyword">month</span> <span class="keyword">string</span>, <span class="keyword">day</span> <span class="keyword">string</span>, cookieid <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/cookie5.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> cookie5;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> cookie5;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411211611341-917550002.png" alt="img"></p><h2 id="玩一玩GROUPING-SETS和GROUPING-ID"><a href="#玩一玩GROUPING-SETS和GROUPING-ID" class="headerlink" title="玩一玩GROUPING SETS和GROUPING__ID"></a>玩一玩GROUPING SETS和GROUPING__ID</h2><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>在一个GROUP BY查询中，根据不同的维度组合进行聚合，等价于将不同维度的GROUP BY结果集进行UNION ALL</p><p><strong>GROUPING__ID</strong>，表示结果属于哪一个分组集合。</p><h3 id="查询语句"><a href="#查询语句" class="headerlink" title="查询语句"></a>查询语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">  <span class="keyword">month</span>,</span><br><span class="line">  <span class="keyword">day</span>,</span><br><span class="line">  <span class="keyword">count</span>(<span class="keyword">distinct</span> cookieid) <span class="keyword">as</span> uv,</span><br><span class="line">  GROUPING__ID</span><br><span class="line"><span class="keyword">from</span> cookie.cookie5 </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">month</span>,<span class="keyword">day</span> </span><br><span class="line"><span class="keyword">grouping</span> <span class="keyword">sets</span> (<span class="keyword">month</span>,<span class="keyword">day</span>) </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> GROUPING__ID;</span><br></pre></td></tr></table></figure><h4 id="等价于"><a href="#等价于" class="headerlink" title="等价于"></a>等价于</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="literal">NULL</span>,<span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">1</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> cookie5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span> </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="literal">NULL</span>,<span class="keyword">day</span>,<span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">2</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> cookie5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span></span><br></pre></td></tr></table></figure><h3 id="查询结果"><a href="#查询结果" class="headerlink" title="查询结果"></a>查询结果</h3><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411212505871-491842082.png" alt="img"></p><h3 id="结果说明"><a href="#结果说明" class="headerlink" title="结果说明"></a>结果说明</h3><p>第一列是按照month进行分组</p><p>第二列是按照day进行分组</p><p>第三列是按照month或day分组是，统计这一组有几个不同的cookieid</p><p>第四列grouping_id表示这一组结果属于哪个分组集合，根据grouping sets中的分组条件month，day，1是代表month，2是代表day</p><h3 id="再比如"><a href="#再比如" class="headerlink" title="再比如"></a>再比如</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  <span class="keyword">month</span>, <span class="keyword">day</span>,</span><br><span class="line"><span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,</span><br><span class="line">GROUPING__ID </span><br><span class="line"><span class="keyword">FROM</span> cookie5 </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span> </span><br><span class="line"><span class="keyword">GROUPING</span> <span class="keyword">SETS</span> (<span class="keyword">month</span>,<span class="keyword">day</span>,(<span class="keyword">month</span>,<span class="keyword">day</span>)) </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br></pre></td></tr></table></figure><h4 id="等价于-1"><a href="#等价于-1" class="headerlink" title="等价于"></a>等价于</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="literal">NULL</span>,<span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">1</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> cookie5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span> </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="literal">NULL</span>,<span class="keyword">day</span>,<span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">2</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> cookie5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span></span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">day</span>,<span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">3</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> cookie5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411214043088-850311452.png" alt="img"></p><h2 id="玩一玩CUBE"><a href="#玩一玩CUBE" class="headerlink" title="玩一玩CUBE"></a>玩一玩CUBE</h2><h3 id="说明-1"><a href="#说明-1" class="headerlink" title="说明"></a>说明</h3><p>根据GROUP BY的维度的所有组合进行聚合</p><h3 id="查询语句-1"><a href="#查询语句-1" class="headerlink" title="查询语句"></a>查询语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  <span class="keyword">month</span>, <span class="keyword">day</span>,</span><br><span class="line"><span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,</span><br><span class="line">GROUPING__ID </span><br><span class="line"><span class="keyword">FROM</span> cookie5 </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span> </span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">CUBE</span> </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br></pre></td></tr></table></figure><h4 id="等价于-2"><a href="#等价于-2" class="headerlink" title="等价于"></a>等价于</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="literal">NULL</span>,<span class="literal">NULL</span>,<span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">0</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> cookie5</span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="literal">NULL</span>,<span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">1</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> cookie5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span> </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="literal">NULL</span>,<span class="keyword">day</span>,<span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">2</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> cookie5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span></span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">day</span>,<span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">3</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> cookie5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span></span><br></pre></td></tr></table></figure><h3 id="查询结果-1"><a href="#查询结果-1" class="headerlink" title="查询结果"></a>查询结果</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411214320837-245868149.png" alt="img"></p><h2 id="玩一玩ROLLUP"><a href="#玩一玩ROLLUP" class="headerlink" title="玩一玩ROLLUP"></a>玩一玩ROLLUP</h2><h3 id="说明-2"><a href="#说明-2" class="headerlink" title="说明"></a>说明</h3><p>是CUBE的子集，以最左侧的维度为主，从该维度进行层级聚合</p><h3 id="查询语句-2"><a href="#查询语句-2" class="headerlink" title="查询语句"></a>查询语句</h3><p>– 比如，以month维度进行层级聚合</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  <span class="keyword">month</span>, <span class="keyword">day</span>, <span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv, GROUPING__ID  </span><br><span class="line"><span class="keyword">FROM</span> cookie5 </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span> <span class="keyword">WITH</span> <span class="keyword">ROLLUP</span>  <span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br></pre></td></tr></table></figure><p>可以实现这样的上钻过程：<br>月天的UV-&gt;月的UV-&gt;总UV</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411214644148-1207189210.png" alt="img"></p><p>–把month和day调换顺序，则以day维度进行层级聚合：</p><p>可以实现这样的上钻过程：<br>天月的UV-&gt;天的UV-&gt;总UV<br>（这里，根据天和月进行聚合，和根据天聚合结果一样，因为有父子关系，如果是其他维度组合的话，就会不一样）</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411214849921-1392308790.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （十六）Hive分析窗口函数(四) LAG、LEAD、FIRST_VALUE和LAST_VALUE</title>
      <link href="/2019-04-16-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89Hive%E5%88%86%E6%9E%90%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0(%E5%9B%9B)%20LAG%E3%80%81LEAD%E3%80%81FIRST_VALUE%E5%92%8CLAST_VALUE.html"/>
      <url>/2019-04-16-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89Hive%E5%88%86%E6%9E%90%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0(%E5%9B%9B)%20LAG%E3%80%81LEAD%E3%80%81FIRST_VALUE%E5%92%8CLAST_VALUE.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （十六）Hive分析窗口函数(四) LAG、LEAD、FIRST_VALUE和LAST_VALUE：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （十六）Hive分析窗口函数(四) LAG、LEAD、FIRST_VALUE和LAST_VALUE</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><h3 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h3><p>cookie4.txt</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cookie1,2015-04-10 10:00:02,url2</span><br><span class="line">cookie1,2015-04-10 10:00:00,url1</span><br><span class="line">cookie1,2015-04-10 10:03:04,1url3</span><br><span class="line">cookie1,2015-04-10 10:50:05,url6</span><br><span class="line">cookie1,2015-04-10 11:00:00,url7</span><br><span class="line">cookie1,2015-04-10 10:10:00,url4</span><br><span class="line">cookie1,2015-04-10 10:50:01,url5</span><br><span class="line">cookie2,2015-04-10 10:00:02,url22</span><br><span class="line">cookie2,2015-04-10 10:00:00,url11</span><br><span class="line">cookie2,2015-04-10 10:03:04,1url33</span><br><span class="line">cookie2,2015-04-10 10:50:05,url66</span><br><span class="line">cookie2,2015-04-10 11:00:00,url77</span><br><span class="line">cookie2,2015-04-10 10:10:00,url44</span><br><span class="line">cookie2,2015-04-10 10:50:01,url55</span><br></pre></td></tr></table></figure><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">use cookie;</span><br><span class="line">drop table if exists cookie4;</span><br><span class="line">create table cookie4(cookieid string, createtime string, url string) </span><br><span class="line">row format delimited fields terminated by &apos;,&apos;;</span><br><span class="line">load data local inpath &quot;/home/hadoop/cookie4.txt&quot; into table cookie4;</span><br><span class="line">select * from cookie4;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411201621843-342659508.png" alt="img"></p><h2 id="玩一玩LAG"><a href="#玩一玩LAG" class="headerlink" title="玩一玩LAG"></a>玩一玩LAG</h2><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>LAG(col,n,DEFAULT) 用于统计窗口内往上第n行值</p><blockquote><p>第一个参数为列名，<br>第二个参数为往上第n行（可选，默认为1），<br>第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL）</p></blockquote><h3 id="查询语句"><a href="#查询语句" class="headerlink" title="查询语句"></a>查询语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">  cookieid, </span><br><span class="line">  createtime, </span><br><span class="line">  <span class="keyword">url</span>, </span><br><span class="line">  row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> rn, </span><br><span class="line">  LAG(createtime,<span class="number">1</span>,<span class="string">'1970-01-01 00:00:00'</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> last_1_time, </span><br><span class="line">  LAG(createtime,<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> last_2_time </span><br><span class="line"><span class="keyword">from</span> cookie.cookie4;</span><br></pre></td></tr></table></figure><h3 id="查询结果"><a href="#查询结果" class="headerlink" title="查询结果"></a>查询结果</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411202336839-169636398.png" alt="img"></p><h3 id="结果说明"><a href="#结果说明" class="headerlink" title="结果说明"></a>结果说明</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">last_1_time: 指定了往上第1行的值，default为&apos;1970-01-01 00:00:00&apos;  </span><br><span class="line">　　　　　　　　cookie1第一行，往上1行为NULL,因此取默认值 1970-01-01 00:00:00</span><br><span class="line">　　　　　　　　cookie1第三行，往上1行值为第二行值，2015-04-10 10:00:02</span><br><span class="line">　　　　　　　　cookie1第六行，往上1行值为第五行值，2015-04-10 10:50:01</span><br><span class="line">last_2_time: 指定了往上第2行的值，为指定默认值</span><br><span class="line">　　　　　　　　cookie1第一行，往上2行为NULL</span><br><span class="line">　　　　　　　　cookie1第二行，往上2行为NULL</span><br><span class="line">　　　　　　　　cookie1第四行，往上2行为第二行值，2015-04-10 10:00:02</span><br><span class="line">　　　　　　　　cookie1第七行，往上2行为第五行值，2015-04-10 10:50:01</span><br></pre></td></tr></table></figure><h2 id="玩一玩LEAD"><a href="#玩一玩LEAD" class="headerlink" title="玩一玩LEAD"></a>玩一玩LEAD</h2><h3 id="说明-1"><a href="#说明-1" class="headerlink" title="说明"></a>说明</h3><p>与LAG相反</p><p>LEAD(col,n,DEFAULT) 用于统计窗口内往下第n行值</p><blockquote><p>第一个参数为列名，<br>第二个参数为往下第n行（可选，默认为1），<br>第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL）</p></blockquote><h3 id="查询语句-1"><a href="#查询语句-1" class="headerlink" title="查询语句"></a>查询语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">  cookieid, </span><br><span class="line">  createtime, </span><br><span class="line">  <span class="keyword">url</span>, </span><br><span class="line">  row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> rn, </span><br><span class="line">  <span class="keyword">LEAD</span>(createtime,<span class="number">1</span>,<span class="string">'1970-01-01 00:00:00'</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> next_1_time, </span><br><span class="line">  <span class="keyword">LEAD</span>(createtime,<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> next_2_time </span><br><span class="line"><span class="keyword">from</span> cookie.cookie4;</span><br></pre></td></tr></table></figure><h3 id="查询结果-1"><a href="#查询结果-1" class="headerlink" title="查询结果"></a>查询结果</h3><h3 id><a href="#" class="headerlink" title></a><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411203316368-518018530.png" alt="img"></h3><h3 id="结果说明-1"><a href="#结果说明-1" class="headerlink" title="结果说明"></a>结果说明</h3><p>–逻辑与LAG一样，只不过LAG是往上，LEAD是往下。</p><h2 id="玩一玩FIRST-VALUE"><a href="#玩一玩FIRST-VALUE" class="headerlink" title="玩一玩FIRST_VALUE"></a>玩一玩FIRST_VALUE</h2><h3 id="说明-2"><a href="#说明-2" class="headerlink" title="说明"></a>说明</h3><p>取分组内排序后，截止到当前行，第一个值</p><h3 id="查询语句-2"><a href="#查询语句-2" class="headerlink" title="查询语句"></a>查询语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">  cookieid, </span><br><span class="line">  createtime, </span><br><span class="line">  <span class="keyword">url</span>, </span><br><span class="line">  row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> rn, </span><br><span class="line">  <span class="keyword">first_value</span>(<span class="keyword">url</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> first1 </span><br><span class="line"><span class="keyword">from</span> cookie.cookie4;</span><br></pre></td></tr></table></figure><h3 id="查询结果-2"><a href="#查询结果-2" class="headerlink" title="查询结果"></a>查询结果</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411204128827-493017140.png" alt="img"></p><h2 id="玩一玩LAST-VALUE"><a href="#玩一玩LAST-VALUE" class="headerlink" title="玩一玩LAST_VALUE"></a>玩一玩LAST_VALUE</h2><h3 id="说明-3"><a href="#说明-3" class="headerlink" title="说明"></a>说明</h3><p>取分组内排序后，截止到当前行，最后一个值</p><h3 id="查询语句-3"><a href="#查询语句-3" class="headerlink" title="查询语句"></a>查询语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">  cookieid, </span><br><span class="line">  createtime, </span><br><span class="line">  <span class="keyword">url</span>, </span><br><span class="line">  row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> rn, </span><br><span class="line">  <span class="keyword">last_value</span>(<span class="keyword">url</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> last1 </span><br><span class="line"><span class="keyword">from</span> cookie.cookie4;</span><br></pre></td></tr></table></figure><h3 id="查询结果-3"><a href="#查询结果-3" class="headerlink" title="查询结果"></a>查询结果</h3><h3 id="-1"><a href="#-1" class="headerlink" title></a><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411205221871-1107163476.png" alt="img"></h3><h3 id="如果不指定ORDER-BY，则默认按照记录在文件中的偏移量进行排序，会出现错误的结果"><a href="#如果不指定ORDER-BY，则默认按照记录在文件中的偏移量进行排序，会出现错误的结果" class="headerlink" title="如果不指定ORDER BY，则默认按照记录在文件中的偏移量进行排序，会出现错误的结果"></a>如果不指定ORDER BY，则默认按照记录在文件中的偏移量进行排序，会出现错误的结果</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411205841092-1288201073.png" alt="img"></p><h3 id="如果想要取分组内排序后最后一个值，则需要变通一下"><a href="#如果想要取分组内排序后最后一个值，则需要变通一下" class="headerlink" title="如果想要取分组内排序后最后一个值，则需要变通一下"></a><strong>如果想要取分组内排序后最后一个值，则需要变通一下</strong></h3><h4 id="查询语句-4"><a href="#查询语句-4" class="headerlink" title="查询语句"></a><strong>查询语句</strong></h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">  cookieid, </span><br><span class="line">  createtime, </span><br><span class="line">  <span class="keyword">url</span>, </span><br><span class="line">  row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> rn,</span><br><span class="line">  <span class="keyword">LAST_VALUE</span>(<span class="keyword">url</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> last1,</span><br><span class="line">  <span class="keyword">FIRST_VALUE</span>(<span class="keyword">url</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">desc</span>) <span class="keyword">as</span> last2 </span><br><span class="line"><span class="keyword">from</span> cookie.cookie4 </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> cookieid,createtime;</span><br></pre></td></tr></table></figure><h4 id="查询结果-4"><a href="#查询结果-4" class="headerlink" title="查询结果"></a>查询结果</h4><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411211123369-588038997.png" alt="img"></p><p><strong>提示：在使用分析函数的过程中，要特别注意ORDER BY子句，用的不恰当，统计出的结果就不是你所期望的。</strong></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （十五）Hive分析窗口函数(三) CUME_DIST和PERCENT_RANK</title>
      <link href="/2019-04-15-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89Hive%E5%88%86%E6%9E%90%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0(%E4%B8%89)%20CUME_DIST%E5%92%8CPERCENT_RANK.html"/>
      <url>/2019-04-15-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89Hive%E5%88%86%E6%9E%90%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0(%E4%B8%89)%20CUME_DIST%E5%92%8CPERCENT_RANK.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （十五）Hive分析窗口函数(三) CUME_DIST和PERCENT_RANK：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （十五）Hive分析窗口函数(三) CUME_DIST和PERCENT_RANK</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><h3 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h3><p>cookie3.txt</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">d1,user1,1000</span><br><span class="line">d1,user2,2000</span><br><span class="line">d1,user3,3000</span><br><span class="line">d2,user4,4000</span><br><span class="line">d2,user5,5000</span><br></pre></td></tr></table></figure><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> cookie;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> cookie3;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> cookie3(dept <span class="keyword">string</span>, userid <span class="keyword">string</span>, sal <span class="built_in">int</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/cookie3.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> cookie3;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> cookie3;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411194816992-676797656.png" alt="img"></p><h2 id="玩一玩CUME-DIST"><a href="#玩一玩CUME-DIST" class="headerlink" title="玩一玩CUME_DIST"></a>玩一玩CUME_DIST</h2><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>–<strong>CUME_DIST ：</strong>小于等于当前值的行数/分组内总行数</p><h3 id="查询语句"><a href="#查询语句" class="headerlink" title="查询语句"></a>查询语句</h3><p>比如，统计小于等于当前薪水的人数，所占总人数的比例</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">  dept,</span><br><span class="line">  userid,</span><br><span class="line">  sal,</span><br><span class="line">  <span class="keyword">cume_dist</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> sal) <span class="keyword">as</span> rn1,</span><br><span class="line">  <span class="keyword">cume_dist</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> sal) <span class="keyword">as</span> rn2</span><br><span class="line"><span class="keyword">from</span> cookie.cookie3;</span><br></pre></td></tr></table></figure><h3 id="查询结果"><a href="#查询结果" class="headerlink" title="查询结果"></a>查询结果</h3><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411195322786-2102329526.png" alt="img"></p><h3 id="结果说明"><a href="#结果说明" class="headerlink" title="结果说明"></a>结果说明</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rn1: 没有partition,所有数据均为1组，总行数为5，</span><br><span class="line">     第一行：小于等于1000的行数为1，因此，1/5=0.2</span><br><span class="line">     第三行：小于等于3000的行数为3，因此，3/5=0.6</span><br><span class="line">rn2: 按照部门分组，dpet=d1的行数为3,</span><br><span class="line">     第二行：小于等于2000的行数为2，因此，2/3=0.6666666666666666</span><br></pre></td></tr></table></figure><h2 id="玩一玩PERCENT-RANK"><a href="#玩一玩PERCENT-RANK" class="headerlink" title="玩一玩PERCENT_RANK"></a>玩一玩PERCENT_RANK</h2><h3 id="说明-1"><a href="#说明-1" class="headerlink" title="说明"></a>说明</h3><p> <strong>–PERCENT_RANK</strong> ：分组内当前行的RANK值-1/分组内总行数-1</p><h3 id="查询语句-1"><a href="#查询语句-1" class="headerlink" title="查询语句"></a>查询语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">  dept,</span><br><span class="line">  userid,</span><br><span class="line">  sal,</span><br><span class="line">  <span class="keyword">percent_rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> sal) <span class="keyword">as</span> rn1, <span class="comment">--分组内</span></span><br><span class="line">  <span class="keyword">rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> sal) <span class="keyword">as</span> rn11, <span class="comment">--分组内的rank值</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="number">1</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> <span class="literal">null</span>) <span class="keyword">as</span> rn12, <span class="comment">--分组内总行数</span></span><br><span class="line">  <span class="keyword">percent_rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> sal) <span class="keyword">as</span> rn2,</span><br><span class="line">  <span class="keyword">rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dept <span class="keyword">order</span> <span class="keyword">by</span> sal) <span class="keyword">as</span> rn21,</span><br><span class="line">  <span class="keyword">sum</span>(<span class="number">1</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> dept) <span class="keyword">as</span> rn22 </span><br><span class="line"><span class="keyword">from</span> cookie.cookie3;</span><br></pre></td></tr></table></figure><h3 id="查询结果-1"><a href="#查询结果-1" class="headerlink" title="查询结果"></a>查询结果</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411201215380-2136016706.png" alt="img"></p><h3 id="结果说明-1"><a href="#结果说明-1" class="headerlink" title="结果说明"></a>结果说明</h3><p><strong>–PERCENT_RANK</strong> ：分组内当前行的RANK值-1/分组内总行数-1</p><p>rn1 ==  (rn11-1) / (rn12-1)</p><p>rn2 ==  (rn21-1) / (rn22-1)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rn1: rn1 = (rn11-1) / (rn12-1) </span><br><span class="line">       第一行,(1-1)/(5-1)=0/4=0</span><br><span class="line">       第二行,(2-1)/(5-1)=1/4=0.25</span><br><span class="line">       第四行,(4-1)/(5-1)=3/4=0.75</span><br><span class="line">rn2: 按照dept分组，</span><br><span class="line">     dept=d1的总行数为3</span><br><span class="line">     第一行，(1-1)/(3-1)=0</span><br><span class="line">     第三行，(3-1)/(3-1)=1</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （十四）Hive分析窗口函数(二) NTILE,ROW_NUMBER,RANK,DENSE_RANK</title>
      <link href="/2019-04-14-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89Hive%E5%88%86%E6%9E%90%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0(%E4%BA%8C)%20NTILE,ROW_NUMBER,RANK,DENSE_RANK.html"/>
      <url>/2019-04-14-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89Hive%E5%88%86%E6%9E%90%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0(%E4%BA%8C)%20NTILE,ROW_NUMBER,RANK,DENSE_RANK.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （十四）Hive分析窗口函数(二) NTILE,ROW_NUMBER,RANK,DENSE_RANK：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        本文中介绍前几个序列函数，NTILE,ROW_NUMBER,RANK,DENSE_RANK，下面会一一解释各自的用途。</p><p><strong>注意： 序列函数不支持WINDOW子句。（ROWS BETWEEN）</strong></p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><h3 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cookie1,2015-04-10,1</span><br><span class="line">cookie1,2015-04-11,5</span><br><span class="line">cookie1,2015-04-12,7</span><br><span class="line">cookie1,2015-04-13,3</span><br><span class="line">cookie1,2015-04-14,2</span><br><span class="line">cookie1,2015-04-15,4</span><br><span class="line">cookie1,2015-04-16,4</span><br><span class="line">cookie2,2015-04-10,2</span><br><span class="line">cookie2,2015-04-11,3</span><br><span class="line">cookie2,2015-04-12,5</span><br><span class="line">cookie2,2015-04-13,6</span><br><span class="line">cookie2,2015-04-14,3</span><br><span class="line">cookie2,2015-04-15,9</span><br><span class="line">cookie2,2015-04-16,7</span><br></pre></td></tr></table></figure><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> cookie;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> cookie2;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> cookie2(cookieid <span class="keyword">string</span>, createtime <span class="keyword">string</span>, pv <span class="built_in">int</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/cookie2.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> cookie2;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> cookie2;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411190330410-382259848.png" alt="img"></p><h2 id="玩一玩NTILE"><a href="#玩一玩NTILE" class="headerlink" title="玩一玩NTILE"></a>玩一玩NTILE</h2><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>NTILE(n)，用于将分组数据按照顺序切分成n片，返回当前切片值<br>NTILE不支持ROWS BETWEEN，比如 NTILE(2) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)<br>如果切片不均匀，默认增加第一个切片的分布</p><h3 id="查询语句"><a href="#查询语句" class="headerlink" title="查询语句"></a>查询语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  cookieid,</span><br><span class="line">  createtime,</span><br><span class="line">  pv,</span><br><span class="line">  ntile(<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> rn1, <span class="comment">--分组内将数据分成2片</span></span><br><span class="line">  ntile(<span class="number">3</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> rn2, <span class="comment">--分组内将数据分成2片</span></span><br><span class="line">  ntile(<span class="number">4</span>) <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> rn3 <span class="comment">--将所有数据分成4片</span></span><br><span class="line"><span class="keyword">from</span> cookie.cookie2 </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> cookieid,createtime;</span><br></pre></td></tr></table></figure><h3 id="查询结果"><a href="#查询结果" class="headerlink" title="查询结果"></a>查询结果</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411191813159-31132130.png" alt="img"></p><h3 id="比如，统计一个cookie，pv数最多的前1-3的天"><a href="#比如，统计一个cookie，pv数最多的前1-3的天" class="headerlink" title="比如，统计一个cookie，pv数最多的前1/3的天"></a>比如，统计一个cookie，pv数最多的前1/3的天</h3><h4 id="查询语句-1"><a href="#查询语句-1" class="headerlink" title="查询语句"></a>查询语句</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  cookieid,</span><br><span class="line">  createtime,</span><br><span class="line">  pv,</span><br><span class="line">  ntile(<span class="number">3</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> pv <span class="keyword">desc</span> ) <span class="keyword">as</span> rn </span><br><span class="line"><span class="keyword">from</span> cookie.cookie2;</span><br></pre></td></tr></table></figure><h4 id="查询结果-1"><a href="#查询结果-1" class="headerlink" title="查询结果"></a>查询结果</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411192539356-1545943983.png" alt="img"></p><p>–rn = 1 的记录，就是我们想要的结果</p><h2 id="玩一玩ROW-NUMBER"><a href="#玩一玩ROW-NUMBER" class="headerlink" title="玩一玩ROW_NUMBER"></a>玩一玩ROW_NUMBER</h2><h3 id="说明-1"><a href="#说明-1" class="headerlink" title="说明"></a>说明</h3><p>ROW_NUMBER() –从1开始，按照顺序，生成分组内记录的序列<br>–比如，按照pv降序排列，生成分组内每天的pv名次<br>ROW_NUMBER() 的应用场景非常多，再比如，获取分组内排序第一的记录;获取一个session中的第一条refer等。</p><h3 id="分组排序"><a href="#分组排序" class="headerlink" title="分组排序"></a>分组排序</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  cookieid,</span><br><span class="line">  createtime,</span><br><span class="line">  pv,</span><br><span class="line">  row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> pv <span class="keyword">desc</span>) <span class="keyword">as</span> rn</span><br><span class="line"><span class="keyword">from</span> cookie.cookie2;</span><br></pre></td></tr></table></figure><h3 id="查询结果-2"><a href="#查询结果-2" class="headerlink" title="查询结果"></a>查询结果</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411193336998-1717025751.png" alt="img"></p><p>– 所以如果需要取每一组的前3名，只需要rn&lt;=3即可，适合TopN</p><h2 id="玩一玩RANK-和-DENSE-RANK"><a href="#玩一玩RANK-和-DENSE-RANK" class="headerlink" title="玩一玩RANK 和 DENSE_RANK"></a>玩一玩RANK 和 DENSE_RANK</h2><p>—RANK() 生成数据项在分组中的排名，排名相等会在名次中留下空位<br>—DENSE_RANK() 生成数据项在分组中的排名，排名相等会在名次中不会留下空位</p><h3 id="查询语句-2"><a href="#查询语句-2" class="headerlink" title="查询语句"></a>查询语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  cookieid,</span><br><span class="line">  createtime,</span><br><span class="line">  pv,</span><br><span class="line">  <span class="keyword">rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> pv <span class="keyword">desc</span>) <span class="keyword">as</span> rn1,</span><br><span class="line">  <span class="keyword">dense_rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> pv <span class="keyword">desc</span>) <span class="keyword">as</span> rn2,</span><br><span class="line">  row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> pv <span class="keyword">desc</span>) <span class="keyword">as</span> rn3</span><br><span class="line"><span class="keyword">from</span> cookie.cookie2 </span><br><span class="line"><span class="keyword">where</span> cookieid=<span class="string">'cookie1'</span>;</span><br></pre></td></tr></table></figure><h3 id="查询结果-3"><a href="#查询结果-3" class="headerlink" title="查询结果"></a>查询结果</h3><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180411193913711-1567210105.png" alt="img"></p><h2 id="ROW-NUMBER、RANK和DENSE-RANK的区别"><a href="#ROW-NUMBER、RANK和DENSE-RANK的区别" class="headerlink" title="ROW_NUMBER、RANK和DENSE_RANK的区别"></a>ROW_NUMBER、RANK和DENSE_RANK的区别</h2><p><strong>row_number</strong>： 按顺序编号，不留空位<br><strong>rank</strong>： 按顺序编号，相同的值编相同号，留空位<br><strong>dense_rank</strong>： 按顺序编号，相同的值编相同的号，不留空位</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （十三）Hive分析窗口函数(一) SUM,AVG,MIN,MAX</title>
      <link href="/2019-04-13-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89Hive%E5%88%86%E6%9E%90%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0(%E4%B8%80)%20SUM,AVG,MIN,MAX.html"/>
      <url>/2019-04-13-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89Hive%E5%88%86%E6%9E%90%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0(%E4%B8%80)%20SUM,AVG,MIN,MAX.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （十三）Hive分析窗口函数(一) SUM,AVG,MIN,MAX：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （十三）Hive分析窗口函数(一) SUM,AVG,MIN,MAX</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><h3 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cookie1,2015-04-10,1</span><br><span class="line">cookie1,2015-04-11,5</span><br><span class="line">cookie1,2015-04-12,7</span><br><span class="line">cookie1,2015-04-13,3</span><br><span class="line">cookie1,2015-04-14,2</span><br><span class="line">cookie1,2015-04-15,4</span><br><span class="line">cookie1,2015-04-16,4</span><br></pre></td></tr></table></figure><h3 id="创建数据库及表"><a href="#创建数据库及表" class="headerlink" title="创建数据库及表"></a>创建数据库及表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists cookie;</span><br><span class="line">use cookie;</span><br><span class="line">drop table if exists cookie1;</span><br><span class="line">create table cookie1(cookieid string, createtime string, pv int) row format delimited fields terminated by &apos;,&apos;;</span><br><span class="line">load data local inpath &quot;/home/hadoop/cookie1.txt&quot; into table cookie1;</span><br><span class="line">select * from cookie1;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410205530168-1924440479.png" alt="img"></p><h2 id="玩一玩SUM"><a href="#玩一玩SUM" class="headerlink" title="玩一玩SUM"></a>玩一玩SUM</h2><h3 id="查询语句"><a href="#查询语句" class="headerlink" title="查询语句"></a>查询语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">   cookieid, </span><br><span class="line">   createtime, </span><br><span class="line">   pv, </span><br><span class="line">   <span class="keyword">sum</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> pv1, </span><br><span class="line">   <span class="keyword">sum</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> pv2, </span><br><span class="line">   <span class="keyword">sum</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid) <span class="keyword">as</span> pv3, </span><br><span class="line">   <span class="keyword">sum</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> pv4, </span><br><span class="line">   <span class="keyword">sum</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="number">1</span> <span class="keyword">following</span>) <span class="keyword">as</span> pv5, </span><br><span class="line">   <span class="keyword">sum</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">unbounded</span> <span class="keyword">following</span>) <span class="keyword">as</span> pv6 </span><br><span class="line"><span class="keyword">from</span> cookie1;</span><br></pre></td></tr></table></figure><h3 id="查询结果"><a href="#查询结果" class="headerlink" title="查询结果"></a>查询结果</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410211647233-1245106876.png" alt="img"></p><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pv1: 分组内从起点到当前行的pv累积，如，11号的pv1=10号的pv+11号的pv, 12号=10号+11号+12号</span><br><span class="line">pv2: 同pv1</span><br><span class="line">pv3: 分组内(cookie1)所有的pv累加</span><br><span class="line">pv4: 分组内当前行+往前3行，如，11号=10号+11号， 12号=10号+11号+12号， 13号=10号+11号+12号+13号， 14号=11号+12号+13号+14号</span><br><span class="line">pv5: 分组内当前行+往前3行+往后1行，如，14号=11号+12号+13号+14号+15号=5+7+3+2+4=21</span><br><span class="line">pv6: 分组内当前行+往后所有行，如，13号=13号+14号+15号+16号=3+2+4+4=13，14号=14号+15号+16号=2+4+4=10</span><br></pre></td></tr></table></figure><p>如果不指定ROWS BETWEEN,默认为从起点到当前行;<br>如果不指定ORDER BY，则将分组内所有值累加;<br>关键是理解ROWS BETWEEN含义,也叫做WINDOW子句：<br>PRECEDING：往前<br>FOLLOWING：往后<br>CURRENT ROW：当前行<br>UNBOUNDED：起点，</p><p>　　UNBOUNDED PRECEDING 表示从前面的起点，</p><p>　　UNBOUNDED FOLLOWING：表示到后面的终点<br>–其他AVG，MIN，MAX，和SUM用法一样。</p><h2 id="玩一玩AVG"><a href="#玩一玩AVG" class="headerlink" title="玩一玩AVG"></a>玩一玩AVG</h2><h3 id="查询语句-1"><a href="#查询语句-1" class="headerlink" title="查询语句"></a>查询语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">   cookieid, </span><br><span class="line">   createtime, </span><br><span class="line">   pv, </span><br><span class="line">   <span class="keyword">avg</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> pv1, <span class="comment">-- 默认为从起点到当前行</span></span><br><span class="line">   <span class="keyword">avg</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> pv2, <span class="comment">--从起点到当前行，结果同pv1</span></span><br><span class="line">   <span class="keyword">avg</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid) <span class="keyword">as</span> pv3, <span class="comment">--分组内所有行</span></span><br><span class="line">   <span class="keyword">avg</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> pv4, <span class="comment">--当前行+往前3行</span></span><br><span class="line">   <span class="keyword">avg</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="number">1</span> <span class="keyword">following</span>) <span class="keyword">as</span> pv5, <span class="comment">--当前行+往前3行+往后1行</span></span><br><span class="line">   <span class="keyword">avg</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">unbounded</span> <span class="keyword">following</span>) <span class="keyword">as</span> pv6  <span class="comment">--当前行+往后所有行</span></span><br><span class="line"><span class="keyword">from</span> cookie1;</span><br></pre></td></tr></table></figure><h3 id="查询结果-1"><a href="#查询结果-1" class="headerlink" title="查询结果"></a>查询结果</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410212726807-1076693371.png" alt="img"></p><h2 id="玩一玩MIN"><a href="#玩一玩MIN" class="headerlink" title="玩一玩MIN"></a>玩一玩MIN</h2><h3 id="查询语句-2"><a href="#查询语句-2" class="headerlink" title="查询语句"></a>查询语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">   cookieid, </span><br><span class="line">   createtime, </span><br><span class="line">   pv, </span><br><span class="line">   <span class="keyword">min</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> pv1, <span class="comment">-- 默认为从起点到当前行</span></span><br><span class="line">   <span class="keyword">min</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> pv2, <span class="comment">--从起点到当前行，结果同pv1</span></span><br><span class="line">   <span class="keyword">min</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid) <span class="keyword">as</span> pv3, <span class="comment">--分组内所有行</span></span><br><span class="line">   <span class="keyword">min</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> pv4, <span class="comment">--当前行+往前3行</span></span><br><span class="line">   <span class="keyword">min</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="number">1</span> <span class="keyword">following</span>) <span class="keyword">as</span> pv5, <span class="comment">--当前行+往前3行+往后1行</span></span><br><span class="line">   <span class="keyword">min</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">unbounded</span> <span class="keyword">following</span>) <span class="keyword">as</span> pv6  <span class="comment">--当前行+往后所有行</span></span><br><span class="line"><span class="keyword">from</span> cookie1;</span><br></pre></td></tr></table></figure><h3 id="查询结果-2"><a href="#查询结果-2" class="headerlink" title="查询结果"></a>查询结果</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410212927297-2042376017.png" alt="img"></p><h2 id="玩一玩MAX"><a href="#玩一玩MAX" class="headerlink" title="玩一玩MAX"></a>玩一玩MAX</h2><h3 id="查询语句-3"><a href="#查询语句-3" class="headerlink" title="查询语句"></a>查询语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">   cookieid, </span><br><span class="line">   createtime, </span><br><span class="line">   pv, </span><br><span class="line">   <span class="keyword">max</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> pv1, <span class="comment">-- 默认为从起点到当前行</span></span><br><span class="line">   <span class="keyword">max</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> pv2, <span class="comment">--从起点到当前行，结果同pv1</span></span><br><span class="line">   <span class="keyword">max</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid) <span class="keyword">as</span> pv3, <span class="comment">--分组内所有行</span></span><br><span class="line">   <span class="keyword">max</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> pv4, <span class="comment">--当前行+往前3行</span></span><br><span class="line">   <span class="keyword">max</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="number">1</span> <span class="keyword">following</span>) <span class="keyword">as</span> pv5, <span class="comment">--当前行+往前3行+往后1行</span></span><br><span class="line">   <span class="keyword">max</span>(pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">unbounded</span> <span class="keyword">following</span>) <span class="keyword">as</span> pv6  <span class="comment">--当前行+往后所有行</span></span><br><span class="line"><span class="keyword">from</span> cookie1;</span><br></pre></td></tr></table></figure><h3 id="查询结果-3"><a href="#查询结果-3" class="headerlink" title="查询结果"></a>查询结果</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410213346492-1184970532.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （十二）Hive SQL练习之影评案例</title>
      <link href="/2019-04-12-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89Hive%20SQL%E7%BB%83%E4%B9%A0%E4%B9%8B%E5%BD%B1%E8%AF%84%E6%A1%88%E4%BE%8B.html"/>
      <url>/2019-04-12-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89Hive%20SQL%E7%BB%83%E4%B9%A0%E4%B9%8B%E5%BD%B1%E8%AF%84%E6%A1%88%E4%BE%8B.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （十二）Hive SQL练习之影评案例：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （十二）Hive SQL练习之影评案例</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="案例说明"><a href="#案例说明" class="headerlink" title="案例说明"></a>案例说明</h2><p>现有如此三份数据：<br>1、users.dat 数据格式为： 2::M::56::16::70072，</p><p>共有6040条数据<br>对应字段为：UserID BigInt, Gender String, Age Int, Occupation String, Zipcode String<br>对应字段中文解释：用户id，性别，年龄，职业，邮政编码</p><p>2、movies.dat    数据格式为： 2::Jumanji (1995)::Adventure|Children’s|Fantasy，</p><p>共有3883条数据<br>对应字段为：MovieID BigInt, Title String, Genres String<br>对应字段中文解释：电影ID，电影名字，电影类型</p><p>3、ratings.dat    数据格式为： 1::1193::5::978300760，</p><p>共有1000209条数据<br>对应字段为：UserID BigInt, MovieID BigInt, Rating Double, Timestamped String<br>对应字段中文解释：用户ID，电影ID，评分，评分时间戳</p><p>题目要求</p><p>　　数据要求：<br>　　　　（1）写shell脚本清洗数据。（hive不支持解析多字节的分隔符，也就是说hive只能解析’:’, 不支持解析’::’，所以用普通方式建表来使用是行不通的，要求对数据做一次简单清洗）<br>　　　　（2）使用Hive能解析的方式进行</p><p>　　Hive要求：<br>　　　　（1）正确建表，导入数据（三张表，三份数据），并验证是否正确</p><p>　　　　（2）求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）</p><p>　　　　（3）分别求男性，女性当中评分最高的10部电影（性别，电影名，影评分）</p><p>　　　　（4）求movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，影评分）</p><p>　　　　（5）求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（观影者，电影名，影评分）</p><p>　　　　（6）求好片（评分&gt;=4.0）最多的那个年份的最好看的10部电影</p><p>　　　　（7）求1997年上映的电影中，评分最高的10部Comedy类电影</p><p>　　　　（8）该影评库中各种类型电影中评价最高的5部电影（类型，电影名，平均影评分）</p><p>　　　　（9）各年评分最高的电影类型（年份，类型，影评分）</p><p>　　　　（10）每个地区最高评分的电影名，把结果存入HDFS（地区，电影名，影评分）</p><h2 id="数据下载"><a href="#数据下载" class="headerlink" title="数据下载"></a>数据下载</h2><p><a href="https://files.cnblogs.com/files/qingyunzong/hive影评案例.zip" target="_blank" rel="noopener">https://files.cnblogs.com/files/qingyunzong/hive%E5%BD%B1%E8%AF%84%E6%A1%88%E4%BE%8B.zip</a></p><h2 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h2><p>之前已经使用MapReduce程序将3张表格进行合并，所以只需要将合并之后的表格导入对应的表中进行查询即可。</p><h3 id="1、正确建表，导入数据（三张表，三份数据），并验证是否正确"><a href="#1、正确建表，导入数据（三张表，三份数据），并验证是否正确" class="headerlink" title="1、正确建表，导入数据（三张表，三份数据），并验证是否正确"></a>1、正确建表，导入数据（三张表，三份数据），并验证是否正确</h3><h4 id="（1）分析需求"><a href="#（1）分析需求" class="headerlink" title="（1）分析需求"></a>（1）分析需求</h4><p>需要创建一个数据库movie，在movie数据库中创建3张表，t_user，t_movie，t_rating</p><blockquote><p><strong>t_user</strong>:userid bigint,sex string,age int,occupation string,zipcode string<br><strong>t_movie</strong>:movieid bigint,moviename string,movietype string<br><strong>t_rating</strong>:userid bigint,movieid bigint,rate double,times string</p></blockquote><p>原始数据是以::进行切分的，所以需要使用能解析多字节分隔符的Serde即可</p><p>使用RegexSerde</p><p>需要两个参数：<br>input.regex = “(.<em>)::(.</em>)::(.*)”<br>output.format.string = “%1$s %2$s %3$s”</p><h4 id="（2）创建数据库"><a href="#（2）创建数据库" class="headerlink" title="（2）创建数据库"></a>（2）创建数据库</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">exists</span> movie;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> movie;</span><br><span class="line"><span class="keyword">use</span> movie;</span><br></pre></td></tr></table></figure><h4 id="（3）创建t-user表"><a href="#（3）创建t-user表" class="headerlink" title="（3）创建t_user表"></a>（3）创建t_user表</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_user(</span><br><span class="line">userid <span class="built_in">bigint</span>,</span><br><span class="line">sex <span class="keyword">string</span>,</span><br><span class="line">age <span class="built_in">int</span>,</span><br><span class="line">occupation <span class="keyword">string</span>,</span><br><span class="line">zipcode <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> serde <span class="string">'org.apache.hadoop.hive.serde2.RegexSerDe'</span> </span><br><span class="line"><span class="keyword">with</span> serdeproperties(<span class="string">'input.regex'</span>=<span class="string">'(.*)::(.*)::(.*)::(.*)::(.*)'</span>,<span class="string">'output.format.string'</span>=<span class="string">'%1$s %2$s %3$s %4$s %5$s'</span>)</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure><h4 id="（4）创建t-movie表"><a href="#（4）创建t-movie表" class="headerlink" title="（4）创建t_movie表"></a>（4）创建t_movie表</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> movie;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_movie(</span><br><span class="line">movieid <span class="built_in">bigint</span>,</span><br><span class="line">moviename <span class="keyword">string</span>,</span><br><span class="line">movietype <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> serde <span class="string">'org.apache.hadoop.hive.serde2.RegexSerDe'</span> </span><br><span class="line"><span class="keyword">with</span> serdeproperties(<span class="string">'input.regex'</span>=<span class="string">'(.*)::(.*)::(.*)'</span>,<span class="string">'output.format.string'</span>=<span class="string">'%1$s %2$s %3$s'</span>)</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure><h4 id="（5）创建t-rating表"><a href="#（5）创建t-rating表" class="headerlink" title="（5）创建t_rating表"></a>（5）创建t_rating表</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> movie;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_rating(</span><br><span class="line">userid <span class="built_in">bigint</span>,</span><br><span class="line">movieid <span class="built_in">bigint</span>,</span><br><span class="line">rate <span class="keyword">double</span>,</span><br><span class="line">times <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> serde <span class="string">'org.apache.hadoop.hive.serde2.RegexSerDe'</span> </span><br><span class="line"><span class="keyword">with</span> serdeproperties(<span class="string">'input.regex'</span>=<span class="string">'(.*)::(.*)::(.*)::(.*)'</span>,<span class="string">'output.format.string'</span>=<span class="string">'%1$s %2$s %3$s %4$s'</span>)</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure><h4 id="（6）导入数据"><a href="#（6）导入数据" class="headerlink" title="（6）导入数据"></a>（6）导入数据</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; load data local inpath "/home/hadoop/movie/users.dat" into table t_user;</span><br><span class="line">No rows affected (0.928 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt; load data local inpath "/home/hadoop/movie/movies.dat" into table t_movie;</span><br><span class="line">No rows affected (0.538 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt; load data local inpath "/home/hadoop/movie/ratings.dat" into table t_rating;</span><br><span class="line">No rows affected (0.963 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><h4 id="（7）验证"><a href="#（7）验证" class="headerlink" title="（7）验证"></a>（7）验证</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t.* <span class="keyword">from</span> t_user t;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410163511811-1216719632.png" alt="img"></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t.* <span class="keyword">from</span> t_movie t;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410163646769-516781465.png" alt="img"></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t.* <span class="keyword">from</span> t_rating t;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410163751235-1412738103.png" alt="img"></p><h3 id="2、求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）"><a href="#2、求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）" class="headerlink" title="2、求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）"></a>2、求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）</h3><h4 id="（1）思路分析："><a href="#（1）思路分析：" class="headerlink" title="（1）思路分析："></a>（1）思路分析：</h4><p>　　1、需求字段：电影名        t_movie.moviename</p><p>　　　　　　　　  评分次数    t_rating.rate          count()</p><p>　　2、核心SQL：按照电影名进行分组统计，求出每部电影的评分次数并按照评分次数降序排序</p><h4 id="（2）完整SQL："><a href="#（2）完整SQL：" class="headerlink" title="（2）完整SQL："></a>（2）完整SQL：</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer2 <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> a.moviename <span class="keyword">as</span> moviename,<span class="keyword">count</span>(a.moviename) <span class="keyword">as</span> total </span><br><span class="line"><span class="keyword">from</span> t_movie a <span class="keyword">join</span> t_rating b <span class="keyword">on</span> a.movieid=b.movieid </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.moviename </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> total <span class="keyword">desc</span> </span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> answer2;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410164633081-629022259.png" alt="img"></p><h3 id="3、分别求男性，女性当中评分最高的10部电影（性别，电影名，影评分）"><a href="#3、分别求男性，女性当中评分最高的10部电影（性别，电影名，影评分）" class="headerlink" title="3、分别求男性，女性当中评分最高的10部电影（性别，电影名，影评分）"></a>3、分别求男性，女性当中评分最高的10部电影（性别，电影名，影评分）</h3><h4 id="（1）分析思路："><a href="#（1）分析思路：" class="headerlink" title="（1）分析思路："></a>（1）分析思路：</h4><p>　　1、需求字段：性别　　t_user.sex</p><p>　　　　　　　　  电影名　t_movie.moviename</p><p>　　　　　　　　  影评分　t_rating.rate</p><p>　　2、核心SQL：三表联合查询，按照性别过滤条件，电影名作为分组条件，影评分作为排序条件进行查询</p><h4 id="（2）完整SQL：-1"><a href="#（2）完整SQL：-1" class="headerlink" title="（2）完整SQL："></a>（2）完整SQL：</h4><p>女性当中评分最高的10部电影（性别，电影名，影评分）评论次数大于等于50次</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer3_F <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> <span class="string">"F"</span> <span class="keyword">as</span> sex, c.moviename <span class="keyword">as</span> <span class="keyword">name</span>, <span class="keyword">avg</span>(a.rate) <span class="keyword">as</span> avgrate, <span class="keyword">count</span>(c.moviename) <span class="keyword">as</span> total  </span><br><span class="line"><span class="keyword">from</span> t_rating a </span><br><span class="line"><span class="keyword">join</span> t_user b <span class="keyword">on</span> a.userid=b.userid </span><br><span class="line"><span class="keyword">join</span> t_movie c <span class="keyword">on</span> a.movieid=c.movieid </span><br><span class="line"><span class="keyword">where</span> b.sex=<span class="string">"F"</span> </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> c.moviename </span><br><span class="line"><span class="keyword">having</span> total &gt;= <span class="number">50</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> avgrate <span class="keyword">desc</span> </span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> answer3_F；</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410170214113-1840938708.png" alt="img"></p><p>男性当中评分最高的10部电影（性别，电影名，影评分）评论次数大于等于50次</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer3_M <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> <span class="string">"M"</span> <span class="keyword">as</span> sex, c.moviename <span class="keyword">as</span> <span class="keyword">name</span>, <span class="keyword">avg</span>(a.rate) <span class="keyword">as</span> avgrate, <span class="keyword">count</span>(c.moviename) <span class="keyword">as</span> total  </span><br><span class="line"><span class="keyword">from</span> t_rating a </span><br><span class="line"><span class="keyword">join</span> t_user b <span class="keyword">on</span> a.userid=b.userid </span><br><span class="line"><span class="keyword">join</span> t_movie c <span class="keyword">on</span> a.movieid=c.movieid </span><br><span class="line"><span class="keyword">where</span> b.sex=<span class="string">"M"</span> </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> c.moviename </span><br><span class="line"><span class="keyword">having</span> total &gt;= <span class="number">50</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> avgrate <span class="keyword">desc</span> </span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> answer3_M；</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410170316416-1654694572.png" alt="img"></p><h3 id="4、求movieid-2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，影评分）"><a href="#4、求movieid-2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，影评分）" class="headerlink" title="4、求movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，影评分）"></a>4、求movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，影评分）</h3><h4 id="（1）分析思路：-1"><a href="#（1）分析思路：-1" class="headerlink" title="（1）分析思路："></a>（1）分析思路：</h4><p>　　1、需求字段：年龄段　　t_user.age</p><p>　　　　　　　　  影评分　t_rating.rate</p><p>　　2、核心SQL：t_user和t_rating表进行联合查询，用movieid=2116作为过滤条件，用年龄段作为分组条件</p><h4 id="（2）完整SQL：-2"><a href="#（2）完整SQL：-2" class="headerlink" title="（2）完整SQL："></a>（2）完整SQL：</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer4 <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> a.age <span class="keyword">as</span> age, <span class="keyword">avg</span>(b.rate) <span class="keyword">as</span> avgrate </span><br><span class="line"><span class="keyword">from</span> t_user a <span class="keyword">join</span> t_rating b <span class="keyword">on</span> a.userid=b.userid </span><br><span class="line"><span class="keyword">where</span> b.movieid=<span class="number">2116</span> </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.age;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> answer4;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410170836707-1154639273.png" alt="img"></p><h3 id="5、求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（观影者，电影名，影评分）"><a href="#5、求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（观影者，电影名，影评分）" class="headerlink" title="5、求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（观影者，电影名，影评分）"></a>5、求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（观影者，电影名，影评分）</h3><h4 id="（1）分析思路：-2"><a href="#（1）分析思路：-2" class="headerlink" title="（1）分析思路："></a>（1）分析思路：</h4><p>　　1、需求字段：观影者　t_rating.userid</p><p>　　　　　　　　  电影名　t_movie.moviename</p><p>　　　　　　　　  影评分　t_rating.rate</p><p>　　2、核心SQL：</p><p>　　　　A.　　需要先求出最喜欢看电影的那位女性</p><p>　　　　　　　　　　需要查询的字段：性别：t_user.sex</p><p>　　　　　　　　　　　　　　　　　　观影次数：count(t_rating.userid)</p><p>　　　　B.　　根据A中求出的女性userid作为where过滤条件，以看过的电影的影评分rate作为排序条件进行排序，求出评分最高的10部电影</p><p>　　　　　　　　　　需要查询的字段：电影的ID：t_rating.movieid</p><p>　　　　C.　　求出B中10部电影的平均影评分</p><p>　　　　　　　　　　需要查询的字段：电影的ID：answer5_B.movieid</p><p>　　　　　　　　　　　　　　　　　　影评分：t_rating.rate</p><h4 id="（2）完整SQL：-3"><a href="#（2）完整SQL：-3" class="headerlink" title="（2）完整SQL："></a>（2）完整SQL：</h4><p> A.　　需要先求出最喜欢看电影的那位女性</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.userid, <span class="keyword">count</span>(a.userid) <span class="keyword">as</span> total </span><br><span class="line"><span class="keyword">from</span> t_rating a <span class="keyword">join</span> t_user b <span class="keyword">on</span> a.userid = b.userid </span><br><span class="line"><span class="keyword">where</span> b.sex=<span class="string">"F"</span> </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.userid </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> total <span class="keyword">desc</span> </span><br><span class="line"><span class="keyword">limit</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410172157137-2052414004.png" alt="img"></p><p>B.　　根据A中求出的女性userid作为where过滤条件，以看过的电影的影评分rate作为排序条件进行排序，求出评分最高的10部电影</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer5_B <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> a.movieid <span class="keyword">as</span> movieid, a.rate <span class="keyword">as</span> rate  </span><br><span class="line"><span class="keyword">from</span> t_rating a </span><br><span class="line"><span class="keyword">where</span> a.userid=<span class="number">1150</span> </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> rate <span class="keyword">desc</span> </span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> answer5_B;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410172614162-1700464534.png" alt="img"></p><p>C.　　求出B中10部电影的平均影评分</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer5_C <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> b.movieid <span class="keyword">as</span> movieid, c.moviename <span class="keyword">as</span> moviename, <span class="keyword">avg</span>(b.rate) <span class="keyword">as</span> avgrate </span><br><span class="line"><span class="keyword">from</span> answer5_B a </span><br><span class="line"><span class="keyword">join</span> t_rating b <span class="keyword">on</span> a.movieid=b.movieid </span><br><span class="line"><span class="keyword">join</span> t_movie c <span class="keyword">on</span> b.movieid=c.movieid </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> b.movieid,c.moviename;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> answer5_C;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410173209987-971071603.png" alt="img"></p><h3 id="6、求好片（评分-gt-4-0）最多的那个年份的最好看的10部电影"><a href="#6、求好片（评分-gt-4-0）最多的那个年份的最好看的10部电影" class="headerlink" title="6、求好片（评分&gt;=4.0）最多的那个年份的最好看的10部电影"></a>6、求好片（评分&gt;=4.0）最多的那个年份的最好看的10部电影</h3><h4 id="（1）分析思路：-3"><a href="#（1）分析思路：-3" class="headerlink" title="（1）分析思路："></a>（1）分析思路：</h4><p>　　1、需求字段：电影id　t_rating.movieid</p><p>　　　　　　　　  电影名　t_movie.moviename（包含年份）</p><p>　　　　　　　　  影评分　t_rating.rate</p><p>　　　　　　　　  上映年份　xxx.years</p><p>　　2、核心SQL：</p><p>　　　　A.　　需要将t_rating和t_movie表进行联合查询，将电影名当中的上映年份截取出来，保存到临时表answer6_A中</p><p>　　　　　　　　　　需要查询的字段：电影id　t_rating.movieid</p><p>　　　　　　　　　　　　　　　　　　电影名　t_movie.moviename（包含年份）</p><p>　　　　　　　　　　　　　　　　　　影评分　t_rating.rate</p><p>　　　　B.　　从answer6_A按照年份进行分组条件，按照评分&gt;=4.0作为where过滤条件，按照count(years)作为排序条件进行查询</p><p>　　　　　　　　　　需要查询的字段：电影的ID：answer6_A.years</p><p>　　　　C.　　从answer6_A按照years=1998作为where过滤条件，按照评分作为排序条件进行查询</p><p>　　　　　　　　　　需要查询的字段：电影的ID：answer6_A.moviename</p><p>　　　　　　　　　　　　　　　　　　影评分：answer6_A.avgrate</p><h4 id="（2）完整SQL：-4"><a href="#（2）完整SQL：-4" class="headerlink" title="（2）完整SQL："></a>（2）完整SQL：</h4><p>A.　　需要将t_rating和t_movie表进行联合查询，将电影名当中的上映年份截取出来</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer6_A <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span>  a.movieid <span class="keyword">as</span> movieid, a.moviename <span class="keyword">as</span> moviename, <span class="keyword">substr</span>(a.moviename,<span class="number">-5</span>,<span class="number">4</span>) <span class="keyword">as</span> <span class="keyword">years</span>, <span class="keyword">avg</span>(b.rate) <span class="keyword">as</span> avgrate</span><br><span class="line"><span class="keyword">from</span> t_movie a <span class="keyword">join</span> t_rating b <span class="keyword">on</span> a.movieid=b.movieid </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.movieid, a.moviename;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> answer6_A;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410184026515-566278137.png" alt="img"></p><p>B.　　从answer6_A按照年份进行分组条件，按照评分&gt;=4.0作为where过滤条件，按照count(years)作为排序条件进行查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">years</span>, <span class="keyword">count</span>(<span class="keyword">years</span>) <span class="keyword">as</span> total </span><br><span class="line"><span class="keyword">from</span> answer6_A a </span><br><span class="line"><span class="keyword">where</span> avgrate &gt;= <span class="number">4.0</span> </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">years</span> </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> total <span class="keyword">desc</span> </span><br><span class="line"><span class="keyword">limit</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410184710241-171102133.png" alt="img"></p><p>C.　　从answer6_A按照years=1998作为where过滤条件，按照评分作为排序条件进行查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer6_C <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> a.moviename <span class="keyword">as</span> <span class="keyword">name</span>, a.avgrate <span class="keyword">as</span> rate </span><br><span class="line"><span class="keyword">from</span> answer6_A a </span><br><span class="line"><span class="keyword">where</span> a.years=<span class="number">1998</span> </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> rate <span class="keyword">desc</span> </span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> answer6_C;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410185149876-690755903.png" alt="img"></p><h3 id="7、求1997年上映的电影中，评分最高的10部Comedy类电影"><a href="#7、求1997年上映的电影中，评分最高的10部Comedy类电影" class="headerlink" title="7、求1997年上映的电影中，评分最高的10部Comedy类电影"></a>7、求1997年上映的电影中，评分最高的10部Comedy类电影</h3><h4 id="（1）分析思路：-4"><a href="#（1）分析思路：-4" class="headerlink" title="（1）分析思路："></a>（1）分析思路：</h4><p>　　1、需求字段：电影id　t_rating.movieid</p><p>　　　　　　　　  电影名　t_movie.moviename（包含年份）</p><p>　　　　　　　　  影评分　t_rating.rate</p><p>　　　　　　　　 上映年份　xxx.years（最终查询结果可不显示）</p><p>　　　　　　　　 电影类型　xxx.type（最终查询结果可不显示）</p><p>　　2、核心SQL：</p><p>　　　　A.　　需要电影类型，所有可以将第六步中求出answer6_A表和t_movie表进行联合查询</p><p>　　　　　　　　　　需要查询的字段：电影id　answer6_A.movieid</p><p>　　　　　　　　　　　　　　　　　　电影名　answer6_A.moviename</p><p>　　　　　　　　　　　　　　　　　　影评分　answer6_A.rate</p><p>　　　　　　　　　　　　　　　　　　电影类型　t_movie.movietype　</p><p>　　　　　　　　　　　　　　　　　　上映年份　answer6_A.years</p><p>　　　　B.　　从answer7_A按照电影类型中是否包含Comedy和按上映年份作为where过滤条件，按照评分作为排序条件进行查询，将结果保存到answer7_B中</p><p>　　　　　　　　　　需要查询的字段：电影的ID：answer7_A.id</p><p>　　　　　　　　　　　　　　　　　　电影的名称：answer7_A.name</p><p>　　　　　　　　　　　　　　　　　　电影的评分：answer7_A.rate</p><h4 id="（2）完整SQL：-5"><a href="#（2）完整SQL：-5" class="headerlink" title="（2）完整SQL："></a>（2）完整SQL：</h4><p>A.　　需要电影类型，所有可以将第六步中求出answer6_A表和t_movie表进行联合查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer7_A <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> b.movieid <span class="keyword">as</span> <span class="keyword">id</span>, b.moviename <span class="keyword">as</span> <span class="keyword">name</span>, b.years <span class="keyword">as</span> <span class="keyword">years</span>, b.avgrate <span class="keyword">as</span> rate, a.movietype <span class="keyword">as</span> <span class="keyword">type</span> </span><br><span class="line"><span class="keyword">from</span> t_movie a <span class="keyword">join</span> answer6_A b <span class="keyword">on</span> a.movieid=b.movieid;</span><br><span class="line"><span class="keyword">select</span> t.* <span class="keyword">from</span> answer7_A t;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410192030317-443456540.png" alt="img"></p><p>B.　　从answer7_A按照电影类型中是否包含Comedy和按照评分&gt;=4.0作为where过滤条件，按照评分作为排序条件进行查询，将结果保存到answer7_B中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer7_B <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> t.id <span class="keyword">as</span> <span class="keyword">id</span>, t.name <span class="keyword">as</span> <span class="keyword">name</span>, t.rate <span class="keyword">as</span> rate </span><br><span class="line"><span class="keyword">from</span> answer7_A t </span><br><span class="line"><span class="keyword">where</span> t.years=<span class="number">1997</span> <span class="keyword">and</span> <span class="keyword">instr</span>(<span class="keyword">lcase</span>(t.type),<span class="string">'comedy'</span>) &gt;<span class="number">0</span> </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> rate <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> answer7_B;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410193635839-727010468.png" alt="img"></p><h3 id="8、该影评库中各种类型电影中评价最高的5部电影（类型，电影名，平均影评分）"><a href="#8、该影评库中各种类型电影中评价最高的5部电影（类型，电影名，平均影评分）" class="headerlink" title="8、该影评库中各种类型电影中评价最高的5部电影（类型，电影名，平均影评分）"></a>8、该影评库中各种类型电影中评价最高的5部电影（类型，电影名，平均影评分）</h3><h4 id="（1）分析思路：-5"><a href="#（1）分析思路：-5" class="headerlink" title="（1）分析思路："></a>（1）分析思路：</h4><p>　　1、需求字段：电影id　movieid</p><p>　　　　　　　　  电影名　moviename</p><p>　　　　　　　　  影评分　rate（排序条件）</p><p>　　　　　　　　 电影类型　type（分组条件）</p><p>　　2、核心SQL：</p><p>　　　　A.　　需要电影类型，所有需要将answer7_A中的type字段进行裂变，将结果保存到answer8_A中</p><p>　　　　　　　　　　需要查询的字段：电影id　answer7_A.id</p><p>　　　　　　　　　　　　　　　　　　电影名　answer7_A.name（包含年份）</p><p>　　　　　　　　　　　　　　　　　　上映年份　answer7_A.years</p><p>　　　　　　　　　　　　　　　　　　影评分　answer7_A.rate</p><p>　　　　　　　　　　　　　　　　　　电影类型　answer7_A.movietype　</p><p>　　　　B.　　求TopN，按照type分组，需要添加一列来记录每组的顺序，将结果保存到answer8_B中</p><blockquote><p>row_number() ：用来生成 num字段的值</p><p>distribute by movietype ：按照type进行分组</p><p>sort by avgrate desc ：每组数据按照rate排降序</p><p>num：新列， 值就是每一条记录在每一组中按照排序规则计算出来的排序值</p></blockquote><p>　　　　C.　　从answer8_B中取出num列序号&lt;=5的</p><h4 id="（2）完整SQL：-6"><a href="#（2）完整SQL：-6" class="headerlink" title="（2）完整SQL："></a>（2）完整SQL：</h4><p>A.　　需要电影类型，所有需要将answer7_A中的type字段进行裂变，将结果保存到answer8_A中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer8_A <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> a.id <span class="keyword">as</span> <span class="keyword">id</span>, a.name <span class="keyword">as</span> <span class="keyword">name</span>, a.years <span class="keyword">as</span> <span class="keyword">years</span>, a.rate <span class="keyword">as</span> rate, tv.type <span class="keyword">as</span> <span class="keyword">type</span> </span><br><span class="line"><span class="keyword">from</span> answer7_A a </span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(a.type,<span class="string">"\\|"</span>)) tv <span class="keyword">as</span> <span class="keyword">type</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> answer8_A;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410194801735-397806737.png" alt="img"></p><p>B.　　求TopN，按照type分组，需要添加一列来记录每组的顺序，将结果保存到answer8_B中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer8_B <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,<span class="keyword">years</span>,rate,<span class="keyword">type</span>,row_number() <span class="keyword">over</span>(<span class="keyword">distribute</span> <span class="keyword">by</span> <span class="keyword">type</span> <span class="keyword">sort</span> <span class="keyword">by</span> rate <span class="keyword">desc</span> ) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> answer8_A;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> answer8_B;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410195900115-1011015068.png" alt="img"></p><p>C.　　从answer8_B中取出num列序号&lt;=5的</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.* <span class="keyword">from</span> answer8_B a <span class="keyword">where</span> a.num &lt;= <span class="number">5</span>;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410200238591-1498325084.png" alt="img"></p><h3 id="9、各年评分最高的电影类型（年份，类型，影评分）"><a href="#9、各年评分最高的电影类型（年份，类型，影评分）" class="headerlink" title="9、各年评分最高的电影类型（年份，类型，影评分）"></a>9、各年评分最高的电影类型（年份，类型，影评分）</h3><h4 id="（1）分析思路：-6"><a href="#（1）分析思路：-6" class="headerlink" title="（1）分析思路："></a>（1）分析思路：</h4><p>　　1、需求字段：电影id　movieid</p><p>　　　　　　　　  电影名　moviename</p><p>　　　　　　　　  影评分　rate（排序条件）</p><p>　　　　　　　　  电影类型　type（分组条件）</p><p>　　　　　　　　 上映年份　years（分组条件）</p><p>　　2、核心SQL：</p><p>　　　　A.　　需要按照电影类型和上映年份进行分组，按照影评分进行排序，将结果保存到answer9_A中</p><p>　　　　　　　　　　需要查询的字段：</p><p>　　　　　　　　　　　　　　　　　　上映年份　answer7_A.years</p><p>　　　　　　　　　　　　　　　　　　影评分　answer7_A.rate</p><p>　　　　　　　　　　　　　　　　　　电影类型　answer7_A.movietype　</p><p>　　　　B.　　求TopN，按照years分组，需要添加一列来记录每组的顺序，将结果保存到answer9_B中</p><p>　　　　C.　　按照num=1作为where过滤条件取出结果数据</p><h4 id="（2）完整SQL：-7"><a href="#（2）完整SQL：-7" class="headerlink" title="（2）完整SQL："></a>（2）完整SQL：</h4><p>A.　　需要按照电影类型和上映年份进行分组，按照影评分进行排序，将结果保存到answer9_A中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer9_A <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> a.years <span class="keyword">as</span> <span class="keyword">years</span>, a.type <span class="keyword">as</span> <span class="keyword">type</span>, <span class="keyword">avg</span>(a.rate) <span class="keyword">as</span> rate </span><br><span class="line"><span class="keyword">from</span> answer8_A a </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.years,a.type </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> rate <span class="keyword">desc</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> answer9_A;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410201710877-165323305.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410201710877-165323305.png" alt="img"></p><p>B.　　求TopN，按照years分组，需要添加一列来记录每组的顺序，将结果保存到answer9_B中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer9_B <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> <span class="keyword">years</span>,<span class="keyword">type</span>,rate,row_number() <span class="keyword">over</span> (<span class="keyword">distribute</span> <span class="keyword">by</span> <span class="keyword">years</span> <span class="keyword">sort</span> <span class="keyword">by</span> rate) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> answer9_A;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> answer9_B;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410202105613-1494957407.png" alt="img"></p><p>C.　　按照num=1作为where过滤条件取出结果数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> answer9_B <span class="keyword">where</span> <span class="keyword">num</span>=<span class="number">1</span>;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410202237858-1872842838.png" alt="img"></p><h3 id="10、每个地区最高评分的电影名，把结果存入HDFS（地区，电影名，影评分）"><a href="#10、每个地区最高评分的电影名，把结果存入HDFS（地区，电影名，影评分）" class="headerlink" title="10、每个地区最高评分的电影名，把结果存入HDFS（地区，电影名，影评分）"></a>10、每个地区最高评分的电影名，把结果存入HDFS（地区，电影名，影评分）</h3><h4 id="（1）分析思路：-7"><a href="#（1）分析思路：-7" class="headerlink" title="（1）分析思路："></a>（1）分析思路：</h4><p>　　1、需求字段：电影id　t_movie.movieid</p><p>　　　　　　　　  电影名　t_movie.moviename</p><p>　　　　　　　　  影评分　t_rating.rate（排序条件）</p><p>　　　　　　　　  地区　t_user.zipcode（分组条件）</p><p>　　2、核心SQL：</p><p>　　　　A.　　需要把三张表进行联合查询，取出电影id、电影名称、影评分、地区，将结果保存到answer10_A表中</p><p>　　　　　　　　　　需要查询的字段：电影id　t_movie.movieid</p><p>　　　　　　　　  　　　　　　　　　 电影名　t_movie.moviename</p><p>　　　　　　　　  　　　　　　　　　 影评分　t_rating.rate（排序条件）</p><p>　　　　　　　　  　　　　　　　　　 地区　t_user.zipcode（分组条件）</p><p>　　　　B.　　求TopN，按照地区分组，按照平均排序，添加一列num用来记录地区排名，将结果保存到answer10_B表中</p><p>　　　　C.　　按照num=1作为where过滤条件取出结果数据</p><h4 id="（2）完整SQL：-8"><a href="#（2）完整SQL：-8" class="headerlink" title="（2）完整SQL："></a>（2）完整SQL：</h4><p> A.　　需要把三张表进行联合查询，取出电影id、电影名称、影评分、地区，将结果保存到answer10_A表中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer10_A <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> c.movieid, c.moviename, <span class="keyword">avg</span>(b.rate) <span class="keyword">as</span> avgrate, a.zipcode</span><br><span class="line"><span class="keyword">from</span> t_user a </span><br><span class="line"><span class="keyword">join</span> t_rating b <span class="keyword">on</span> a.userid=b.userid </span><br><span class="line"><span class="keyword">join</span> t_movie c <span class="keyword">on</span> b.movieid=c.movieid </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.zipcode,c.movieid, c.moviename;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t.* <span class="keyword">from</span> answer10_A t;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410203328988-1213590724.png" alt="img"></p><p>B.　　求TopN，按照地区分组，按照平均排序，添加一列num用来记录地区排名，将结果保存到answer10_B表中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> answer10_B <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> movieid,moviename,avgrate,zipcode, row_number() <span class="keyword">over</span> (<span class="keyword">distribute</span> <span class="keyword">by</span> zipcode <span class="keyword">sort</span> <span class="keyword">by</span> avgrate) <span class="keyword">as</span> <span class="keyword">num</span> </span><br><span class="line"><span class="keyword">from</span> answer10_A; </span><br><span class="line"><span class="keyword">select</span> t.* <span class="keyword">from</span> answer10_B t;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410204043457-1325388358.png" alt="img"></p><p>C.　　按照num=1作为where过滤条件取出结果数据并保存到HDFS上</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">directory</span> <span class="string">"/movie/answer10/"</span> <span class="keyword">select</span> t.* <span class="keyword">from</span> answer10_B t <span class="keyword">where</span> t.num=<span class="number">1</span>;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180410204402851-479530041.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （十一）Hive的5个面试题</title>
      <link href="/2019-04-11-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89Hive%E7%9A%845%E4%B8%AA%E9%9D%A2%E8%AF%95%E9%A2%98.html"/>
      <url>/2019-04-11-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89Hive%E7%9A%845%E4%B8%AA%E9%9D%A2%E8%AF%95%E9%A2%98.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （十一）Hive的5个面试题：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （十一）Hive的5个面试题</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、求单月访问次数和总访问次数"><a href="#一、求单月访问次数和总访问次数" class="headerlink" title="一、求单月访问次数和总访问次数"></a>一、求单月访问次数和总访问次数</h2><h3 id="1、数据说明"><a href="#1、数据说明" class="headerlink" title="1、数据说明"></a>1、数据说明</h3><h4 id="数据字段说明"><a href="#数据字段说明" class="headerlink" title="数据字段说明"></a>数据字段说明</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户名，月份，访问次数</span><br></pre></td></tr></table></figure><h4 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">A,2015-01,5</span><br><span class="line">A,2015-01,15</span><br><span class="line">B,2015-01,5</span><br><span class="line">A,2015-01,8</span><br><span class="line">B,2015-01,25</span><br><span class="line">A,2015-01,5</span><br><span class="line">A,2015-02,4</span><br><span class="line">A,2015-02,6</span><br><span class="line">B,2015-02,10</span><br><span class="line">B,2015-02,5</span><br><span class="line">A,2015-03,16</span><br><span class="line">A,2015-03,22</span><br><span class="line">B,2015-03,23</span><br><span class="line">B,2015-03,10</span><br><span class="line">B,2015-03,1</span><br></pre></td></tr></table></figure><h3 id="2、数据准备"><a href="#2、数据准备" class="headerlink" title="2、数据准备"></a>2、数据准备</h3><h4 id="（1）创建表"><a href="#（1）创建表" class="headerlink" title="（1）创建表"></a>（1）创建表</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> myhive;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> t_access(</span><br><span class="line">uname <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'用户名'</span>,</span><br><span class="line">umonth <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'月份'</span>,</span><br><span class="line">ucount <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'访问次数'</span></span><br><span class="line">) <span class="keyword">comment</span> <span class="string">'用户访问表'</span> </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span> </span><br><span class="line">location <span class="string">"/hive/t_access"</span>;</span><br></pre></td></tr></table></figure><h4 id="（2）导入数据"><a href="#（2）导入数据" class="headerlink" title="（2）导入数据"></a>（2）导入数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &quot;/home/hadoop/access.txt&quot; into table t_access;</span><br></pre></td></tr></table></figure><h4 id="（3）验证数据"><a href="#（3）验证数据" class="headerlink" title="（3）验证数据"></a>（3）验证数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from t_access;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408191422095-618397996.png" alt="img"></p><h3 id="3、结果需求"><a href="#3、结果需求" class="headerlink" title="3、结果需求"></a>3、结果需求</h3><p>现要求出：<br>每个用户截止到每月为止的最大单月访问次数和累计到该月的总访问次数，结果数据格式如下</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408191641383-2108507539.png" alt="img"></p><h3 id="4、需求分析"><a href="#4、需求分析" class="headerlink" title="4、需求分析"></a>4、需求分析</h3><p>此结果需要根据用户+月份进行分组</p><h4 id="（1）先求出当月访问次数"><a href="#（1）先求出当月访问次数" class="headerlink" title="（1）先求出当月访问次数"></a>（1）先求出当月访问次数</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--求当月访问次数</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tmp_access(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">mon <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span> <span class="built_in">int</span></span><br><span class="line">); </span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> tmp_access </span><br><span class="line"><span class="keyword">select</span> uname,umonth,<span class="keyword">sum</span>(ucount)</span><br><span class="line"> <span class="keyword">from</span> t_access t <span class="keyword">group</span> <span class="keyword">by</span> t.uname,t.umonth;<span class="keyword">select</span> * <span class="keyword">from</span> tmp_access;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408212426965-852872067.png" alt="img"></p><h4 id="（2）tmp-access进行自连接视图"><a href="#（2）tmp-access进行自连接视图" class="headerlink" title="（2）tmp_access进行自连接视图"></a>（2）tmp_access进行自连接视图</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> tmp_view <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> a.name anme,a.mon amon,a.num anum,b.name bname,b.mon bmon,b.num bnum <span class="keyword">from</span> tmp_access a <span class="keyword">join</span> tmp_access b </span><br><span class="line"><span class="keyword">on</span> a.name=b.name;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tmp_view;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408213008759-536376094.png" alt="img"></p><h4 id="（3）进行比较统计"><a href="#（3）进行比较统计" class="headerlink" title="（3）进行比较统计"></a>（3）进行比较统计</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> anme,amon,anum,<span class="keyword">max</span>(bnum) <span class="keyword">as</span> max_access,<span class="keyword">sum</span>(bnum) <span class="keyword">as</span> sum_access </span><br><span class="line"><span class="keyword">from</span> tmp_view </span><br><span class="line"><span class="keyword">where</span> amon&gt;=bmon </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> anme,amon,anum;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408213548473-1116107765.png" alt="img"></p><h2 id="二、学生课程成绩"><a href="#二、学生课程成绩" class="headerlink" title="二、学生课程成绩"></a>二、学生课程成绩</h2><h3 id="1、说明"><a href="#1、说明" class="headerlink" title="1、说明"></a>1、说明</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> myhive;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`course`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">int</span>,</span><br><span class="line">  <span class="string">`sid`</span> <span class="built_in">int</span> ,</span><br><span class="line">  <span class="string">`course`</span> <span class="keyword">string</span>,</span><br><span class="line">  <span class="string">`score`</span> <span class="built_in">int</span> </span><br><span class="line">) ;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 插入数据</span><br><span class="line">// 字段解释：id, 学号， 课程， 成绩</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`course`</span> <span class="keyword">VALUES</span> (<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yuwen'</span>, <span class="number">43</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`course`</span> <span class="keyword">VALUES</span> (<span class="number">2</span>, <span class="number">1</span>, <span class="string">'shuxue'</span>, <span class="number">55</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`course`</span> <span class="keyword">VALUES</span> (<span class="number">3</span>, <span class="number">2</span>, <span class="string">'yuwen'</span>, <span class="number">77</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`course`</span> <span class="keyword">VALUES</span> (<span class="number">4</span>, <span class="number">2</span>, <span class="string">'shuxue'</span>, <span class="number">88</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`course`</span> <span class="keyword">VALUES</span> (<span class="number">5</span>, <span class="number">3</span>, <span class="string">'yuwen'</span>, <span class="number">98</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`course`</span> <span class="keyword">VALUES</span> (<span class="number">6</span>, <span class="number">3</span>, <span class="string">'shuxue'</span>, <span class="number">65</span>);</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408215422080-584424294.png" alt="img"></p><h3 id="2、需求"><a href="#2、需求" class="headerlink" title="2、需求"></a>2、需求</h3><p>求：所有数学课程成绩 大于 语文课程成绩的学生的学号</p><h4 id="1、使用case…when…将不同的课程名称转换成不同的列"><a href="#1、使用case…when…将不同的课程名称转换成不同的列" class="headerlink" title="1、使用case…when…将不同的课程名称转换成不同的列"></a>1、使用case…when…将不同的课程名称转换成不同的列</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> tmp_course_view <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">sid</span>, <span class="keyword">case</span> course <span class="keyword">when</span> <span class="string">"shuxue"</span> <span class="keyword">then</span> score <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>  <span class="keyword">as</span> shuxue,  </span><br><span class="line"><span class="keyword">case</span> course <span class="keyword">when</span> <span class="string">"yuwen"</span> <span class="keyword">then</span> score <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>  <span class="keyword">as</span> yuwen <span class="keyword">from</span> course;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tmp_course_view;</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408215759602-1208331286.png" alt="img"></p><h4 id="2、以sid分组合并取各成绩最大值"><a href="#2、以sid分组合并取各成绩最大值" class="headerlink" title="2、以sid分组合并取各成绩最大值"></a>2、以sid分组合并取各成绩最大值</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> tmp_course_view1 <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> aa.sid, <span class="keyword">max</span>(aa.shuxue) <span class="keyword">as</span> shuxue, <span class="keyword">max</span>(aa.yuwen) <span class="keyword">as</span> yuwen <span class="keyword">from</span> tmp_course_view aa <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">sid</span>;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tmp_course_view1;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408220127658-1596886525.png" alt="img"></p><h4 id="3、比较结果"><a href="#3、比较结果" class="headerlink" title="3、比较结果"></a>3、比较结果</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tmp_course_view1 <span class="keyword">where</span> shuxue &gt; yuwen;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408220333411-2122468849.png" alt="img"></p><h2 id="三、求每一年最大气温的那一天-温度"><a href="#三、求每一年最大气温的那一天-温度" class="headerlink" title="三、求每一年最大气温的那一天  + 温度"></a>三、求每一年最大气温的那一天  + 温度</h2><h3 id="1、说明-1"><a href="#1、说明-1" class="headerlink" title="1、说明"></a>1、说明</h3><p>数据格式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2010012325</span><br></pre></td></tr></table></figure><p>具体数据</p><p><img src="https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif" alt="img"> View Code</p><p>数据解释</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2010012325表示在2010年01月23日的气温为25度</span><br></pre></td></tr></table></figure><h3 id="2、-需求"><a href="#2、-需求" class="headerlink" title="2、 需求"></a>2、 需求</h3><p>比如：2010012325表示在2010年01月23日的气温为25度。现在要求使用hive，计算每一年出现过的最大气温的日期+温度。<br>要计算出每一年的最大气温。我用<br>select substr(data,1,4),max(substr(data,9,2)) from table2 group by substr(data,1,4);<br>出来的是 年份 + 温度 这两列数据例如 2015 99</p><p>但是如果我是想select 的是：具体每一年最大气温的那一天 + 温度 。例如 20150109 99<br>请问该怎么执行hive语句。。<br>group by 只需要substr(data,1,4)，<br>但是select substr(data,1,8)，又不在group by 的范围内。<br>是我陷入了思维死角。一直想不出所以然。。求大神指点一下。<br>在select 如果所需要的。不在group by的条件里。这种情况如何去分析？</p><h3 id="3、解析"><a href="#3、解析" class="headerlink" title="3、解析"></a>3、解析</h3><h4 id="（1）创建一个临时表tmp-weather，将数据切分"><a href="#（1）创建一个临时表tmp-weather，将数据切分" class="headerlink" title="（1）创建一个临时表tmp_weather，将数据切分"></a>（1）创建一个临时表tmp_weather，将数据切分</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tmp_weather <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> <span class="keyword">substr</span>(<span class="keyword">data</span>,<span class="number">1</span>,<span class="number">4</span>) <span class="keyword">years</span>,<span class="keyword">substr</span>(<span class="keyword">data</span>,<span class="number">5</span>,<span class="number">2</span>) <span class="keyword">months</span>,<span class="keyword">substr</span>(<span class="keyword">data</span>,<span class="number">7</span>,<span class="number">2</span>) <span class="keyword">days</span>,<span class="keyword">substr</span>(<span class="keyword">data</span>,<span class="number">9</span>,<span class="number">2</span>) temp <span class="keyword">from</span> weather;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tmp_weather;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180409100419037-1519099272.png" alt="img"></p><h4 id="（2）创建一个临时表tmp-year-weather"><a href="#（2）创建一个临时表tmp-year-weather" class="headerlink" title="（2）创建一个临时表tmp_year_weather"></a>（2）创建一个临时表tmp_year_weather</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tmp_year_weather <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> <span class="keyword">substr</span>(<span class="keyword">data</span>,<span class="number">1</span>,<span class="number">4</span>) <span class="keyword">years</span>,<span class="keyword">max</span>(<span class="keyword">substr</span>(<span class="keyword">data</span>,<span class="number">9</span>,<span class="number">2</span>)) max_temp <span class="keyword">from</span> weather <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">substr</span>(<span class="keyword">data</span>,<span class="number">1</span>,<span class="number">4</span>);</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tmp_year_weather;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180409100254081-790110773.png" alt="img"></p><h4 id="（3）将2个临时表进行连接查询"><a href="#（3）将2个临时表进行连接查询" class="headerlink" title="（3）将2个临时表进行连接查询"></a>（3）将2个临时表进行连接查询</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tmp_year_weather a <span class="keyword">join</span> tmp_weather b <span class="keyword">on</span> a.years=b.years <span class="keyword">and</span> a.max_temp=b.temp;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180409100655695-981143896.png" alt="img"></p><h2 id="四、求学生选课情况"><a href="#四、求学生选课情况" class="headerlink" title="四、求学生选课情况"></a>四、求学生选课情况</h2><h2 id="1、数据说明-1"><a href="#1、数据说明-1" class="headerlink" title="1、数据说明"></a>1、数据说明</h2><h4 id="（1）数据格式"><a href="#（1）数据格式" class="headerlink" title="（1）数据格式"></a>（1）数据格式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">id course </span><br><span class="line">1,a </span><br><span class="line">1,b </span><br><span class="line">1,c </span><br><span class="line">1,e </span><br><span class="line">2,a </span><br><span class="line">2,c </span><br><span class="line">2,d </span><br><span class="line">2,f </span><br><span class="line">3,a </span><br><span class="line">3,b </span><br><span class="line">3,c </span><br><span class="line">3,e</span><br></pre></td></tr></table></figure><h4 id="（2）字段含义"><a href="#（2）字段含义" class="headerlink" title="（2）字段含义"></a>（2）字段含义</h4><p>表示有id为1,2,3的学生选修了课程a,b,c,d,e,f中其中几门。</p><h2 id="2、数据准备-1"><a href="#2、数据准备-1" class="headerlink" title="2、数据准备"></a>2、数据准备</h2><h4 id="（1）建表t-course"><a href="#（1）建表t-course" class="headerlink" title="（1）建表t_course"></a>（1）建表t_course</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_course(<span class="keyword">id</span> <span class="built_in">int</span>,course <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180409101747464-781937248.png" alt="img"></p><h4 id="（2）导入数据-1"><a href="#（2）导入数据-1" class="headerlink" title="（2）导入数据"></a>（2）导入数据</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/course/course.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> t_course;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180409101834214-242067281.png" alt="img"></p><h2 id="3、需求"><a href="#3、需求" class="headerlink" title="3、需求"></a>3、需求</h2><p>编写Hive的HQL语句来实现以下结果：表中的1表示选修，表中的0表示未选修</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">id    a    b    c    d    e    f</span><br><span class="line">1     1    1    1    0    1    0</span><br><span class="line">2     1    0    1    1    0    1</span><br><span class="line">3     1    1    1    0    1    0</span><br></pre></td></tr></table></figure><h2 id="4、解析"><a href="#4、解析" class="headerlink" title="4、解析"></a>4、解析</h2><p>第一步：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> collect_set(course) <span class="keyword">as</span> courses <span class="keyword">from</span> id_course;</span><br></pre></td></tr></table></figure><p>第二步：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.strict.checks.cartesian.product=<span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> id_courses <span class="keyword">as</span> <span class="keyword">select</span> t1.id <span class="keyword">as</span> <span class="keyword">id</span>,t1.course <span class="keyword">as</span> id_courses,t2.course courses </span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">( <span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">as</span> <span class="keyword">id</span>,collect_set(course) <span class="keyword">as</span> course <span class="keyword">from</span> id_course <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span> ) t1 </span><br><span class="line"><span class="keyword">join</span> </span><br><span class="line">(<span class="keyword">select</span> collect_set(course) <span class="keyword">as</span> course <span class="keyword">from</span> id_course) t2;</span><br></pre></td></tr></table></figure><blockquote><p>启用严格模式：hive.mapred.mode = strict // Deprecated<br>hive.strict.checks.large.query = true<br>该设置会禁用：1. 不指定分页的orderby<br>　　　　　　   2. 对分区表不指定分区进行查询<br>　　　　　　   3. 和数据量无关，只是一个查询模式</p><p>hive.strict.checks.type.safety = true<br>严格类型安全，该属性不允许以下操作：1. bigint和string之间的比较<br>　　　　　　　　　　　　　　　　　　2. bigint和double之间的比较</p><p>hive.strict.checks.cartesian.product = true<br>该属性不允许笛卡尔积操作</p></blockquote><p>第三步：得出最终结果：<br>思路：<br>拿出course字段中的每一个元素在id_courses中进行判断，看是否存在。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,</span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> array_contains(id_courses, courses[<span class="number">0</span>]) <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span> <span class="keyword">as</span> a,</span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> array_contains(id_courses, courses[<span class="number">1</span>]) <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span> <span class="keyword">as</span> b,</span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> array_contains(id_courses, courses[<span class="number">2</span>]) <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span> <span class="keyword">as</span> c,</span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> array_contains(id_courses, courses[<span class="number">3</span>]) <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span> <span class="keyword">as</span> d,</span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> array_contains(id_courses, courses[<span class="number">4</span>]) <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span> <span class="keyword">as</span> e,</span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> array_contains(id_courses, courses[<span class="number">5</span>]) <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span> <span class="keyword">as</span> f </span><br><span class="line"><span class="keyword">from</span> id_courses;</span><br></pre></td></tr></table></figure><h2 id="五、求月销售额和总销售额"><a href="#五、求月销售额和总销售额" class="headerlink" title="五、求月销售额和总销售额"></a>五、求月销售额和总销售额</h2><h3 id="1、数据说明-2"><a href="#1、数据说明-2" class="headerlink" title="1、数据说明"></a>1、数据说明</h3><h4 id="（1）数据格式-1"><a href="#（1）数据格式-1" class="headerlink" title="（1）数据格式"></a>（1）数据格式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a,01,150</span><br><span class="line">a,01,200</span><br><span class="line">b,01,1000</span><br><span class="line">b,01,800</span><br><span class="line">c,01,250</span><br><span class="line">c,01,220</span><br><span class="line">b,01,6000</span><br><span class="line">a,02,2000</span><br><span class="line">a,02,3000</span><br><span class="line">b,02,1000</span><br><span class="line">b,02,1500</span><br><span class="line">c,02,350</span><br><span class="line">c,02,280</span><br><span class="line">a,03,350</span><br><span class="line">a,03,250</span><br></pre></td></tr></table></figure><h4 id="（2）字段含义-1"><a href="#（2）字段含义-1" class="headerlink" title="（2）字段含义"></a>（2）字段含义</h4><p>店铺，月份，金额</p><h3 id="2、数据准备-2"><a href="#2、数据准备-2" class="headerlink" title="2、数据准备"></a>2、数据准备</h3><h4 id="（1）创建数据库表t-store"><a href="#（1）创建数据库表t-store" class="headerlink" title="（1）创建数据库表t_store"></a>（1）创建数据库表t_store</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> <span class="keyword">class</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_store(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">months</span> <span class="built_in">int</span>,</span><br><span class="line">money <span class="built_in">int</span></span><br><span class="line">) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br></pre></td></tr></table></figure><h4 id="（2）导入数据-2"><a href="#（2）导入数据-2" class="headerlink" title="（2）导入数据"></a>（2）导入数据</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/home/hadoop/store.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> t_store;</span><br></pre></td></tr></table></figure><h3 id="3、需求-1"><a href="#3、需求-1" class="headerlink" title="3、需求"></a>3、需求</h3><p>编写Hive的HQL语句求出每个店铺的当月销售额和累计到当月的总销售额</p><h3 id="4、解析-1"><a href="#4、解析-1" class="headerlink" title="4、解析"></a>4、解析</h3><p>（1）按照商店名称和月份进行分组统计</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tmp_store1 <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,<span class="keyword">months</span>,<span class="keyword">sum</span>(money) <span class="keyword">as</span> money <span class="keyword">from</span> t_store <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>,<span class="keyword">months</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tmp_store1;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180409113229256-890349636.png" alt="img"></p><p>（2）对tmp_store1 表里面的数据进行自连接</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tmp_store2 <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> a.name aname,a.months amonths,a.money amoney,b.name bname,b.months bmonths,b.money bmoney <span class="keyword">from</span> tmp_store1 a </span><br><span class="line"><span class="keyword">join</span> tmp_store1 b <span class="keyword">on</span> a.name=b.name <span class="keyword">order</span> <span class="keyword">by</span> aname,amonths;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tmp_store2;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180409113405020-775197507.png" alt="img"></p><p>（3）比较统计</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> aname,amonths,amoney,<span class="keyword">sum</span>(bmoney) <span class="keyword">as</span> total <span class="keyword">from</span> tmp_store2 <span class="keyword">where</span> amonths &gt;= bmonths <span class="keyword">group</span> <span class="keyword">by</span> aname,amonths,amoney;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180409113709570-551757814.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （十）Hive的高级操作</title>
      <link href="/2019-04-10-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%EF%BC%89Hive%E7%9A%84%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C.html"/>
      <url>/2019-04-10-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%EF%BC%89Hive%E7%9A%84%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （十）Hive的高级操作：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （十）Hive的高级操作</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、负责数据类型"><a href="#一、负责数据类型" class="headerlink" title="一、负责数据类型"></a>一、负责数据类型</h2><h3 id="1、array"><a href="#1、array" class="headerlink" title="1、array"></a>1、array</h3><p> 现有数据如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 huangbo guangzhou,xianggang,shenzhen a1:30,a2:20,a3:100 beijing,112233,13522334455,500</span><br><span class="line">2xuzhengxianggangb2:50,b3:40tianjin,223344,13644556677,600</span><br><span class="line">3wangbaoqiangbeijing,zhejinagc1:200chongqinjg,334455,15622334455,20</span><br></pre></td></tr></table></figure><p>建表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> <span class="keyword">class</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> cdt(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, </span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>, </span><br><span class="line">work_location <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;, </span><br><span class="line">piaofang <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="built_in">bigint</span>&gt;, </span><br><span class="line">address <span class="keyword">struct</span>&lt;location:<span class="keyword">string</span>,zipcode:<span class="built_in">int</span>,phone:<span class="keyword">string</span>,<span class="keyword">value</span>:<span class="built_in">int</span>&gt;) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span> </span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span> </span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">":"</span> </span><br><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\n"</span>;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408173043585-1275161000.png" alt="img"></p><p>导入数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; load data local inpath "/home/hadoop/cdt.txt" into table cdt;</span><br></pre></td></tr></table></figure><p>查询语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> cdt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408183049458-1912277812.png" alt="img"></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span> <span class="keyword">from</span> cdt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408183202688-414846691.png" alt="img"></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> work_location <span class="keyword">from</span> cdt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408183336575-889255211.png" alt="img"></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> work_location[<span class="number">0</span>] <span class="keyword">from</span> cdt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408183446633-233162014.png" alt="img"></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> work_location[<span class="number">1</span>] <span class="keyword">from</span> cdt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408183519651-594024573.png" alt="img"></p><h3 id="2、map"><a href="#2、map" class="headerlink" title="2、map"></a>2、map</h3><p>建表语句、导入数据同1</p><p>查询语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> piaofang <span class="keyword">from</span> cdt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408183748427-1339156513.png" alt="img"></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> piaofang[<span class="string">"a1"</span>] <span class="keyword">from</span> cdt;</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408183858137-827620470.png" alt="img"></p><h3 id="3、struct"><a href="#3、struct" class="headerlink" title="3、struct"></a>3、struct</h3><p>建表语句、导入数据同1</p><p>查询语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> address <span class="keyword">from</span> cdt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408184016402-1457666185.png" alt="img"></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> address.location <span class="keyword">from</span> cdt;</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408184050383-1588329051.png" alt="img"></p><h3 id="4、uniontype"><a href="#4、uniontype" class="headerlink" title="4、uniontype"></a>4、uniontype</h3><p>很少使用</p><p>参考资料：<a href="http://yugouai.iteye.com/blog/1849192" target="_blank" rel="noopener">http://yugouai.iteye.com/blog/1849192</a></p><h2 id="二、视图"><a href="#二、视图" class="headerlink" title="二、视图"></a>二、视图</h2><h3 id="1、Hive-的视图和关系型数据库的视图区别"><a href="#1、Hive-的视图和关系型数据库的视图区别" class="headerlink" title="1、Hive 的视图和关系型数据库的视图区别"></a>1、Hive 的视图和关系型数据库的视图区别</h3><p>和关系型数据库一样，Hive 也提供了视图的功能，不过请注意，Hive 的视图和关系型数据库的数据还是有很大的区别：</p><p>　　（1）只有逻辑视图，没有物化视图；</p><p>　　（2）视图只能查询，不能 Load/Insert/Update/Delete 数据；</p><p>　　（3）视图在创建时候，只是保存了一份元数据，当查询视图的时候，才开始执行视图对应的 那些子查询</p><h3 id="2、Hive视图的创建语句"><a href="#2、Hive视图的创建语句" class="headerlink" title="2、Hive视图的创建语句"></a>2、Hive视图的创建语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> view_cdt <span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> cdt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408184549702-1005718852.png" alt="img"></p><h3 id="3、Hive视图的查看语句"><a href="#3、Hive视图的查看语句" class="headerlink" title="3、Hive视图的查看语句"></a>3、Hive视图的查看语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> views;</span><br><span class="line">desc view_cdt;<span class="comment">-- 查看某个具体视图的信息</span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408184741330-1744317581.png" alt="img"></p><h3 id="4、Hive视图的使用语句"><a href="#4、Hive视图的使用语句" class="headerlink" title="4、Hive视图的使用语句"></a>4、Hive视图的使用语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> view_cdt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408184854110-707403466.png" alt="img"></p><h3 id="5、Hive视图的删除语句"><a href="#5、Hive视图的删除语句" class="headerlink" title="5、Hive视图的删除语句"></a>5、Hive视图的删除语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">view</span> view_cdt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408185042082-261986868.png" alt="img"></p><h2 id="三、函数"><a href="#三、函数" class="headerlink" title="三、函数"></a>三、函数</h2><h3 id="1、内置函数"><a href="#1、内置函数" class="headerlink" title="1、内置函数"></a>1、内置函数</h3><p>具体可看<a href="http://www.cnblogs.com/qingyunzong/p/8744593.html" target="_blank" rel="noopener">http://www.cnblogs.com/qingyunzong/p/8744593.html</a></p><h4 id="（1）查看内置函数"><a href="#（1）查看内置函数" class="headerlink" title="（1）查看内置函数"></a>（1）查看内置函数</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> functions;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408185346738-551528669.png" alt="img"></p><h4 id="（2）显示函数的详细信息"><a href="#（2）显示函数的详细信息" class="headerlink" title="（2）显示函数的详细信息"></a>（2）显示函数的详细信息</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc function substr;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408185504054-1979368578.png" alt="img"></p><h4 id="（3）显示函数的扩展信息"><a href="#（3）显示函数的扩展信息" class="headerlink" title="（3）显示函数的扩展信息"></a>（3）显示函数的扩展信息</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc function extended substr;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180408185612437-1436305025.png" alt="img"></p><h3 id="2、自定义函数UDF"><a href="#2、自定义函数UDF" class="headerlink" title="2、自定义函数UDF"></a>2、自定义函数UDF</h3><p>当 Hive 提供的内置函数无法满足业务处理需要时，此时就可以考虑使用用户自定义函数。</p><p><strong>UDF</strong>（user-defined function）作用于单个数据行，产生一个数据行作为输出。（数学函数，字 符串函数）</p><p><strong>UDAF</strong>（用户定义聚集函数 User- Defined Aggregation Funcation）：接收多个输入数据行，并产 生一个输出数据行。（count，max）</p><p><strong>UDTF</strong>（表格生成函数 User-Defined Table Functions）：接收一行输入，输出多行（explode）</p><h3 id="1-简单UDF示例"><a href="#1-简单UDF示例" class="headerlink" title="(1) 简单UDF示例"></a>(1) 简单UDF示例</h3><h4 id="A-导入hive需要的jar包，自定义一个java类继承UDF，重载-evaluate-方法"><a href="#A-导入hive需要的jar包，自定义一个java类继承UDF，重载-evaluate-方法" class="headerlink" title="A.　导入hive需要的jar包，自定义一个java类继承UDF，重载 evaluate 方法"></a>A.　导入hive需要的jar包，自定义一个java类继承UDF，重载 evaluate 方法</h4><p>ToLowerCase.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ToLowerCase</span> <span class="keyword">extends</span> <span class="title">UDF</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 必须是 public，并且 evaluate 方法可以重载</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(String field)</span> </span>&#123;</span><br><span class="line">    String result = field.toLowerCase();</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="B-打成-jar-包上传到服务器"><a href="#B-打成-jar-包上传到服务器" class="headerlink" title="B.　打成 jar 包上传到服务器"></a>B.　打成 jar 包上传到服务器</h4><h4 id="C-将-jar-包添加到-hive-的-classpath"><a href="#C-将-jar-包添加到-hive-的-classpath" class="headerlink" title="C.　将 jar 包添加到 hive 的 classpath"></a>C.　将 jar 包添加到 hive 的 classpath</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add JAR /home/hadoop/udf.jar;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180414105512185-1132423466.png" alt="img"></p><h4 id="D-创建临时函数与开发好的-class-关联起来"><a href="#D-创建临时函数与开发好的-class-关联起来" class="headerlink" title="D.　创建临时函数与开发好的 class 关联起来"></a>D.　创建临时函数与开发好的 class 关联起来</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; create temporary function tolowercase as 'com.study.hive.udf.ToLowerCase';</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180414105730255-1712623480.png" alt="img"></p><h4 id="E-至此，便可以在-hql-在使用自定义的函数"><a href="#E-至此，便可以在-hql-在使用自定义的函数" class="headerlink" title="E.　至此，便可以在 hql 在使用自定义的函数"></a>E.　至此，便可以在 hql 在使用自定义的函数</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; select tolowercase('HELLO');</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180414105920517-647807254.png" alt="img"></p><h3 id="2-JSON数据解析UDF开发"><a href="#2-JSON数据解析UDF开发" class="headerlink" title="(2) JSON数据解析UDF开发"></a>(2) JSON数据解析UDF开发</h3><p>现有原始 json 数据（rating.json）如下</p><blockquote><p>{“movie”:”1193”,”rate”:”5”,”timeStamp”:”978300760”,”uid”:”1”}</p><p>{“movie”:”661”,”rate”:”3”,”timeStamp”:”978302109”,”uid”:”1”}</p><p>{“movie”:”914”,”rate”:”3”,”timeStamp”:”978301968”,”uid”:”1”}</p><p>{“movie”:”3408”,”rate”:”4”,”timeStamp”:”978300275”,”uid”:”1”}</p><p>{“movie”:”2355”,”rate”:”5”,”timeStamp”:”978824291”,”uid”:”1”}</p><p>{“movie”:”1197”,”rate”:”3”,”timeStamp”:”978302268”,”uid”:”1”}</p><p>{“movie”:”1287”,”rate”:”5”,”timeStamp”:”978302039”,”uid”:”1”}</p><p>{“movie”:”2804”,”rate”:”5”,”timeStamp”:”978300719”,”uid”:”1”}</p><p>{“movie”:”594”,”rate”:”4”,”timeStamp”:”978302268”,”uid”:”1”}</p></blockquote><p>现在需要将数据导入到 hive 仓库中，并且最终要得到这么一个结果：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180414110149949-679183333.png" alt="img"></p><p>该怎么做、？？？（提示：可用内置 get_json_object 或者自定义函数完成）</p><h4 id="A-get-json-object-string-json-string-string-path"><a href="#A-get-json-object-string-json-string-string-path" class="headerlink" title="A.　get_json_object(string json_string, string path)"></a>A.　get_json_object(string json_string, string path)</h4><p>返回值: string  </p><p>说明：解析json的字符串json_string,返回path指定的内容。如果输入的json字符串无效，那么返回NULL。  这个函数每次只能返回一个数据项。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; select get_json_object('&#123;"movie":"594","rate":"4","timeStamp":"978302268","uid":"1"&#125;','$.movie');</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180414111001588-234254636.png" alt="img"></p><p>创建json表并将数据导入进去</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; create table json(data string);</span><br><span class="line">No rows affected (0.983 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt; load data local inpath '/home/hadoop/json.txt' into table json;</span><br><span class="line">No rows affected (1.046 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180414111449488-1854085883.png" alt="img"></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; select </span><br><span class="line">. . . . . . . . . . . . . . .&gt; get_json_object(data,'$.movie') as movie </span><br><span class="line">. . . . . . . . . . . . . . .&gt; from json；</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180414111805847-1824092036.png" alt="img"></p><h4 id="B-json-tuple-jsonStr-k1-k2-…"><a href="#B-json-tuple-jsonStr-k1-k2-…" class="headerlink" title="B.　json_tuple(jsonStr, k1, k2, …)"></a>B.　json_tuple(jsonStr, k1, k2, …)</h4><p>参数为一组键k1，k2……和JSON字符串，返回值的元组。该方法比 <code>get_json_object</code> 高效，因为可以在一次调用中输入多个键</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; select </span><br><span class="line">. . . . . . . . . . . . . . .&gt;   b.b_movie,</span><br><span class="line">. . . . . . . . . . . . . . .&gt;   b.b_rate,</span><br><span class="line">. . . . . . . . . . . . . . .&gt;   b.b_timeStamp,</span><br><span class="line">. . . . . . . . . . . . . . .&gt;   b.b_uid   </span><br><span class="line">. . . . . . . . . . . . . . .&gt; from json a </span><br><span class="line">. . . . . . . . . . . . . . .&gt; lateral view json_tuple(a.data,&apos;movie&apos;,&apos;rate&apos;,&apos;timeStamp&apos;,&apos;uid&apos;) b as b_movie,b_rate,b_timeStamp,b_uid;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180414113012552-2019392285.png" alt="img"></p><h3 id="3-Transform实现"><a href="#3-Transform实现" class="headerlink" title="(3) Transform实现"></a>(3) Transform实现</h3><p>Hive 的 TRANSFORM 关键字提供了在 SQL 中调用自写脚本的功能。适合实现 Hive 中没有的 功能又不想写 UDF 的情况</p><p>具体以一个实例讲解。</p><p>Json 数据： {“movie”:”1193”,”rate”:”5”,”timeStamp”:”978300760”,”uid”:”1”}</p><p>需求：把 timestamp 的值转换成日期编号</p><p>1、先加载 rating.json 文件到 hive 的一个原始表 rate_json</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rate_json(line <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/rating.json'</span> <span class="keyword">into</span> <span class="keyword">table</span> rate_json;</span><br></pre></td></tr></table></figure><p>2、创建 rate 这张表用来存储解析 json 出来的字段：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rate(movie <span class="built_in">int</span>, rate <span class="built_in">int</span>, unixtime <span class="built_in">int</span>, userid <span class="built_in">int</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span></span><br><span class="line"><span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p>解析 json，得到结果之后存入 rate 表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> rate <span class="keyword">select</span></span><br><span class="line">get_json_object(line,<span class="string">'$.movie'</span>) <span class="keyword">as</span> moive,</span><br><span class="line">get_json_object(line,<span class="string">'$.rate'</span>) <span class="keyword">as</span> rate,</span><br><span class="line">get_json_object(line,<span class="string">'$.timeStamp'</span>) <span class="keyword">as</span> unixtime,</span><br><span class="line">get_json_object(line,<span class="string">'$.uid'</span>) <span class="keyword">as</span> userid</span><br><span class="line"><span class="keyword">from</span> rate_json;</span><br></pre></td></tr></table></figure><p>3、使用 transform+python 的方式去转换 unixtime 为 weekday</p><p>先编辑一个 python 脚本文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">########python######代码</span></span><br><span class="line"><span class="comment">## vi weekday_mapper.py</span></span><br><span class="line"><span class="comment">#!/bin/python</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line"> line = line.strip()</span><br><span class="line"> movie,rate,unixtime,userid = line.split(<span class="string">'\t'</span>)</span><br><span class="line"> weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()</span><br><span class="line"> <span class="keyword">print</span> <span class="string">'\t'</span>.join([movie, rate, str(weekday),userid])</span><br></pre></td></tr></table></figure><p>保存文件 然后，将文件加入 hive 的 classpath：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;add file /home/hadoop/weekday_mapper.py;</span><br><span class="line">hive&gt; insert into table lastjsontable select transform(movie,rate,unixtime,userid)</span><br><span class="line">using 'python weekday_mapper.py' as(movie,rate,weekday,userid) from rate;</span><br></pre></td></tr></table></figure><p>创建最后的用来存储调用 python 脚本解析出来的数据的表：lastjsontable</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> lastjsontable(movie <span class="built_in">int</span>, rate <span class="built_in">int</span>, <span class="keyword">weekday</span> <span class="built_in">int</span>, userid <span class="built_in">int</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p>最后查询看数据是否正确</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span>(<span class="keyword">weekday</span>) <span class="keyword">from</span> lastjsontable;</span><br></pre></td></tr></table></figure><h2 id="四、特殊分隔符处理"><a href="#四、特殊分隔符处理" class="headerlink" title="四、特殊分隔符处理"></a>四、特殊分隔符处理</h2><p>补充：hive 读取数据的机制：</p><p>1、 首先用 InputFormat&lt;默认是：org.apache.hadoop.mapred.TextInputFormat &gt;的一个具体实 现类读入文件数据，返回一条一条的记录（可以是行，或者是你逻辑中的“行”）</p><p>2、 然后利用 SerDe&lt;默认：org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&gt;的一个具体 实现类，对上面返回的一条一条的记录进行字段切割</p><p>Hive 对文件中字段的分隔符默认情况下只支持单字节分隔符，如果数据文件中的分隔符是多 字符的，如下所示：</p><p>01||huangbo</p><p>02||xuzheng</p><p>03||wangbaoqiang</p><h3 id="1、使用RegexSerDe正则表达式解析"><a href="#1、使用RegexSerDe正则表达式解析" class="headerlink" title="1、使用RegexSerDe正则表达式解析"></a>1、使用RegexSerDe正则表达式解析</h3><p>创建表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_bi_reg(<span class="keyword">id</span> <span class="keyword">string</span>,<span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> serde <span class="string">'org.apache.hadoop.hive.serde2.RegexSerDe'</span></span><br><span class="line"><span class="keyword">with</span> serdeproperties(<span class="string">'input.regex'</span>=<span class="string">'(.*)\\|\\|(.*)'</span>,<span class="string">'output.format.string'</span>=<span class="string">'%1$s %2$s'</span>)</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180414113909764-1659213834.png" alt="img"></p><p>导入数据并查询</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; load data local inpath '/home/hadoop/data.txt' into table t_bi_reg;</span><br><span class="line">No rows affected (0.747 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt; select a.* from t_bi_reg a;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180414114100544-189365287.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （九）Hive的内置函数</title>
      <link href="/2019-04-09-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B9%9D%EF%BC%89Hive%E7%9A%84%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0.html"/>
      <url>/2019-04-09-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B9%9D%EF%BC%89Hive%E7%9A%84%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （九）Hive的内置函数：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （九）Hive的内置函数</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="数学函数"><a href="#数学函数" class="headerlink" title="数学函数"></a><strong>数学函数</strong></h2><table><thead><tr><th><strong>Return Type</strong></th><th><strong>Name (Signature)</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr><td>DOUBLE</td><td>round(DOUBLE a)</td><td>Returns the rounded <code>BIGINT</code> value of <code>a</code>.<strong>返回对a四舍五入的BIGINT值</strong></td></tr><tr><td>DOUBLE</td><td>round(DOUBLE a, INT d)</td><td>Returns <code>a</code> rounded to <code>d</code> decimal places.<strong>返回DOUBLE型d的保留n位小数的DOUBLW型的近似值</strong></td></tr><tr><td>DOUBLE</td><td>bround(DOUBLE a)</td><td>Returns the rounded BIGINT value of <code>a</code> using HALF_EVEN rounding mode (as of <a href="https://issues.apache.org/jira/browse/HIVE-11103" target="_blank" rel="noopener">Hive 1.3.0, 2.0.0</a>). Also known as Gaussian rounding or bankers’ rounding. Example: bround(2.5) = 2, bround(3.5) = 4. <strong>银行家舍入法（1<del>4：舍，6</del>9：进，5-&gt;前位数是偶：舍，5-&gt;前位数是奇：进）</strong></td></tr><tr><td>DOUBLE</td><td>bround(DOUBLE a, INT d)</td><td>Returns <code>a</code> rounded to <code>d</code> decimal places using HALF_EVEN rounding mode (as of <a href="https://issues.apache.org/jira/browse/HIVE-11103" target="_blank" rel="noopener">Hive 1.3.0, 2.0.0</a>). Example: bround(8.25, 1) = 8.2, bround(8.35, 1) = 8.4. <strong>银行家舍入法,保留d位小数</strong></td></tr><tr><td>BIGINT</td><td>floor(DOUBLE a)</td><td>Returns the maximum <code>BIGINT</code> value that is equal to or less than <code>a</code><strong>向下取整，最数轴上最接近要求的值的左边的值  如：6.10-&gt;6   -3.4-&gt;-4</strong></td></tr><tr><td>BIGINT</td><td>ceil(DOUBLE a), ceiling(DOUBLE a)</td><td>Returns the minimum BIGINT value that is equal to or greater than <code>a</code>.<strong>求其不小于小给定实数的最小整数如：ceil(6) = ceil(6.1)= ceil(6.9) = 6</strong></td></tr><tr><td>DOUBLE</td><td>rand(), rand(INT seed)</td><td>Returns a random number (that changes from row to row) that is distributed uniformly from 0 to 1. Specifying the seed will make sure the generated random number sequence is deterministic.<strong>每行返回一个DOUBLE型随机数seed是随机因子</strong></td></tr><tr><td>DOUBLE</td><td>exp(DOUBLE a), exp(DECIMAL a)</td><td>Returns <code>ea</code> where <code>e</code> is the base of the natural logarithm. Decimal version added in <a href="https://issues.apache.org/jira/browse/HIVE-6327" target="_blank" rel="noopener">Hive 0.13.0</a>.<strong>返回e的a幂次方， a可为小数</strong></td></tr><tr><td>DOUBLE</td><td>ln(DOUBLE a), ln(DECIMAL a)</td><td>Returns the natural logarithm of the argument <code>a</code>. Decimal version added in <a href="https://issues.apache.org/jira/browse/HIVE-6327" target="_blank" rel="noopener">Hive 0.13.0</a>.<strong>以自然数为底d的对数，a可为小数</strong></td></tr><tr><td>DOUBLE</td><td>log10(DOUBLE a), log10(DECIMAL a)</td><td>Returns the base-10 logarithm of the argument <code>a</code>. Decimal version added in <a href="https://issues.apache.org/jira/browse/HIVE-6327" target="_blank" rel="noopener">Hive 0.13.0</a>.<strong>以10为底d的对数，a可为小数</strong></td></tr><tr><td>DOUBLE</td><td>log2(DOUBLE a), log2(DECIMAL a)</td><td>Returns the base-2 logarithm of the argument <code>a</code>. Decimal version added in <a href="https://issues.apache.org/jira/browse/HIVE-6327" target="_blank" rel="noopener">Hive 0.13.0</a>.<strong>以2为底数d的对数，a可为小数</strong></td></tr><tr><td>DOUBLE</td><td>log(DOUBLE base, DOUBLE a)log(DECIMAL base, DECIMAL a)</td><td>Returns the base-<code>base</code> logarithm of the argument <code>a</code>. Decimal versions added in <a href="https://issues.apache.org/jira/browse/HIVE-6327" target="_blank" rel="noopener">Hive 0.13.0</a>.<strong>以base为底的对数，base 与 a都是DOUBLE类型</strong></td></tr><tr><td>DOUBLE</td><td>pow(DOUBLE a, DOUBLE p), power(DOUBLE a, DOUBLE p)</td><td>Returns <code>ap</code>.<strong>计算a的p次幂</strong></td></tr><tr><td>DOUBLE</td><td>sqrt(DOUBLE a), sqrt(DECIMAL a)</td><td>Returns the square root of <code>a</code>. Decimal version added in <a href="https://issues.apache.org/jira/browse/HIVE-6327" target="_blank" rel="noopener">Hive 0.13.0</a>.<strong>计算a的平方根</strong></td></tr><tr><td>STRING</td><td>bin(BIGINT a)</td><td>Returns the number in binary format (see <a href="http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_bin).**计算二进制a的STRING类型，a为BIGINT类型" target="_blank" rel="noopener">http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_bin).**计算二进制a的STRING类型，a为BIGINT类型</a>**</td></tr><tr><td>STRING</td><td>hex(BIGINT a) hex(STRING a) hex(BINARY a)</td><td>If the argument is an <code>INT</code> or <code>binary</code>, <code>hex</code> returns the number as a <code>STRING</code> in hexadecimal format. Otherwise if the number is a <code>STRING</code>, it converts each character into its hexadecimal representation and returns the resulting <code>STRING</code>. (See<a href="http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_hex" target="_blank" rel="noopener">http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_hex</a>, <code>BINARY</code> version as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-2482" target="_blank" rel="noopener">0.12.0</a>.)<strong>计算十六进制a的STRING类型，如果a为STRING类型就转换成字符相对应的十六进制</strong></td></tr><tr><td>BINARY</td><td>unhex(STRING a)</td><td>Inverse of hex. Interprets each pair of characters as a hexadecimal number and converts to the byte representation of the number. (<code>BINARY</code> version as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-2482" target="_blank" rel="noopener">0.12.0</a>, used to return a string.)<strong>hex的逆方法</strong></td></tr><tr><td>STRING</td><td>conv(BIGINT num, INT from_base, INT to_base), conv(STRING num, INT from_base, INT to_base)</td><td>Converts a number from a given base to another (see <a href="http://dev.mysql.com/doc/refman/5.0/en/mathematical-functions.html#function_conv).**将GIGINT/STRING类型的num从from_base进制转换成to_base进制" target="_blank" rel="noopener">http://dev.mysql.com/doc/refman/5.0/en/mathematical-functions.html#function_conv).**将GIGINT/STRING类型的num从from_base进制转换成to_base进制</a>**</td></tr><tr><td>DOUBLE</td><td>abs(DOUBLE a)</td><td>Returns the absolute value.<strong>计算a的绝对值</strong></td></tr><tr><td>INT or DOUBLE</td><td>pmod(INT a, INT b), pmod(DOUBLE a, DOUBLE b)</td><td>Returns the positive value of <code>a mod b</code>.<strong>a对b取模</strong></td></tr><tr><td>DOUBLE</td><td>sin(DOUBLE a), sin(DECIMAL a)</td><td>Returns the sine of <code>a</code> (<code>a</code> is in radians). Decimal version added in <a href="https://issues.apache.org/jira/browse/HIVE-6327" target="_blank" rel="noopener">Hive 0.13.0</a>.<strong>求a的正弦值</strong></td></tr><tr><td>DOUBLE</td><td>asin(DOUBLE a), asin(DECIMAL a)</td><td>Returns the arc sin of <code>a</code> if -1&lt;=a&lt;=1 or NULL otherwise. Decimal version added in <a href="https://issues.apache.org/jira/browse/HIVE-6327" target="_blank" rel="noopener">Hive 0.13.0</a>.<strong>求d的反正弦值</strong></td></tr><tr><td>DOUBLE</td><td>cos(DOUBLE a), cos(DECIMAL a)</td><td>Returns the cosine of <code>a</code> (<code>a</code> is in radians). Decimal version added in <a href="https://issues.apache.org/jira/browse/HIVE-6327" target="_blank" rel="noopener">Hive 0.13.0</a>.<strong>求余弦值</strong></td></tr><tr><td>DOUBLE</td><td>acos(DOUBLE a), acos(DECIMAL a)</td><td>Returns the arccosine of <code>a</code> if -1&lt;=a&lt;=1 or NULL otherwise. Decimal version added in <a href="https://issues.apache.org/jira/browse/HIVE-6327" target="_blank" rel="noopener">Hive 0.13.0</a>.<strong>求反余弦值</strong></td></tr><tr><td>DOUBLE</td><td>tan(DOUBLE a), tan(DECIMAL a)</td><td>Returns the tangent of <code>a</code> (<code>a</code> is in radians). Decimal version added in <a href="https://issues.apache.org/jira/browse/HIVE-6327" target="_blank" rel="noopener">Hive 0.13.0</a>.<strong>求正切值</strong></td></tr><tr><td>DOUBLE</td><td>atan(DOUBLE a), atan(DECIMAL a)</td><td>Returns the arctangent of <code>a</code>. Decimal version added in <a href="https://issues.apache.org/jira/browse/HIVE-6327" target="_blank" rel="noopener">Hive 0.13.0</a>.<strong>求反正切值</strong></td></tr><tr><td>DOUBLE</td><td>degrees(DOUBLE a), degrees(DECIMAL a)</td><td>Converts value of <code>a</code> from radians to degrees. Decimal version added in <a href="https://issues.apache.org/jira/browse/HIVE-6385" target="_blank" rel="noopener">Hive 0.13.0</a>.<strong>奖弧度值转换角度值</strong></td></tr><tr><td>DOUBLE</td><td>radians(DOUBLE a), radians(DOUBLE a)</td><td>Converts value of <code>a</code> from degrees to radians. Decimal version added in <a href="https://issues.apache.org/jira/browse/HIVE-6327" target="_blank" rel="noopener">Hive 0.13.0</a>.<strong>将角度值转换成弧度值</strong></td></tr><tr><td>INT or DOUBLE</td><td>positive(INT a), positive(DOUBLE a)</td><td>Returns <code>a</code>.<strong>返回a</strong></td></tr><tr><td>INT or DOUBLE</td><td>negative(INT a), negative(DOUBLE a)</td><td>Returns <code>-a</code>.<strong>返回a的相反数</strong></td></tr><tr><td>DOUBLE or INT</td><td>sign(DOUBLE a), sign(DECIMAL a)</td><td>Returns the sign of <code>a</code> as ‘1.0’ (if <code>a</code> is positive) or ‘-1.0’ (if <code>a</code> is negative), ‘0.0’ otherwise. The decimal version returns INT instead of DOUBLE. Decimal version added in <a href="https://issues.apache.org/jira/browse/HIVE-6246" target="_blank" rel="noopener">Hive 0.13.0</a>.<strong>如果a是正数则返回1.0，是负数则返回-1.0，否则返回0.0</strong></td></tr><tr><td>DOUBLE</td><td>e()</td><td>Returns the value of <code>e</code>.<strong>数学常数e</strong></td></tr><tr><td>DOUBLE</td><td>pi()</td><td>Returns the value of <code>pi</code>.<strong>数学常数pi</strong></td></tr><tr><td>BIGINT</td><td>factorial(INT a)</td><td>Returns the factorial of <code>a</code> (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-9857" target="_blank" rel="noopener">1.2.0</a>). Valid <code>a</code> is [0..20]. <strong>求a的阶乘</strong></td></tr><tr><td>DOUBLE</td><td>cbrt(DOUBLE a)</td><td>Returns the cube root of <code>a</code> double value (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-9858" target="_blank" rel="noopener">1.2.0</a>). <strong>求a的立方根</strong></td></tr><tr><td>INT BIGINT</td><td>shiftleft(TINYINT|SMALLINT|INT a, INT b)shiftleft(BIGINT a, INT b)</td><td>Bitwise left shift (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-9859" target="_blank" rel="noopener">1.2.0</a>). Shifts <code>a</code> <code>b</code> positions to the left.Returns int for tinyint, smallint and int <code>a</code>. Returns bigint for bigint <code>a</code>.<strong>按位左移</strong></td></tr><tr><td>INTBIGINT</td><td>shiftright(TINYINT|SMALLINT|INT a, INTb)shiftright(BIGINT a, INT b)</td><td>Bitwise right shift (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-9859" target="_blank" rel="noopener">1.2.0</a>). Shifts <code>a</code> <code>b</code> positions to the right.Returns int for tinyint, smallint and int <code>a</code>. Returns bigint for bigint <code>a</code>.<strong>按拉右移</strong></td></tr><tr><td>INTBIGINT</td><td>shiftrightunsigned(TINYINT|SMALLINT|INTa, INT b),shiftrightunsigned(BIGINT a, INT b)</td><td>Bitwise unsigned right shift (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-9859" target="_blank" rel="noopener">1.2.0</a>). Shifts <code>a</code> <code>b</code> positions to the right.Returns int for tinyint, smallint and int <code>a</code>. Returns bigint for bigint <code>a</code>.<strong>无符号按位右移（&lt;&lt;&lt;）</strong></td></tr><tr><td>T</td><td>greatest(T v1, T v2, …)</td><td>Returns the greatest value of the list of values (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-9402" target="_blank" rel="noopener">1.1.0</a>). Fixed to return NULL when one or more arguments are NULL, and strict type restriction relaxed, consistent with “&gt;” operator (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-12082" target="_blank" rel="noopener">2.0.0</a>). <strong>求最大值</strong></td></tr><tr><td>T</td><td>least(T v1, T v2, …)</td><td>Returns the least value of the list of values (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-9402" target="_blank" rel="noopener">1.1.0</a>). Fixed to return NULL when one or more arguments are NULL, and strict type restriction relaxed, consistent with “&lt;” operator (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-12082" target="_blank" rel="noopener">2.0.0</a>). <strong>求最小值</strong></td></tr></tbody></table><h2 id="集合函数"><a href="#集合函数" class="headerlink" title="集合函数"></a><strong>集合函数</strong></h2><table><thead><tr><th><strong>Return Type</strong></th><th><strong>Name(Signature)</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr><td>int</td><td>size(Map&lt;K.V&gt;)</td><td>Returns the number of elements in the map type.<strong>求map的长度</strong></td></tr><tr><td>int</td><td>size(Array<t>)</t></td><td>Returns the number of elements in the array type.<strong>求数组的长度</strong></td></tr><tr><td>array<k></k></td><td>map_keys(Map&lt;K.V&gt;)</td><td>Returns an unordered array containing the keys of the input map.<strong>返回map中的所有key</strong></td></tr><tr><td>array<v></v></td><td>map_values(Map&lt;K.V&gt;)</td><td>Returns an unordered array containing the values of the input map.<strong>返回map中的所有value</strong></td></tr><tr><td>boolean</td><td>array_contains(Array<t>, value)</t></td><td>Returns TRUE if the array contains value.<strong>如该数组Array<t>包含value返回true。，否则返回false</t></strong></td></tr><tr><td>array</td><td>sort_array(Array<t>)</t></td><td>Sorts the input array in ascending order according to the natural ordering of the array elements and returns it (as of version <a href="https://issues.apache.org/jira/browse/HIVE-2279" target="_blank" rel="noopener">0.9.0</a>).<strong>按自然顺序对数组进行排序并返回</strong></td></tr></tbody></table><h2 id="类型转换函数"><a href="#类型转换函数" class="headerlink" title="类型转换函数"></a><strong>类型转换函数</strong></h2><table><thead><tr><th><strong>Return Type</strong></th><th><strong>Name(Signature)</strong></th><th>Description</th></tr></thead><tbody><tr><td>binary</td><td>binary(string|binary)</td><td>Casts the parameter into a binary.<strong>将输入的值转换成二进制</strong></td></tr><tr><td><strong>Expected “=” to follow “type”</strong></td><td>cast(expr as <type>)</type></td><td>Converts the results of the expression expr to <type>. For example, cast(‘1’ as BIGINT) will convert the string ‘1’ to its integral representation. A null is returned if the conversion does not succeed. If cast(expr as boolean) Hive returns true for a non-empty string.<strong>将expr转换成type类型 如：cast(“1” as BIGINT) 将字符串1转换成了BIGINT类型，如果转换失败将返回NULL</strong></type></td></tr></tbody></table><h2 id="日期函数"><a href="#日期函数" class="headerlink" title="日期函数"></a><strong>日期函数</strong></h2><table><thead><tr><th><strong>Return Type</strong></th><th><strong>Name(Signature)</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr><td>string</td><td>from_unixtime(bigint unixtime[, string format])</td><td>Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the format of “1970-01-01 00:00:00”.<strong>将时间的秒值转换成format格式（format可为“yyyy-MM-dd hh:mm:ss”,“yyyy-MM-dd hh”,“yyyy-MM-dd hh:mm”等等）如from_unixtime(1250111000,”yyyy-MM-dd”) 得到2009-03-12</strong></td></tr><tr><td>bigint</td><td>unix_timestamp()</td><td>Gets current Unix timestamp in seconds.<strong>获取本地时区下的时间戳</strong></td></tr><tr><td>bigint</td><td>unix_timestamp(string date)</td><td>Converts time string in format <code>yyyy-MM-dd HH:mm:ss</code> to Unix timestamp (in seconds), using the default timezone and the default locale, return 0 if fail: unix_timestamp(‘2009-03-20 11:30:01’) = 1237573801<strong>将格式为yyyy-MM-dd HH:mm:ss的时间字符串转换成时间戳  如unix_timestamp(‘2009-03-20 11:30:01’) = 1237573801</strong></td></tr><tr><td>bigint</td><td>unix_timestamp(string date, string pattern)</td><td>Convert time string with given pattern (see [<a href="http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html]" target="_blank" rel="noopener">http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html]</a>) to Unix time stamp (in seconds), return 0 if fail: unix_timestamp(‘2009-03-20’, ‘yyyy-MM-dd’) = 1237532400.<strong>将指定时间字符串格式字符串转换成Unix时间戳，如果格式不对返回0 如：unix_timestamp(‘2009-03-20’, ‘yyyy-MM-dd’) = 1237532400</strong></td></tr><tr><td>string</td><td>to_date(string timestamp)</td><td>Returns the date part of a timestamp string: to_date(“1970-01-01 00:00:00”) = “1970-01-01”.<strong>返回时间字符串的日期部分</strong></td></tr><tr><td>int</td><td>year(string date)</td><td>Returns the year part of a date or a timestamp string: year(“1970-01-01 00:00:00”) = 1970, year(“1970-01-01”) = 1970.<strong>返回时间字符串的年份部分</strong></td></tr><tr><td>int</td><td>quarter(date/timestamp/string)</td><td>Returns the quarter of the year for a date, timestamp, or string in the range 1 to 4 (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-3404" target="_blank" rel="noopener">1.3.0</a>). Example: quarter(‘2015-04-08’) = 2.<strong>返回当前时间属性哪个季度 如quarter(‘2015-04-08’) = 2</strong></td></tr><tr><td>int</td><td>month(string date)</td><td>Returns the month part of a date or a timestamp string: month(“1970-11-01 00:00:00”) = 11, month(“1970-11-01”) = 11.<strong>返回时间字符串的月份部分</strong></td></tr><tr><td>int</td><td>day(string date) dayofmonth(date)</td><td>Returns the day part of a date or a timestamp string: day(“1970-11-01 00:00:00”) = 1, day(“1970-11-01”) = 1.<strong>返回时间字符串的天</strong></td></tr><tr><td>int</td><td>hour(string date)</td><td>Returns the hour of the timestamp: hour(‘2009-07-30 12:58:59’) = 12, hour(‘12:58:59’) = 12.<strong>返回时间字符串的小时</strong></td></tr><tr><td>int</td><td>minute(string date)</td><td>Returns the minute of the timestamp.<strong>返回时间字符串的分钟</strong></td></tr><tr><td>int</td><td>second(string date)</td><td>Returns the second of the timestamp.<strong>返回时间字符串的秒</strong></td></tr><tr><td>int</td><td>weekofyear(string date)</td><td>Returns the week number of a timestamp string: weekofyear(“1970-11-01 00:00:00”) = 44, weekofyear(“1970-11-01”) = 44.<strong>返回时间字符串位于一年中的第几个周内  如weekofyear(“1970-11-01 00:00:00”) = 44, weekofyear(“1970-11-01”) = 44</strong></td></tr><tr><td>int</td><td>datediff(string enddate, string startdate)</td><td>Returns the number of days from startdate to enddate: datediff(‘2009-03-01’, ‘2009-02-27’) = 2.<strong>计算开始时间startdate到结束时间enddate相差的天数</strong></td></tr><tr><td>string</td><td>date_add(string startdate, int days)</td><td>Adds a number of days to startdate: date_add(‘2008-12-31’, 1) = ‘2009-01-01’.<strong>从开始时间startdate加上days</strong></td></tr><tr><td>string</td><td>date_sub(string startdate, int days)</td><td>Subtracts a number of days to startdate: date_sub(‘2008-12-31’, 1) = ‘2008-12-30’.<strong>从开始时间startdate减去days</strong></td></tr><tr><td>timestamp</td><td>from_utc_timestamp(timestamp, string timezone)</td><td>Assumes given timestamp is UTC and converts to given timezone (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-2272" target="_blank" rel="noopener">0.8.0</a>). For example, from_utc_timestamp(‘1970-01-01 08:00:00’,’PST’) returns 1970-01-01 00:00:00.<strong>如果给定的时间戳并非UTC，则将其转化成指定的时区下时间戳</strong></td></tr><tr><td>timestamp</td><td>to_utc_timestamp(timestamp, string timezone)</td><td>Assumes given timestamp is in given timezone and converts to UTC (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-2272" target="_blank" rel="noopener">0.8.0</a>). For example, to_utc_timestamp(‘1970-01-01 00:00:00’,’PST’) returns 1970-01-01 08:00:00.<strong>如果给定的时间戳指定的时区下时间戳，则将其转化成UTC下的时间戳</strong></td></tr><tr><td>date</td><td>current_date</td><td>Returns the current date at the start of query evaluation (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-5472" target="_blank" rel="noopener">1.2.0</a>). All calls of current_date within the same query return the same value.<strong>返回当前时间日期</strong></td></tr><tr><td>timestamp</td><td>current_timestamp</td><td>Returns the current timestamp at the start of query evaluation (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-5472" target="_blank" rel="noopener">1.2.0</a>). All calls of current_timestamp within the same query return the same value.<strong>返回当前时间戳</strong></td></tr><tr><td>string</td><td>add_months(string start_date, int num_months)</td><td>Returns the date that is num_months after start_date (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-9357" target="_blank" rel="noopener">1.1.0</a>). start_date is a string, date or timestamp. num_months is an integer. The time part of start_date is ignored. If start_date is the last day of the month or if the resulting month has fewer days than the day component of start_date, then the result is the last day of the resulting month. Otherwise, the result has the same day component as start_date.<strong>返回当前时间下再增加num_months个月的日期</strong></td></tr><tr><td>string</td><td>last_day(string date)</td><td>Returns the last day of the month which the date belongs to (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-9358" target="_blank" rel="noopener">1.1.0</a>). date is a string in the format ‘yyyy-MM-dd HH:mm:ss’ or ‘yyyy-MM-dd’. The time part of date is ignored.<strong>返回这个月的最后一天的日期，忽略时分秒部分（HH:mm:ss）</strong></td></tr><tr><td>string</td><td>next_day(string start_date, string day_of_week)</td><td>Returns the first date which is later than start_date and named as day_of_week (as of Hive<a href="https://issues.apache.org/jira/browse/HIVE-9520" target="_blank" rel="noopener">1.2.0</a>). start_date is a string/date/timestamp. day_of_week is 2 letters, 3 letters or full name of the day of the week (e.g. Mo, tue, FRIDAY). The time part of start_date is ignored. Example: next_day(‘2015-01-14’, ‘TU’) = 2015-01-20.<strong>返回当前时间的下一个星期X所对应的日期 如：next_day(‘2015-01-14’, ‘TU’) = 2015-01-20  以2015-01-14为开始时间，其下一个星期二所对应的日期为2015-01-20</strong></td></tr><tr><td>string</td><td>trunc(string date, string format)</td><td>Returns date truncated to the unit specified by the format (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-9480" target="_blank" rel="noopener">1.2.0</a>). Supported formats: MONTH/MON/MM, YEAR/YYYY/YY. Example: trunc(‘2015-03-17’, ‘MM’) = 2015-03-01.<strong>返回时间的最开始年份或月份  如trunc(“2016-06-26”,“MM”)=2016-06-01  trunc(“2016-06-26”,“YY”)=2016-01-01   注意所支持的格式为MONTH/MON/MM, YEAR/YYYY/YY</strong></td></tr><tr><td>double</td><td>months_between(date1, date2)</td><td>Returns number of months between dates date1 and date2 (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-9518" target="_blank" rel="noopener">1.2.0</a>). If date1 is later than date2, then the result is positive. If date1 is earlier than date2, then the result is negative. If date1 and date2 are either the same days of the month or both last days of months, then the result is always an integer. Otherwise the UDF calculates the fractional portion of the result based on a 31-day month and considers the difference in time components date1 and date2. date1 and date2 type can be date, timestamp or string in the format ‘yyyy-MM-dd’ or ‘yyyy-MM-dd HH:mm:ss’. The result is rounded to 8 decimal places. Example: months_between(‘1997-02-28 10:30:00’, ‘1996-10-30’) = 3.94959677<strong>返回date1与date2之间相差的月份，如date1&gt;date2，则返回正，如果date1&lt;date2,则返回负，否则返回0.0  如：months_between(‘1997-02-28 10:30:00’, ‘1996-10-30’) = 3.94959677  1997-02-28 10:30:00与1996-10-30相差3.94959677个月</strong></td></tr><tr><td>string</td><td>date_format(date/timestamp/string ts, string fmt)</td><td>Converts a date/timestamp/string to a value of string in the format specified by the date format fmt (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-10276" target="_blank" rel="noopener">1.2.0</a>). Supported formats are Java SimpleDateFormat formats –<a href="https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html" target="_blank" rel="noopener">https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html</a>. The second argument fmt should be constant. Example: date_format(‘2015-04-08’, ‘y’) = ‘2015’.date_format can be used to implement other UDFs, e.g.:dayname(date) is date_format(date, ‘EEEE’)dayofyear(date) is date_format(date, ‘D’)<strong>按指定格式返回时间date 如：date_format(“2016-06-22”,”MM-dd”)=06-22</strong></td></tr></tbody></table><h2 id="条件函数"><a href="#条件函数" class="headerlink" title="条件函数"></a><strong>条件函数</strong></h2><table><thead><tr><th><strong>Return Type</strong></th><th><strong>Name(Signature)</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr><td>T</td><td>if(boolean testCondition, T valueTrue, T valueFalseOrNull)</td><td>Returns valueTrue when testCondition is true, returns valueFalseOrNull otherwise.<strong>如果testCondition 为true就返回valueTrue,否则返回valueFalseOrNull ，（valueTrue，valueFalseOrNull为泛型）</strong></td></tr><tr><td>T</td><td>nvl(T value, T default_value)</td><td>Returns default value if value is null else returns value (as of HIve <a href="https://issues.apache.org/jira/browse/HIVE-2288" target="_blank" rel="noopener">0.11</a>).<strong>如果value值为NULL就返回default_value,否则返回value</strong></td></tr><tr><td>T</td><td>COALESCE(T v1, T v2, …)</td><td>Returns the first v that is not NULL, or NULL if all v’s are NULL.<strong>返回第一非null的值，如果全部都为NULL就返回NULL  如：COALESCE (NULL,44,55)=44/strong&gt;</strong></td></tr><tr><td>T</td><td>CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END</td><td>When a = b, returns c; when a = d, returns e; else returns f.<strong>如果a=b就返回c,a=d就返回e，否则返回f  如CASE 4 WHEN 5  THEN 5 WHEN 4 THEN 4 ELSE 3 END 将返回4</strong></td></tr><tr><td>T</td><td>CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END</td><td>When a = true, returns b; when c = true, returns d; else returns e.<strong>如果a=ture就返回b,c= ture就返回d，否则返回e  如：CASE WHEN  5&gt;0  THEN 5 WHEN 4&gt;0 THEN 4 ELSE 0 END 将返回5；CASE WHEN  5&lt;0  THEN 5 WHEN 4&lt;0 THEN 4 ELSE 0 END 将返回0</strong></td></tr><tr><td>boolean</td><td>isnull( a )</td><td>Returns true if a is NULL and false otherwise.<strong>如果a为null就返回true，否则返回false</strong></td></tr><tr><td>boolean</td><td>isnotnull ( a )</td><td>Returns true if a is not NULL and false otherwise.<strong>如果a为非null就返回true，否则返回false</strong></td></tr></tbody></table><h2 id="字符函数"><a href="#字符函数" class="headerlink" title="字符函数"></a><strong>字符函数</strong></h2><table><thead><tr><th><strong>Return Type</strong></th><th><strong>Name(Signature)</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr><td>int</td><td>ascii(string str)</td><td>Returns the numeric value of the first  character of str.<strong>返回str中首个ASCII字符串的整数值</strong></td></tr><tr><td>string</td><td>base64(binary bin)</td><td>Converts the argument from binary to a base 64 string (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-2482" target="_blank" rel="noopener">0.12.0</a>)..<strong>将二进制bin转换成64位的字符串</strong></td></tr><tr><td>string</td><td>concat(string|binary A, string|binary B…)</td><td>Returns the string or bytes resulting from concatenating the strings or bytes passed in as parameters in order. For example, concat(‘foo’, ‘bar’) results in ‘foobar’. Note that this function can take any number of input strings..<strong>对二进制字节码或字符串按次序进行拼接</strong></td></tr><tr><td>array&lt;struct&lt;string,double&gt;&gt;</td><td>context_ngrams(array&lt;array<string>&gt;, array<string>, int K, int pf)</string></string></td><td>Returns the top-k contextual N-grams from a set of tokenized sentences, given a string of “context”. See <a href="https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining" target="_blank" rel="noopener">StatisticsAndDataMining</a> for more information..<strong>与ngram类似，但context_ngram()允许你预算指定上下文(数组)来去查找子序列，具体看StatisticsAndDataMining(这里的解释更易懂)</strong></td></tr><tr><td>string</td><td>concat_ws(string SEP, string A, string B…)</td><td>Like concat() above, but with custom separator SEP..<strong>与concat()类似，但使用指定的分隔符喜进行分隔</strong></td></tr><tr><td>string</td><td>concat_ws(string SEP, array<string>)</string></td><td>Like concat_ws() above, but taking an array of strings. (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-2203" target="_blank" rel="noopener">0.9.0</a>).<strong>拼接Array中的元素并用指定分隔符进行分隔</strong></td></tr><tr><td>string</td><td>decode(binary bin, string charset)</td><td>Decodes the first argument into a String using the provided character set (one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’). If either argument is null, the result will also be null. (As of Hive <a href="https://issues.apache.org/jira/browse/HIVE-2482" target="_blank" rel="noopener">0.12.0</a>.).<strong>使用指定的字符集charset将二进制值bin解码成字符串，支持的字符集有：’US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’，如果任意输入参数为NULL都将返回NULL</strong></td></tr><tr><td>binary</td><td>encode(string src, string charset)</td><td>Encodes the first argument into a BINARY using the provided character set (one of ‘US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’). If either argument is null, the result will also be null. (As of Hive <a href="https://issues.apache.org/jira/browse/HIVE-2482" target="_blank" rel="noopener">0.12.0</a>.).<strong>使用指定的字符集charset将字符串编码成二进制值，支持的字符集有：’US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’，如果任一输入参数为NULL都将返回NULL</strong></td></tr><tr><td>int</td><td>find_in_set(string str, string strList)</td><td>Returns the first occurance of str in strList where strList is a comma-delimited string. Returns null if either argument is null. Returns 0 if the first argument contains any commas. For example, find_in_set(‘ab’, ‘abc,b,ab,c,def’) returns 3..<strong>返回以逗号分隔的字符串中str出现的位置，如果参数str为逗号或查找失败将返回0，如果任一参数为NULL将返回NULL回</strong></td></tr><tr><td>string</td><td>format_number(number x, int d)</td><td>Formats the number X to a format like ‘#,###,###.##’, rounded to D decimal places, and returns the result as a string. If D is 0, the result has no decimal point or fractional part. (As of Hive <a href="https://issues.apache.org/jira/browse/HIVE-2694" target="_blank" rel="noopener">0.10.0</a>; bug with float types fixed in <a href="https://issues.apache.org/jira/browse/HIVE-7257" target="_blank" rel="noopener">Hive 0.14.0</a>, decimal type support added in <a href="https://issues.apache.org/jira/browse/HIVE-7279" target="_blank" rel="noopener">Hive 0.14.0</a>).<strong>将数值X转换成”#,###,###.##”格式字符串，并保留d位小数，如果d为0，将进行四舍五入且不保留小数</strong></td></tr><tr><td>string</td><td>get_json_object(string json_string, string path)</td><td>Extracts json object from a json string based on json path specified, and returns json string of the extracted json object. It will return null if the input json string is invalid. <strong>NOTE: The json path can only have the characters [0-9a-z_], i.e., no upper-case or special characters. Also, the keys *cannot start with numbers.*</strong> This is due to restrictions on Hive column names..<strong>从指定路径上的JSON字符串抽取出JSON对象，并返回这个对象的JSON格式，如果输入的JSON是非法的将返回NULL,注意此路径上JSON字符串只能由数字 字母 下划线组成且不能有大写字母和特殊字符，且key不能由数字开头，这是由于Hive对列名的限制</strong></td></tr><tr><td>boolean</td><td>in_file(string str, string filename)</td><td>Returns true if the string str appears as an entire line in filename..<strong>如果文件名为filename的文件中有一行数据与字符串str匹配成功就返回true</strong></td></tr><tr><td>int</td><td>instr(string str, string substr)</td><td>Returns the position of the first occurrence of <code>substr</code> in <code>str</code>. Returns <code>null</code> if either of the arguments are <code>null</code> and returns <code>0</code> if <code>substr</code> could not be found in <code>str</code>. Be aware that this is not zero based. The first character in <code>str</code> has index 1..<strong>查找字符串str中子字符串substr出现的位置，如果查找失败将返回0，如果任一参数为Null将返回null，注意位置为从1开始的</strong></td></tr><tr><td>int</td><td>length(string A)</td><td>Returns the length of the string..<strong>返回字符串的长度</strong></td></tr><tr><td>int</td><td>locate(string substr, string str[, int pos])</td><td>Returns the position of the first occurrence of substr in str after position pos..<strong>查找字符串str中的pos位置后字符串substr第一次出现的位置</strong></td></tr><tr><td>string</td><td>lower(string A) lcase(string A)</td><td>Returns the string resulting from converting all characters of B to lower case. For example, lower(‘fOoBaR’) results in ‘foobar’..<strong>将字符串A的所有字母转换成小写字母</strong></td></tr><tr><td>string</td><td>lpad(string str, int len, string pad)</td><td>Returns str, left-padded with pad to a length of len..<strong>从左边开始对字符串str使用字符串pad填充，最终len长度为止，如果字符串str本身长度比len大的话，将去掉多余的部分</strong></td></tr><tr><td>string</td><td>ltrim(string A)</td><td>Returns the string resulting from trimming spaces from the beginning(left hand side) of A. For example, ltrim(‘ foobar ‘) results in ‘foobar ‘..<strong>去掉字符串A前面的空格</strong></td></tr><tr><td>array&lt;struct&lt;string,double&gt;&gt;</td><td>ngrams(array&lt;array<string>&gt;, int N, int K, int pf)</string></td><td>Returns the top-k N-grams from a set of tokenized sentences, such as those returned by the sentences() UDAF. See <a href="https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining" target="_blank" rel="noopener">StatisticsAndDataMining</a> for more information..<strong>返回出现次数TOP K的的子序列,n表示子序列的长度，具体看StatisticsAndDataMining (这里的解释更易懂)</strong></td></tr><tr><td>string</td><td>parse_url(string urlString, string partToExtract [, string keyToExtract])</td><td>Returns the specified part from the URL. Valid values for partToExtract include HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO. For example, parse_url(‘<a href="http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;" target="_blank" rel="noopener">http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;</a>, ‘HOST’) returns ‘facebook.com’. Also a value of a particular key in QUERY can be extracted by providing the key as the third argument, for example, parse_url(‘<a href="http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;" target="_blank" rel="noopener">http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;</a>, ‘QUERY’, ‘k1’) returns ‘v1’..<strong>返回从URL中抽取指定部分的内容，参数url是URL字符串，而参数partToExtract是要抽取的部分，这个参数包含(HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO,例如：parse_url(‘<a href="http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;" target="_blank" rel="noopener">http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;</a>, ‘HOST’) =’facebook.com’，如果参数partToExtract值为QUERY则必须指定第三个参数key  如：parse_url(‘<a href="http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;" target="_blank" rel="noopener">http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;</a>, ‘QUERY’, ‘k1’) =‘v1’</strong></td></tr><tr><td>string</td><td>printf(String format, Obj… args)</td><td>Returns the input formatted according do printf-style format strings (as of Hive<a href="https://issues.apache.org/jira/browse/HIVE-2695" target="_blank" rel="noopener">0.9.0</a>)..<strong>按照printf风格格式输出字符串</strong></td></tr><tr><td>string</td><td>regexp_extract(string subject, string pattern, int index)</td><td>Returns the string extracted using the pattern. For example, regexp_extract(‘foothebar’, ‘foo(.<em>?)(bar)’, 2) returns ‘bar.’ Note that some care is necessary in using predefined character classes: using ‘\s’ as the second argument will match the letter s; ‘\s’ is necessary to match whitespace, etc. The ‘index’ parameter is the Java regex Matcher group() method index. See docs/api/java/util/regex/Matcher.html for more information on the ‘index’ or Java regex group() method..*</em>抽取字符串subject中符合正则表达式pattern的第index个部分的子字符串，注意些预定义字符的使用，如第二个参数如果使用’\s’将被匹配到s,’\s’才是匹配空格**</td></tr><tr><td>string</td><td>regexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT)</td><td>Returns the string resulting from replacing all substrings in INITIAL_STRING that match the java regular expression syntax defined in PATTERN with instances of REPLACEMENT. For example, regexp_replace(“foobar”, “oo|ar”, “”) returns ‘fb.’ Note that some care is necessary in using predefined character classes: using ‘\s’ as the second argument will match the letter s; ‘\s’ is necessary to match whitespace, etc..<strong>按照Java正则表达式PATTERN将字符串INTIAL_STRING中符合条件的部分成REPLACEMENT所指定的字符串，如里REPLACEMENT这空的话，抽符合正则的部分将被去掉  如：regexp_replace(“foobar”, “oo|ar”, “”) = ‘fb.’ 注意些预定义字符的使用，如第二个参数如果使用’\s’将被匹配到s,’\s’才是匹配空格</strong></td></tr><tr><td>string</td><td>repeat(string str, int n)</td><td>Repeats str n times..<strong>重复输出n次字符串str</strong></td></tr><tr><td>string</td><td>reverse(string A)</td><td>Returns the reversed string..<strong>反转字符串</strong></td></tr><tr><td>string</td><td>rpad(string str, int len, string pad)</td><td>Returns str, right-padded with pad to a length of len..<strong>从右边开始对字符串str使用字符串pad填充，最终len长度为止，如果字符串str本身长度比len大的话，将去掉多余的部分</strong></td></tr><tr><td>string</td><td>rtrim(string A)</td><td>Returns the string resulting from trimming spaces from the end(right hand side) of A. For example, rtrim(‘ foobar ‘) results in ‘ foobar’..<strong>去掉字符串后面出现的空格</strong></td></tr><tr><td>array&lt;array<string>&gt;</string></td><td>sentences(string str, string lang, string locale)</td><td>Tokenizes a string of natural language text into words and sentences, where each sentence is broken at the appropriate sentence boundary and returned as an array of words. The ‘lang’ and ‘locale’ are optional arguments. For example, sentences(‘Hello there! How are you?’) returns ( (“Hello”, “there”), (“How”, “are”, “you”) )..<strong>字符串str将被转换成单词数组，如：sentences(‘Hello there! How are you?’) =( (“Hello”, “there”), (“How”, “are”, “you”) )</strong></td></tr><tr><td>string</td><td>space(int n)</td><td>Returns a string of n spaces..<strong>返回n个空格</strong></td></tr><tr><td>array</td><td>split(string str, string pat)</td><td>Splits str around pat (pat is a regular expression)..<strong>按照正则表达式pat来分割字符串str,并将分割后的数组字符串的形式返回</strong></td></tr><tr><td>map&lt;string,string&gt;</td><td>str_to_map(text[, delimiter1, delimiter2])</td><td>Splits text into key-value pairs using two delimiters. Delimiter1 separates text into K-V pairs, and Delimiter2 splits each K-V pair. Default delimiters are ‘,’ for delimiter1 and ‘=’ for delimiter2..<strong>将字符串str按照指定分隔符转换成Map，第一个参数是需要转换字符串，第二个参数是键值对之间的分隔符，默认为逗号;第三个参数是键值之间的分隔符，默认为”=”</strong></td></tr><tr><td>string</td><td>substr(string|binary A, int start) substring(string|binary A, int start)</td><td>Returns the substring or slice of the byte array of A starting from start position till the end of string A. For example, substr(‘foobar’, 4) results in ‘bar’ (see [<a href="http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_substr])..**对于字符串A,从start位置开始截取字符串并返回" target="_blank" rel="noopener">http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_substr])..**对于字符串A,从start位置开始截取字符串并返回</a>**</td></tr><tr><td>string</td><td>substr(string|binary A, int start, int len) substring(string|binary A, int start, int len)</td><td>Returns the substring or slice of the byte array of A starting from start position with length len. For example, substr(‘foobar’, 4, 1) results in ‘b’ (see [<a href="http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_substr])..**对于二进制/字符串A,从start位置开始截取长度为length的字符串并返回" target="_blank" rel="noopener">http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_substr])..**对于二进制/字符串A,从start位置开始截取长度为length的字符串并返回</a>**</td></tr><tr><td>string</td><td>substring_index(string A, string delim, int count)</td><td>Returns the substring from string A before count occurrences of the delimiter delim (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-686" target="_blank" rel="noopener">1.3.0</a>). If count is positive, everything to the left of the final delimiter (counting from the left) is returned. If count is negative, everything to the right of the final delimiter (counting from the right) is returned. Substring_index performs a case-sensitive match when searching for delim. Example: substring_index(‘<a href="http://www.apache.org&#39;" target="_blank" rel="noopener">www.apache.org&#39;</a>, ‘.’, 2) = ‘<a href="http://www.apache&#39;..**截取第count分隔符之前的字符串，如count为正则从左边开始截取，如果为负则从右边开始截取" target="_blank" rel="noopener">www.apache&#39;..**截取第count分隔符之前的字符串，如count为正则从左边开始截取，如果为负则从右边开始截取</a>**</td></tr><tr><td>string</td><td>translate(string|char|varchar input, string|char|varchar from, string|char|varchar to)</td><td>Translates the input string by replacing the characters present in the <code>from</code> string with the corresponding characters in the <code>to</code> string. This is similar to the <code>translate</code>function in <a href="http://www.postgresql.org/docs/9.1/interactive/functions-string.html" target="_blank" rel="noopener">PostgreSQL</a>. If any of the parameters to this UDF are NULL, the result is NULL as well. (Available as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-2418" target="_blank" rel="noopener">0.10.0</a>, for string types)Char/varchar support added as of <a href="https://issues.apache.org/jira/browse/HIVE-6622" target="_blank" rel="noopener">Hive 0.14.0</a>..<strong>将input出现在from中的字符串替换成to中的字符串 如：translate(“MOBIN”,”BIN”,”M”)=”MOM”</strong></td></tr><tr><td>string</td><td>trim(string A)</td><td>Returns the string resulting from trimming spaces from both ends of A. For example, trim(‘ foobar ‘) results in ‘foobar’.<strong>将字符串A前后出现的空格去掉</strong></td></tr><tr><td>binary</td><td>unbase64(string str)</td><td>Converts the argument from a base 64 string to BINARY. (As of Hive <a href="https://issues.apache.org/jira/browse/HIVE-2482" target="_blank" rel="noopener">0.12.0</a>.).<strong>将64位的字符串转换二进制值</strong></td></tr><tr><td>string</td><td>upper(string A) ucase(string A)</td><td>Returns the string resulting from converting all characters of A to upper case. For example, upper(‘fOoBaR’) results in ‘FOOBAR’..<strong>将字符串A中的字母转换成大写字母</strong></td></tr><tr><td>string</td><td>initcap(string A)</td><td>Returns string, with the first letter of each word in uppercase, all other letters in lowercase. Words are delimited by whitespace. (As of Hive <a href="https://issues.apache.org/jira/browse/HIVE-3405" target="_blank" rel="noopener">1.1.0</a>.).<strong>将字符串A转换第一个字母大写其余字母的字符串</strong></td></tr><tr><td>int</td><td>levenshtein(string A, string B)</td><td>Returns the Levenshtein distance between two strings (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-9556" target="_blank" rel="noopener">1.2.0</a>). For example, levenshtein(‘kitten’, ‘sitting’) results in 3..<strong>计算两个字符串之间的差异大小  如：levenshtein(‘kitten’, ‘sitting’) = 3</strong></td></tr><tr><td>string</td><td>soundex(string A)</td><td>Returns soundex code of the string (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-9738" target="_blank" rel="noopener">1.2.0</a>). For example, soundex(‘Miller’) results in M460..<strong>将普通字符串转换成soundex字符串</strong></td></tr></tbody></table><h2 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a><strong>聚合函数</strong></h2><table><thead><tr><th><strong>Return Type</strong></th><th><strong>Name(Signature)</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr><td>BIGINT</td><td>count(*), count(expr), count(DISTINCT expr[, expr…])</td><td>count(<em>) - Returns the total number of retrieved rows, including rows containing NULL values.*</em>统计总行数，包括含有NULL值的行<strong>count(expr) - Returns the number of rows for which the supplied expression is non-NULL.</strong>统计提供非NULL的expr表达式值的行数<strong>count(DISTINCT expr[, expr]) - Returns the number of rows for which the supplied expression(s) are unique and non-NULL. Execution of this can be optimized with <a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.optimize.distinct.rewrite" target="_blank" rel="noopener">hive.optimize.distinct.rewrite</a>.</strong>统计提供非NULL且去重后的expr表达式值的行数**</td></tr><tr><td>DOUBLE</td><td>sum(col), sum(DISTINCT col)</td><td>Returns the sum of the elements in the group or the sum of the distinct values of the column in the group.<strong>sum(col),表示求指定列的和，sum(DISTINCT col)表示求去重后的列的和</strong></td></tr><tr><td>DOUBLE</td><td>avg(col), avg(DISTINCT col)</td><td>Returns the average of the elements in the group or the average of the distinct values of the column in the group.<strong>avg(col),表示求指定列的平均值，avg(DISTINCT col)表示求去重后的列的平均值</strong></td></tr><tr><td>DOUBLE</td><td>min(col)</td><td>Returns the minimum of the column in the group.<strong>求指定列的最小值</strong></td></tr><tr><td>DOUBLE</td><td>max(col)</td><td>Returns the maximum value of the column in the group.<strong>求指定列的最大值</strong></td></tr><tr><td>DOUBLE</td><td>variance(col), var_pop(col)</td><td>Returns the variance of a numeric column in the group.<strong>求指定列数值的方差</strong></td></tr><tr><td>DOUBLE</td><td>var_samp(col)</td><td>Returns the unbiased sample variance of a numeric column in the group.<strong>求指定列数值的样本方差</strong></td></tr><tr><td>DOUBLE</td><td>stddev_pop(col)</td><td>Returns the standard deviation of a numeric column in the group.<strong>求指定列数值的标准偏差</strong></td></tr><tr><td>DOUBLE</td><td>stddev_samp(col)</td><td>Returns the unbiased sample standard deviation of a numeric column in the group.<strong>求指定列数值的样本标准偏差</strong></td></tr><tr><td>DOUBLE</td><td>covar_pop(col1, col2)</td><td>Returns the population covariance of a pair of numeric columns in the group.<strong>求指定列数值的协方差</strong></td></tr><tr><td>DOUBLE</td><td>covar_samp(col1, col2)</td><td>Returns the sample covariance of a pair of a numeric columns in the group.<strong>求指定列数值的样本协方差</strong></td></tr><tr><td>DOUBLE</td><td>corr(col1, col2)</td><td>Returns the Pearson coefficient of correlation of a pair of a numeric columns in the group.<strong>返回两列数值的相关系数</strong></td></tr><tr><td>DOUBLE</td><td>percentile(BIGINT col, p)</td><td>Returns the exact pth percentile of a column in the group (does not work with floating point types). p must be between 0 and 1. NOTE: A true percentile can only be computed for integer values. Use PERCENTILE_APPROX if your input is non-integral.<strong>返回col的p%分位数</strong></td></tr></tbody></table><h2 id="表生成函数"><a href="#表生成函数" class="headerlink" title="表生成函数"></a><strong>表生成函数</strong></h2><table><thead><tr><th><strong>Return Type</strong></th><th><strong>Name(Signature)</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr><td>Array Type</td><td>explode(array&lt;<em>TYPE</em>&gt; a)</td><td>For each element in a, generates a row containing that element.<strong>对于a中的每个元素，将生成一行且包含该元素</strong></td></tr><tr><td>N rows</td><td>explode(ARRAY)</td><td>Returns one row for each element from the array..<strong>每行对应数组中的一个元素</strong></td></tr><tr><td>N rows</td><td>explode(MAP)</td><td>Returns one row for each key-value pair from the input map with two columns in each row: one for the key and another for the value. (As of Hive <a href="https://issues.apache.org/jira/browse/HIVE-1735" target="_blank" rel="noopener">0.8.0</a>.).<strong>每行对应每个map键-值，其中一个字段是map的键，另一个字段是map的值</strong></td></tr><tr><td>N rows</td><td>posexplode(ARRAY)</td><td>Behaves like <code>explode</code> for arrays, but includes the position of items in the original array by returning a tuple of <code>(pos, value)</code>. (As of <a href="https://issues.apache.org/jira/browse/HIVE-4943" target="_blank" rel="noopener">Hive 0.13.0</a>.).<strong>与explode类似，不同的是还返回各元素在数组中的位置</strong></td></tr><tr><td>N rows</td><td>stack(INT n, v_1, v_2, …, v_k)</td><td>Breaks up v_1, …, v_k into n rows. Each row will have k/n columns. n must be constant..<strong>把M列转换成N行，每行有M/N个字段，其中n必须是个常数</strong></td></tr><tr><td>tuple</td><td>json_tuple(jsonStr, k1, k2, …)</td><td>Takes a set of names (keys) and a JSON string, and returns a tuple of values. This is a more efficient version of the <code>get_json_object</code> UDF because it can get multiple keys with just one call..<strong>从一个JSON字符串中获取多个键并作为一个元组返回，与get_json_object不同的是此函数能一次获取多个键值</strong></td></tr><tr><td>tuple</td><td>parse_url_tuple(url, p1, p2, …)</td><td>This is similar to the <code>parse_url()</code> UDF but can extract multiple parts at once out of a URL. Valid part names are: HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, USERINFO, QUERY:<key>..<strong>返回从URL中抽取指定N部分的内容，参数url是URL字符串，而参数p1,p2,….是要抽取的部分，这个参数包含HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, USERINFO, QUERY:<key></key></strong></key></td></tr><tr><td></td><td>inline(ARRAY&lt;STRUCT[,STRUCT]&gt;)</td><td>Explodes an array of structs into a table. (As of Hive <a href="https://issues.apache.org/jira/browse/HIVE-3238" target="_blank" rel="noopener">0.10</a>.).<strong>将结构体数组提取出来并插入到表中</strong></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （八）Hive中文乱码</title>
      <link href="/2019-04-08-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%85%AB%EF%BC%89Hive%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81.html"/>
      <url>/2019-04-08-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%85%AB%EF%BC%89Hive%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （八）Hive中文乱码：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （八）Hive中文乱码</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="Hive注释中文乱码"><a href="#Hive注释中文乱码" class="headerlink" title="Hive注释中文乱码"></a>Hive注释中文乱码</h2><p>创建表的时候，comment说明字段包含中文，表成功创建成功之后，中文说明显示乱码</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> movie(</span><br><span class="line">userID <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'用户ID'</span>,</span><br><span class="line">movieID <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'电影ID'</span>,</span><br><span class="line">rating <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'电影评分'</span>,</span><br><span class="line">timestamped <span class="built_in">bigint</span> <span class="keyword">comment</span> <span class="string">'评分时间戳'</span>,</span><br><span class="line">movieName <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'电影名字'</span>, </span><br><span class="line">movieType <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'电影类型'</span>, </span><br><span class="line">sex <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'性别'</span>, </span><br><span class="line">age <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'年龄'</span>, </span><br><span class="line">occupation <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'职业'</span>, </span><br><span class="line">zipcode <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'邮政编码'</span></span><br><span class="line">) <span class="keyword">comment</span> <span class="string">'影评三表合一'</span> </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span></span><br><span class="line">location <span class="string">'/hive/movie'</span>;</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405195942097-172064801.png" alt="img"></p><p>这是因为在MySQL中的元数据出现乱码</p><h3 id="针对元数据库metastore中的表-分区-视图的编码设置"><a href="#针对元数据库metastore中的表-分区-视图的编码设置" class="headerlink" title="针对元数据库metastore中的表,分区,视图的编码设置"></a>针对元数据库metastore中的表,分区,视图的编码设置</h3><p>因为我们知道 metastore 支持数据库级别，表级别的字符集是 latin1</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405200316676-1087898740.png" alt="img"></p><p>那么我们只需要把相应注释的地方的字符集由 latin1 改成 utf-8，就可以了。用到注释的就三个地方，表、分区、视图。如下修改分为两个步骤：</p><h3 id="1、进入数据库-Metastore-中执行以下-5-条-SQL-语句"><a href="#1、进入数据库-Metastore-中执行以下-5-条-SQL-语句" class="headerlink" title="1、进入数据库 Metastore 中执行以下 5 条 SQL 语句"></a>1、进入数据库 Metastore 中执行以下 5 条 SQL 语句</h3><h4 id="（1）修改表字段注解和表注解"><a href="#（1）修改表字段注解和表注解" class="headerlink" title="（1）修改表字段注解和表注解"></a>（1）修改表字段注解和表注解</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8；</span><br><span class="line">alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8；</span><br></pre></td></tr></table></figure><h4 id="（2）修改分区字段注解"><a href="#（2）修改分区字段注解" class="headerlink" title="（2）修改分区字段注解"></a>（2）修改分区字段注解</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 ;</span><br><span class="line">alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;</span><br></pre></td></tr></table></figure><h4 id="（3）修改索引注解"><a href="#（3）修改索引注解" class="headerlink" title="（3）修改索引注解"></a>（3）修改索引注解</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</span><br></pre></td></tr></table></figure><h3 id="2、修改-metastore-的连接-URL"><a href="#2、修改-metastore-的连接-URL" class="headerlink" title="2、修改 metastore 的连接 URL"></a>2、修改 metastore 的连接 URL</h3><p> 修改hive-site.xml配置文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://IP:3306/db_name?createDatabaseIfNotExist=true&amp;amp;useUnicode=true&amp;characterEncoding=UTF-8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>做完可以解决乱码问题</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405201734055-866615241.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （七）Hive的DDL操作</title>
      <link href="/2019-04-07-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%83%EF%BC%89Hive%E7%9A%84DDL%E6%93%8D%E4%BD%9C.html"/>
      <url>/2019-04-07-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%83%EF%BC%89Hive%E7%9A%84DDL%E6%93%8D%E4%BD%9C.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （七）Hive的DDL操作：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （七）Hive的DDL操作</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h1 id="库操作"><a href="#库操作" class="headerlink" title="库操作"></a>库操作</h1><h2 id="1、创建库"><a href="#1、创建库" class="headerlink" title="1、创建库"></a>1、创建库</h2><h3 id="语法结构"><a href="#语法结构" class="headerlink" title="语法结构"></a>语法结构</h3><blockquote><p>CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name</p><p>　　[COMMENT database_comment]　　　　　　//关于数据块的描述</p><p>　　[LOCATION hdfs_path]　　　　　　　　　　//指定数据库在HDFS上的存储位置</p><p>　　[WITH DBPROPERTIES (property_name=property_value, …)];　　　　//指定数据块属性</p></blockquote><p>　　默认地址：/user/hive/warehouse/db_name.db/table_name/partition_name/…</p><h3 id="创建库的方式"><a href="#创建库的方式" class="headerlink" title="创建库的方式"></a>创建库的方式</h3><h4 id="（1）创建普通的数据库"><a href="#（1）创建普通的数据库" class="headerlink" title="（1）创建普通的数据库"></a>（1）创建普通的数据库</h4><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; create database t1;</span><br><span class="line">No rows affected (0.308 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt; show databases;</span><br><span class="line">+<span class="comment">----------------+</span></span><br><span class="line">| database_name  |</span><br><span class="line">+<span class="comment">----------------+</span></span><br><span class="line">| default        |</span><br><span class="line">| myhive         |</span><br><span class="line">| t1             |</span><br><span class="line">+<span class="comment">----------------+</span></span><br><span class="line">3 rows selected (0.393 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><h4 id="（2）创建库的时候检查存与否"><a href="#（2）创建库的时候检查存与否" class="headerlink" title="（2）创建库的时候检查存与否"></a>（2）创建库的时候检查存与否</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; create database if not exists t1;</span><br><span class="line">No rows affected (0.176 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><h4 id="（3）创建库的时候带注释"><a href="#（3）创建库的时候带注释" class="headerlink" title="（3）创建库的时候带注释"></a>（3）创建库的时候带注释</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; create database if not exists t2 comment 'learning hive';</span><br><span class="line">No rows affected (0.217 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405151447453-54696878.png" alt="img"></p><h4 id="（4）创建带属性的库"><a href="#（4）创建带属性的库" class="headerlink" title="（4）创建带属性的库"></a>（4）创建带属性的库</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; create database if not exists t3 with dbproperties('creator'='hadoop','date'='2018-04-05');</span><br><span class="line">No rows affected (0.255 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><h2 id="2、查看库"><a href="#2、查看库" class="headerlink" title="2、查看库"></a>2、查看库</h2><h3 id="查看库的方式"><a href="#查看库的方式" class="headerlink" title="查看库的方式"></a>查看库的方式</h3><h4 id="（1）查看有哪些数据库"><a href="#（1）查看有哪些数据库" class="headerlink" title="（1）查看有哪些数据库"></a>（1）查看有哪些数据库</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; show databases;</span><br><span class="line">+<span class="comment">----------------+</span></span><br><span class="line">| database_name |</span><br><span class="line">+<span class="comment">----------------+</span></span><br><span class="line">| default |</span><br><span class="line">| myhive |</span><br><span class="line">| t1 |</span><br><span class="line">| t2 |</span><br><span class="line">| t3 |</span><br><span class="line">+<span class="comment">----------------+</span></span><br><span class="line">5 rows selected (0.164 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405152147476-1818091794.png" alt="img"></p><h4 id="（2）显示数据库的详细属性信息"><a href="#（2）显示数据库的详细属性信息" class="headerlink" title="（2）显示数据库的详细属性信息"></a>（2）显示数据库的详细属性信息</h4><p>语法</p><blockquote><p>desc database [extended] dbname;</p></blockquote><p>示例</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; desc database extended t3;</span><br><span class="line">+<span class="comment">----------+----------+------------------------------------------+-------------+-------------+------------------------------------+</span></span><br><span class="line">| db_name  | <span class="keyword">comment</span>  |                 location                 | owner_name  | owner_type  |             <span class="keyword">parameters</span>             |</span><br><span class="line">+<span class="comment">----------+----------+------------------------------------------+-------------+-------------+------------------------------------+</span></span><br><span class="line">| t3       |          | hdfs://myha01/<span class="keyword">user</span>/hive/warehouse/t3.db  | hadoop      | <span class="keyword">USER</span>        | &#123;<span class="built_in">date</span>=<span class="number">2018</span><span class="number">-04</span><span class="number">-05</span>, creator=hadoop&#125;  |</span><br><span class="line">+<span class="comment">----------+----------+------------------------------------------+-------------+-------------+------------------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="keyword">row</span> selected (<span class="number">0.11</span> <span class="keyword">seconds</span>)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2://hadoop3:<span class="number">10000</span>&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405152346031-867635387.png" alt="img"></p><h4 id="（3）查看正在使用哪个库"><a href="#（3）查看正在使用哪个库" class="headerlink" title="（3）查看正在使用哪个库"></a>（3）查看正在使用哪个库</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; select current_database();</span><br><span class="line">+<span class="comment">----------+</span></span><br><span class="line">|   _c0    |</span><br><span class="line">+<span class="comment">----------+</span></span><br><span class="line">| default  |</span><br><span class="line">+<span class="comment">----------+</span></span><br><span class="line">1 row selected (1.36 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405152511882-533323414.png" alt="img"></p><h4 id="（4）查看创建库的详细语句"><a href="#（4）查看创建库的详细语句" class="headerlink" title="（4）查看创建库的详细语句"></a>（4）查看创建库的详细语句</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; show create database t3;</span><br><span class="line">+<span class="comment">----------------------------------------------+</span></span><br><span class="line">|                createdb_stmt                 |</span><br><span class="line">+<span class="comment">----------------------------------------------+</span></span><br><span class="line">| <span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> <span class="string">`t3`</span>                         |</span><br><span class="line">| LOCATION                                     |</span><br><span class="line">|   <span class="string">'hdfs://myha01/user/hive/warehouse/t3.db'</span>  |</span><br><span class="line">| <span class="keyword">WITH</span> DBPROPERTIES (                          |</span><br><span class="line">|   <span class="string">'creator'</span>=<span class="string">'hadoop'</span>,                        |</span><br><span class="line">|   <span class="string">'date'</span>=<span class="string">'2018-04-05'</span>)                       |</span><br><span class="line">+<span class="comment">----------------------------------------------+</span></span><br><span class="line"><span class="number">6</span> <span class="keyword">rows</span> selected (<span class="number">0.155</span> <span class="keyword">seconds</span>)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2://hadoop3:<span class="number">10000</span>&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405152613117-1681533126.png" alt="img"></p><h2 id="3、删除库"><a href="#3、删除库" class="headerlink" title="3、删除库"></a>3、删除库</h2><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>删除库操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">drop database dbname;</span><br><span class="line">drop database if exists dbname;</span><br></pre></td></tr></table></figure><p>默认情况下，hive 不允许删除包含表的数据库，有两种解决办法：</p><p>1、 手动删除库下所有表，然后删除库</p><p>2、 使用 cascade 关键字</p><blockquote><p><strong>drop database if exists dbname cascade;</strong></p><p>默认情况下就是 restrict drop database if exists myhive ==== drop database if exists myhive restrict</p></blockquote><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><h4 id="（1）删除不含表的数据库"><a href="#（1）删除不含表的数据库" class="headerlink" title="（1）删除不含表的数据库"></a>（1）删除不含表的数据库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; show tables in t1;</span><br><span class="line">+-----------+</span><br><span class="line">| tab_name  |</span><br><span class="line">+-----------+</span><br><span class="line">+-----------+</span><br><span class="line">No rows selected (0.147 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt; drop database t1;</span><br><span class="line">No rows affected (0.178 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt; show databases;</span><br><span class="line">+----------------+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+</span><br><span class="line">| default        |</span><br><span class="line">| myhive         |</span><br><span class="line">| t2             |</span><br><span class="line">| t3             |</span><br><span class="line">+----------------+</span><br><span class="line">4 rows selected (0.124 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405153207185-1206431168.png" alt="img"></p><h4 id="（2）删除含有表的数据库"><a href="#（2）删除含有表的数据库" class="headerlink" title="（2）删除含有表的数据库"></a>（2）删除含有表的数据库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; drop database if exists t3 cascade;</span><br><span class="line">No rows affected (1.56 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405153816995-802101520.png" alt="img"></p><h2 id="4、切换库"><a href="#4、切换库" class="headerlink" title="4、切换库"></a>4、切换库</h2><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><blockquote><p>use database_name</p></blockquote><h3 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; use t2;</span><br><span class="line">No rows affected (0.109 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405154024694-578835712.png" alt="img"></p><h1 id="表操作"><a href="#表操作" class="headerlink" title="表操作"></a>表操作</h1><h2 id="1、创建表"><a href="#1、创建表" class="headerlink" title="1、创建表"></a>1、创建表</h2><h3 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h3><blockquote><p><strong>CREATE</strong> [EXTERNAL] <strong>TABLE</strong> [IF NOT EXISTS] table_name</p><p>　　[(col_name data_type [COMMENT col_comment], …)]</p><p>　　[<strong>COMMENT</strong> table_comment]</p><p>　　[<strong>PARTITIONED BY</strong> (col_name data_type [COMMENT col_comment], …)]</p><p>　　[<strong>CLUSTERED BY</strong> (col_name, col_name, …)</p><p>　　　　[<strong>SORTED BY</strong> (col_name [ASC|DESC], …)] <strong>INTO</strong> num_buckets <strong>BUCKETS</strong>]</p><p>　　[<strong>ROW FORMAT</strong> row_format]</p><p>　　[<strong>STORED AS</strong> file_format]</p><p>　　[<strong>LOCATION</strong> hdfs_path]</p></blockquote><p>详情请参见： <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualD" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualD</a> DL-CreateTable</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">•CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXIST 选项来忽略这个异常</span><br><span class="line">•EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION）</span><br><span class="line">•LIKE 允许用户复制现有的表结构，但是不复制数据</span><br><span class="line">•COMMENT可以为表与字段增加描述</span><br><span class="line">•PARTITIONED BY 指定分区•ROW FORMAT 　　DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] 　　　　MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] 　　　　| SERDE serde_name [WITH SERDEPROPERTIES 　　　　(property_name=property_value, property_name=property_value, ...)] 　　用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive 通过 SerDe 确定表的具体的列的数据。 •STORED AS 　　SEQUENCEFILE //序列化文件　　| TEXTFILE //普通的文本文件格式　　| RCFILE　　//行列存储相结合的文件　　| INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname //自定义文件格式　　如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCE 。</span><br><span class="line">•LOCATION指定表在HDFS的存储路径</span><br></pre></td></tr></table></figure><p>最佳实践：<br>　　如果一份数据已经存储在HDFS上，并且要被多个用户或者客户端使用，最好创建外部表<br>　　反之，最好创建内部表。</p><p>　　如果不指定，就按照默认的规则存储在默认的仓库路径中。</p><h3 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h3><p>使用t2数据库进行操作</p><h4 id="（1）创建默认的内部表"><a href="#（1）创建默认的内部表" class="headerlink" title="（1）创建默认的内部表"></a>（1）创建默认的内部表</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; create table student(id int, name string, sex string, age int,department string) row format delimited fields terminated by ",";</span><br><span class="line">No rows affected (0.222 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt; desc student;</span><br><span class="line">+<span class="comment">-------------+------------+----------+</span></span><br><span class="line">|  col_name   | data_type  | <span class="keyword">comment</span>  |</span><br><span class="line">+<span class="comment">-------------+------------+----------+</span></span><br><span class="line">| <span class="keyword">id</span>          | <span class="built_in">int</span>        |          |</span><br><span class="line">| <span class="keyword">name</span>        | <span class="keyword">string</span>     |          |</span><br><span class="line">| sex         | <span class="keyword">string</span>     |          |</span><br><span class="line">| age         | <span class="built_in">int</span>        |          |</span><br><span class="line">| department  | <span class="keyword">string</span>     |          |</span><br><span class="line">+<span class="comment">-------------+------------+----------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.168</span> <span class="keyword">seconds</span>)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2://hadoop3:<span class="number">10000</span>&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405160259794-1130927260.png" alt="img"></p><h4 id="（2）外部表"><a href="#（2）外部表" class="headerlink" title="（2）外部表"></a>（2）外部表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; create external table student_ext(id int, name string, sex string, age int,department string) row format delimited fields terminated by &quot;,&quot; location &quot;/hive/student&quot;;</span><br><span class="line">No rows affected (0.248 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><h4 id="（3）分区表"><a href="#（3）分区表" class="headerlink" title="（3）分区表"></a>（3）分区表</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; create external table student_ptn(id int, name string, sex string, age int,department string)</span><br><span class="line">. . . . . . . . . . . . . . .&gt; partitioned by (city string)</span><br><span class="line">. . . . . . . . . . . . . . .&gt; row format delimited fields terminated by ","</span><br><span class="line">. . . . . . . . . . . . . . .&gt; location "/hive/student_ptn";</span><br><span class="line">No rows affected (0.24 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><p>添加分区</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; alter table student_ptn add partition(city=&quot;beijing&quot;);</span><br><span class="line">No rows affected (0.269 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt; alter table student_ptn add partition(city=&quot;shenzhen&quot;);</span><br><span class="line">No rows affected (0.236 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><p>如果某张表是分区表。那么每个分区的定义，其实就表现为了这张表的数据存储目录下的一个子目录<br>如果是分区表。那么数据文件一定要存储在某个分区中，而不能直接存储在表中。</p><h4 id="（4）分桶表"><a href="#（4）分桶表" class="headerlink" title="（4）分桶表"></a>（4）分桶表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; create external table student_bck(id int, name string, sex string, age int,department string)</span><br><span class="line">. . . . . . . . . . . . . . .&gt; clustered by (id) sorted by (id asc, name desc) into 4 buckets</span><br><span class="line">. . . . . . . . . . . . . . .&gt; row format delimited fields terminated by &quot;,&quot;</span><br><span class="line">. . . . . . . . . . . . . . .&gt; location &quot;/hive/student_bck&quot;;</span><br><span class="line">No rows affected (0.216 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><h4 id="（5）使用CTAS创建表"><a href="#（5）使用CTAS创建表" class="headerlink" title="（5）使用CTAS创建表"></a>（5）使用CTAS创建表</h4><p>作用： 就是从一个查询SQL的结果来创建一个表进行存储</p><p>现象student表中导入数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; load data local inpath &quot;/home/hadoop/student.txt&quot; into table student;</span><br><span class="line">No rows affected (0.715 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt; select * from student;</span><br><span class="line">+-------------+---------------+--------------+--------------+---------------------+</span><br><span class="line">| student.id  | student.name  | student.sex  | student.age  | student.department  |</span><br><span class="line">+-------------+---------------+--------------+--------------+---------------------+</span><br><span class="line">| 95002       | 刘晨            | 女            | 19           | IS                  |</span><br><span class="line">| 95017       | 王风娟           | 女            | 18           | IS                  |</span><br><span class="line">| 95018       | 王一            | 女            | 19           | IS                  |</span><br><span class="line">| 95013       | 冯伟            | 男            | 21           | CS                  |</span><br><span class="line">| 95014       | 王小丽           | 女            | 19           | CS                  |</span><br><span class="line">| 95019       | 邢小丽           | 女            | 19           | IS                  |</span><br><span class="line">| 95020       | 赵钱            | 男            | 21           | IS                  |</span><br><span class="line">| 95003       | 王敏            | 女            | 22           | MA                  |</span><br><span class="line">| 95004       | 张立            | 男            | 19           | IS                  |</span><br><span class="line">| 95012       | 孙花            | 女            | 20           | CS                  |</span><br><span class="line">| 95010       | 孔小涛           | 男            | 19           | CS                  |</span><br><span class="line">| 95005       | 刘刚            | 男            | 18           | MA                  |</span><br><span class="line">| 95006       | 孙庆            | 男            | 23           | CS                  |</span><br><span class="line">| 95007       | 易思玲           | 女            | 19           | MA                  |</span><br><span class="line">| 95008       | 李娜            | 女            | 18           | CS                  |</span><br><span class="line">| 95021       | 周二            | 男            | 17           | MA                  |</span><br><span class="line">| 95022       | 郑明            | 男            | 20           | MA                  |</span><br><span class="line">| 95001       | 李勇            | 男            | 20           | CS                  |</span><br><span class="line">| 95011       | 包小柏           | 男            | 18           | MA                  |</span><br><span class="line">| 95009       | 梦圆圆           | 女            | 18           | MA                  |</span><br><span class="line">| 95015       | 王君            | 男            | 18           | MA                  |</span><br><span class="line">+-------------+---------------+--------------+--------------+---------------------+</span><br><span class="line">21 rows selected (0.342 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><p>使用CTAS创建表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; create table student_ctas as select * from student where id &lt; 95012;</span><br><span class="line">WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">No rows affected (34.514 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt; select * from student_ctas</span><br><span class="line">. . . . . . . . . . . . . . .&gt; ;</span><br><span class="line">+------------------+--------------------+-------------------+-------------------+--------------------------+</span><br><span class="line">| student_ctas.id  | student_ctas.name  | student_ctas.sex  | student_ctas.age  | student_ctas.department  |</span><br><span class="line">+------------------+--------------------+-------------------+-------------------+--------------------------+</span><br><span class="line">| 95002            | 刘晨                 | 女                 | 19                | IS                       |</span><br><span class="line">| 95003            | 王敏                 | 女                 | 22                | MA                       |</span><br><span class="line">| 95004            | 张立                 | 男                 | 19                | IS                       |</span><br><span class="line">| 95010            | 孔小涛                | 男                 | 19                | CS                       |</span><br><span class="line">| 95005            | 刘刚                 | 男                 | 18                | MA                       |</span><br><span class="line">| 95006            | 孙庆                 | 男                 | 23                | CS                       |</span><br><span class="line">| 95007            | 易思玲                | 女                 | 19                | MA                       |</span><br><span class="line">| 95008            | 李娜                 | 女                 | 18                | CS                       |</span><br><span class="line">| 95001            | 李勇                 | 男                 | 20                | CS                       |</span><br><span class="line">| 95011            | 包小柏                | 男                 | 18                | MA                       |</span><br><span class="line">| 95009            | 梦圆圆                | 女                 | 18                | MA                       |</span><br><span class="line">+------------------+--------------------+-------------------+-------------------+--------------------------+</span><br><span class="line">11 rows selected (0.445 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405162052001-528212443.png" alt="img"></p><h4 id="（6）复制表结构"><a href="#（6）复制表结构" class="headerlink" title="（6）复制表结构"></a>（6）复制表结构</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; create table student_copy like student;</span><br><span class="line">No rows affected (0.217 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><p>注意：</p><p>如果在table的前面没有加external关键字，那么复制出来的新表。无论如何都是内部表<br>如果在table的前面有加external关键字，那么复制出来的新表。无论如何都是外部表</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405162246038-1109569199.png" alt="img"></p><h2 id="2、查看表"><a href="#2、查看表" class="headerlink" title="2、查看表"></a>2、查看表</h2><h3 id="（1）查看表列表"><a href="#（1）查看表列表" class="headerlink" title="（1）查看表列表"></a>（1）查看表列表</h3><h4 id="查看当前使用的数据库中有哪些表"><a href="#查看当前使用的数据库中有哪些表" class="headerlink" title="查看当前使用的数据库中有哪些表"></a>查看当前使用的数据库中有哪些表</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; show tables;</span><br><span class="line">+<span class="comment">---------------+</span></span><br><span class="line">|   tab_name    |</span><br><span class="line">+<span class="comment">---------------+</span></span><br><span class="line">| student       |</span><br><span class="line">| student_bck   |</span><br><span class="line">| student_copy  |</span><br><span class="line">| student_ctas  |</span><br><span class="line">| student_ext   |</span><br><span class="line">| student_ptn   |</span><br><span class="line">+<span class="comment">---------------+</span></span><br><span class="line">6 rows selected (0.163 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><h4 id="查看非当前使用的数据库中有哪些表"><a href="#查看非当前使用的数据库中有哪些表" class="headerlink" title="查看非当前使用的数据库中有哪些表"></a>查看非当前使用的数据库中有哪些表</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; show tables in myhive;</span><br><span class="line">+<span class="comment">-----------+</span></span><br><span class="line">| tab_name  |</span><br><span class="line">+<span class="comment">-----------+</span></span><br><span class="line">| student   |</span><br><span class="line">+<span class="comment">-----------+</span></span><br><span class="line">1 row selected (0.144 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><h4 id="查看数据库中以xxx开头的表"><a href="#查看数据库中以xxx开头的表" class="headerlink" title="查看数据库中以xxx开头的表"></a>查看数据库中以xxx开头的表</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; show tables like 'student_c*';</span><br><span class="line">+<span class="comment">---------------+</span></span><br><span class="line">|   tab_name    |</span><br><span class="line">+<span class="comment">---------------+</span></span><br><span class="line">| student_copy  |</span><br><span class="line">| student_ctas  |</span><br><span class="line">+<span class="comment">---------------+</span></span><br><span class="line">2 rows selected (0.13 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><h3 id="（2）查看表的详细信息"><a href="#（2）查看表的详细信息" class="headerlink" title="（2）查看表的详细信息"></a>（2）查看表的详细信息</h3><h3 id="查看表的信息"><a href="#查看表的信息" class="headerlink" title="查看表的信息"></a>查看表的信息</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; desc student;</span><br><span class="line">+<span class="comment">-------------+------------+----------+</span></span><br><span class="line">|  col_name   | data_type  | <span class="keyword">comment</span>  |</span><br><span class="line">+<span class="comment">-------------+------------+----------+</span></span><br><span class="line">| <span class="keyword">id</span>          | <span class="built_in">int</span>        |          |</span><br><span class="line">| <span class="keyword">name</span>        | <span class="keyword">string</span>     |          |</span><br><span class="line">| sex         | <span class="keyword">string</span>     |          |</span><br><span class="line">| age         | <span class="built_in">int</span>        |          |</span><br><span class="line">| department  | <span class="keyword">string</span>     |          |</span><br><span class="line">+<span class="comment">-------------+------------+----------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.149</span> <span class="keyword">seconds</span>)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2://hadoop3:<span class="number">10000</span>&gt;</span><br></pre></td></tr></table></figure><h4 id="查看表的详细信息（格式不友好）"><a href="#查看表的详细信息（格式不友好）" class="headerlink" title="查看表的详细信息（格式不友好）"></a>查看表的详细信息（格式不友好）</h4><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; desc extended student;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405163049174-722059489.png" alt="img"></p><h4 id="查看表的详细信息（格式友好）"><a href="#查看表的详细信息（格式友好）" class="headerlink" title="查看表的详细信息（格式友好）"></a>查看表的详细信息（格式友好）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; desc formatted student;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405163229214-1215954993.png" alt="img"></p><h4 id="查看分区信息"><a href="#查看分区信息" class="headerlink" title="查看分区信息"></a>查看分区信息</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; show partitions student_ptn;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405163334107-2114870049.png" alt="img"></p><h3 id="（3）查看表的详细建表语句"><a href="#（3）查看表的详细建表语句" class="headerlink" title="（3）查看表的详细建表语句"></a>（3）查看表的详细建表语句</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; show create table student_ptn;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405163449651-416776974.png" alt="img"></p><h2 id="3、修改表"><a href="#3、修改表" class="headerlink" title="3、修改表"></a>3、修改表</h2><h3 id="（1）修改表名"><a href="#（1）修改表名" class="headerlink" title="（1）修改表名"></a>（1）修改表名</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; alter table student rename to new_student;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405163823328-646565023.png" alt="img"></p><h3 id="（2）修改字段定义"><a href="#（2）修改字段定义" class="headerlink" title="（2）修改字段定义"></a>（2）修改字段定义</h3><h4 id="A-增加一个字段"><a href="#A-增加一个字段" class="headerlink" title="A. 增加一个字段"></a>A. 增加一个字段</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; alter table new_student add columns (score int);</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405164132833-857553391.png" alt="img"></p><h4 id="B-修改一个字段的定义"><a href="#B-修改一个字段的定义" class="headerlink" title="B. 修改一个字段的定义"></a>B. 修改一个字段的定义</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; alter table new_student change name new_name string;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405164318857-437618298.png" alt="img"></p><h4 id="C-删除一个字段"><a href="#C-删除一个字段" class="headerlink" title="C. 删除一个字段"></a>C. 删除一个字段</h4><p>不支持</p><h4 id="D-替换所有字段"><a href="#D-替换所有字段" class="headerlink" title="D. 替换所有字段"></a>D. 替换所有字段</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; alter table new_student replace columns (id int, name string, address string);</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405164553454-371632555.png" alt="img"></p><h3 id="（3）修改分区信息"><a href="#（3）修改分区信息" class="headerlink" title="（3）修改分区信息"></a>（3）修改分区信息</h3><h4 id="A-添加分区"><a href="#A-添加分区" class="headerlink" title="A. 添加分区"></a>A. 添加分区</h4><p>静态分区</p><p>　　添加一个</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; alter table student_ptn add partition(city=&quot;chongqing&quot;);</span><br></pre></td></tr></table></figure><p>　　添加多个</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; alter table student_ptn add partition(city=&quot;chongqing2&quot;) partition(city=&quot;chongqing3&quot;) partition(city=&quot;chongqing4&quot;);</span><br></pre></td></tr></table></figure><p>动态分区</p><p>先向student_ptn表中插入数据，数据格式如下图</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; load data local inpath &quot;/home/hadoop/student.txt&quot; into table student_ptn partition(city=&quot;beijing&quot;);</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405170132089-387032843.png" alt="img"></p><p>现在我把这张表的内容直接插入到另一张表student_ptn_age中，并实现sex为动态分区（不指定到底是哪中性别，让系统自己分配决定）</p><p>首先创建student_ptn_age并指定分区为age</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; create table student_ptn_age(id int,name string,sex string,department string) partitioned by (age int);</span><br></pre></td></tr></table></figure><p>从student_ptn表中查询数据并插入student_ptn_age表中</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; insert overwrite table student_ptn_age partition(age)</span><br><span class="line">. . . . . . . . . . . . . . .&gt; select id,name,sex,department，age from student_ptn;</span><br><span class="line">WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">No rows affected (27.905 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405172637613-1037021873.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405173022085-1302800258.png" alt="img"></p><h4 id="B-修改分区"><a href="#B-修改分区" class="headerlink" title="B. 修改分区"></a>B. 修改分区</h4><p>修改分区，一般来说，都是指修改分区的数据存储目录</p><p>在添加分区的时候，直接指定当前分区的数据存储目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; alter table student_ptn add if not exists partition(city=&apos;beijing&apos;) </span><br><span class="line">. . . . . . . . . . . . . . .&gt; location &apos;/student_ptn_beijing&apos; partition(city=&apos;cc&apos;) location &apos;/student_cc&apos;;</span><br><span class="line">No rows affected (0.306 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><p>修改已经指定好的分区的数据存储目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; alter table student_ptn partition (city=&apos;beijing&apos;) set location &apos;/student_ptn_beijing&apos;;</span><br></pre></td></tr></table></figure><p>此时原先的分区文件夹仍存在，但是在往分区添加数据时，只会添加到新的分区目录</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405174224289-1981643823.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405174337967-1343200403.png" alt="img"></p><h4 id="C-删除分区"><a href="#C-删除分区" class="headerlink" title="C. 删除分区"></a>C. 删除分区</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; alter table student_ptn drop partition (city=&apos;beijing&apos;);</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405174611617-1503113488.png" alt="img"></p><h2 id="4、删除表"><a href="#4、删除表" class="headerlink" title="4、删除表"></a>4、删除表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; drop table new_student;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405174718166-1433538751.png" alt="img"></p><h2 id="5、清空表"><a href="#5、清空表" class="headerlink" title="5、清空表"></a>5、清空表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop3:10000&gt; truncate table student_ptn;</span><br></pre></td></tr></table></figure><h1 id="其他辅助命令"><a href="#其他辅助命令" class="headerlink" title="其他辅助命令"></a>其他辅助命令</h1><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405175841608-959744694.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180405175903108-1636139803.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （六）Hive SQL之数据类型和存储格式</title>
      <link href="/2019-04-06-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%85%AD%EF%BC%89Hive%20SQL%E4%B9%8B%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%92%8C%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F.html"/>
      <url>/2019-04-06-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%85%AD%EF%BC%89Hive%20SQL%E4%B9%8B%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%92%8C%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （六）Hive SQL之数据类型和存储格式：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （六）Hive SQL之数据类型和存储格式</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、数据类型"><a href="#一、数据类型" class="headerlink" title="一、数据类型"></a>一、数据类型</h2><h3 id="1、基本数据类型"><a href="#1、基本数据类型" class="headerlink" title="1、基本数据类型"></a>1、基本数据类型</h3><p>Hive 支持关系型数据中大多数基本数据类型</p><table><thead><tr><th>类型</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>boolean</td><td>true/false</td><td>TRUE</td></tr><tr><td>tinyint</td><td>1字节的有符号整数</td><td>-128~127 1Y</td></tr><tr><td>smallint</td><td>2个字节的有符号整数，-32768~32767</td><td>1S</td></tr><tr><td>int</td><td>4个字节的带符号整数</td><td>1</td></tr><tr><td>bigint</td><td>8字节带符号整数</td><td>1L</td></tr><tr><td>float</td><td>4字节单精度浮点数</td><td>1.0</td></tr><tr><td>double</td><td>8字节双精度浮点数</td><td>1.0</td></tr><tr><td>deicimal</td><td>任意精度的带符号小数</td><td>1.0</td></tr><tr><td>String</td><td>字符串，变长</td><td>“a”,’b’</td></tr><tr><td>varchar</td><td>变长字符串</td><td>“a”,’b’</td></tr><tr><td>char</td><td>固定长度字符串</td><td>“a”,’b’</td></tr><tr><td>binary</td><td>字节数组</td><td>无法表示</td></tr><tr><td>timestamp</td><td>时间戳，纳秒精度</td><td>122327493795</td></tr><tr><td>date</td><td>日期</td><td>‘2018-04-07’</td></tr></tbody></table><p>和其他的SQL语言一样，这些都是保留字。需要注意的是所有的这些数据类型都是对Java中接口的实现，因此这些类型的具体行为细节和Java中对应的类型是完全一致的。例如，string类型实现的是Java中的String，float实现的是Java中的float，等等。</p><h3 id="2、复杂类型"><a href="#2、复杂类型" class="headerlink" title="2、复杂类型"></a>2、复杂类型</h3><table><thead><tr><th>类型</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>array</td><td>有序的的同类型的集合</td><td>array(1,2)</td></tr><tr><td>map</td><td>key-value,key必须为原始类型，value可以任意类型</td><td>map(‘a’,1,’b’,2)</td></tr><tr><td>struct</td><td>字段集合,类型可以不同</td><td>struct(‘1’,1,1.0), named_stract(‘col1’,’1’,’col2’,1,’clo3’,1.0)</td></tr></tbody></table><h2 id="二、存储格式"><a href="#二、存储格式" class="headerlink" title="二、存储格式"></a>二、存储格式</h2><p>Hive会为每个创建的数据库在HDFS上创建一个目录，该数据库的表会以子目录形式存储，表中的数据会以表目录下的文件形式存储。对于default数据库，默认的缺省数据库没有自己的目录，default数据库下的表默认存放在/user/hive/warehouse目录下。</p><h3 id="（1）textfile"><a href="#（1）textfile" class="headerlink" title="（1）textfile"></a>（1）textfile</h3><p>textfile为默认格式，存储方式为行存储。数据不做压缩，磁盘开销大，数据解析开销大。 </p><h3 id="（2）SequenceFile"><a href="#（2）SequenceFile" class="headerlink" title="（2）SequenceFile"></a>（2）SequenceFile</h3><p>SequenceFile是Hadoop API提供的一种二进制文件支持，其具有使用方便、可分割、可压缩的特点。 </p><p>SequenceFile支持三种压缩选择：NONE, RECORD, BLOCK。 Record压缩率低，一般建议使用BLOCK压缩。 </p><h3 id="（3）RCFile"><a href="#（3）RCFile" class="headerlink" title="（3）RCFile"></a>（3）RCFile</h3><p>一种行列存储相结合的存储方式。 </p><h3 id="（4）ORCFile"><a href="#（4）ORCFile" class="headerlink" title="（4）ORCFile"></a>（4）ORCFile</h3><p>数据按照行分块，每个块按照列存储，其中每个块都存储有一个索引。hive给出的新格式，属于RCFILE的升级版,性能有大幅度提升,而且数据可以压缩存储,压缩快 快速列存取。 </p><h3 id="（5）Parquet"><a href="#（5）Parquet" class="headerlink" title="（5）Parquet"></a>（5）Parquet</h3><p>Parquet也是一种行式存储，同时具有很好的压缩性能；同时可以减少大量的表扫描和反序列化的时间。</p><h2 id="三、数据格式"><a href="#三、数据格式" class="headerlink" title="三、数据格式"></a>三、数据格式</h2><p>当数据存储在文本文件中，必须按照一定格式区别行和列，并且在Hive中指明这些区分符。Hive默认使用了几个平时很少出现的字符，这些字符一般不会作为内容出现在记录中。</p><p>Hive默认的行和列分隔符如下表所示。</p><table><thead><tr><th>分隔符</th><th>描述</th></tr></thead><tbody><tr><td>\n</td><td>对于文本文件来说，每行是一条记录，所以\n 来分割记录</td></tr><tr><td>^A (Ctrl+A)</td><td>分割字段，也可以用\001 来表示</td></tr><tr><td>^B (Ctrl+B)</td><td>用于分割 Arrary 或者 Struct 中的元素，或者用于 map 中键值之间的分割，也可以用\002 分割。</td></tr><tr><td>^C</td><td>用于 map 中键和值自己分割，也可以用\003 表示。</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （五）DbVisualizer配置连接hive</title>
      <link href="/2019-04-05-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%94%EF%BC%89DbVisualizer%E9%85%8D%E7%BD%AE%E8%BF%9E%E6%8E%A5hive.html"/>
      <url>/2019-04-05-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%94%EF%BC%89DbVisualizer%E9%85%8D%E7%BD%AE%E8%BF%9E%E6%8E%A5hive.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （五）DbVisualizer配置连接hive：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （五）DbVisualizer配置连接hive</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、安装DbVisualizer"><a href="#一、安装DbVisualizer" class="headerlink" title="一、安装DbVisualizer"></a>一、安装DbVisualizer</h2><p>下载地址<a href="http://www.dbvis.com/" target="_blank" rel="noopener">http://www.dbvis.com/</a></p><p>也可以从网上下载破解版程序，此处使用的版本是DbVisualizer 9.1.1</p><p>具体的安装步骤可以百度，或是修改安装目录之后默认安装就可以</p><h2 id="二、配置DbVisualizer里的hive-jdbc"><a href="#二、配置DbVisualizer里的hive-jdbc" class="headerlink" title="二、配置DbVisualizer里的hive jdbc"></a>二、配置DbVisualizer里的hive jdbc</h2><h3 id="1、在DbVisualizer的安装目录jdbc文件夹下新建hive文件夹"><a href="#1、在DbVisualizer的安装目录jdbc文件夹下新建hive文件夹" class="headerlink" title="1、在DbVisualizer的安装目录jdbc文件夹下新建hive文件夹"></a>1、在DbVisualizer的安装目录jdbc文件夹下新建hive文件夹</h3><p>D:\Program Files\DbVisualizer\jdbc</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404130621506-310954602.png" alt="img"></p><h3 id="2、拷贝Hadoop的相关jar包放入新建的hive文件夹里面"><a href="#2、拷贝Hadoop的相关jar包放入新建的hive文件夹里面" class="headerlink" title="2、拷贝Hadoop的相关jar包放入新建的hive文件夹里面"></a>2、拷贝Hadoop的相关jar包放入新建的hive文件夹里面</h3><p>jar包位置:　　</p><h4 id="1-hadoop-2-7-5-share-hadoop-common-hadoop-common-2-7-5-jar"><a href="#1-hadoop-2-7-5-share-hadoop-common-hadoop-common-2-7-5-jar" class="headerlink" title="(1)　　hadoop-2.7.5/share/hadoop/common/hadoop-common-2.7.5.jar"></a>(1)　　hadoop-2.7.5/share/hadoop/common/hadoop-common-2.7.5.jar</h4><p>把图中红框中的jar包拷贝到新建的hive文件夹里面</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404130944730-1360228643.png" alt="img"></p><h4 id="2-hadoop-2-7-5-share-hadoop-common-lib"><a href="#2-hadoop-2-7-5-share-hadoop-common-lib" class="headerlink" title="(2)　　hadoop-2.7.5/share/hadoop/common/lib/"></a>(2)　　hadoop-2.7.5/share/hadoop/common/lib/</h4><p>把图中涉及到的jar包拷贝到新建的hive文件夹里面</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404135500221-969827221.png" alt="img"></p><h3 id="3、拷贝Hive的相关jar包放入新建的hive文件夹里面"><a href="#3、拷贝Hive的相关jar包放入新建的hive文件夹里面" class="headerlink" title="3、拷贝Hive的相关jar包放入新建的hive文件夹里面"></a>3、拷贝Hive的相关jar包放入新建的hive文件夹里面</h3><p>jar包位置:　　</p><h4 id="1-apache-hive-2-3-3-bin-jdbc-lib"><a href="#1-apache-hive-2-3-3-bin-jdbc-lib" class="headerlink" title="(1)　　apache-hive-2.3.3-bin/jdbc/lib"></a>(1)　　apache-hive-2.3.3-bin/jdbc/lib</h4><p>把图中涉及到的jar包拷贝到新建的hive文件夹里面</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404135543042-1208593430.png" alt="img"></p><h3 id="4、结果"><a href="#4、结果" class="headerlink" title="4、结果"></a>4、结果</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404135722643-192560741.png" alt="img"></p><h3 id="5、在tools-Driver-manager中进行配置"><a href="#5、在tools-Driver-manager中进行配置" class="headerlink" title="5、在tools/Driver manager中进行配置"></a>5、在tools/Driver manager中进行配置</h3><p> 打开DbVisualizer，此时会进行加载刚添加的jar包</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404135954967-49547825.png" alt="img"></p><h3 id="6、在Tool–Driver-manager中进行配置"><a href="#6、在Tool–Driver-manager中进行配置" class="headerlink" title="6、在Tool–Driver manager中进行配置"></a>6、在Tool–Driver manager中进行配置</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404140106979-609721263.png" alt="img"></p><p>点击左上角的添加</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404140151086-476092228.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404140500114-1367641987.png" alt="img"></p><p>完成之后关闭窗口</p><p>点击添加连接数据库</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404140704331-601865750.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404140721278-889862110.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404140758891-1791585887.png" alt="img"></p><p>选择驱动</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404140826037-708008515.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404140947419-1363418024.png" alt="img"></p><p>点击完成</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404141035449-800125990.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404145450132-447921151.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （四）Hive的连接3种连接方式</title>
      <link href="/2019-04-04-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%9B%9B%EF%BC%89Hive%E7%9A%84%E8%BF%9E%E6%8E%A53%E7%A7%8D%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F.html"/>
      <url>/2019-04-04-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%9B%9B%EF%BC%89Hive%E7%9A%84%E8%BF%9E%E6%8E%A53%E7%A7%8D%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （四）Hive的连接3种连接方式：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （四）Hive的连接3种连接方式</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、CLI连接"><a href="#一、CLI连接" class="headerlink" title="一、CLI连接"></a>一、CLI连接</h2><p>进入到 bin 目录下，直接输入命令： </p><p>[hadoop@hadoop3 ~]$ <strong>hive</strong><br>SLF4J: Class path contains multiple SLF4J bindings.<br>SLF4J: Found binding in [jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]<br>SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]<br>SLF4J: See <a href="http://www.slf4j.org/codes.html#multiple_bindings" target="_blank" rel="noopener">http://www.slf4j.org/codes.html#multiple_bindings</a> for an explanation.<br>SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</p><p>Logging initialized using configuration in jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/hive-common-2.3.3.jar!/hive-log4j2.properties Async: true<br>Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.<br>hive&gt; <strong>show databases;</strong><br>OK<br>default<br>myhive<br>Time taken: 6.569 seconds, Fetched: 2 row(s)<br>hive&gt;</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404094203204-1654342331.png" alt="img"></p><p>启动成功的话如上图所示，接下来便可以做 hive 相关操作</p><p>补充：</p><p>　　1、上面的 hive 命令相当于在启动的时候执行：<strong>hive –service cli</strong></p><p>　　2、使用 <strong>hive –help</strong>，可以查看 hive 命令可以启动那些服务</p><p>　　3、通过 <strong>hive –service serviceName –help</strong> 可以查看某个具体命令的使用方式</p><h2 id="二、HiveServer2-beeline"><a href="#二、HiveServer2-beeline" class="headerlink" title="二、HiveServer2/beeline"></a>二、HiveServer2/beeline</h2><p>在现在使用的最新的 hive-2.3.3 版本中：都需要对 hadoop 集群做如下改变，否则无法使用</p><h3 id="1、修改-hadoop-集群的-hdfs-site-xml-配置文件"><a href="#1、修改-hadoop-集群的-hdfs-site-xml-配置文件" class="headerlink" title="1、修改 hadoop 集群的 hdfs-site.xml 配置文件"></a>1、修改 hadoop 集群的 hdfs-site.xml 配置文件</h3><p>加入一条配置信息，表示启用 webhdfs</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h3 id="2、修改-hadoop-集群的-core-site-xml-配置文件"><a href="#2、修改-hadoop-集群的-core-site-xml-配置文件" class="headerlink" title="2、修改 hadoop 集群的 core-site.xml 配置文件"></a>2、修改 hadoop 集群的 core-site.xml 配置文件</h3><p>加入两条配置信息：表示设置 hadoop 的代理用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>配置解析：</p><p>hadoop.proxyuser.hadoop.hosts 配置成*的意义，表示任意节点使用 hadoop 集群的代理用户 hadoop 都能访问 hdfs 集群，hadoop.proxyuser.hadoop.groups 表示代理用户的组所属</p><p>以上操作做好了之后（最好重启一下HDFS集群），请继续做如下两步：</p><p><strong>第一步：先启动 hiveserver2 服务</strong></p><p>启动方式，（假如是在 hadoop3 上）：</p><p>启动为前台：hiveserver2</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ hiveserver2</span><br><span class="line">2018-04-04 10:21:49: Starting HiveServer2</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404102339802-1449905367.png" alt="img"></p><p>启动会多一个进程</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404102444648-1107890873.png" alt="img"></p><p>启动为后台：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nohup hiveserver2 1&gt;/home/hadoop/hiveserver.log 2&gt;/home/hadoop/hiveserver.err &amp;</span><br><span class="line">或者：nohup hiveserver2 1&gt;/dev/null 2&gt;/dev/null &amp;</span><br><span class="line">或者：nohup hiveserver2 &gt;/dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>以上 3 个命令是等价的，第一个表示记录日志，第二个和第三个表示不记录日志</p><p>命令中的 1 和 2 的意义分别是：</p><p><strong>1：表示标准日志输出</strong></p><p><strong>2：表示错误日志输出 如果我没有配置日志的输出路径，日志会生成在当前工作目录，默认的日志名称叫做： nohup.xxx</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ nohup hiveserver2 1&gt;/home/hadoop/log/hivelog/hiveserver.log 2&gt;/home/hadoop/log/hivelog/hiveserver.err &amp;</span><br><span class="line">[1] 4352</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404102754557-1513350234.png" alt="img"></p><p>PS：nohup 命令：如果你正在运行一个进程，而且你觉得在退出帐户时该进程还不会结束， 那么可以使用 nohup 命令。该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。 nohup 就是不挂起的意思(no hang up)。 该命令的一般形式为：nohup command &amp;</p><p><strong>第二步：然后启动 beeline 客户端去连接：</strong></p><p>执行命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ beeline -u jdbc:hive2//hadoop3:10000 -n hadoop</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">scan complete in 1ms</span><br><span class="line">scan complete in 2374ms</span><br><span class="line">No known driver to handle &quot;jdbc:hive2//hadoop3:10000&quot;</span><br><span class="line">Beeline version 2.3.3 by Apache Hive</span><br><span class="line">beeline&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404103024076-887839080.png" alt="img"></p><p>-u : 指定元数据库的链接信息</p><p>-n : 指定用户名和密码</p><p>另外还有一种方式也可以去连接：</p><p>先执行 beeline</p><p>然后按图所示输入：!connect jdbc:hive2://hadoop02:10000</p><p>按回车，然后输入用户名，这个 用户名就是安装 hadoop 集群的用户名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ beeline</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Beeline version 2.3.3 by Apache Hive</span><br><span class="line">beeline&gt; !connect jdbc:hive2://hadoop3:10000</span><br><span class="line">Connecting to jdbc:hive2://hadoop3:10000</span><br><span class="line">Enter username for jdbc:hive2://hadoop3:10000: hadoop</span><br><span class="line">Enter password for jdbc:hive2://hadoop3:10000: ******</span><br><span class="line">Connected to: Apache Hive (version 2.3.3)</span><br><span class="line">Driver: Hive JDBC (version 2.3.3)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://hadoop3:10000&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404103449430-1687396282.png" alt="img"></p><p>接下来便可以做 hive 操作</p><h2 id="三、Web-UI"><a href="#三、Web-UI" class="headerlink" title="三、Web UI"></a>三、Web UI</h2><p> 1、 下载对应版本的 src 包：apache-hive-2.3.2-src.tar.gz</p><p>2、 上传，解压</p><p>tar -zxvf apache-hive-2.3.2-src.tar.gz</p><p>3、 然后进入目录${HIVE_SRC_HOME}/hwi/web，执行打包命令:</p><p>jar -cvf hive-hwi-2.3.2.war *</p><p>在当前目录会生成一个 hive-hwi-2.3.2.war</p><p>4、 得到 hive-hwi-2.3.2.war 文件，复制到 hive 下的 lib 目录中</p><p>cp hive-hwi-2.3.2.war ${HIVE_HOME}/lib/</p><p>5、 修改配置文件 hive-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;hive.hwi.listen.host&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;0.0.0.0&lt;/value&gt;</span><br><span class="line"> &lt;description&gt;监听的地址&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;hive.hwi.listen.port&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;9999&lt;/value&gt;</span><br><span class="line"> &lt;description&gt;监听的端口号&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;hive.hwi.war.file&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;lib/hive-hwi-2.3.2.war&lt;/value&gt;</span><br><span class="line"> &lt;description&gt;war 包所在的地址&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>6、 复制所需 jar 包</p><p>　　1、cp ${JAVA_HOME}/lib/tools.jar ${HIVE_HOME}/lib</p><p>　　2、再寻找三个 jar 包，都放入${HIVE_HOME}/lib 目录：</p><p>　　　　commons-el-1.0.jar</p><p>　　　　jasper-compiler-5.5.23.jar</p><p>　　　　jasper-runtime-5.5.23.jar</p><p>　　　　不然启动 hwi 服务的时候会报错。</p><p>7、 安装 ant</p><blockquote><p>1、 上传 ant 包：apache-ant-1.9.4-bin.tar.gz</p><p>2、 解压 tar -zxvf apache-ant-1.9.4-bin.tar.gz -C ~/apps/</p><p>3、 配置环境变量 vi /etc/profile 在最后增加两行： export ANT_HOME=/home/hadoop/apps/apache-ant-1.9.4 export PATH=$PATH:$ANT_HOME/bin 配置完环境变量别忘记执行：source /etc/profile</p><p>4、 验证是否安装成功</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404105055045-2058439962.png" alt="img"></p></blockquote><p>8、上面的步骤都配置完，基本就大功告成了。进入${HIVE_HOME}/bin 目录：</p><p>　　 ${HIVE_HOME}/bin/hive –service hwi</p><p>　　或者让在后台运行： nohup bin/hive –service hwi &gt; /dev/null 2&gt; /dev/null &amp;</p><p>9、 前面配置了端口号为 9999，所以这里直接在浏览器中输入: hadoop02:9999/hwi</p><p>10、至此大功告成</p><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180404105149756-1584607098.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （三）Hive元数据信息对应MySQL数据库表</title>
      <link href="/2019-04-03-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%89%EF%BC%89Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF%E5%AF%B9%E5%BA%94MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8.html"/>
      <url>/2019-04-03-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%89%EF%BC%89Hive%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF%E5%AF%B9%E5%BA%94MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （三）Hive元数据信息对应MySQL数据库表：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （三）Hive元数据信息对应MySQL数据库表</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Hive 的元数据信息通常存储在关系型数据库中，常用MySQL数据库作为元数据库管理。上一篇hive的安装也是将元数据信息存放在MySQL数据库中。</p><p>Hive的元数据信息在MySQL数据中有57张表</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403183125168-938574711.png" alt="img"></p><h2 id="一、存储Hive版本的元数据表（VERSION）"><a href="#一、存储Hive版本的元数据表（VERSION）" class="headerlink" title="一、存储Hive版本的元数据表（VERSION）"></a>一、存储Hive版本的元数据表（VERSION）</h2><p> VERSION   – 查询版本信息</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403183216187-173310376.png" alt="img"></p><p>该表比较简单，但很重要。</p><table><thead><tr><th><strong>VER_ID</strong></th><th><strong>SCHEMA_VERSION</strong></th><th><strong>VERSION_COMMENT</strong></th></tr></thead><tbody><tr><td>ID主键</td><td>Hive版本</td><td>版本说明</td></tr><tr><td>1</td><td>0.13.0</td><td>Set by MetaStore</td></tr></tbody></table><p>如果该表出现问题，根本进入不了Hive-Cli。</p><p>比如该表不存在，当启动Hive-Cli时候，就会报错”Table ‘hive.version’ doesn’t exist”。</p><h2 id="二、Hive数据库相关的元数据表（DBS、DATABASE-PARAMS）"><a href="#二、Hive数据库相关的元数据表（DBS、DATABASE-PARAMS）" class="headerlink" title="二、Hive数据库相关的元数据表（DBS、DATABASE_PARAMS）"></a>二、Hive数据库相关的元数据表（DBS、DATABASE_PARAMS）</h2><h3 id="1、DBS"><a href="#1、DBS" class="headerlink" title="1、DBS"></a>1、DBS</h3><p>DBS　　　　 – 存储Hive中所有数据库的基本信息</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403183504628-542757406.png" alt="img"></p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>DB_ID</strong></td><td>数据库ID</td><td>2</td></tr><tr><td><strong>DESC</strong></td><td>数据库描述</td><td>测试库</td></tr><tr><td><strong>DB_LOCATION_URI</strong></td><td>数据库HDFS路径</td><td>hdfs://namenode/user/hive/warehouse/lxw1234.db</td></tr><tr><td><strong>NAME</strong></td><td>数据库名</td><td>lxw1234</td></tr><tr><td><strong>OWNER_NAME</strong></td><td>数据库所有者用户名</td><td>lxw1234</td></tr><tr><td><strong>OWNER_TYPE</strong></td><td>所有者角色</td><td>USER</td></tr></tbody></table><h3 id="2、DATABASE-PARAMS"><a href="#2、DATABASE-PARAMS" class="headerlink" title="2、DATABASE_PARAMS"></a>2、DATABASE_PARAMS</h3><p>DATABASE_PARAMS　　–该表存储数据库的相关参数，在CREATE DATABASE时候用</p><p>WITH DBPROPERTIES (property_name=property_value, …)指定的参数。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403184800892-1555435977.png" alt="img"></p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>DB_ID</strong></td><td>数据库ID</td><td>2</td></tr><tr><td><strong>PARAM_KEY</strong></td><td>参数名</td><td>createdby</td></tr><tr><td><strong>PARAM_VALUE</strong></td><td>参数值</td><td>lxw1234</td></tr></tbody></table><p><strong>注意：</strong></p><p><strong>DBS和DATABASE_PARAMS这两张表通过DB_ID字段关联。</strong></p><h2 id="三、Hive表和视图相关的元数据表"><a href="#三、Hive表和视图相关的元数据表" class="headerlink" title="三、Hive表和视图相关的元数据表"></a>三、Hive表和视图相关的元数据表</h2><p><strong>主要有TBLS、TABLE_PARAMS、TBL_PRIVS，这三张表通过TBL_ID关联。</strong></p><h3 id="1、TBLS"><a href="#1、TBLS" class="headerlink" title="1、TBLS"></a>1、TBLS</h3><p> 该表中存储Hive表、视图、索引表的基本信息。</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>TBL_ID</strong></td><td>表ID</td><td>1</td></tr><tr><td><strong>CREATE_TIME</strong></td><td>创建时间</td><td>1436317071</td></tr><tr><td><strong>DB_ID</strong></td><td>数据库ID</td><td>2，对应DBS中的DB_ID</td></tr><tr><td><strong>LAST_ACCESS_TIME</strong></td><td>上次访问时间</td><td>1436317071</td></tr><tr><td><strong>OWNER</strong></td><td>所有者</td><td>liuxiaowen</td></tr><tr><td><strong>RETENTION</strong></td><td>保留字段</td><td>0</td></tr><tr><td><strong>SD_ID</strong></td><td>序列化配置信息</td><td>86，对应SDS表中的SD_ID</td></tr><tr><td><strong>TBL_NAME</strong></td><td>表名</td><td>lxw1234</td></tr><tr><td><strong>TBL_TYPE</strong></td><td>表类型</td><td>MANAGED_TABLE、EXTERNAL_TABLE、INDEX_TABLE、VIRTUAL_VIEW</td></tr><tr><td><strong>VIEW_EXPANDED_TEXT</strong></td><td>视图的详细HQL语句</td><td>select <code>lxw1234</code>.<code>pt</code>, <code>lxw1234</code>.<code>pcid</code> from <code>liuxiaowen</code>.<code>lxw1234</code></td></tr><tr><td><strong>VIEW_ORIGINAL_TEXT</strong></td><td>视图的原始HQL语句</td><td>select * from lxw1234</td></tr></tbody></table><h3 id="2、TABLE-PARAMS"><a href="#2、TABLE-PARAMS" class="headerlink" title="2、TABLE_PARAMS"></a>2、<strong>TABLE_PARAMS</strong></h3><p>该表存储表/视图的属性信息。</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>TBL_ID</strong></td><td>表ID</td><td>1</td></tr><tr><td><strong>PARAM_KEY</strong></td><td>属性名</td><td>totalSize、numRows、EXTERNAL</td></tr><tr><td><strong>PARAM_VALUE</strong></td><td>属性值</td><td>970107336、21231028、TRUE</td></tr></tbody></table><h3 id="3、TBL-PRIVS"><a href="#3、TBL-PRIVS" class="headerlink" title="3、TBL_PRIVS"></a>3、<strong>TBL_PRIVS</strong></h3><p> 该表存储表/视图的授权信息</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>TBL_GRANT_ID</strong></td><td>授权ID</td><td>1</td></tr><tr><td><strong>CREATE_TIME</strong></td><td>授权时间</td><td>1436320455</td></tr><tr><td><strong>GRANT_OPTION</strong></td><td></td><td>0</td></tr><tr><td><strong>GRANTOR</strong></td><td>授权执行用户</td><td>liuxiaowen</td></tr><tr><td><strong>GRANTOR_TYPE</strong></td><td>授权者类型</td><td>USER</td></tr><tr><td><strong>PRINCIPAL_NAME</strong></td><td>被授权用户</td><td>username</td></tr><tr><td><strong>PRINCIPAL_TYPE</strong></td><td>被授权用户类型</td><td>USER</td></tr><tr><td><strong>TBL_PRIV</strong></td><td>权限</td><td>Select、Alter</td></tr><tr><td><strong>TBL_ID</strong></td><td>表ID</td><td>22，对应TBLS表中的TBL_ID</td></tr></tbody></table><h2 id="四、Hive文件存储信息相关的元数据表"><a href="#四、Hive文件存储信息相关的元数据表" class="headerlink" title="四、Hive文件存储信息相关的元数据表"></a>四、Hive文件存储信息相关的元数据表</h2><p>　　主要涉及SDS、SD_PARAMS、SERDES、SERDE_PARAMS</p><p>　　由于HDFS支持的文件格式很多，而建Hive表时候也可以指定各种文件格式，Hive在将HQL解析成MapReduce时候，需要知道去哪里，使用哪种格式去读写HDFS文件，而这些信息就保存在这几张表中。</p><h3 id="1、SDS"><a href="#1、SDS" class="headerlink" title="1、SDS"></a>1、SDS</h3><p>　　该表保存文件存储的基本信息，如INPUT_FORMAT、OUTPUT_FORMAT、是否压缩等。</p><p>　　<strong>TBLS表中的SD_ID与该表关联，可以获取Hive表的存储信息。</strong></p><table><thead><tr><th>元数据表字段</th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>SD_ID</strong></td><td>存储信息ID</td><td>1</td></tr><tr><td><strong>CD_ID</strong></td><td>字段信息ID</td><td>21，对应CDS表</td></tr><tr><td><strong>INPUT_FORMAT</strong></td><td>文件输入格式</td><td>org.apache.hadoop.mapred.TextInputFormat</td></tr><tr><td><strong>IS_COMPRESSED</strong></td><td>是否压缩</td><td>0</td></tr><tr><td><strong>IS_STOREDASSUBDIRECTORIES</strong></td><td>是否以子目录存储</td><td>0</td></tr><tr><td><strong>LOCATION</strong></td><td>HDFS路径</td><td>hdfs://namenode/hivedata/warehouse/ut.db/t_lxw</td></tr><tr><td><strong>NUM_BUCKETS</strong></td><td>分桶数量</td><td>5</td></tr><tr><td><strong>OUTPUT_FORMAT</strong></td><td>文件输出格式</td><td>org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</td></tr><tr><td><strong>SERDE_ID</strong></td><td>序列化类ID</td><td>3，对应SERDES表</td></tr></tbody></table><h3 id="2、SD-PARAMS"><a href="#2、SD-PARAMS" class="headerlink" title="2、SD_PARAMS"></a>2、SD_PARAMS</h3><p>　　该表存储Hive存储的属性信息，在创建表时候使用</p><p>　　STORED BY ‘storage.handler.class.name’ [WITH SERDEPROPERTIES (…)指定。</p><table><thead><tr><th><strong>元数据表字段</strong></th><th>说明</th><th>示例数据</th></tr></thead><tbody><tr><td><strong>SD_ID</strong></td><td>存储配置ID</td><td>1</td></tr><tr><td><strong>PARAM_KEY</strong></td><td>存储属性名</td><td></td></tr><tr><td><strong>PARAM_VALUE</strong></td><td>存储属性值</td><td></td></tr></tbody></table><h3 id="3、SERDES"><a href="#3、SERDES" class="headerlink" title="3、SERDES"></a>3、SERDES</h3><p> 该表存储序列化使用的类信息</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>SERDE_ID</strong></td><td>序列化类配置ID</td><td>1</td></tr><tr><td><strong>NAME</strong></td><td>序列化类别名</td><td></td></tr><tr><td><strong>SLIB</strong></td><td>序列化类</td><td>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</td></tr></tbody></table><h3 id="4、SERDE-PARAMS"><a href="#4、SERDE-PARAMS" class="headerlink" title="4、SERDE_PARAMS"></a>4、SERDE_PARAMS</h3><p> 该表存储序列化的一些属性、格式信息,比如：行、列分隔符</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>SERDE_ID</strong></td><td>序列化类配置ID</td><td>1</td></tr><tr><td><strong>PARAM_KEY</strong></td><td>属性名</td><td>field.delim</td></tr><tr><td><strong>PARAM_VALUE</strong></td><td>属性值</td><td>,</td></tr></tbody></table><h2 id="五、Hive表字段相关的元数据表"><a href="#五、Hive表字段相关的元数据表" class="headerlink" title="五、Hive表字段相关的元数据表"></a>五、Hive表字段相关的元数据表</h2><p>主要涉及COLUMNS_V2</p><h3 id="1、COLUMNS-V2"><a href="#1、COLUMNS-V2" class="headerlink" title="1、COLUMNS_V2"></a>1、COLUMNS_V2</h3><p>该表存储表对应的字段信息。</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>CD_ID</strong></td><td>字段信息ID</td><td>1</td></tr><tr><td><strong>COMMENT</strong></td><td>字段注释</td><td></td></tr><tr><td><strong>COLUMN_NAME</strong></td><td>字段名</td><td>pt</td></tr><tr><td><strong>TYPE_NAME</strong></td><td>字段类型</td><td>string</td></tr><tr><td><strong>INTEGER_IDX</strong></td><td>字段顺序</td><td>2</td></tr></tbody></table><h2 id="六、Hive表分区相关的元数据表"><a href="#六、Hive表分区相关的元数据表" class="headerlink" title="六、Hive表分区相关的元数据表"></a>六、Hive表分区相关的元数据表</h2><p>主要涉及PARTITIONS、PARTITION_KEYS、PARTITION_KEY_VALS、PARTITION_PARAMS</p><h3 id="1、PARTITIONS"><a href="#1、PARTITIONS" class="headerlink" title="1、PARTITIONS"></a>1、PARTITIONS</h3><p> 该表存储表分区的基本信息。</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>PART_ID</strong></td><td>分区ID</td><td>1</td></tr><tr><td><strong>CREATE_TIME</strong></td><td>分区创建时间</td><td></td></tr><tr><td><strong>LAST_ACCESS_TIME</strong></td><td>最后一次访问时间</td><td></td></tr><tr><td><strong>PART_NAME</strong></td><td>分区名</td><td>pt=2015-06-12</td></tr><tr><td><strong>SD_ID</strong></td><td>分区存储ID</td><td>21</td></tr><tr><td><strong>TBL_ID</strong></td><td>表ID</td><td>2</td></tr></tbody></table><h3 id="2、PARTITION-KEYS"><a href="#2、PARTITION-KEYS" class="headerlink" title="2、PARTITION_KEYS"></a>2、PARTITION_KEYS</h3><p>该表存储分区的字段信息。</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>TBL_ID</strong></td><td>表ID</td><td>2</td></tr><tr><td><strong>PKEY_COMMENT</strong></td><td>分区字段说明</td><td></td></tr><tr><td><strong>PKEY_NAME</strong></td><td>分区字段名</td><td>pt</td></tr><tr><td><strong>PKEY_TYPE</strong></td><td>分区字段类型</td><td>string</td></tr><tr><td><strong>INTEGER_IDX</strong></td><td>分区字段顺序</td><td>1</td></tr></tbody></table><h3 id="3、PARTITION-KEY-VALS"><a href="#3、PARTITION-KEY-VALS" class="headerlink" title="3、PARTITION_KEY_VALS"></a>3、PARTITION_KEY_VALS</h3><p>该表存储分区字段值。</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>PART_ID</strong></td><td>分区ID</td><td>2</td></tr><tr><td><strong>PART_KEY_VAL</strong></td><td>分区字段值</td><td>2015-06-12</td></tr><tr><td><strong>INTEGER_IDX</strong></td><td>分区字段值顺序</td><td>0</td></tr></tbody></table><h3 id="4、PARTITION-PARAMS"><a href="#4、PARTITION-PARAMS" class="headerlink" title="4、PARTITION_PARAMS"></a>4、PARTITION_PARAMS</h3><p>该表存储分区的属性信息。</p><table><thead><tr><th><strong>元数据表字段</strong></th><th><strong>说明</strong></th><th><strong>示例数据</strong></th></tr></thead><tbody><tr><td><strong>PART_ID</strong></td><td>分区ID</td><td>2</td></tr><tr><td><strong>PARAM_KEY</strong></td><td>分区属性名</td><td>numFiles、numRows</td></tr><tr><td><strong>PARAM_VALUE</strong></td><td>分区属性值</td><td>15、502195</td></tr></tbody></table><h2 id="七、其他不常用的元数据表"><a href="#七、其他不常用的元数据表" class="headerlink" title="七、其他不常用的元数据表"></a>七、其他不常用的元数据表</h2><ul><li><strong>DB_PRIVS</strong></li></ul><p>数据库权限信息表。通过GRANT语句对数据库授权后，将会在这里存储。</p><ul><li><strong>IDXS</strong></li></ul><p>索引表，存储Hive索引相关的元数据</p><ul><li><strong>INDEX_PARAMS</strong></li></ul><p>索引相关的属性信息。</p><ul><li><strong>TAB_COL_STATS</strong></li></ul><p>表字段的统计信息。使用ANALYZE语句对表字段分析后记录在这里。</p><ul><li><strong>TBL_COL_PRIVS</strong></li></ul><p>表字段的授权信息</p><ul><li><strong>PART_PRIVS</strong></li></ul><p>分区的授权信息</p><ul><li><strong>PART_COL_STATS</strong></li></ul><p>分区字段的统计信息。</p><ul><li><strong>PART_COL_PRIVS</strong></li></ul><p>分区字段的权限信息。</p><ul><li><strong>FUNCS</strong></li></ul><p>用户注册的函数信息</p><ul><li><strong>FUNC_RU</strong></li></ul><p>用户注册函数的资源信息</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （二）Hive安装</title>
      <link href="/2019-04-02-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%EF%BC%89Hive%E5%AE%89%E8%A3%85.html"/>
      <url>/2019-04-02-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%EF%BC%89Hive%E5%AE%89%E8%A3%85.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （二）Hive安装：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （二）Hive安装</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="Hive的下载"><a href="#Hive的下载" class="headerlink" title="Hive的下载"></a>Hive的下载</h2><p>下载地址<a href="http://mirrors.hust.edu.cn/apache/" target="_blank" rel="noopener">http://mirrors.hust.edu.cn/apache/</a></p><p>选择合适的Hive版本进行下载，进到stable-2文件夹可以看到稳定的2.x的版本是2.3.3</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403130724778-219253901.png" alt="img"></p><h2 id="Hive的安装"><a href="#Hive的安装" class="headerlink" title="Hive的安装"></a>Hive的安装</h2><h3 id="1、本人使用MySQL做为Hive的元数据库，所以先安装MySQL。"><a href="#1、本人使用MySQL做为Hive的元数据库，所以先安装MySQL。" class="headerlink" title="1、本人使用MySQL做为Hive的元数据库，所以先安装MySQL。"></a>1、本人使用MySQL做为Hive的元数据库，所以先安装MySQL。</h3><p>MySql安装过程<a href="http://www.cnblogs.com/qingyunzong/p/8294876.html" target="_blank" rel="noopener">http://www.cnblogs.com/qingyunzong/p/8294876.html</a></p><h3 id="2、上传Hive安装包"><a href="#2、上传Hive安装包" class="headerlink" title="2、上传Hive安装包"></a>2、上传Hive安装包</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403132436680-660336254.png" alt="img"></p><h3 id="3、解压安装包"><a href="#3、解压安装包" class="headerlink" title="3、解压安装包"></a>3、解压安装包</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ tar -zxvf apache-hive-2.3.3-bin.tar.gz -C apps/</span><br></pre></td></tr></table></figure><h3 id="4、修改配置文件"><a href="#4、修改配置文件" class="headerlink" title="4、修改配置文件"></a>4、修改配置文件</h3><p>配置文件所在目录apache-hive-2.3.3-bin/conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 apps]$ cd apache-hive-2.3.3-bin/</span><br><span class="line">[hadoop@hadoop3 apache-hive-2.3.3-bin]$ ls</span><br><span class="line">bin  binary-package-licenses  conf  examples  hcatalog  jdbc  lib  LICENSE  NOTICE  RELEASE_NOTES.txt  scripts</span><br><span class="line">[hadoop@hadoop3 apache-hive-2.3.3-bin]$ cd conf/</span><br><span class="line">[hadoop@hadoop3 conf]$ ls</span><br><span class="line">beeline-log4j2.properties.template    ivysettings.xml</span><br><span class="line">hive-default.xml.template             llap-cli-log4j2.properties.template</span><br><span class="line">hive-env.sh.template                  llap-daemon-log4j2.properties.template</span><br><span class="line">hive-exec-log4j2.properties.template  parquet-logging.properties</span><br><span class="line">hive-log4j2.properties.template</span><br><span class="line">[hadoop@hadoop3 conf]$ pwd</span><br><span class="line">/home/hadoop/apps/apache-hive-2.3.3-bin/conf</span><br><span class="line">[hadoop@hadoop3 conf]$</span><br></pre></td></tr></table></figure><p>新建hive-site.xml并添加以下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 conf]$ touch hive-site.xml</span><br><span class="line">[hadoop@hadoop3 conf]$ vi hive-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop1:3306/hivedb?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- 如果 mysql 和 hive 在同一个服务器节点，那么请更改 hadoop02 为 localhost --&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>以下可选配置，该配置信息用来指定 Hive 数据仓库的数据存储在 HDFS 上的目录</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>hive default warehouse, if nessecory, change it<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="5、-一定要记得加入-MySQL-驱动包（mysql-connector-java-5-1-40-bin-jar）该-jar-包放置在-hive-的根路径下的-lib-目录"><a href="#5、-一定要记得加入-MySQL-驱动包（mysql-connector-java-5-1-40-bin-jar）该-jar-包放置在-hive-的根路径下的-lib-目录" class="headerlink" title="5、 一定要记得加入 MySQL 驱动包（mysql-connector-java-5.1.40-bin.jar）该 jar 包放置在 hive 的根路径下的 lib 目录"></a>5、 一定要记得加入 MySQL 驱动包（mysql-connector-java-5.1.40-bin.jar）该 jar 包放置在 hive 的根路径下的 lib 目录</h3><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403133935984-1544488446.png" alt="img"></p><h3 id="6、-安装完成，配置环境变量"><a href="#6、-安装完成，配置环境变量" class="headerlink" title="6、 安装完成，配置环境变量"></a>6、 安装完成，配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 lib]$ vi ~/.bashrc </span><br><span class="line"><span class="meta">#</span><span class="bash">Hive</span></span><br><span class="line">export HIVE_HOME=/home/hadoop/apps/apache-hive-2.3.3-bin</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br></pre></td></tr></table></figure><p>使修改的配置文件立即生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 lib]$ source ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="7、-验证-Hive-安装"><a href="#7、-验证-Hive-安装" class="headerlink" title="7、 验证 Hive 安装"></a>7、 验证 Hive 安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ hive --help</span><br><span class="line">Usage ./hive &lt;parameters&gt; --service serviceName &lt;service parameters&gt;</span><br><span class="line">Service List: beeline cleardanglingscratchdir cli hbaseimport hbaseschematool help hiveburninclient hiveserver2 hplsql jar lineage llapdump llap llapstatus metastore metatool orcfiledump rcfilecat schemaTool version </span><br><span class="line">Parameters parsed:</span><br><span class="line">  --auxpath : Auxiliary jars </span><br><span class="line">  --config : Hive configuration directory</span><br><span class="line">  --service : Starts specific service/component. cli is default</span><br><span class="line">Parameters used:</span><br><span class="line">  HADOOP_HOME or HADOOP_PREFIX : Hadoop install directory</span><br><span class="line">  HIVE_OPT : Hive options</span><br><span class="line">For help on a particular service:</span><br><span class="line">  ./hive --service serviceName --help</span><br><span class="line">Debug help:  ./hive --debug --help</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403134314992-1941171163.png" alt="img"></p><h3 id="8、-初始化元数据库"><a href="#8、-初始化元数据库" class="headerlink" title="8、 初始化元数据库"></a>8、 初始化元数据库</h3><p>　　注意：当使用的 hive 是 2.x 之前的版本，不做初始化也是 OK 的，当 hive 第一次启动的 时候会自动进行初始化，只不过会不会生成足够多的元数据库中的表。在使用过程中会 慢慢生成。但最后进行初始化。如果使用的 2.x 版本的 Hive，那么就必须手动初始化元 数据库。使用命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ schematool -dbType mysql -initSchema</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Metastore connection URL:     jdbc:mysql://hadoop1:3306/hivedb?createDatabaseIfNotExist=true</span><br><span class="line">Metastore Connection Driver :     com.mysql.jdbc.Driver</span><br><span class="line">Metastore connection User:     root</span><br><span class="line">Starting metastore schema initialization to 2.3.0</span><br><span class="line">Initialization script hive-schema-2.3.0.mysql.sql</span><br><span class="line">Initialization script completed</span><br><span class="line">schemaTool completed</span><br><span class="line">[hadoop@hadoop3 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403134541826-654934912.png" alt="img"></p><h3 id="9、-启动-Hive-客户端"><a href="#9、-启动-Hive-客户端" class="headerlink" title="9、 启动 Hive 客户端"></a>9、 启动 Hive 客户端</h3><p><strong>hive –service cli和hive效果一样</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ hive --service cli</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:file:/home/hadoop/apps/apache-hive-2.3.3-bin/lib/hive-common-2.3.3.jar!/hive-log4j2.properties Async: true</span><br><span class="line">Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403163639692-1199542716.png" alt="img"></p><h2 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h2><p>现有一个文件student.txt，将其存入hive中，student.txt数据格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">95002,刘晨,女,19,IS</span><br><span class="line">95017,王风娟,女,18,IS</span><br><span class="line">95018,王一,女,19,IS</span><br><span class="line">95013,冯伟,男,21,CS</span><br><span class="line">95014,王小丽,女,19,CS</span><br><span class="line">95019,邢小丽,女,19,IS</span><br><span class="line">95020,赵钱,男,21,IS</span><br><span class="line">95003,王敏,女,22,MA</span><br><span class="line">95004,张立,男,19,IS</span><br><span class="line">95012,孙花,女,20,CS</span><br><span class="line">95010,孔小涛,男,19,CS</span><br><span class="line">95005,刘刚,男,18,MA</span><br><span class="line">95006,孙庆,男,23,CS</span><br><span class="line">95007,易思玲,女,19,MA</span><br><span class="line">95008,李娜,女,18,CS</span><br><span class="line">95021,周二,男,17,MA</span><br><span class="line">95022,郑明,男,20,MA</span><br><span class="line">95001,李勇,男,20,CS</span><br><span class="line">95011,包小柏,男,18,MA</span><br><span class="line">95009,梦圆圆,女,18,MA</span><br><span class="line">95015,王君,男,18,MA</span><br></pre></td></tr></table></figure><h3 id="1、创建一个数据库myhive"><a href="#1、创建一个数据库myhive" class="headerlink" title="1、创建一个数据库myhive"></a>1、创建一个数据库myhive</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create database myhive;</span><br><span class="line">OK</span><br><span class="line">Time taken: 7.847 seconds</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403173156107-1549458905.png" alt="img"></p><h3 id="2、使用新的数据库myhive"><a href="#2、使用新的数据库myhive" class="headerlink" title="2、使用新的数据库myhive"></a>2、使用新的数据库myhive</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; use myhive;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.047 seconds</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403173305344-1780862637.png" alt="img"></p><h3 id="3、查看当前正在使用的数据库"><a href="#3、查看当前正在使用的数据库" class="headerlink" title="3、查看当前正在使用的数据库"></a>3、查看当前正在使用的数据库</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select current_database();</span><br><span class="line">OK</span><br><span class="line">myhive</span><br><span class="line">Time taken: 0.728 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403173425413-1583852832.png" alt="img"></p><h3 id="4、在数据库myhive创建一张student表"><a href="#4、在数据库myhive创建一张student表" class="headerlink" title="4、在数据库myhive创建一张student表"></a>4、在数据库myhive创建一张student表</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table student(id int, name string, sex string, age int, department string) row format delimited fields terminated by ",";</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.718 seconds</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403181521923-461375650.png" alt="img"></p><h3 id="5、往表中加载数据"><a href="#5、往表中加载数据" class="headerlink" title="5、往表中加载数据"></a>5、往表中加载数据</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data local inpath "/home/hadoop/student.txt" into table student;</span><br><span class="line">Loading data to table myhive.student</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.854 seconds</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403181841534-1261507478.png" alt="img"></p><h3 id="6、查询数据"><a href="#6、查询数据" class="headerlink" title="6、查询数据"></a>6、查询数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br><span class="line">OK</span><br><span class="line">95002    刘晨    女    19    IS</span><br><span class="line">95017    王风娟    女    18    IS</span><br><span class="line">95018    王一    女    19    IS</span><br><span class="line">95013    冯伟    男    21    CS</span><br><span class="line">95014    王小丽    女    19    CS</span><br><span class="line">95019    邢小丽    女    19    IS</span><br><span class="line">95020    赵钱    男    21    IS</span><br><span class="line">95003    王敏    女    22    MA</span><br><span class="line">95004    张立    男    19    IS</span><br><span class="line">95012    孙花    女    20    CS</span><br><span class="line">95010    孔小涛    男    19    CS</span><br><span class="line">95005    刘刚    男    18    MA</span><br><span class="line">95006    孙庆    男    23    CS</span><br><span class="line">95007    易思玲    女    19    MA</span><br><span class="line">95008    李娜    女    18    CS</span><br><span class="line">95021    周二    男    17    MA</span><br><span class="line">95022    郑明    男    20    MA</span><br><span class="line">95001    李勇    男    20    CS</span><br><span class="line">95011    包小柏    男    18    MA</span><br><span class="line">95009    梦圆圆    女    18    MA</span><br><span class="line">95015    王君    男    18    MA</span><br><span class="line">Time taken: 2.455 seconds, Fetched: 21 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403182000435-2086698927.png" alt="img"></p><h3 id="7、查看表结构"><a href="#7、查看表结构" class="headerlink" title="7、查看表结构"></a>7、查看表结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc student;</span><br><span class="line">OK</span><br><span class="line">id                      int                                         </span><br><span class="line">name                    string                                      </span><br><span class="line">sex                     string                                      </span><br><span class="line">age                     int                                         </span><br><span class="line">department              string                                      </span><br><span class="line">Time taken: 0.102 seconds, Fetched: 5 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc extended student;</span><br><span class="line">OK</span><br><span class="line">id                      int                                         </span><br><span class="line">name                    string                                      </span><br><span class="line">sex                     string                                      </span><br><span class="line">age                     int                                         </span><br><span class="line">department              string                                      </span><br><span class="line">          </span><br><span class="line">Detailed Table Information    Table(tableName:student, dbName:myhive, owner:hadoop, createTime:1522750487, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:null), FieldSchema(name:name, type:string, comment:null), FieldSchema(name:sex, type:string, comment:null), FieldSchema(name:age, type:int, comment:null), FieldSchema(name:department, type:string, comment:null)], location:hdfs://myha01/user/hive/warehouse/myhive.db/student, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:&#123;serialization.format=,, field.delim=,&#125;), bucketCols:[], sortCols:[], parameters:&#123;&#125;, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:&#123;&#125;), storedAsSubDirectories:false), partitionKeys:[], parameters:&#123;transient_lastDdlTime=1522750695, totalSize=523, numRows=0, rawDataSize=0, numFiles=1&#125;, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, rewriteEnabled:false)    </span><br><span class="line">Time taken: 0.127 seconds, Fetched: 7 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc formatted student;</span><br><span class="line">OK</span><br><span class="line"># col_name                data_type               comment             </span><br><span class="line">          </span><br><span class="line">id                      int                                         </span><br><span class="line">name                    string                                      </span><br><span class="line">sex                     string                                      </span><br><span class="line">age                     int                                         </span><br><span class="line">department              string                                      </span><br><span class="line">          </span><br><span class="line"># Detailed Table Information          </span><br><span class="line">Database:               myhive                   </span><br><span class="line">Owner:                  hadoop                   </span><br><span class="line">CreateTime:             Tue Apr 03 18:14:47 CST 2018     </span><br><span class="line">LastAccessTime:         UNKNOWN                  </span><br><span class="line">Retention:              0                        </span><br><span class="line">Location:               hdfs://myha01/user/hive/warehouse/myhive.db/student     </span><br><span class="line">Table Type:             MANAGED_TABLE            </span><br><span class="line">Table Parameters:          </span><br><span class="line">    numFiles                1                   </span><br><span class="line">    numRows                 0                   </span><br><span class="line">    rawDataSize             0                   </span><br><span class="line">    totalSize               523                 </span><br><span class="line">    transient_lastDdlTime    1522750695          </span><br><span class="line">          </span><br><span class="line"># Storage Information          </span><br><span class="line">SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe     </span><br><span class="line">InputFormat:            org.apache.hadoop.mapred.TextInputFormat     </span><br><span class="line">OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat     </span><br><span class="line">Compressed:             No                       </span><br><span class="line">Num Buckets:            -1                       </span><br><span class="line">Bucket Columns:         []                       </span><br><span class="line">Sort Columns:           []                       </span><br><span class="line">Storage Desc Params:          </span><br><span class="line">    field.delim             ,                   </span><br><span class="line">    serialization.format    ,                   </span><br><span class="line">Time taken: 0.13 seconds, Fetched: 34 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习之路 （一）Hive初识</title>
      <link href="/2019-04-01-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%80%EF%BC%89Hive%E5%88%9D%E8%AF%86.html"/>
      <url>/2019-04-01-Hive%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%80%EF%BC%89Hive%E5%88%9D%E8%AF%86.html</url>
      
        <content type="html"><![CDATA[<p>** Hive学习之路 （一）Hive初识：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hive学习之路 （一）Hive初识</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="Hive-简介"><a href="#Hive-简介" class="headerlink" title="Hive 简介"></a>Hive 简介</h2><h3 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h3><p><strong>1、Hive 由 Facebook 实现并开源</strong></p><p><strong>2、是基于 Hadoop 的一个数据仓库工具</strong></p><p><strong>3、可以将结构化的数据映射为一张数据库表</strong></p><p><strong>4、并提供 HQL(Hive SQL)查询功能</strong></p><p><strong>5、底层数据是存储在 HDFS 上</strong></p><p><strong>6、Hive的本质是将 SQL 语句转换为 MapReduce 任务运行</strong></p><p><strong>7、使不熟悉 MapReduce 的用户很方便地利用 HQL 处理和计算 HDFS 上的结构化的数据，适用于离线的批量数据计算。</strong></p><p>　　数据仓库之父比尔·恩门（Bill Inmon）在 1991 年出版的“Building the Data Warehouse”（《建 立数据仓库》）一书中所提出的定义被广泛接受——数据仓库（Data Warehouse）是一个面 向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反映历史 变化（Time Variant）的数据集合，用于支持管理决策(Decision Making Support)。</p><p>　　Hive 依赖于 HDFS 存储数据，Hive 将 HQL 转换成 MapReduce 执行，所以说 Hive 是基于 Hadoop 的一个数据仓库工具，实质就是一款基于 HDFS 的 MapReduce 计算框架，对存储在 HDFS 中的数据进行分析和管理</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403192903767-826182114.png" alt="img"></p><h3 id="为什么使用-Hive"><a href="#为什么使用-Hive" class="headerlink" title="为什么使用 Hive"></a>为什么使用 Hive</h3><p>直接使用 MapReduce 所面临的问题：</p><p>　　1、人员学习成本太高</p><p>　　2、项目周期要求太短</p><p>　　3、MapReduce实现复杂查询逻辑开发难度太大</p><p>为什么要使用 Hive：</p><p>　　1、更友好的接口：操作接口采用类 SQL 的语法，提供快速开发的能力</p><p>　　2、更低的学习成本：避免了写 MapReduce，减少开发人员的学习成本</p><p>　　3、更好的扩展性：可自由扩展集群规模而无需重启服务，还支持用户自定义函数</p><h3 id="Hive-特点"><a href="#Hive-特点" class="headerlink" title="Hive 特点"></a>Hive 特点</h3><p><strong>优点</strong>：</p><p>　　1、<strong>可扩展性,横向扩展</strong>，Hive 可以自由的扩展集群的规模，一般情况下不需要重启服务 横向扩展：通过分担压力的方式扩展集群的规模 纵向扩展：一台服务器cpu i7-6700k 4核心8线程，8核心16线程，内存64G =&gt; 128G</p><p>　　2、<strong>延展性</strong>，Hive 支持自定义函数，用户可以根据自己的需求来实现自己的函数</p><p>　　3、<strong>良好的容错性</strong>，可以保障即使有节点出现问题，SQL 语句仍可完成执行</p><p><strong>缺点</strong>：</p><p>　　1、<strong>Hive 不支持记录级别的增删改操作</strong>，但是用户可以通过查询生成新表或者将查询结 果导入到文件中（当前选择的 hive-2.3.2 的版本支持记录级别的插入操作）</p><p>　　2、<strong>Hive 的查询延时很严重</strong>，因为 MapReduce Job 的启动过程消耗很长时间，所以不能 用在交互查询系统中。</p><p>　　3、<strong>Hive 不支持事务</strong>（因为不没有增删改，所以主要用来做 OLAP（联机分析处理），而 不是 OLTP（联机事务处理），这就是数据处理的两大级别）。</p><h3 id="Hive-和-RDBMS-的对比"><a href="#Hive-和-RDBMS-的对比" class="headerlink" title="Hive 和 RDBMS 的对比"></a>Hive 和 RDBMS 的对比</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403193352838-1398998715.png" alt="img"></p><p>总结：</p><p>　　Hive 具有 SQL 数据库的外表，但应用场景完全不同，<strong>Hive 只适合用来做海量离线数 据统计分析，也就是数据仓库</strong>。</p><h2 id="Hive的架构"><a href="#Hive的架构" class="headerlink" title="Hive的架构"></a>Hive的架构</h2><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403193501903-1989526977.png" alt="img"></p><p>从上图看出hive的内部架构由四部分组成：</p><h3 id="1、用户接口-shell-CLI-jdbc-odbc-webui-Command-Line-Interface"><a href="#1、用户接口-shell-CLI-jdbc-odbc-webui-Command-Line-Interface" class="headerlink" title="1、用户接口: shell/CLI, jdbc/odbc, webui Command Line Interface"></a>1、用户接口: shell/CLI, jdbc/odbc, webui Command Line Interface</h3><p>　　CLI，Shell 终端命令行（Command Line Interface），采用交互形式使用 Hive 命令行与 Hive 进行交互，最常用（学习，调试，生产）</p><p>　　JDBC/ODBC，是 Hive 的基于 JDBC 操作提供的客户端，用户（开发员，运维人员）通过 这连接至 Hive server 服务</p><p>　　Web UI，通过浏览器访问 Hive</p><h3 id="2、跨语言服务-：-thrift-server-提供了一种能力，让用户可以使用多种不同的语言来操纵hive"><a href="#2、跨语言服务-：-thrift-server-提供了一种能力，让用户可以使用多种不同的语言来操纵hive" class="headerlink" title="2、跨语言服务 ： thrift server 提供了一种能力，让用户可以使用多种不同的语言来操纵hive"></a>2、跨语言服务 ： thrift server 提供了一种能力，让用户可以使用多种不同的语言来操纵hive</h3><p>　　Thrift 是 Facebook 开发的一个软件框架，可以用来进行可扩展且跨语言的服务的开发， Hive 集成了该服务，能让不同的编程语言调用 Hive 的接口</p><h3 id="3、底层的Driver：-驱动器Driver，编译器Compiler，优化器Optimizer，执行器Executor"><a href="#3、底层的Driver：-驱动器Driver，编译器Compiler，优化器Optimizer，执行器Executor" class="headerlink" title="3、底层的Driver： 驱动器Driver，编译器Compiler，优化器Optimizer，执行器Executor"></a>3、底层的Driver： 驱动器Driver，编译器Compiler，优化器Optimizer，执行器Executor</h3><p>　　Driver 组件完成 HQL 查询语句从词法分析，语法分析，编译，优化，以及生成逻辑执行 计划的生成。生成的逻辑执行计划存储在 HDFS 中，并随后由 MapReduce 调用执行</p><p>　　Hive 的核心是驱动引擎， 驱动引擎由四部分组成：</p><p>　　　　(1) 解释器：解释器的作用是将 HiveSQL 语句转换为抽象语法树（AST）</p><p>　　　　(2) 编译器：编译器是将语法树编译为逻辑执行计划</p><p>　　　　(3) 优化器：优化器是对逻辑执行计划进行优化</p><p>　　　　(4) 执行器：执行器是调用底层的运行框架执行逻辑执行计划</p><h3 id="4、元数据存储系统-：-RDBMS-MySQL"><a href="#4、元数据存储系统-：-RDBMS-MySQL" class="headerlink" title="4、元数据存储系统 ： RDBMS MySQL"></a>4、元数据存储系统 ： RDBMS MySQL</h3><p>　　<strong>元数据</strong>，通俗的讲，就是存储在 Hive 中的数据的描述信息。</p><p>　　Hive 中的元数据通常包括：表的名字，表的列和分区及其属性，表的属性（内部表和 外部表），表的数据所在目录</p><p>　　Metastore 默认存在自带的 Derby 数据库中。缺点就是不适合多用户操作，并且数据存 储目录不固定。数据库跟着 Hive 走，极度不方便管理</p><p>　　解决方案：通常存我们自己创建的 MySQL 库（本地 或 远程）</p><p>　　Hive 和 MySQL 之间通过 MetaStore 服务交互</p><h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h3><p>　　HiveQL 通过命令行或者客户端提交，经过 Compiler 编译器，运用 MetaStore 中的元数 据进行类型检测和语法分析，生成一个逻辑方案(Logical Plan)，然后通过的优化处理，产生 一个 MapReduce 任务。</p><h2 id="Hive的数据组织"><a href="#Hive的数据组织" class="headerlink" title="Hive的数据组织"></a>Hive的数据组织</h2><p>1、Hive 的存储结构包括<strong>数据库、表、视图、分区和表数据</strong>等。数据库，表，分区等等都对 应 HDFS 上的一个目录。表数据对应 HDFS 对应目录下的文件。</p><p>2、Hive 中所有的数据都存储在 HDFS 中，没有专门的数据存储格式，因为 <strong>Hive 是读模式</strong> （Schema On Read），可支持 TextFile，SequenceFile，RCFile 或者自定义格式等</p><p>3、 只需要在创建表的时候告诉 Hive 数据中的<strong>列分隔符和行分隔符</strong>，Hive 就可以解析数据</p><p>　　Hive 的默认列分隔符：控制符 <strong>Ctrl + A，\x01 Hive</strong> 的</p><p>　　Hive 的默认行分隔符：换行符 <strong>\n</strong></p><p>4、Hive 中包含以下数据模型：</p><p>　　<strong>database</strong>：在 HDFS 中表现为${hive.metastore.warehouse.dir}目录下一个文件夹</p><p>　　<strong>table</strong>：在 HDFS 中表现所属 database 目录下一个文件夹</p><p>　　<strong>external table</strong>：与 table 类似，不过其数据存放位置可以指定任意 HDFS 目录路径</p><p>　　<strong>partition</strong>：在 HDFS 中表现为 table 目录下的子目录</p><p>　　<strong>bucket</strong>：在 HDFS 中表现为同一个表目录或者分区目录下根据某个字段的值进行 hash 散 列之后的多个文件</p><p>　　<strong>view</strong>：与传统数据库类似，只读，基于基本表创建</p><p>5、Hive 的元数据存储在 RDBMS 中，除元数据外的其它所有数据都基于 HDFS 存储。默认情 况下，Hive 元数据保存在内嵌的 Derby 数据库中，只能允许一个会话连接，只适合简单的 测试。实际生产环境中不适用，为了支持多用户会话，则需要一个独立的元数据库，使用 MySQL 作为元数据库，Hive 内部对 MySQL 提供了很好的支持。</p><p>6、Hive 中的表分为内部表、外部表、分区表和 Bucket 表</p><p><strong>内部表和外部表的区别：</strong></p><p>　　<strong>删除内部表，删除表元数据和数据</strong></p><p>　　<strong>删除外部表，删除元数据，不删除数据</strong></p><p><strong>内部表和外部表的使用选择：</strong></p><p>　　大多数情况，他们的区别不明显，如果数据的所有处理都在 Hive 中进行，那么倾向于 选择内部表，但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。</p><p>　　使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中</p><p>　　使用外部表的场景是针对一个数据集有多个不同的 Schema</p><p>　　通过外部表和内部表的区别和使用选择的对比可以看出来，hive 其实仅仅只是对存储在 HDFS 上的数据提供了一种新的抽象。而不是管理存储在 HDFS 上的数据。所以不管创建内部 表还是外部表，都可以对 hive 表的数据存储目录中的数据进行增删操作。</p><p><strong>分区表和分桶表的区别：</strong> </p><p>　　Hive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。同 时表和分区也可以进一步被划分为 Buckets，分桶表的原理和 MapReduce 编程中的 HashPartitioner 的原理类似。</p><p>　　分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于 Hive 是读模式，所 以对添加进分区的数据不做模式校验，分桶表中的数据是按照某些分桶字段进行 hash 散列 形成的多个文件，所以数据的准确性也高很多</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文读懂 Spark</title>
      <link href="/2018-10-30-spark-%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82spark.html"/>
      <url>/2018-10-30-spark-%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82spark.html</url>
      
        <content type="html"><![CDATA[<p>** 一文读懂 Spark：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1-前言"><a href="#1-1-前言" class="headerlink" title="1.1 前言"></a>1.1 前言</h2><p>​        Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。</p><p>​        Apache Spark 诞生于大名鼎鼎的 AMPLab（这里还诞生过 Mesos 和 Alluxio），从创立之初就带有浓厚的学术气质，其设计目标是为各种大数据处理需求提供一个统一的技术栈。如今 Spark 背后的商业公司 Databricks 创始人也是来自 AMPLab 的博士毕业生。</p><p>​        Spark 本身使用 Scala 语言编写，Scala 是一门融合了面向对象与函数式的“双范式”语言，运行在 JVM 之上。Spark 大量使用了它的函数式、即时代码生成等特性。Spark 目前提供了 Java、Scala、Python、R 四种语言的 API，前两者因为同样运行在 JVM 上可以达到更原生的支持。</p><p><strong>MapReduce 的问题所在</strong></p><p>​        Hadoop 是大数据处理领域的开创者。严格来说，Hadoop 不只是一个软件，而是一整套生态系统，例如 MapReduce 负责进行分布式计算，而 HDFS 负责存储大量文件。</p><p>​        MapReduce 模型的诞生是大数据处理从无到有的飞跃。但随着技术的进步，对大数据处理的需求也变得越来越复杂，MapReduce 的问题也日渐凸显。通常，我们将 MapReduce 的输入和输出数据保留在 HDFS 上，很多时候，<strong>复杂的 ETL、数据清洗等工作无法用一次 MapReduce 完成，所以需要将多个 MapReduce 过程连接起来</strong>：</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/d3f3c77e124e420bb147bc19389c6db1.jpeg" alt="img"></p><p>▲ 上图中只有两个 MapReduce 串联，实际上可能有几十个甚至更多，依赖关系也更复杂。</p><p>这种方式下，<strong>每次中间结果都要写入 HDFS 落盘保存，代价很大</strong>（别忘了，HDFS 的每份数据都需要冗余若干份拷贝）。另外，由于本质上是多次 MapReduce 任务，调度也比较麻烦，实时性无从谈起。</p><h2 id="Spark-与-RDD-模型"><a href="#Spark-与-RDD-模型" class="headerlink" title="Spark 与 RDD 模型"></a>Spark 与 RDD 模型</h2><p>针对上面的问题，如果能把中间结果保存在内存里，岂不是快的多？之所以不能这么做，最大的障碍是：分布式系统必须能容忍一定的故障，所谓 fault-tolerance。如果只是放在内存中，一旦某个计算节点宕机，其他节点无法恢复出丢失的数据，只能重启整个计算任务，这对于动辄成百上千节点的集群来说是不可接受的。</p><p>一般来说，想做到 fault-tolerance 只有两个方案：要么存储到外部（例如 HDFS），要么拷贝到多个副本。<strong>Spark 大胆地提出了第三种——重算一遍。但是之所以能做到这一点，是依赖于一个额外的假设：所有计算过程都是确定性的（deterministic）。</strong>Spark 借鉴了函数式编程思想，提出了 RDD（Resilient Distributed Datasets），译作“弹性分布式数据集”。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/afc23758a78b4d25958338fd7649b3e0.jpeg" alt="img"></p><p><strong>RDD 是一个只读的、分区的（partitioned）数据集合</strong>。RDD 要么来源于不可变的外部文件（例如 HDFS 上的文件），要么由确定的算子由其他 RDD 计算得到。<strong>RDD 通过算子连接构成有向无环图（DAG）</strong>，上图演示了一个简单的例子，其中节点对应 RDD，边对应算子。</p><p>回到刚刚的问题，RDD 如何做到 fault-tolerance？很简单，RDD 中的每个分区都能被确定性的计算出来，所以<strong>一旦某个分区丢失了，另一个计算节点可以从它的前继节点出发、用同样的计算过程重算一次，即可得到完全一样的 RDD 分区</strong>。这个过程可以递归的进行下去。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/ac3c7bd069244036817c416494610da2.jpeg" alt="img"></p><p>▲ 上图演示了 RDD 分区的恢复。为了简洁并没有画出分区，实际上恢复是以分区为单位的。</p><p>Spark 的编程接口和 Java 8 的 Stream 很相似：RDD 作为数据，在多种算子间变换，构成对执行计划 DAG 的描述。最后，一旦遇到类似 collect()这样的输出命令，执行计划会被发往 Spark 集群、开始计算。不难发现，算子分成两类：</p><ul><li>map()、filter()、join() 等算子称为 Transformation，它们输入一个或多个 RDD，输出一个 RDD。</li><li>collect()、count()、save() 等算子称为 Action，它们通常是将数据收集起来返回；</li></ul><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/87e40f722c334e2aba35c0e76c98c7ae.jpeg" alt="img"></p><p>▲ 上图的例子用来收集包含“HDFS”关键字的错误日志时间戳。当执行到 collect() 时，右边的执行计划开始运行。</p><p>像之前提到的，RDD 的数据由多个分区（partition）构成，这些分区可以分布在集群的各个机器上，这也就是 RDD 中 “distributed” 的含义。熟悉 DBMS 的同学可以把 RDD 理解为逻辑执行计划，partition 理解为物理执行计划。</p><p>此外，RDD 还包含它的每个分区的依赖分区（dependency），以及一个函数指出如何计算出本分区的数据。Spark 的设计者发现，依赖关系依据执行方式的不同可以很自然地分成两种：<strong>窄依赖（Narrow Dependency）和宽依赖（Wide Dependency）</strong>，举例来说：</p><ul><li>map()、filter() 等算子构成窄依赖：生产的每个分区只依赖父 RDD 中的一个分区。</li><li>groupByKey() 等算子构成宽依赖：生成的每个分区依赖父 RDD 中的多个分区（往往是全部分区）。</li></ul><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/b5373ed5294948b5a2f652ad489fcca3.jpeg" alt="img"></p><p>▲ 左图展示了宽依赖和窄依赖，其中 Join 算子因为 Join key 分区情况不同二者皆有；右图展示了执行过程，由于宽依赖的存在，执行计划被分成 3 个阶段。</p><p>在执行时，窄依赖可以很容易的按流水线（pipeline）的方式计算：对于每个分区从前到后依次代入各个算子即可。<strong>然而，宽依赖需要等待前继 RDD 中所有分区计算完成；换句话说，宽依赖就像一个栅栏（barrier）会阻塞到之前的所有计算完成。</strong>整个计算过程被宽依赖分割成多个阶段（stage），如上右图所示。</p><blockquote><p>了解 MapReduce 的同学可能已经发现，宽依赖本质上就是一个 MapReduce 过程。但是相比 MapReduce 自己写 Map 和 Reduce 函数的编程接口，Spark 的接口要容易的多；并且在 Spark 中，多个阶段的 MapReduce 只需要构造一个 DAG 即可。</p></blockquote><h2 id="声明式接口：Spark-SQL"><a href="#声明式接口：Spark-SQL" class="headerlink" title="声明式接口：Spark SQL"></a>声明式接口：Spark SQL</h2><p>命令式编程中，你需要编写一个程序。下面给出了一种伪代码实现：</p><p>employees = db.getAllEmployees() countByDept = dict() // 统计各部门女生人数 (dept_id -&gt; count) for employee in employees: if (employee.gender == ‘female’) countByDept[employee.dept_id] += 1 results = list() // 加上 dept.name 列 depts = db.getAllDepartments() for dept in depts: if (countByDept containsKey dept.id) results.add(row(dept.id, dept.name, countByDept[dept.id])) return results;</p><p>声明式编程中，你只要用关系代数的运算表达出结果：</p><p>employees.join(dept, employees.deptId == dept.id) .where(employees.gender == ‘female’) .groupBy(dept.id, dept.name) .agg()</p><blockquote><p>等价地，如果你更熟悉 SQL，也可以写成这样：</p><p>SELECTdept.id,dept.name,COUNT(*)FROMemployees JOINdept ONemployees.dept_id ==dept.idWHEREemployees.gender =’female’GROUPBYdept.id,dept.name</p></blockquote><p>显然，声明式的要简洁的多！但声明式编程依赖于执行者产生真正的程序代码，所以除了上面这段程序，还需要把数据模型（即 schema）一并告知执行者。声明式编程最广为人知的形式就是 SQL。</p><p>Spark SQL 就是这样一个基于 SQL 的声明式编程接口。<strong>你可以将它看作在 Spark 之上的一层封装，在 RDD 计算模型的基础上，提供了 DataFrame API 以及一个内置的 SQL 执行计划优化器 Catalyst。</strong></p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/a4d07f1924c44c968fd3a19e64183042.jpeg" alt="img"></p><p>▲ 上图黄色部分是 Spark SQL 中新增的部分。</p><p><strong>DataFrame 就像数据库中的表，除了数据之外它还保存了数据的 schema 信息。</strong>计算中，schema 信息也会经过算子进行相应的变换。DataFrame 的数据是行（row）对象组成的 RDD，对 DataFrame 的操作最终会变成对底层 RDD 的操作。</p><p><strong>Catalyst 是一个内置的 SQL 优化器，负责把用户输入的 SQL 转化成执行计划。</strong>Catelyst 强大之处是它利用了 Scala 提供的代码生成（codegen）机制，物理执行计划经过编译，产出的执行代码效率很高，和直接操作 RDD 的命令式代码几乎没有分别。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/5558627b9ae347ac9dd7e17a8cfa0eff.jpeg" alt="img"></p><p>▲ 上图是 Catalyst 的工作流程，与大多数 SQL 优化器一样是一个 Cost-Based Optimizer (CBO)，但最后使用代码生成（codegen）转化成直接对 RDD 的操作。</p><h2 id="流计算框架：Spark-Streaming"><a href="#流计算框架：Spark-Streaming" class="headerlink" title="流计算框架：Spark Streaming"></a>流计算框架：Spark Streaming</h2><p>以往，批处理和流计算被看作大数据系统的两个方面。我们常常能看到这样的架构——以 Kafka、Storm 为代表的流计算框架用于实时计算，而 Spark 或 MapReduce 则负责每天、每小时的数据批处理。在 ETL 等场合，这样的设计常常导致同样的计算逻辑被实现两次，耗费人力不说，保证一致性也是个问题。</p><p>Spark Streaming 正是诞生于此类需求。传统的流计算框架大多注重于低延迟，采用了持续的（continuous）算子模型；而 Spark Streaming 基于 Spark，另辟蹊径提出了 <strong>D-Stream（Discretized Streams）方案：将流数据切成很小的批（micro-batch），用一系列的短暂、无状态、确定性的批处理实现流处理。</strong></p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/091d8858c96c4b09895b70c24c138932.jpeg" alt="img"></p><p>Spark Streaming 的做法在流计算框架中很有创新性，它虽然牺牲了低延迟（一般流计算能做到 100ms 级别，Spark Streaming 延迟一般为 1s 左右），但是带来了三个诱人的优势：</p><ul><li>更高的吞吐量（大约是 Storm 的 2-5 倍）</li><li>更快速的失败恢复（通常只要 1-2s），因此对于 straggler（性能拖后腿的节点）直接杀掉即可</li><li>开发者只需要维护一套 ETL 逻辑即可同时用于批处理和流计算</li></ul><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/49926e28d6014358ae51e234771c089e.jpeg" alt="img"></p><p>▲ 上左图中，为了在持续算子模型的流计算系统中保证一致性，不得不在主备机之间使用同步机制，导致性能损失，Spark Streaming 完全没有这个问题；右图是 D-Stream 的原理示意图。</p><p>你可能会困惑，流计算中的状态一直是个难题。但我们刚刚提到 D-Stream 方案是无状态的，那诸如 word count 之类的问题，怎么做到保持 count 算子的状态呢？</p><p>答案是通过 RDD：<strong>将前一个时间步的 RDD 作为当前时间步的 RDD 的前继节点，就能造成状态不断更替的效果</strong>。实际上，新的状态 RDD 总是不断生成，而旧的 RDD 并不会被“替代”，而是作为新 RDD 的前继依赖。对于底层的 Spark 框架来说，并没有时间步的概念，有的只是不断扩张的 DAG 图和新的 RDD 节点。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/f0696a56517c463f8be35e35dba5739d.jpeg" alt="img"></p><p>▲ 上图是流式计算 word count 的例子，count 结果在不同时间步中不断累积。</p><p>那么另一个问题也随之而来：随着时间的推进，上图中的状态 RDD counts会越来越多，他的祖先（lineage）变得越来越长，极端情况下，恢复过程可能溯源到很久之前。这是不可接受的！<strong>因此，Spark Streming 会定期地对状态 RDD 做 checkpoint，将其持久化到 HDFS 等存储中，这被称为 lineage cut</strong>，在它之前更早的 RDD 就可以没有顾虑地清理掉了。</p><blockquote><p>关于流行的几个开源流计算框架的对比，可以参考文章 Comparison of Apache Stream Processing Frameworks。</p></blockquote><h2 id="流计算与-SQL：Spark-Structured-Streaming"><a href="#流计算与-SQL：Spark-Structured-Streaming" class="headerlink" title="流计算与 SQL：Spark Structured Streaming"></a>流计算与 SQL：Spark Structured Streaming</h2><blockquote><p>出人意料的是，Spark Structured Streaming 的流式计算引擎并没有复用 Spark Streaming，而是在 Spark SQL 上设计了新的一套引擎。因此，从 Spark SQL 迁移到 Spark Structured Streaming 十分容易，但从 Spark Streaming 迁移过来就要困难得多。</p></blockquote><p>很自然的，基于这样的模型，Spark SQL 中的大部分接口、实现都得以在 Spark Structured Streaming 中直接复用。将用户的 SQL 执行计划转化成流计算执行计划的过程被称为<strong>增量化</strong>（incrementalize），这一步是由 Spark 框架自动完成的。对于用户来说只要知道：每次计算的输入是某一小段时间的流数据，而输出是对应数据产生的计算结果。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/e492c7df930e461ab18abe0be8f960c0.jpeg" alt="img"></p><p>▲ 左图是 Spark Structured Streaming 模型示意图；右图展示了同一个任务的批处理、流计算版本，可以看到，除了输入输出不同，内部计算过程完全相同。</p><p>与 Spark SQL 相比，流式 SQL 计算还有两个额外的特性，分别是窗口（window）和水位（watermark）。</p><p><strong>窗口（window）是对过去某段时间的定义。</strong>批处理中，查询通常是全量的（例如：总用户量是多少）；而流计算中，我们通常关心近期一段时间的数据（例如：最近24小时新增的用户量是多少）。用户通过选用合适的窗口来获得自己所需的计算结果，常见的窗口有滑动窗口（Sliding Window）、滚动窗口（Tumbling Window）等。</p><p><strong>水位（watermark）用来丢弃过早的数据。</strong>在流计算中，上游的输入事件可能存在不确定的延迟，而流计算系统的内存是有限的、只能保存有限的状态，一定时间之后必须丢弃历史数据。以双流 A JOIN B 为例，假设窗口为 1 小时，那么 A 中比当前时间减 1 小时更早的数据（行）会被丢弃；如果 B 中出现 1 小时前的事件，因为无法处理只能忽略。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/5f85be06a9664db1885b48902d3d27cf.jpeg" alt="img"></p><p>▲ 上图为水位的示意图，“迟到”太久的数据（行）由于已经低于当前水位无法处理，将被忽略。</p><p>水位和窗口的概念都是因时间而来。在其他流计算系统中，也存在相同或类似的概念。</p><blockquote><p>关于 SQL 的流计算模型，常常被拿来对比的还有另一个流计算框架 Apache Flink。与 Spark 相比，它们的实现思路有很大不同，但在模型上是很相似的。</p></blockquote><h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/3128a0aecac04e2db7d18fc9b05af266.jpeg" alt="img"></p><p><strong>驱动程序（Driver）</strong>即用户编写的程序，对应一个 SparkContext，负责任务的构造、调度、故障恢复等。驱动程序可以直接运行在客户端，例如用户的应用程序中；也可以托管在 Master 上，这被称为集群模式（cluster mode），通常用于流计算等长期任务。</p><p><strong>Cluster Manager</strong>顾名思义负责集群的资源分配，Spark 自带的 Spark Master 支持任务的资源分配，并包含一个 Web UI 用来监控任务运行状况。多个 Master 可以构成一主多备，通过 ZooKeeper 进行协调和故障恢复。通常 Spark 集群使用 Spark Master 即可，但如果用户的集群中不仅有 Spark 框架、还要承担其他任务，官方推荐使用 Mesos 作为集群调度器。</p><p><strong>Worker</strong>节点负责执行计算任务，上面保存了 RDD 等数据。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Spark 是一个同时支持批处理和流计算的分布式计算系统。Spark 的所有计算均构建于 RDD 之上，RDD 通过算子连接形成 DAG 的执行计划，RDD 的确定性及不可变性是 Spark 实现故障恢复的基础。Spark Streaming 的 D-Stream 本质上也是将输入数据分成一个个 micro-batch 的 RDD。</p><p>Spark SQL 是在 RDD 之上的一层封装，相比原始 RDD，DataFrame API 支持数据表的 schema 信息，从而可以执行 SQL 关系型查询，大幅降低了开发成本。Spark Structured Streaming 是 Spark SQL 的流计算版本，它将输入的数据流看作不断追加的数据行。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase学习之路 （十一）HBase的协过滤器</title>
      <link href="/2018-06-11-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89HBase%E7%9A%84%E5%8D%8F%E8%BF%87%E6%BB%A4%E5%99%A8.html"/>
      <url>/2018-06-11-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89HBase%E7%9A%84%E5%8D%8F%E8%BF%87%E6%BB%A4%E5%99%A8.html</url>
      
        <content type="html"><![CDATA[<p>** HBase学习之路 （十一）HBase的协过滤器：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        HBase学习之路 （十一）HBase的协过滤器</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="协处理器—Coprocessor"><a href="#协处理器—Coprocessor" class="headerlink" title="协处理器—Coprocessor"></a>协处理器—Coprocessor</h2><h3 id="1、-起源"><a href="#1、-起源" class="headerlink" title="1、 起源"></a>1、 起源</h3><p>　　Hbase 作为列族数据库最经常被人诟病的特性包括：无法轻易建立“二级索引”，难以执 行求和、计数、排序等操作。比如，在旧版本的(&lt;0.92)Hbase 中，统计数据表的总行数，需 要使用 Counter 方法，执行一次 MapReduce Job 才能得到。虽然 HBase 在数据存储层中集成 了 MapReduce，能够有效用于数据表的分布式计算。然而在很多情况下，做一些简单的相 加或者聚合计算的时候，<strong>如果直接将计算过程放置在 server 端，能够减少通讯开销，从而获 得很好的性能提升。</strong>于是，HBase 在 0.92 之后引入了协处理器(coprocessors)，实现一些激动 人心的新特性：能够轻易建立二次索引、复杂过滤器(谓词下推)以及访问控制等。</p><h2 id="2、介绍"><a href="#2、介绍" class="headerlink" title="2、介绍"></a>2、介绍</h2><p>　　协处理器有两种：<strong>observer 和 endpoint</strong></p><p>　　Observer 类似于传统数据库中的触发器，当发生某些事件的时候这类协处理器会被 Server 端调用。Observer Coprocessor 就是一些散布在 HBase Server 端代码中的 hook 钩子， 在固定的事件发生时被调用。比如：put 操作之前有钩子函数 prePut，该函数在 put 操作执 行前会被 Region Server 调用；在 put 操作之后则有 postPut 钩子函数</p><p>　　以 HBase0.92 版本为例，它提供了三种观察者接口：</p><blockquote><p><strong>RegionObserver</strong>：提供客户端的数据操纵事件钩子：Get、Put、Delete、Scan 等。</p><p><strong>WALObserver</strong>：提供 WAL 相关操作钩子。</p><p><strong>MasterObserver</strong>：提供 DDL-类型的操作钩子。如创建、删除、修改数据表等。</p><p>到 0.96 版本又新增一个 <strong>RegionServerObserver</strong></p></blockquote><p>　　下图是以 RegionObserver 为例子讲解 Observer 这种协处理器的原理：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403130003416-1058757311.png" alt="img"></p><p>　　1、客户端发出 put 请求</p><p>　　2、该请求被分派给合适的 RegionServer 和 region</p><p>　　3、coprocessorHost 拦截该请求，然后在该表上登记的每个 RegionObserver 上调用 prePut()</p><p>　　4、如果没有被 prePut()拦截，该请求继续送到 region，然后进行处理</p><p>　　5、region 产生的结果再次被 CoprocessorHost 拦截，调用 postPut()</p><p>　　6、假如没有 postPut()拦截该响应，最终结果被返回给客户端</p><p>　　Endpoint 协处理器类似传统数据库中的存储过程，客户端可以调用这些 Endpoint 协处 理器执行一段 Server 端代码，并将 Server 端代码的结果返回给客户端进一步处理，最常见 的用法就是进行聚集操作。如果没有协处理器，当用户需要找出一张表中的最大数据，即 max 聚合操作，就必须进行全表扫描，在客户端代码内遍历扫描结果，并执行求最大值的 操作。这样的方法无法利用底层集群的并发能力，而将所有计算都集中到 Client 端统一执行， 势必效率低下。利用 Coprocessor，用户可以将求最大值的代码部署到 HBase Server 端，HBase 将利用底层 cluster 的多个节点并发执行求最大值的操作。即在每个 Region 范围内执行求最 大值的代码，将每个 Region 的最大值在 Region Server 端计算出，仅仅将该 max 值返回给客 户端。在客户端进一步将多个 Region 的最大值进一步处理而找到其中的最大值。这样整体 的执行效率就会提高很多</p><p>　　下图是 EndPoint 的工作原理：</p><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180403130107289-1349588066.png" alt="img"></p><h3 id="3、总结"><a href="#3、总结" class="headerlink" title="3、总结"></a>3、总结</h3><p>　　Observer 允许集群在正常的客户端操作过程中可以有不同的行为表现</p><p>　　Endpoint 允许扩展集群的能力，对客户端应用开放新的运算命令</p><p>　　Observer 类似于 RDBMS 中的触发器，主要在服务端工作</p><p>　　Endpoint 类似于 RDBMS 中的存储过程，主要在服务端工作</p><p>　　Observer 可以实现权限管理、优先级设置、监控、ddl 控制、<strong>二级索引</strong>等功能</p><p>　　Endpoint 可以实现 <strong>min、max、avg、sum、distinct、group by</strong> 等功能</p><h2 id="协处理加载方式"><a href="#协处理加载方式" class="headerlink" title="协处理加载方式"></a>协处理加载方式</h2><p>　　协处理器的加载方式有两种，我们称之为<strong>静态加载方式（Static Load）和动态加载方式 （Dynamic Load）</strong>。静态加载的协处理器称之为 System Coprocessor，动态加载的协处理器称 之为 Table Coprocessor。</p><h3 id="1、-静态加载"><a href="#1、-静态加载" class="headerlink" title="1、 静态加载"></a>1、 静态加载</h3><p>通过修改 hbase-site.xml 这个文件来实现，启动全局 aggregation，能过操纵所有的表上 的数据。只需要添加如下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hbase.coprocessor.user.region.classes&lt;/name&gt;</span><br><span class="line">&lt;value&gt;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>为所有 table 加载了一个 cp class，可以用”,”分割加载多个 class</p><h3 id="2、-动态加载"><a href="#2、-动态加载" class="headerlink" title="2、 动态加载"></a>2、 动态加载</h3><p>启用表 aggregation，只对特定的表生效。通过 HBase Shell 来实现。</p><p>（1）停用表　　disable ‘guanzhu’</p><p>（2）添加协处理器　　alter ‘guanzhu’, METHOD =&gt; ‘table_att’, ‘coprocessor’ =&gt; ‘hdfs://myha01/hbase/guanzhu.jar|com.study.hbase.cp.HbaseCoprocessorTest|1001|’</p><p>（3）启用表　　enable ‘guanzhu’</p><h3 id="3、-协处理器卸载"><a href="#3、-协处理器卸载" class="headerlink" title="3、 协处理器卸载"></a>3、 协处理器卸载</h3><p>同样是3步</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">disable &apos;mytable&apos;</span><br><span class="line">alter &apos;mytable&apos;,METHOD=&gt;&apos;table_att_unset&apos;,NAME=&gt;&apos;coprocessor$1&apos;</span><br><span class="line">enable &apos;mytable&apos;</span><br></pre></td></tr></table></figure><h3 id="案例（二级索引）"><a href="#案例（二级索引）" class="headerlink" title="案例（二级索引）"></a>案例（二级索引）</h3><p>说明：二狗子是王宝强的粉丝</p><p>关注表：二狗子关注了王宝强　　rowKey=’ergouzi’　　cell=”star:wangbaoqiang”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">put &apos;guanzhu&apos;, &apos;ergouzi&apos;, &apos;cf:star&apos;, &apos;wangbaoqiang&apos;</span><br></pre></td></tr></table></figure><p>粉丝表：二狗子是王宝强的粉丝　　rowKey=”wangbaoqiang”　　cell=”fensi:ergouzi”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">put &apos;fans&apos;, &apos;wangbaoqiang&apos;, &apos;cf:fensi&apos;, &apos;ergouzi&apos;</span><br></pre></td></tr></table></figure><p>java实现代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">public class HbaseCoprocessorTest extends BaseRegionObserver&#123;</span><br><span class="line"></span><br><span class="line">    static Configuration conf = HBaseConfiguration.create();</span><br><span class="line">    static Connection conn = null;</span><br><span class="line">    static Table table = null;</span><br><span class="line">    </span><br><span class="line">    static &#123;</span><br><span class="line">        conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;);</span><br><span class="line">        try &#123;</span><br><span class="line">            conn = ConnectionFactory.createConnection(conf);</span><br><span class="line">            table = conn.getTable(TableName.valueOf(&quot;fans&quot;));</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    /**</span><br><span class="line">     * 此方法是在真正的put方法调用之前进行调用</span><br><span class="line">     * 参数put为table.put(put)里面的参数put对象，是要进行插入的那条数据</span><br><span class="line">     * </span><br><span class="line">     * 例如：要向关注表里面插入一条数据    姓名：二狗子    关注的明星：王宝强</span><br><span class="line">     * shell语句：put &apos;guanzhu&apos;,&apos;ergouzi&apos;, &apos;cf:star&apos;, &apos;wangbaoqiang&apos;</span><br><span class="line">     *</span><br><span class="line">     * */</span><br><span class="line">    @Override</span><br><span class="line">    public void prePut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability)</span><br><span class="line">            throws IOException &#123;</span><br><span class="line">        //获取put对象里面的rowkey&apos;ergouzi&apos;</span><br><span class="line">        byte[] row = put.getRow();</span><br><span class="line">        //获取put对象里面的cell</span><br><span class="line">        List&lt;Cell&gt; list = put.get(&quot;cf&quot;.getBytes(), &quot;star&quot;.getBytes());</span><br><span class="line">        Cell cell = list.get(0);</span><br><span class="line">        </span><br><span class="line">        //创建一个新的put对象</span><br><span class="line">        Put new_put = new Put(cell.getValueArray());</span><br><span class="line">        new_put.addColumn(&quot;cf&quot;.getBytes(), &quot;fensi&quot;.getBytes(), row);</span><br><span class="line">        table.put(new_put);</span><br><span class="line">        conn.close();</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>打成jar包，命名为guanzhu.jar，将其上传到HDFS目录/hbase下面</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -put guanzhu.jar /hbase</span><br></pre></td></tr></table></figure><p>打开hbase shell命令，按顺序呢执行（提前已经创建好guanzhu和fans表）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; disable &apos;guanzhu&apos;</span><br><span class="line">0 row(s) in 2.8850 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):002:0&gt; alter &apos;guanzhu&apos;, METHOD =&gt; &apos;table_att&apos;, &apos;coprocessor&apos; =&gt; &apos;hdfs://myha01/hbase/guanzhu.jar|com.study.hbase.cp.HbaseCoprocessorTest|1001|&apos;</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1/1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">0 row(s) in 2.7570 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):003:0&gt; enable &apos;guanzhu&apos;</span><br><span class="line">0 row(s) in 2.3400 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):004:0&gt; desc &apos;guanzhu&apos;</span><br><span class="line">Table guanzhu is ENABLED                                                                                        </span><br><span class="line">guanzhu, &#123;TABLE_ATTRIBUTES =&gt; &#123;coprocessor$1 =&gt; &apos;hdfs://myha01/hbase/guanzhu.jar|com.study.hbase.cp.HbaseCoproce</span><br><span class="line">ssorTest|1001|&apos;&#125;                                                                                                </span><br><span class="line">COLUMN FAMILIES DESCRIPTION                                                                                     </span><br><span class="line">&#123;NAME =&gt; &apos;cf&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DATA_</span><br><span class="line">BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BL</span><br><span class="line">OCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;                                                                   </span><br><span class="line">1 row(s) in 0.0500 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):005:0&gt; put &apos;guanzhu&apos;, &apos;ergouzi&apos;, &apos;cf:star&apos;, &apos;wangbaoqiang&apos;</span><br><span class="line">0 row(s) in 0.3050 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):006:0&gt; scan &apos;guanzhu&apos;</span><br><span class="line">ROW                           COLUMN+CELL                                                                       </span><br><span class="line"> ergouzi                      column=cf:star, timestamp=1522759023001, value=wangbaoqiang                       </span><br><span class="line">1 row(s) in 0.0790 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):007:0&gt; scan &apos;fans&apos;</span><br><span class="line">ROW                           COLUMN+CELL                                                                       </span><br><span class="line"> \x00\x00\x00\x19\x00\x00\x00 column=cf:fensi, timestamp=1522759022996, value=ergouzi                           </span><br><span class="line"> \x0C\x00\x07ergouzi\x02cfsta                                                                                   </span><br><span class="line"> r\x7F\xFF\xFF\xFF\xFF\xFF\xF                                                                                   </span><br><span class="line"> F\xFF\x04wangbaoqiang                                                                                          </span><br><span class="line">1 row(s) in 0.0330 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):008:0&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase学习之路 （十）HBase表的设计原则</title>
      <link href="/2018-06-10-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%EF%BC%89HBase%E8%A1%A8%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99.html"/>
      <url>/2018-06-10-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%EF%BC%89HBase%E8%A1%A8%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99.html</url>
      
        <content type="html"><![CDATA[<p>** HBase学习之路 （十）HBase表的设计原则：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        HBase学习之路 （十）HBase表的设计原则</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="建表高级属性"><a href="#建表高级属性" class="headerlink" title="建表高级属性"></a>建表高级属性</h2><p>　　下面几个 shell 命令在 hbase 操作中可以起到很大的作用，且主要体现在建表的过程中，看 下面几个 create 属性</p><h3 id="1、-BLOOMFILTER"><a href="#1、-BLOOMFILTER" class="headerlink" title="1、 BLOOMFILTER"></a>1、 BLOOMFILTER</h3><p>　　默认是 NONE 是否使用布隆过虑及使用何种方式，布隆过滤可以每列族单独启用 使用 HColumnDescriptor.setBloomFilterType(NONE | ROW | ROWCOL) 对列族单独启用布隆</p><p>　　Default = ROW 对行进行布隆过滤</p><p>　　对 ROW，行键的哈希在每次插入行时将被添加到布隆</p><p>　　对 ROWCOL，行键 + 列族 + 列族修饰的哈希将在每次插入行时添加到布隆</p><p>　　使用方法: create ‘table’,{BLOOMFILTER =&gt;’ROW’}</p><p>　　作用：用布隆过滤可以节省读磁盘过程，可以有助于降低读取延迟</p><h3 id="2、-VERSIONS"><a href="#2、-VERSIONS" class="headerlink" title="2、 VERSIONS"></a>2、 VERSIONS</h3><p>　　默认是 1 这个参数的意思是数据保留 1 个 版本，如果我们认为我们的数据没有这么大 的必要保留这么多，随时都在更新，而老版本的数据对我们毫无价值，那将此参数设为 1 能 节约 2/3 的空间</p><p>　　使用方法: create ‘table’,{VERSIONS=&gt;’2’}</p><p>　　附：MIN_VERSIONS =&gt; ‘0’是说在 compact 操作执行之后，至少要保留的版本</p><h3 id="3、-COMPRESSION"><a href="#3、-COMPRESSION" class="headerlink" title="3、 COMPRESSION"></a>3、 COMPRESSION</h3><p>　　默认值是 NONE 即不使用压缩，这个参数意思是该列族是否采用压缩，采用什么压缩算 法，方法: create ‘table’,{NAME=&gt;’info’,COMPRESSION=&gt;’SNAPPY’} ，建议采用 SNAPPY 压缩算 法 ，HBase 中，在 Snappy 发布之前（Google 2011 年对外发布 Snappy），采用的 LZO 算法，目标是达到尽可能快的压缩和解压速度，同时减少对 CPU 的消耗；</p><p>　　在 Snappy 发布之后，建议采用 Snappy 算法（参考《HBase: The Definitive Guide》），具体 可以根据实际情况对 LZO 和 Snappy 做过更详细的对比测试后再做选择。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180402203133302-774610222.png" alt="img"></p><p>　　如果建表之初没有压缩，后来想要加入压缩算法，可以通过 alter 修改 schema</p><h3 id="4、-TTL"><a href="#4、-TTL" class="headerlink" title="4、 TTL"></a>4、 TTL</h3><p>　　默认是 2147483647 即：Integer.MAX_VALUE 值大概是 68 年，这个参数是说明该列族数据的存活时间，单位是 s</p><p>　　这个参数可以根据具体的需求对数据设定存活时间，超过存过时间的数据将在表中不在 显示，待下次 major compact 的时候再彻底删除数据</p><p>　　注意的是 TTL 设定之后 MIN_VERSIONS=&gt;’0’ 这样设置之后，TTL 时间戳过期后，将全部 彻底删除该 family 下所有的数据，如果 MIN_VERSIONS 不等于 0 那将保留最新的 MIN_VERSIONS 个版本的数据，其它的全部删除，比如 MIN_VERSIONS=&gt;’1’ 届时将保留一个 最新版本的数据，其它版本的数据将不再保存。</p><h3 id="5、-alter"><a href="#5、-alter" class="headerlink" title="5、 alter"></a>5、 alter</h3><p>使用方法：</p><p>　　如 修改压缩算法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">disable &apos;table&apos;</span><br><span class="line"></span><br><span class="line">alter &apos;table&apos;,&#123;NAME=&gt;&apos;info&apos;,COMPRESSION=&gt;&apos;snappy&apos;&#125;</span><br><span class="line"></span><br><span class="line">enable &apos;table&apos;</span><br></pre></td></tr></table></figure><p>　　但是需要执行 major_compact ‘table’ 命令之后 才会做实际的操作。 </p><h3 id="6、-describe-desc"><a href="#6、-describe-desc" class="headerlink" title="6、 describe/desc"></a>6、 describe/desc</h3><p>　　这个命令查看了 create table 的各项参数或者是默认值。</p><p>　　使用方式：describe ‘user_info’</p><h3 id="7、-disable-all-enable-all"><a href="#7、-disable-all-enable-all" class="headerlink" title="7、 disable_all/enable_all"></a>7、 disable_all/enable_all</h3><p>　　disable_all ‘toplist.*’ disable_all 支持正则表达式，并列出当前匹配的表的如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">toplist_a_total_1001</span><br><span class="line">toplist_a_total_1002</span><br><span class="line">toplist_a_total_1008</span><br><span class="line">toplist_a_total_1009</span><br><span class="line">toplist_a_total_1019</span><br><span class="line">toplist_a_total_1035</span><br><span class="line">...</span><br><span class="line">Disable the above 25 tables (y/n)? 并给出确认提示</span><br></pre></td></tr></table></figure><h3 id="8、-drop-all"><a href="#8、-drop-all" class="headerlink" title="8、 drop_all"></a>8、 drop_all</h3><p>　　这个命令和 disable_all 的使用方式是一样的</p><h3 id="9、-hbase-预分区"><a href="#9、-hbase-预分区" class="headerlink" title="9、 hbase 预分区"></a>9、 hbase 预分区</h3><p>　　默认情况下，在创建 HBase 表的时候会自动创建一个 region 分区，当导入数据的时候， 所有的 HBase 客户端都向这一个 region 写数据，直到这个 region 足够大了才进行切分。一 种可以加快批量写入速度的方法是通过预先创建一些空的 regions，这样当数据写入 HBase 时，会按照 region 分区情况，在集群内做数据的负载均衡。</p><p>命令方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># create table with specific split points</span><br><span class="line">hbase&gt;create &apos;table1&apos;,&apos;f1&apos;,SPLITS =&gt; [&apos;\x10\x00&apos;, &apos;\x20\x00&apos;, &apos;\x30\x00&apos;, &apos;\x40\x00&apos;]</span><br><span class="line"># create table with four regions based on random bytes keys</span><br><span class="line">hbase&gt;create &apos;table2&apos;,&apos;f1&apos;, &#123; NUMREGIONS =&gt; 8 , SPLITALGO =&gt; &apos;UniformSplit&apos; &#125;</span><br><span class="line"># create table with five regions based on hex keys</span><br><span class="line">hbase&gt;create &apos;table3&apos;,&apos;f1&apos;, &#123; NUMREGIONS =&gt; 10, SPLITALGO =&gt; &apos;HexStringSplit&apos; &#125;</span><br></pre></td></tr></table></figure><p>　　也可以使用 api 的方式:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.util.RegionSplitter test_table HexStringSplit -c 10 -f info</span><br><span class="line"></span><br><span class="line">hbase org.apache.hadoop.hbase.util.RegionSplitter splitTable HexStringSplit -c 10 -f info</span><br></pre></td></tr></table></figure><p>参数：</p><p>　　test_table 是表名</p><p>　　HexStringSplit 是split 方式</p><p>　　-c 是分 10 个 region</p><p>　　-f 是 family</p><p>可在 UI 上查看结果，如图：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180402203720215-218633229.png" alt="img"></p><p>　　这样就可以将表预先分为 15 个区，减少数据达到 storefile 大小的时候自动分区的时间 消耗，并且还有以一个优势，就是合理设计 rowkey 能让各个 region 的并发请求平均分配(趋 于均匀) 使 IO 效率达到最高，但是预分区需要将 filesize 设置一个较大的值，设置哪个参数 呢 hbase.hregion.max.filesize 这个值默认是 10G 也就是说单个 region 默认大小是 10G</p><p>　　这个参数的默认值在 0.90 到 0.92 到 0.94.3 各版本的变化：256M–1G–10G</p><p>　　但是如果 MapReduce Input 类型为 TableInputFormat 使用 hbase 作为输入的时候，就要注意 了，每个 region 一个 map，如果数据小于 10G 那只会启用一个 map 造成很大的资源浪费， 这时候可以考虑适当调小该参数的值，或者采用预分配 region 的方式，并将检测如果达到 这个值，再手动分配 region。</p><h2 id="表设计"><a href="#表设计" class="headerlink" title="表设计"></a>表设计</h2><h3 id="1、列簇设计"><a href="#1、列簇设计" class="headerlink" title="1、列簇设计"></a>1、列簇设计</h3><p>　　追求的原则是：在合理范围内能尽量少的减少列簇就尽量减少列簇。</p><p>　　最优设计是：将所有相关性很强的 key-value 都放在同一个列簇下，这样既能做到查询效率 最高，也能保持尽可能少的访问不同的磁盘文件。</p><p>　　以用户信息为例，可以将必须的基本信息存放在一个列族，而一些附加的额外信息可以放在 另一列族。</p><h3 id="2、RowKey-设计"><a href="#2、RowKey-设计" class="headerlink" title="2、RowKey 设计"></a>2、RowKey 设计</h3><p>　　HBase 中，表会被划分为 1…n 个 Region，被托管在 RegionServer 中。Region 二个重要的 属性：StartKey 与 EndKey 表示这个 Region 维护的 rowKey 范围，当我们要读/写数据时，如 果 rowKey 落在某个 start-end key 范围内，那么就会定位到目标 region 并且读/写到相关的数 据</p><p>　　那怎么快速精准的定位到我们想要操作的数据，就在于我们的 rowkey 的设计了</p><h2 id="Rowkey-设计三原则"><a href="#Rowkey-设计三原则" class="headerlink" title="Rowkey 设计三原则"></a>Rowkey 设计三原则</h2><h3 id="1、-rowkey-长度原则"><a href="#1、-rowkey-长度原则" class="headerlink" title="1、 rowkey 长度原则"></a>1、 rowkey 长度原则</h3><p>　　Rowkey 是一个二进制码流，Rowkey 的长度被很多开发者建议说设计在 10~100 个字节，不 过建议是越短越好，不要超过 16 个字节。</p><p>　　原因如下：</p><p>　　　　1、数据的持久化文件 HFile 中是按照 KeyValue 存储的，如果 Rowkey 过长比如 100 个字 节，1000 万列数据光 Rowkey 就要占用 100*1000 万=10 亿个字节，将近 1G 数据，这会极大 影响 HFile 的存储效率；</p><p>　　　　2、MemStore 将缓存部分数据到内存，如果 Rowkey 字段过长内存的有效利用率会降低， 系统将无法缓存更多的数据，这会降低检索效率。因此 Rowkey 的字节长度越短越好。</p><p>　　　　3、目前操作系统是都是 64 位系统，内存 8 字节对齐。控制在 16 个字节，8 字节的整数 倍利用操作系统的最佳特性。</p><h3 id="2、rowkey-散列原则"><a href="#2、rowkey-散列原则" class="headerlink" title="2、rowkey 散列原则"></a>2、rowkey 散列原则</h3><p>　　如果 Rowkey 是按时间戳的方式递增，不要将时间放在二进制码的前面，建议将 Rowkey 的高位作为散列字段，由程序循环生成，低位放时间字段，这样将提高数据均衡分布在每个 Regionserver 实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息将产生所有 新数据都在一个 RegionServer 上堆积的热点现象，这样在做数据检索的时候负载将会集中 在个别 RegionServer，降低查询效率。</p><h3 id="3、-rowkey-唯一原则"><a href="#3、-rowkey-唯一原则" class="headerlink" title="3、 rowkey 唯一原则"></a>3、 rowkey 唯一原则</h3><p>　　必须在设计上保证其唯一性。rowkey 是按照字典顺序排序存储的，因此，设计 rowkey 的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问 的数据放到一块。</p><h3 id="数据热点"><a href="#数据热点" class="headerlink" title="数据热点"></a>数据热点</h3><p>　　HBase 中的行是按照 rowkey 的字典顺序排序的，这种设计优化了 scan 操作，可以将相 关的行以及会被一起读取的行存取在临近位置，便于 scan。然而糟糕的 rowkey 设计是热点 的源头。 热点发生在大量的 client 直接访问集群的一个或极少数个节点（访问可能是读， 写或者其他操作）。大量访问会使热点 region 所在的单个机器超出自身承受能力，引起性能 下降甚至 region 不可用，这也会影响同一个 RegionServer 上的其他 region，由于主机无法服 务其他 region 的请求。 设计良好的数据访问模式以使集群被充分，均衡的利用。 为了避免写热点，设计 rowkey 使得不同行在同一个 region，但是在更多数据情况下，数据 应该被写入集群的多个 region，而不是一个。</p><h3 id="防止数据热点的有效措施"><a href="#防止数据热点的有效措施" class="headerlink" title="防止数据热点的有效措施"></a>防止数据热点的有效措施</h3><h4 id="加盐"><a href="#加盐" class="headerlink" title="　　加盐"></a>　　加盐</h4><p>　　这里所说的加盐不是密码学中的加盐，而是在 rowkey 的前面增加随机数，具体就是给 rowkey 分配一个随机前缀以使得它和之前的 rowkey 的开头不同。分配的前缀种类数量应该 和你想使用数据分散到不同的 region 的数量一致。加盐之后的 rowkey 就会根据随机生成的 前缀分散到各个 region 上，以避免热点。</p><h4 id="哈希"><a href="#哈希" class="headerlink" title="　　哈希"></a>　　哈希</h4><p>　　哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是 可以预测的。使用确定的哈希可以让客户端重构完整的 rowkey，可以使用 get 操作准确获取 某一个行数据</p><h4 id="反转"><a href="#反转" class="headerlink" title="　　反转"></a>　　反转</h4><p>　　第三种防止热点的方法是反转固定长度或者数字格式的 rowkey。这样可以使得 rowkey 中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机 rowkey，但是牺 牲了 rowkey 的有序性。</p><p>　　反转 rowkey 的例子以手机号为 rowkey，可以将手机号反转后的字符串作为 rowkey，这 样的就避免了以手机号那样比较固定开头导致热点问题</p><h4 id="时间戳反转"><a href="#时间戳反转" class="headerlink" title="　　时间戳反转"></a>　　时间戳反转</h4><p>　　一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为 rowkey 的一部分对这个问题十分有用，可以用 Long.Max_Value - timestamp 追加到 key 的末尾，例 如 [key][reverse_timestamp] , [key] 的最新值可以通过 scan [key]获得[key]的第一条记录，因 为 HBase 中 rowkey 是有序的，第一条记录是最后录入的数据。比如需要保存一个用户的操 作记录，按照操作时间倒序排序，在设计 rowkey 的时候，可以这样设计 [userId 反转][Long.Max_Value - timestamp]，在查询用户的所有操作记录数据的时候，直接指 定 反 转 后 的 userId ， startRow 是 [userId 反 转 ][000000000000],stopRow 是 [userId 反 转][Long.Max_Value - timestamp]</p><p>　　如果需要查询某段时间的操作记录，startRow 是[user 反转][Long.Max_Value - 起始时间]， stopRow 是[userId 反转][Long.Max_Value - 结束时间]</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase学习之路 （九）HBase phoenix的使用</title>
      <link href="/2018-06-09-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B9%9D%EF%BC%89HBase%20phoenix%E7%9A%84%E4%BD%BF%E7%94%A8.html"/>
      <url>/2018-06-09-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B9%9D%EF%BC%89HBase%20phoenix%E7%9A%84%E4%BD%BF%E7%94%A8.html</url>
      
        <content type="html"><![CDATA[<p>** HBase学习之路 （九）HBase phoenix的使用：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        HBase学习之路 （九）HBase phoenix的使用</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="HBase-phoenix的下载"><a href="#HBase-phoenix的下载" class="headerlink" title="HBase phoenix的下载"></a>HBase phoenix的下载</h2><p>下载地址<a href="http://mirror.bit.edu.cn/apache/phoenix/" target="_blank" rel="noopener">http://mirror.bit.edu.cn/apache/phoenix/</a></p><p>选择对应的hbase版本进行下载，测试使用的是hbase-1.2.6版本</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180402201618877-316419058.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase学习之路 （七）HBase 原理</title>
      <link href="/2018-06-08-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%83%EF%BC%89HBase%20%E5%8E%9F%E7%90%86.html"/>
      <url>/2018-06-08-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%83%EF%BC%89HBase%20%E5%8E%9F%E7%90%86.html</url>
      
        <content type="html"><![CDATA[<p>** HBase学习之路 （七）HBase 原理：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        HBase学习之路 （七）HBase 原理</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><h4 id="错误图解"><a href="#错误图解" class="headerlink" title="错误图解"></a>错误图解</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180402125111282-1966599087.png" alt="img"></p><p>　　这张图是有一个<strong>错误点</strong>：应该是每一个 RegionServer 就只有一个 HLog，而不是一个 Region 有一个 HLog。</p><h4 id="正确图解"><a href="#正确图解" class="headerlink" title="正确图解"></a>正确图解</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180402125236874-140096659.png" alt="img"></p><p>　　从HBase的架构图上可以看出，HBase中的组件包括Client、Zookeeper、HMaster、HRegionServer、HRegion、Store、MemStore、StoreFile、HFile、HLog等，接下来介绍他们的作用。</p><h4 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h4><blockquote><p> 1、HBase 有两张特殊表：</p><p><strong>.META.</strong>：记录了用户所有表拆分出来的的 Region 映射信息，.META.可以有多个 Regoin</p><p><strong>-ROOT-</strong>：记录了.META.表的 Region 信息，-ROOT-只有一个 Region，无论如何不会分裂</p><p>2、Client 访问用户数据前需要首先访问 ZooKeeper，找到-ROOT-表的 Region 所在的位置，然 后访问-ROOT-表，接着访问.META.表，最后才能找到用户数据的位置去访问，中间需要多次 网络操作，不过 client 端会做 cache 缓存。</p></blockquote><h4 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h4><blockquote><p> 1、ZooKeeper 为 HBase 提供 Failover 机制，选举 Master，避免单点 Master 单点故障问题</p><p> 2、存储所有 Region 的寻址入口：-ROOT-表在哪台服务器上。-ROOT-这张表的位置信息</p><p> 3、实时监控 RegionServer 的状态，将 RegionServer 的上线和下线信息实时通知给 Master</p><p> 4、存储 HBase 的 Schema，包括有哪些 Table，每个 Table 有哪些 Column Family</p></blockquote><h4 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h4><blockquote><p>1、为 RegionServer 分配 Region</p><p>2、负责 RegionServer 的负载均衡</p><p>3、发现失效的 RegionServer 并重新分配其上的 Region</p><p>4、HDFS 上的垃圾文件（HBase）回收</p><p>5、处理 Schema 更新请求（表的创建，删除，修改，列簇的增加等等）</p></blockquote><h4 id="RegionServer"><a href="#RegionServer" class="headerlink" title="RegionServer"></a>RegionServer</h4><blockquote><p>1、RegionServer 维护 Master 分配给它的 Region，处理对这些 Region 的 IO 请求</p><p>2、RegionServer 负责 Split 在运行过程中变得过大的 Region，负责 Compact 操作</p><p>可以看到，client 访问 HBase 上数据的过程并不需要 master 参与（寻址访问 zookeeper 和 RegioneServer，数据读写访问 RegioneServer），Master 仅仅维护者 Table 和 Region 的元数据信息，负载很低。</p><p>.META. 存的是所有的 Region 的位置信息，那么 RegioneServer 当中 Region 在进行分裂之后 的新产生的 Region，是由 Master 来决定发到哪个 RegioneServer，这就意味着，只有 Master 知道 new Region 的位置信息，所以，由 Master 来管理.META.这个表当中的数据的 CRUD</p><p>所以结合以上两点表明，在没有 Region 分裂的情况，Master 宕机一段时间是可以忍受的。</p></blockquote><h4 id="HRegion"><a href="#HRegion" class="headerlink" title="HRegion"></a>HRegion</h4><blockquote><p>table在行的方向上分隔为多个Region。Region是HBase中分布式存储和负载均衡的最小单元，即不同的region可以分别在不同的Region Server上，但同一个Region是不会拆分到多个server上。<br>Region按大小分隔，每个表一般是只有一个region。随着数据不断插入表，region不断增大，当region的某个列族达到一个阈值时就会分成两个新的region。<br>每个region由以下信息标识：&lt; 表名,startRowkey,创建时间&gt;<br>由目录表(-ROOT-和.META.)记录该region的endRowkey</p></blockquote><h4 id="Store"><a href="#Store" class="headerlink" title="Store"></a>Store</h4><blockquote><p>每一个region由一个或多个store组成，至少是一个store，hbase会把一起访问的数据放在一个store里面，即为每个 ColumnFamily建一个store，如果有几个ColumnFamily，也就有几个Store。一个Store由一个memStore和0或者 多个StoreFile组成。 HBase以store的大小来判断是否需要切分region</p></blockquote><h4 id="MemStore"><a href="#MemStore" class="headerlink" title="MemStore"></a>MemStore</h4><blockquote><p>memStore 是放在内存里的。保存修改的数据即keyValues。当memStore的大小达到一个阀值（默认128MB）时，memStore会被flush到文 件，即生成一个快照。目前hbase 会有一个线程来负责memStore的flush操作。</p></blockquote><h4 id="StoreFile"><a href="#StoreFile" class="headerlink" title="StoreFile"></a>StoreFile</h4><blockquote><p>memStore内存中的数据写到文件后就是StoreFile，StoreFile底层是以HFile的格式保存。</p></blockquote><h4 id="HFile"><a href="#HFile" class="headerlink" title="HFile"></a>HFile</h4><blockquote><p> HBase中KeyValue数据的存储格式，HFile是Hadoop的 二进制格式文件，实际上StoreFile就是对Hfile做了轻量级包装，即StoreFile底层就是HFile</p></blockquote><h4 id="HLog"><a href="#HLog" class="headerlink" title="HLog"></a>HLog</h4><blockquote><p>HLog(WAL log)：WAL意为write ahead log，用来做灾难恢复使用，HLog记录数据的所有变更，一旦region server 宕机，就可以从log中进行恢复。<br>HLog文件就是一个普通的Hadoop Sequence File， Sequence File的value是key时HLogKey对象，其中记录了写入数据的归属信息，除了table和region名字外，还同时包括sequence number和timestamp，timestamp是写入时间，sequence number的起始值为0，或者是最近一次存入文件系统中的sequence number。 Sequence File的value是HBase的KeyValue对象，即对应HFile中的KeyValue。</p></blockquote><h2 id="物理存储"><a href="#物理存储" class="headerlink" title="物理存储"></a>物理存储</h2><h3 id="整体的物理结构"><a href="#整体的物理结构" class="headerlink" title="整体的物理结构"></a>整体的物理结构</h3><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180402130346713-706113248.png" alt="img"></p><p>　　1、Table 中的所有行都按照 RowKsey 的字典序排列。</p><p>　　2、Table 在行的方向上分割为多个 HRegion。</p><p>　　3、HRegion 按大小分割的(默认 10G)，每个表一开始只有一个 HRegion，随着数据不断插入 表，HRegion 不断增大，当增大到一个阀值的时候，HRegion 就会等分会两个新的 HRegion。 当表中的行不断增多，就会有越来越多的 HRegion。</p><p>　　4、HRegion 是 Hbase 中分布式存储和负载均衡的最小单元。最小单元就表示不同的 HRegion 可以分布在不同的 HRegionserver 上。但一个 HRegion 是不会拆分到多个 server 上的。</p><p>　　5、HRegion 虽然是负载均衡的最小单元，但并不是物理存储的最小单元。事实上，HRegion 由一个或者多个 Store 组成，每个 Store 保存一个 Column Family。每个 Strore 又由一个 memStore 和 0 至多个 StoreFile 组成</p><h3 id="StoreFile-和-HFile-结构"><a href="#StoreFile-和-HFile-结构" class="headerlink" title="StoreFile 和 HFile 结构"></a>StoreFile 和 HFile 结构</h3><p>　　StoreFile 以 HFile 格式保存在 HDFS 上，请看下图 <strong>HFile</strong> 的数据组织格式：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180402130705469-345782346.png" alt="img"></p><p>　　首先 HFile 文件是不定长的，长度固定的只有其中的两块：Trailer 和 FileInfo。</p><p>正如图中所示：</p><p>　　Trailer 中有指针指向其他数据块的起始点。</p><p>　　FileInfo 中记录了文件的一些 Meta 信息，例如：AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY 等。</p><p>HFile 分为六个部分：</p><p>　　<strong>Data Block</strong> 段–保存表中的数据，这部分可以被压缩</p><p>　　<strong>Meta Block</strong> 段 (可选的)–保存用户自定义的 kv 对，可以被压缩。</p><p>　　<strong>File Info</strong> 段–Hfile 的元信息，不被压缩，用户也可以在这一部分添加自己的元信息。</p><p>　　<strong>Data Block Index</strong> 段–Data Block 的索引。每条索引的 key 是被索引的 block 的第一条记录的 key。</p><p>　　<strong>Meta Block Index</strong> 段 (可选的)–Meta Block 的索引。</p><p>　　<strong>Trailer</strong> 段–这一段是定长的。保存了每一段的偏移量，读取一个 HFile 时，会首先读取 Trailer， Trailer保存了每个段的起始位置(段的Magic Number用来做安全check)，然后，DataBlock Index 会被读取到内存中，这样，当检索某个 key 时，不需要扫描整个 HFile，而只需从内存中找 到key所在的block，通过一次磁盘io将整个block读取到内存中，再找到需要的key。DataBlock Index 采用 LRU 机制淘汰。</p><p>　　HFile 的 Data Block，Meta Block 通常采用压缩方式存储，压缩之后可以大大减少网络 IO 和磁 盘 IO，随之而来的开销当然是需要花费 cpu 进行压缩和解压缩。</p><p>目标 Hfile 的压缩支持两种方式：Gzip，LZO。</p><p>　　Data Index 和 Meta Index 块记录了每个 Data 块和 Meta 块的起始点。</p><p>　　Data Block 是 HBase I/O 的基本单元，为了提高效率，HRegionServer 中有基于 LRU 的 Block Cache 机制。每个 Data 块的大小可以在创建一个 Table 的时候通过参数指定，大号的 Block 有利于顺序 Scan，小号 Block 利于随机查询。 每个 Data 块除了开头的 Magic 以外就是一个 个 KeyValue 对拼接而成, Magic 内容就是一些随机数字，目的是防止数据损坏。</p><p>　　HFile 里面的每个 <strong>KeyValue</strong> 对就是一个简单的 byte 数组。但是这个 byte 数组里面包含了很 多项，并且有固定的结构。我们来看看里面的具体结构： </p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180402131032937-1735316959.png" alt="img"></p><p>　　开始是两个固定长度的数值，分别表示 Key 的长度和 Value 的长度。紧接着是 Key，开始是 固定长度的数值，表示 RowKey 的长度，紧接着是 RowKey，然后是固定长度的数值，表示 Family 的长度，然后是 Family，接着是 Qualifier，然后是两个固定长度的数值，表示 Time Stamp 和 Key Type（Put/Delete）。Value 部分没有这么复杂的结构，就是纯粹的二进制数据了。</p><h3 id="MemStore-和-StoreFile"><a href="#MemStore-和-StoreFile" class="headerlink" title="MemStore 和 StoreFile"></a>MemStore 和 StoreFile</h3><p>　　一个 Hregion 由多个 Store 组成，每个 Store 包含一个列族的所有数据。</p><p>　　Store 包括位于内存的一个 memstore 和位于硬盘的多个 storefile 组成。</p><p>　　写操作先写入 memstore，当 memstore 中的数据量达到某个阈值，HRegionServer 启动 flushcache 进程写入 storefile，每次写入形成单独一个 Hfile。</p><p>　　当总 storefile 大小超过一定阈值后，会把当前的 region 分割成两个，并由 HMaster 分配给相 应的 region 服务器，实现负载均衡。</p><p>　　客户端检索数据时，先在 memstore 找，找不到再找 storefile。</p><h2 id="Hbase-WAL-HLog预写"><a href="#Hbase-WAL-HLog预写" class="headerlink" title="Hbase WAL HLog预写"></a>Hbase WAL HLog预写</h2><p>　　WAL 意为 Write ahead log(<a href="http://en.wikipedia.org/wiki/Write-ahead_logging)，类似" target="_blank" rel="noopener">http://en.wikipedia.org/wiki/Write-ahead_logging)，类似</a> mysql 中的 binlog，用来做灾难恢复之用，Hlog 记录数据的所有变更，一旦数据修改，就可以从 log 中 进行恢复。</p><p>　　每个 Region Server 维护一个 Hlog,而不是每个 Region 一个。这样不同 region(来自不同 table) 的日志会混在一起，这样做的目的是不断追加单个文件相对于同时写多个文件而言，可以减 少磁盘寻址次数，因此可以提高对 table 的写性能。带来的麻烦是，如果一台 region server 下线，为了恢复其上的 region，需要将 region server 上的 log 进行拆分，然后分发到其它 region server 上进行恢复。</p><p>　　HLog 文件就是一个普通的 Hadoop Sequence File（序列化文件）：</p><p>　　1、HLog Sequence File 的 Key 是 HLogKey 对象，HLogKey 中记录了写入数据的归属信息，除 了 table 和 region 名字外，同时还包括 sequence number 和 timestamp，timestamp 是”写入 时间”，sequence number 的起始值为 0，或者是最近一次存入文件系统中 sequence number。</p><p>　　2、HLog Sequece File 的 Value 是 HBase 的 KeyValue 对象，即对应 HFile 中的 KeyValue。</p><h2 id="Region-寻址机制"><a href="#Region-寻址机制" class="headerlink" title="Region 寻址机制"></a>Region 寻址机制</h2><p>　　既然读写都在 RegionServer 上发生，我们前面有讲到，每个 RegionSever 为一定数量的 Region 服务，那么 Client 要对某一行数据做读写的时候如何能知道具体要去访问哪个 RegionServer 呢？那就是接下来我们要讨论的问题</p><h3 id="老的-Region-寻址方式"><a href="#老的-Region-寻址方式" class="headerlink" title="老的 Region 寻址方式"></a>老的 Region 寻址方式</h3><p>　　在 HBase-0.96 版本以前，HBase 有两个特殊的表，分别是-ROOT-表和.META.表，其中-ROOT的位置存储在 ZooKeeper 中，-ROOT-本身存储了.META. Table 的 RegionInfo 信息，并且-ROOT不会分裂，只有一个 Region。而.META.表可以被切分成多个 Region。读取的流程如下图所示：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180402182523717-793135327.png" alt="img"></p><p>详细步骤：</p><blockquote><p>第 1 步：Client 请求 ZooKeeper 获得-ROOT-所在的 RegionServer 地址</p><p>第 2 步：Client 请求-ROOT-所在的 RS 地址，获取.META.表的地址，Client 会将-ROOT-的相关 信息 cache 下来，以便下一次快速访问</p><p>第 3 步：Client 请求.META.表的 RegionServer 地址，获取访问数据所在 RegionServer 的地址， Client 会将.META.的相关信息 cache 下来，以便下一次快速访问</p><p>第 4 步：Client 请求访问数据所在 RegionServer 的地址，获取对应的数据</p></blockquote><p>　　从上面的路径我们可以看出，用户需要 3 次请求才能直到用户 Table 真正的位置，这在一定 程序带来了性能的下降。在 0.96 之前使用 3 层设计的主要原因是考虑到元数据可能需要很 大。但是真正集群运行，元数据的大小其实很容易计算出来。在 BigTable 的论文中，每行 METADATA 数据存储大小为 1KB 左右，如果按照一个 Region 为 128M 的计算，3 层设计可以支持的 Region 个数为 2^34 个，采用 2 层设计可以支持 2^17（131072）。那么 2 层设计的情 况下一个集群可以存储 4P 的数据。这仅仅是一个 Region 只有 128M 的情况下。如果是 10G 呢? 因此，通过计算，其实 2 层设计就可以满足集群的需求。因此在 0.96 版本以后就去掉 了-ROOT-表了。</p><h3 id="新的-Region-寻址方式"><a href="#新的-Region-寻址方式" class="headerlink" title="新的 Region 寻址方式"></a>新的 Region 寻址方式</h3><p>　　如上面的计算，2 层结构其实完全能满足业务的需求，因此 0.96 版本以后将-ROOT-表去掉了。 如下图所示：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180402182723566-1452396869.png" alt="img"></p><p>访问路径变成了 3 步：</p><blockquote><p>第 1 步：Client 请求 ZooKeeper 获取.META.所在的 RegionServer 的地址。</p><p>第 2 步：Client 请求.META.所在的 RegionServer 获取访问数据所在的 RegionServer 地址，Client 会将.META.的相关信息 cache 下来，以便下一次快速访问。</p><p>第 3 步：Client 请求数据所在的 RegionServer，获取所需要的数据。</p></blockquote><p><strong>总结去掉-ROOT-的原因有如下 2 点：</strong></p><p>　　<strong>其一：提高性能</strong></p><p>　　<strong>其二：2 层结构已经足以满足集群的需求</strong></p><p>　　这里还有一个问题需要说明，那就是 Client 会缓存.META.的数据，用来加快访问，既然有缓 存，那它什么时候更新？如果.META.更新了，比如 Region1 不在 RerverServer2 上了，被转移 到了 RerverServer3 上。Client 的缓存没有更新会有什么情况？</p><p>　　其实，Client 的元数据缓存不更新，当.META.的数据发生更新。如上面的例子，由于 Region1 的位置发生了变化，Client 再次根据缓存去访问的时候，会出现错误，当出现异常达到重试 次数后就会去.META.所在的 RegionServer 获取最新的数据，如果.META.所在的 RegionServer 也变了，Client 就会去 ZooKeeper 上获取.META.所在的 RegionServer 的最新地址。</p><h2 id="读写过程"><a href="#读写过程" class="headerlink" title="读写过程"></a>读写过程</h2><h3 id="读请求过程"><a href="#读请求过程" class="headerlink" title="读请求过程"></a>读请求过程</h3><blockquote><p>1、客户端通过 ZooKeeper 以及-ROOT-表和.META.表找到目标数据所在的 RegionServer(就是 数据所在的 Region 的主机地址)</p><p>2、联系 RegionServer 查询目标数据</p><p>3、RegionServer 定位到目标数据所在的 Region，发出查询请求</p><p>4、Region 先在 Memstore 中查找，命中则返回</p><p>5、如果在 Memstore 中找不到，则在 Storefile 中扫描 为了能快速的判断要查询的数据在不在这个 StoreFile 中，应用了 BloomFilter</p></blockquote><p><strong>（BloomFilter，布隆过滤器：迅速判断一个元素是不是在一个庞大的集合内，但是他有一个 弱点：它有一定的误判率）</strong></p><p><strong>（误判率：原本不存在与该集合的元素，布隆过滤器有可能会判断说它存在，但是，如果 布隆过滤器，判断说某一个元素不存在该集合，那么该元素就一定不在该集合内）</strong></p><h3 id="写请求过程"><a href="#写请求过程" class="headerlink" title="写请求过程"></a>写请求过程</h3><blockquote><p>1、Client 先根据 RowKey 找到对应的 Region 所在的 RegionServer</p><p>2、Client 向 RegionServer 提交写请求</p><p>3、RegionServer 找到目标 Region</p><p>4、Region 检查数据是否与 Schema 一致</p><p>5、如果客户端没有指定版本，则获取当前系统时间作为数据版本</p><p>6、将更新写入 WAL Log</p><p>7、将更新写入 Memstore</p><p>8、判断 Memstore 的是否需要 flush 为 StoreFile 文件。</p></blockquote><p> 　　Hbase 在做数据插入操作时，首先要找到 RowKey 所对应的的 Region，怎么找到的？其实这 个简单，因为.META.表存储了每张表每个 Region 的起始 RowKey 了。</p><p>　　 <strong>建议：在做海量数据的插入操作，避免出现递增 rowkey 的 put 操作</strong></p><p> 　　如果 put 操作的所有 RowKey 都是递增的，那么试想，当插入一部分数据的时候刚好进行分 裂，那么之后的所有数据都开始往分裂后的第二个 Region 插入，就造成了数据热点现象。</p><p> 　　细节描述：</p><p>　　　　 HBase 使用 MemStore 和 StoreFile 存储对表的更新。</p><p>　　数据在更新时首先写入 HLog(WAL Log)，再写入内存(MemStore)中，MemStore 中的数据是排 序的，当 MemStore 累计到一定阈值时，就会创建一个新的 MemStore，并且将老的 MemStore 添加到 flush 队列，由单独的线程 flush 到磁盘上，成为一个 StoreFile。于此同时，系统会在 ZooKeeper 中记录一个 redo point，表示这个时刻之前的变更已经持久化了。当系统出现意外时，可能导致内存(MemStore)中的数据丢失，此时使用 HLog(WAL Log)来恢复 checkpoint 之后的数据。</p><p>　　StoreFile 是只读的，一旦创建后就不可以再修改。因此 <strong>HBase 的更新/修改其实是不断追加 的操作</strong>。当一个 Store 中的 StoreFile 达到一定的阈值后，就会进行一次合并(minor_compact, major_compact)，将对同一个 key 的修改合并到一起，形成一个大的 StoreFile，当 StoreFile 的大小达到一定阈值后，又会对 StoreFile 进行 split，等分为两个 StoreFile。由于对表的更 新是不断追加的，compact 时，需要访问 Store 中全部的 StoreFile 和 MemStore，将他们按 rowkey 进行合并，由于 StoreFile 和 MemStore 都是经过排序的，并且 StoreFile 带有内存中 索引，合并的过程还是比较快。</p><p>　　<strong>major_compact 和 minor_compact 的区别：</strong></p><p>　　　　<strong>minor_compact 仅仅合并小文件（HFile）</strong></p><p>　　　　<strong>major_compact 合并一个 region 内的所有文件</strong></p><p>　　Client 写入 -&gt; 存入 MemStore，一直到 MemStore 满 -&gt; Flush 成一个 StoreFile，直至增长到 一定阈值 -&gt; 触发 Compact 合并操作 -&gt; 多个 StoreFile 合并成一个 StoreFile，同时进行版本 合并和数据删除 -&gt; 当 StoreFiles Compact 后，逐步形成越来越大的 StoreFile -&gt; 单个 StoreFile 大小超过一定阈值后，触发 Split 操作，把当前 Region Split 成 2 个 Region，Region 会下线， 新 Split 出的 2 个孩子 Region 会被 HMaster 分配到相应的 HRegionServer 上，使得原先 1 个 Region 的压力得以分流到 2 个 Region 上由此过程可知，<strong>HBase 只是增加数据，有所得更新 和删除操作，都是在 Compact 阶段做的，所以，用户写操作只需要进入到内存即可立即返 回，从而保证 I/O 高性能。</strong></p><p>　　写入数据的过程补充：</p><p>　　工作机制：每个 HRegionServer 中都会有一个 HLog 对象，HLog 是一个实现 Write Ahead Log 的类，每次用户操作写入 Memstore 的同时，也会写一份数据到 HLog 文件，HLog 文件定期 会滚动出新，并删除旧的文件(已持久化到 StoreFile 中的数据)。当 HRegionServer 意外终止 后，HMaster 会通过 ZooKeeper 感知，HMaster 首先处理遗留的 HLog 文件，将不同 Region 的 log数据拆分，分别放到相应 Region 目录下，然后再将失效的 Region（带有刚刚拆分的 log） 重新分配，领取到这些 Region 的 HRegionServer 在 load Region 的过程中，会发现有历史 HLog 需要处理，因此会 Replay HLog 中的数据到 MemStore 中，然后 flush 到 StoreFiles，完成数据 恢复。</p><h2 id="RegionServer-工作机制"><a href="#RegionServer-工作机制" class="headerlink" title="RegionServer 工作机制"></a>RegionServer 工作机制</h2><h3 id="Region-分配"><a href="#Region-分配" class="headerlink" title="Region 分配"></a>Region 分配</h3><p>　　任何时刻，一个 Region 只能分配给一个 RegionServer。master 记录了当前有哪些可用的 RegionServer。以及当前哪些 Region 分配给了哪些 RegionServer，哪些 Region 还没有分配。 当需要分配的新的 Region，并且有一个 RegionServer 上有可用空间时，Master 就给这个 RegionServer 发送一个装载请求，把 Region 分配给这个 RegionServer。RegionServer 得到请 求后，就开始对此 Region 提供服务。</p><h3 id="RegionServer-上线"><a href="#RegionServer-上线" class="headerlink" title="RegionServer 上线"></a>RegionServer 上线</h3><p>　　Master 使用 zookeeper 来跟踪 RegionServer 状态。当某个 RegionServer 启动时，会首先在 ZooKeeper 上的 server 目录下建立代表自己的 znode。由于 Master 订阅了 server 目录上的变 更消息，当 server 目录下的文件出现新增或删除操作时，Master 可以得到来自 ZooKeeper 的实时通知。因此一旦 RegionServer 上线，Master 能马上得到消息。</p><h3 id="RegionServer-下线"><a href="#RegionServer-下线" class="headerlink" title="RegionServer 下线"></a>RegionServer 下线</h3><p>　　当 RegionServer 下线时，它和 zookeeper 的会话断开，ZooKeeper 而自动释放代表这台 server 的文件上的独占锁。Master 就可以确定：</p><p>　　1、RegionServer 和 ZooKeeper 之间的网络断开了。</p><p>　　2、RegionServer 挂了。</p><p>　　无论哪种情况，RegionServer都无法继续为它的Region提供服务了，此时Master会删除server 目录下代表这台 RegionServer 的 znode 数据，并将这台 RegionServer 的 Region 分配给其它还 活着的同志。</p><h2 id="Master-工作机制"><a href="#Master-工作机制" class="headerlink" title="Master 工作机制"></a>Master 工作机制</h2><h3 id="Master-上线"><a href="#Master-上线" class="headerlink" title="Master 上线"></a>Master 上线</h3><p>　　Master 启动进行以下步骤:</p><p>　　　　1、从 ZooKeeper 上获取唯一一个代表 Active Master 的锁，用来阻止其它 Master 成为 Master。</p><p>　　　　2、扫描 ZooKeeper 上的 server 父节点，获得当前可用的 RegionServer 列表。</p><p>　　　　3、和每个 RegionServer 通信，获得当前已分配的 Region 和 RegionServer 的对应关系。</p><p>　　　　4、扫描.META. Region 的集合，计算得到当前还未分配的 Region，将他们放入待分配 Region 列表。</p><h3 id="Master-下线"><a href="#Master-下线" class="headerlink" title="Master 下线"></a>Master 下线</h3><p>　　由于 Master 只维护表和 Region 的元数据，而不参与表数据 IO 的过程，Master 下线仅 导致所有元数据的修改被冻结(无法创建删除表，无法修改表的 schema，无法进行 Region 的负载均衡，无法处理 Region 上下线，无法进行 Region 的合并，唯一例外的是 Region 的 split 可以正常进行，因为只有 RegionServer 参与)，表的数据读写还可以正常进行。因此 Master 下线短时间内对整个 hbase 集群没有影响。</p><p>　　从上线过程可以看到，Master 保存的信息全是可以冗余信息（都可以从系统其它地方 收集到或者计算出来）</p><p>　　因此，一般 HBase 集群中总是有一个 Master 在提供服务，还有一个以上的 Master 在等 待时机抢占它的位置。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase学习之路 （六）过滤器</title>
      <link href="/2018-06-07-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%85%AD%EF%BC%89%E8%BF%87%E6%BB%A4%E5%99%A8.html"/>
      <url>/2018-06-07-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%85%AD%EF%BC%89%E8%BF%87%E6%BB%A4%E5%99%A8.html</url>
      
        <content type="html"><![CDATA[<p>** HBase学习之路 （六）过滤器：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        HBase学习之路 （六）过滤器</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="过滤器（Filter）"><a href="#过滤器（Filter）" class="headerlink" title="过滤器（Filter）"></a>过滤器（Filter）</h2><p>　　基础API中的查询操作在面对大量数据的时候是非常苍白的，这里Hbase提供了高级的查询方法：Filter。Filter可以根据簇、列、版本等更多的条件来对数据进行过滤，基于Hbase本身提供的三维有序（主键有序、列有序、版本有序），这些Filter可以高效的完成查询过滤的任务。带有Filter条件的RPC查询请求会把Filter分发到各个RegionServer，是一个服务器端（Server-side）的过滤器，这样也可以降低网络传输的压力。</p><p>　　要完成一个过滤的操作，至少需要两个参数。<strong>一个是抽象的操作符</strong>，Hbase提供了枚举类型的变量来表示这些抽象的操作符：LESS/LESS_OR_EQUAL/EQUAL/NOT_EUQAL等；<strong>另外一个就是具体的比较器（Comparator）</strong>，代表具体的比较逻辑，如果可以提高字节级的比较、字符串级的比较等。有了这两个参数，我们就可以清晰的定义筛选的条件，过滤数据。</p><p><strong>抽象操作符（比较运算符）</strong></p><blockquote><p>LESS &lt;</p><p>LESS_OR_EQUAL &lt;=</p><p>EQUAL =</p><p>NOT_EQUAL &lt;&gt;</p><p>GREATER_OR_EQUAL &gt;=</p><p>GREATER &gt;</p><p>NO_OP 排除所有</p></blockquote><p><strong>比较器（指定比较机制）</strong></p><blockquote><p>BinaryComparator 按字节索引顺序比较指定字节数组，采用 Bytes.compareTo(byte[])</p><p>BinaryPrefixComparator 跟前面相同，只是比较左端的数据是否相同</p><p>NullComparator 判断给定的是否为空</p><p>BitComparator 按位比较</p><p>RegexStringComparator 提供一个正则的比较器，仅支持 EQUAL 和非 EQUAL</p><p>SubstringComparator 判断提供的子串是否出现在 value 中</p></blockquote><h2 id="HBase过滤器的分类"><a href="#HBase过滤器的分类" class="headerlink" title="HBase过滤器的分类"></a>HBase过滤器的分类</h2><h3 id="比较过滤器"><a href="#比较过滤器" class="headerlink" title="比较过滤器"></a>比较过滤器</h3><h4 id="1、行键过滤器-RowFilter"><a href="#1、行键过滤器-RowFilter" class="headerlink" title="1、行键过滤器 RowFilter"></a>1、行键过滤器 RowFilter</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Filter rowFilter = new RowFilter(CompareOp.GREATER, new BinaryComparator(&quot;95007&quot;.getBytes()));</span><br><span class="line">scan.setFilter(rowFilter);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class HbaseFilterTest &#123;</span><br><span class="line"> 2 </span><br><span class="line"> 3     private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;;</span><br><span class="line"> 4     private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;;</span><br><span class="line"> 5 </span><br><span class="line"> 6     private static Connection conn = null;</span><br><span class="line"> 7     private static Admin admin = null;</span><br><span class="line"> 8     </span><br><span class="line"> 9     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">10         </span><br><span class="line">11         Configuration conf = HBaseConfiguration.create();</span><br><span class="line">12         conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);</span><br><span class="line">13         conn = ConnectionFactory.createConnection(conf);</span><br><span class="line">14         admin = conn.getAdmin();</span><br><span class="line">15         Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));</span><br><span class="line">16         </span><br><span class="line">17         Scan scan = new Scan();</span><br><span class="line">18         </span><br><span class="line">19         Filter rowFilter = new RowFilter(CompareOp.GREATER, new BinaryComparator(&quot;95007&quot;.getBytes()));</span><br><span class="line">20         scan.setFilter(rowFilter);</span><br><span class="line">21         ResultScanner resultScanner = table.getScanner(scan);</span><br><span class="line">22         for(Result result : resultScanner) &#123;</span><br><span class="line">23             List&lt;Cell&gt; cells = result.listCells();</span><br><span class="line">24             for(Cell cell : cells) &#123;</span><br><span class="line">25                 System.out.println(cell);</span><br><span class="line">26             &#125;</span><br><span class="line">27         &#125;</span><br><span class="line">28         </span><br><span class="line">29         </span><br><span class="line">30     &#125;</span><br></pre></td></tr></table></figure><p>运行结果部分截图</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180331142700638-33411889.png" alt="img"></p><h4 id="2、列簇过滤器-FamilyFilter"><a href="#2、列簇过滤器-FamilyFilter" class="headerlink" title="2、列簇过滤器 FamilyFilter"></a>2、列簇过滤器 FamilyFilter</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Filter familyFilter = new FamilyFilter(CompareOp.EQUAL, new BinaryComparator(&quot;info&quot;.getBytes()));</span><br><span class="line">scan.setFilter(familyFilter);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class HbaseFilterTest &#123;</span><br><span class="line"> 2 </span><br><span class="line"> 3     private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;;</span><br><span class="line"> 4     private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;;</span><br><span class="line"> 5 </span><br><span class="line"> 6     private static Connection conn = null;</span><br><span class="line"> 7     private static Admin admin = null;</span><br><span class="line"> 8     </span><br><span class="line"> 9     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">10         </span><br><span class="line">11         Configuration conf = HBaseConfiguration.create();</span><br><span class="line">12         conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);</span><br><span class="line">13         conn = ConnectionFactory.createConnection(conf);</span><br><span class="line">14         admin = conn.getAdmin();</span><br><span class="line">15         Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));</span><br><span class="line">16         </span><br><span class="line">17         Scan scan = new Scan();</span><br><span class="line">18 </span><br><span class="line">19         Filter familyFilter = new FamilyFilter(CompareOp.EQUAL, new BinaryComparator(&quot;info&quot;.getBytes()));</span><br><span class="line">20         scan.setFilter(familyFilter);</span><br><span class="line">21         ResultScanner resultScanner = table.getScanner(scan);</span><br><span class="line">22         for(Result result : resultScanner) &#123;</span><br><span class="line">23             List&lt;Cell&gt; cells = result.listCells();</span><br><span class="line">24             for(Cell cell : cells) &#123;</span><br><span class="line">25                 System.out.println(cell);</span><br><span class="line">26             &#125;</span><br><span class="line">27         &#125;</span><br><span class="line">28         </span><br><span class="line">29         </span><br><span class="line">30     &#125;</span><br><span class="line">31     </span><br><span class="line">32     </span><br><span class="line">33 &#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180331142550951-1320138981.png" alt="img"></p><h4 id="3、列过滤器-QualifierFilter"><a href="#3、列过滤器-QualifierFilter" class="headerlink" title="3、列过滤器 QualifierFilter"></a>3、列过滤器 QualifierFilter</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Filter qualifierFilter = new QualifierFilter(CompareOp.EQUAL, new BinaryComparator(&quot;name&quot;.getBytes()));</span><br><span class="line">scan.setFilter(qualifierFilter);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class HbaseFilterTest &#123;</span><br><span class="line"> 2 </span><br><span class="line"> 3     private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;;</span><br><span class="line"> 4     private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;;</span><br><span class="line"> 5 </span><br><span class="line"> 6     private static Connection conn = null;</span><br><span class="line"> 7     private static Admin admin = null;</span><br><span class="line"> 8     </span><br><span class="line"> 9     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">10         </span><br><span class="line">11         Configuration conf = HBaseConfiguration.create();</span><br><span class="line">12         conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);</span><br><span class="line">13         conn = ConnectionFactory.createConnection(conf);</span><br><span class="line">14         admin = conn.getAdmin();</span><br><span class="line">15         Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));</span><br><span class="line">16         </span><br><span class="line">17         Scan scan = new Scan();</span><br><span class="line">18         </span><br><span class="line">19         Filter qualifierFilter = new QualifierFilter(CompareOp.EQUAL, new BinaryComparator(&quot;name&quot;.getBytes()));</span><br><span class="line">20         scan.setFilter(qualifierFilter);</span><br><span class="line">21         ResultScanner resultScanner = table.getScanner(scan);</span><br><span class="line">22         for(Result result : resultScanner) &#123;</span><br><span class="line">23             List&lt;Cell&gt; cells = result.listCells();</span><br><span class="line">24             for(Cell cell : cells) &#123;</span><br><span class="line">25                 System.out.println(cell);</span><br><span class="line">26             &#125;</span><br><span class="line">27         &#125;</span><br><span class="line">28         </span><br><span class="line">29         </span><br><span class="line">30     &#125;</span><br><span class="line">31     </span><br><span class="line">32     </span><br><span class="line">33 &#125;</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180331142440045-1822520821.png" alt="img"></p><h4 id="4、值过滤器-ValueFilter"><a href="#4、值过滤器-ValueFilter" class="headerlink" title="4、值过滤器 ValueFilter"></a>4、值过滤器 ValueFilter</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Filter valueFilter = new ValueFilter(CompareOp.EQUAL, new SubstringComparator(&quot;男&quot;));</span><br><span class="line">scan.setFilter(valueFilter);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class HbaseFilterTest &#123;</span><br><span class="line"> 2 </span><br><span class="line"> 3     private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;;</span><br><span class="line"> 4     private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;;</span><br><span class="line"> 5 </span><br><span class="line"> 6     private static Connection conn = null;</span><br><span class="line"> 7     private static Admin admin = null;</span><br><span class="line"> 8     </span><br><span class="line"> 9     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">10         </span><br><span class="line">11         Configuration conf = HBaseConfiguration.create();</span><br><span class="line">12         conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);</span><br><span class="line">13         conn = ConnectionFactory.createConnection(conf);</span><br><span class="line">14         admin = conn.getAdmin();</span><br><span class="line">15         Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));</span><br><span class="line">16         </span><br><span class="line">17         Scan scan = new Scan();</span><br><span class="line">18         </span><br><span class="line">19         Filter valueFilter = new ValueFilter(CompareOp.EQUAL, new SubstringComparator(&quot;男&quot;));</span><br><span class="line">20         scan.setFilter(valueFilter);</span><br><span class="line">21         ResultScanner resultScanner = table.getScanner(scan);</span><br><span class="line">22         for(Result result : resultScanner) &#123;</span><br><span class="line">23             List&lt;Cell&gt; cells = result.listCells();</span><br><span class="line">24             for(Cell cell : cells) &#123;</span><br><span class="line">25                 System.out.println(cell);</span><br><span class="line">26             &#125;</span><br><span class="line">27         &#125;</span><br><span class="line">28         </span><br><span class="line">29         </span><br><span class="line">30     &#125;</span><br><span class="line">31     </span><br><span class="line">32     </span><br><span class="line">33 &#125;</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180331142810385-1010448988.png" alt="img"></p><h4 id="5、时间戳过滤器-TimestampsFilter"><a href="#5、时间戳过滤器-TimestampsFilter" class="headerlink" title="5、时间戳过滤器 TimestampsFilter"></a>5、时间戳过滤器 TimestampsFilter</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Long&gt; list = new ArrayList&lt;&gt;();</span><br><span class="line">list.add(1522469029503l);</span><br><span class="line">TimestampsFilter timestampsFilter = new TimestampsFilter(list);</span><br><span class="line">scan.setFilter(timestampsFilter);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class HbaseFilterTest &#123;</span><br><span class="line"> 2 </span><br><span class="line"> 3     private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;;</span><br><span class="line"> 4     private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;;</span><br><span class="line"> 5 </span><br><span class="line"> 6     private static Connection conn = null;</span><br><span class="line"> 7     private static Admin admin = null;</span><br><span class="line"> 8     </span><br><span class="line"> 9     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">10         </span><br><span class="line">11         Configuration conf = HBaseConfiguration.create();</span><br><span class="line">12         conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);</span><br><span class="line">13         conn = ConnectionFactory.createConnection(conf);</span><br><span class="line">14         admin = conn.getAdmin();</span><br><span class="line">15         Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));</span><br><span class="line">16         </span><br><span class="line">17         Scan scan = new Scan();</span><br><span class="line">18         </span><br><span class="line">19         List&lt;Long&gt; list = new ArrayList&lt;&gt;();</span><br><span class="line">20         list.add(1522469029503l);</span><br><span class="line">21         TimestampsFilter timestampsFilter = new TimestampsFilter(list);</span><br><span class="line">22         scan.setFilter(timestampsFilter);</span><br><span class="line">23         ResultScanner resultScanner = table.getScanner(scan);</span><br><span class="line">24         for(Result result : resultScanner) &#123;</span><br><span class="line">25             List&lt;Cell&gt; cells = result.listCells();</span><br><span class="line">26             for(Cell cell : cells) &#123;</span><br><span class="line">27                 System.out.println(Bytes.toString(cell.getRow()) + &quot;\t&quot; + Bytes.toString(cell.getFamily()) + &quot;\t&quot; + Bytes.toString(cell.getQualifier())</span><br><span class="line">28                 + &quot;\t&quot; + Bytes.toString(cell.getValue()) + &quot;\t&quot; + cell.getTimestamp());</span><br><span class="line">29             &#125;</span><br><span class="line">30         &#125;</span><br><span class="line">31         </span><br><span class="line">32         </span><br><span class="line">33     &#125;</span><br><span class="line">34     </span><br><span class="line">35     </span><br><span class="line">36 &#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180331143422193-1859430510.png" alt="img"></p><h3 id="专用过滤器"><a href="#专用过滤器" class="headerlink" title="专用过滤器"></a>专用过滤器</h3><h4 id="1、单列值过滤器-SingleColumnValueFilter-—-会返回满足条件的整行"><a href="#1、单列值过滤器-SingleColumnValueFilter-—-会返回满足条件的整行" class="headerlink" title="1、单列值过滤器 SingleColumnValueFilter —-会返回满足条件的整行"></a>1、单列值过滤器 SingleColumnValueFilter —-会返回满足条件的整行</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter(</span><br><span class="line">                &quot;info&quot;.getBytes(), //列簇</span><br><span class="line">                &quot;name&quot;.getBytes(), //列</span><br><span class="line">                CompareOp.EQUAL, </span><br><span class="line">                new SubstringComparator(&quot;刘晨&quot;));</span><br><span class="line">//如果不设置为 true，则那些不包含指定 column 的行也会返回</span><br><span class="line">singleColumnValueFilter.setFilterIfMissing(true);</span><br><span class="line">scan.setFilter(singleColumnValueFilter);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class HbaseFilterTest2 &#123;</span><br><span class="line"> 2 </span><br><span class="line"> 3     private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;;</span><br><span class="line"> 4     private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;;</span><br><span class="line"> 5 </span><br><span class="line"> 6     private static Connection conn = null;</span><br><span class="line"> 7     private static Admin admin = null;</span><br><span class="line"> 8     </span><br><span class="line"> 9     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">10         </span><br><span class="line">11         Configuration conf = HBaseConfiguration.create();</span><br><span class="line">12         conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);</span><br><span class="line">13         conn = ConnectionFactory.createConnection(conf);</span><br><span class="line">14         admin = conn.getAdmin();</span><br><span class="line">15         Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));</span><br><span class="line">16         </span><br><span class="line">17         Scan scan = new Scan();</span><br><span class="line">18         </span><br><span class="line">19         SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter(</span><br><span class="line">20                 &quot;info&quot;.getBytes(), </span><br><span class="line">21                 &quot;name&quot;.getBytes(), </span><br><span class="line">22                 CompareOp.EQUAL, </span><br><span class="line">23                 new SubstringComparator(&quot;刘晨&quot;));</span><br><span class="line">24         singleColumnValueFilter.setFilterIfMissing(true);</span><br><span class="line">25         </span><br><span class="line">26         scan.setFilter(singleColumnValueFilter);</span><br><span class="line">27         ResultScanner resultScanner = table.getScanner(scan);</span><br><span class="line">28         for(Result result : resultScanner) &#123;</span><br><span class="line">29             List&lt;Cell&gt; cells = result.listCells();</span><br><span class="line">30             for(Cell cell : cells) &#123;</span><br><span class="line">31                 System.out.println(Bytes.toString(cell.getRow()) + &quot;\t&quot; + Bytes.toString(cell.getFamily()) + &quot;\t&quot; + Bytes.toString(cell.getQualifier())</span><br><span class="line">32                 + &quot;\t&quot; + Bytes.toString(cell.getValue()) + &quot;\t&quot; + cell.getTimestamp());</span><br><span class="line">33             &#125;</span><br><span class="line">34         &#125;</span><br><span class="line">35         </span><br><span class="line">36         </span><br><span class="line">37     &#125;</span><br><span class="line">38     </span><br><span class="line">39     </span><br><span class="line">40 &#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180331145327073-421109265.png" alt="img"></p><h4 id="2、单列值排除器-SingleColumnValueExcludeFilter"><a href="#2、单列值排除器-SingleColumnValueExcludeFilter" class="headerlink" title="2、单列值排除器 SingleColumnValueExcludeFilter"></a>2、单列值排除器 SingleColumnValueExcludeFilter</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SingleColumnValueExcludeFilter singleColumnValueExcludeFilter = new SingleColumnValueExcludeFilter(</span><br><span class="line">                &quot;info&quot;.getBytes(), </span><br><span class="line">                &quot;name&quot;.getBytes(), </span><br><span class="line">                CompareOp.EQUAL, </span><br><span class="line">                new SubstringComparator(&quot;刘晨&quot;));</span><br><span class="line">singleColumnValueExcludeFilter.setFilterIfMissing(true);</span><br><span class="line">        </span><br><span class="line">scan.setFilter(singleColumnValueExcludeFilter);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class HbaseFilterTest2 &#123;</span><br><span class="line"> 2 </span><br><span class="line"> 3     private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;;</span><br><span class="line"> 4     private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;;</span><br><span class="line"> 5 </span><br><span class="line"> 6     private static Connection conn = null;</span><br><span class="line"> 7     private static Admin admin = null;</span><br><span class="line"> 8     </span><br><span class="line"> 9     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">10         </span><br><span class="line">11         Configuration conf = HBaseConfiguration.create();</span><br><span class="line">12         conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);</span><br><span class="line">13         conn = ConnectionFactory.createConnection(conf);</span><br><span class="line">14         admin = conn.getAdmin();</span><br><span class="line">15         Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));</span><br><span class="line">16         </span><br><span class="line">17         Scan scan = new Scan();</span><br><span class="line">18         </span><br><span class="line">19         SingleColumnValueExcludeFilter singleColumnValueExcludeFilter = new SingleColumnValueExcludeFilter(</span><br><span class="line">20                 &quot;info&quot;.getBytes(), </span><br><span class="line">21                 &quot;name&quot;.getBytes(), </span><br><span class="line">22                 CompareOp.EQUAL, </span><br><span class="line">23                 new SubstringComparator(&quot;刘晨&quot;));</span><br><span class="line">24         singleColumnValueExcludeFilter.setFilterIfMissing(true);</span><br><span class="line">25         </span><br><span class="line">26         scan.setFilter(singleColumnValueExcludeFilter);</span><br><span class="line">27         ResultScanner resultScanner = table.getScanner(scan);</span><br><span class="line">28         for(Result result : resultScanner) &#123;</span><br><span class="line">29             List&lt;Cell&gt; cells = result.listCells();</span><br><span class="line">30             for(Cell cell : cells) &#123;</span><br><span class="line">31                 System.out.println(Bytes.toString(cell.getRow()) + &quot;\t&quot; + Bytes.toString(cell.getFamily()) + &quot;\t&quot; + Bytes.toString(cell.getQualifier())</span><br><span class="line">32                 + &quot;\t&quot; + Bytes.toString(cell.getValue()) + &quot;\t&quot; + cell.getTimestamp());</span><br><span class="line">33             &#125;</span><br><span class="line">34         &#125;</span><br><span class="line">35         </span><br><span class="line">36         </span><br><span class="line">37     &#125;</span><br><span class="line">38     </span><br><span class="line">39     </span><br><span class="line">40 &#125;</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180331145530417-602264605.png" alt="img"></p><h4 id="3、前缀过滤器-PrefixFilter—-针对行键"><a href="#3、前缀过滤器-PrefixFilter—-针对行键" class="headerlink" title="3、前缀过滤器 PrefixFilter—-针对行键"></a>3、前缀过滤器 PrefixFilter—-针对行键</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PrefixFilter prefixFilter = new PrefixFilter(&quot;9501&quot;.getBytes());</span><br><span class="line">        </span><br><span class="line">scan.setFilter(prefixFilter);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class HbaseFilterTest2 &#123;</span><br><span class="line"> 2 </span><br><span class="line"> 3     private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;;</span><br><span class="line"> 4     private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;;</span><br><span class="line"> 5 </span><br><span class="line"> 6     private static Connection conn = null;</span><br><span class="line"> 7     private static Admin admin = null;</span><br><span class="line"> 8     </span><br><span class="line"> 9     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">10         </span><br><span class="line">11         Configuration conf = HBaseConfiguration.create();</span><br><span class="line">12         conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);</span><br><span class="line">13         conn = ConnectionFactory.createConnection(conf);</span><br><span class="line">14         admin = conn.getAdmin();</span><br><span class="line">15         Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));</span><br><span class="line">16         </span><br><span class="line">17         Scan scan = new Scan();</span><br><span class="line">18         </span><br><span class="line">19         PrefixFilter prefixFilter = new PrefixFilter(&quot;9501&quot;.getBytes());</span><br><span class="line">20         </span><br><span class="line">21         scan.setFilter(prefixFilter);</span><br><span class="line">22         ResultScanner resultScanner = table.getScanner(scan);</span><br><span class="line">23         for(Result result : resultScanner) &#123;</span><br><span class="line">24             List&lt;Cell&gt; cells = result.listCells();</span><br><span class="line">25             for(Cell cell : cells) &#123;</span><br><span class="line">26                 System.out.println(Bytes.toString(cell.getRow()) + &quot;\t&quot; + Bytes.toString(cell.getFamily()) + &quot;\t&quot; + Bytes.toString(cell.getQualifier())</span><br><span class="line">27                 + &quot;\t&quot; + Bytes.toString(cell.getValue()) + &quot;\t&quot; + cell.getTimestamp());</span><br><span class="line">28             &#125;</span><br><span class="line">29         &#125;</span><br><span class="line">30         </span><br><span class="line">31         </span><br><span class="line">32     &#125;</span><br><span class="line">33     </span><br><span class="line">34     </span><br><span class="line">35 &#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180331150009860-2063523179.png" alt="img"></p><h4 id="4、列前缀过滤器-ColumnPrefixFilter"><a href="#4、列前缀过滤器-ColumnPrefixFilter" class="headerlink" title="4、列前缀过滤器 ColumnPrefixFilter"></a>4、列前缀过滤器 ColumnPrefixFilter</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ColumnPrefixFilter columnPrefixFilter = new ColumnPrefixFilter(&quot;name&quot;.getBytes());</span><br><span class="line">        </span><br><span class="line">scan.setFilter(columnPrefixFilter);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class HbaseFilterTest2 &#123;</span><br><span class="line"> 2 </span><br><span class="line"> 3     private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;;</span><br><span class="line"> 4     private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;;</span><br><span class="line"> 5 </span><br><span class="line"> 6     private static Connection conn = null;</span><br><span class="line"> 7     private static Admin admin = null;</span><br><span class="line"> 8     </span><br><span class="line"> 9     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">10         </span><br><span class="line">11         Configuration conf = HBaseConfiguration.create();</span><br><span class="line">12         conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);</span><br><span class="line">13         conn = ConnectionFactory.createConnection(conf);</span><br><span class="line">14         admin = conn.getAdmin();</span><br><span class="line">15         Table table = conn.getTable(TableName.valueOf(&quot;student&quot;));</span><br><span class="line">16         </span><br><span class="line">17         Scan scan = new Scan();</span><br><span class="line">18         </span><br><span class="line">19         ColumnPrefixFilter columnPrefixFilter = new ColumnPrefixFilter(&quot;name&quot;.getBytes());</span><br><span class="line">20         </span><br><span class="line">21         scan.setFilter(columnPrefixFilter);</span><br><span class="line">22         ResultScanner resultScanner = table.getScanner(scan);</span><br><span class="line">23         for(Result result : resultScanner) &#123;</span><br><span class="line">24             List&lt;Cell&gt; cells = result.listCells();</span><br><span class="line">25             for(Cell cell : cells) &#123;</span><br><span class="line">26                 System.out.println(Bytes.toString(cell.getRow()) + &quot;\t&quot; + Bytes.toString(cell.getFamily()) + &quot;\t&quot; + Bytes.toString(cell.getQualifier())</span><br><span class="line">27                 + &quot;\t&quot; + Bytes.toString(cell.getValue()) + &quot;\t&quot; + cell.getTimestamp());</span><br><span class="line">28             &#125;</span><br><span class="line">29         &#125;</span><br><span class="line">30         </span><br><span class="line">31         </span><br><span class="line">32     &#125;</span><br><span class="line">33     </span><br><span class="line">34     </span><br><span class="line">35 &#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180331150519676-692732493.png" alt="img"></p><h4 id="5、分页过滤器-PageFilter"><a href="#5、分页过滤器-PageFilter" class="headerlink" title="5、分页过滤器 PageFilter"></a>5、分页过滤器 PageFilter</h4>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase学习之路 （五）MapReduce操作Hbase</title>
      <link href="/2018-06-06-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%94%EF%BC%89MapReduce%E6%93%8D%E4%BD%9CHbase.html"/>
      <url>/2018-06-06-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%94%EF%BC%89MapReduce%E6%93%8D%E4%BD%9CHbase.html</url>
      
        <content type="html"><![CDATA[<p>** HBase学习之路 （五）MapReduce操作Hbase：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        HBase学习之路 （五）MapReduce操作Hbase</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="MapReduce从HDFS读取数据存储到HBase中"><a href="#MapReduce从HDFS读取数据存储到HBase中" class="headerlink" title="MapReduce从HDFS读取数据存储到HBase中"></a>MapReduce从HDFS读取数据存储到HBase中</h2><p>现有HDFS中有一个student.txt文件，格式如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">95002,刘晨,女,19,IS</span><br><span class="line">95017,王风娟,女,18,IS</span><br><span class="line">95018,王一,女,19,IS</span><br><span class="line">95013,冯伟,男,21,CS</span><br><span class="line">95014,王小丽,女,19,CS</span><br><span class="line">95019,邢小丽,女,19,IS</span><br><span class="line">95020,赵钱,男,21,IS</span><br><span class="line">95003,王敏,女,22,MA</span><br><span class="line">95004,张立,男,19,IS</span><br><span class="line">95012,孙花,女,20,CS</span><br><span class="line">95010,孔小涛,男,19,CS</span><br><span class="line">95005,刘刚,男,18,MA</span><br><span class="line">95006,孙庆,男,23,CS</span><br><span class="line">95007,易思玲,女,19,MA</span><br><span class="line">95008,李娜,女,18,CS</span><br><span class="line">95021,周二,男,17,MA</span><br><span class="line">95022,郑明,男,20,MA</span><br><span class="line">95001,李勇,男,20,CS</span><br><span class="line">95011,包小柏,男,18,MA</span><br><span class="line">95009,梦圆圆,女,18,MA</span><br><span class="line">95015,王君,男,18,MA</span><br></pre></td></tr></table></figure><p>将HDFS上的这个文件里面的数据写入到HBase数据块中</p><p>MapReduce实现代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.conf.Configured;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line">import org.apache.hadoop.hbase.client.Put;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableReducer;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line">import org.apache.hadoop.util.Tool;</span><br><span class="line">import org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line">public class ReadHDFSDataToHbaseMR extends Configured implements Tool&#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        </span><br><span class="line">        int run = ToolRunner.run(new ReadHDFSDataToHbaseMR(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public int run(String[] arg0) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = HBaseConfiguration.create();</span><br><span class="line">        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://myha01/&quot;);</span><br><span class="line">        conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;);</span><br><span class="line">        System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">        FileSystem fs = FileSystem.get(conf);</span><br><span class="line">//        conf.addResource(&quot;config/core-site.xml&quot;);</span><br><span class="line">//        conf.addResource(&quot;config/hdfs-site.xml&quot;);</span><br><span class="line">        </span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line">        </span><br><span class="line">        job.setJarByClass(ReadHDFSDataToHbaseMR.class);</span><br><span class="line">        </span><br><span class="line">        job.setMapperClass(HDFSToHbaseMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        TableMapReduceUtil.initTableReducerJob(&quot;student&quot;, HDFSToHbaseReducer.class, job,null,null,null,null,false);</span><br><span class="line">        job.setOutputKeyClass(NullWritable.class);</span><br><span class="line">        job.setOutputValueClass(Put.class);</span><br><span class="line">        </span><br><span class="line">        Path inputPath = new Path(&quot;/student/input/&quot;);</span><br><span class="line">        Path outputPath = new Path(&quot;/student/output/&quot;);</span><br><span class="line">        </span><br><span class="line">        if(fs.exists(outputPath)) &#123;</span><br><span class="line">            fs.delete(outputPath,true);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        FileInputFormat.addInputPath(job, inputPath);</span><br><span class="line">        FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line">        </span><br><span class="line">        boolean isDone = job.waitForCompletion(true);</span><br><span class="line">        </span><br><span class="line">        return isDone ? 0 : 1;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    public static class HDFSToHbaseMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123;</span><br><span class="line">        </span><br><span class="line">        @Override</span><br><span class="line">        protected void map(LongWritable key, Text value, Context context)</span><br><span class="line">                throws IOException, InterruptedException &#123;    </span><br><span class="line">            context.write(value, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    /**</span><br><span class="line">     * 95015,王君,男,18,MA</span><br><span class="line">     * */</span><br><span class="line">    public static class HDFSToHbaseReducer extends TableReducer&lt;Text, NullWritable, NullWritable&gt;&#123;</span><br><span class="line">        </span><br><span class="line">        @Override</span><br><span class="line">        protected void reduce(Text key, Iterable&lt;NullWritable&gt; values,Context context)</span><br><span class="line">                throws IOException, InterruptedException &#123;</span><br><span class="line">            </span><br><span class="line">            String[] split = key.toString().split(&quot;,&quot;);</span><br><span class="line">            </span><br><span class="line">            Put put = new Put(split[0].getBytes());</span><br><span class="line">            </span><br><span class="line">            put.addColumn(&quot;info&quot;.getBytes(), &quot;name&quot;.getBytes(), split[1].getBytes());</span><br><span class="line">            put.addColumn(&quot;info&quot;.getBytes(), &quot;sex&quot;.getBytes(), split[2].getBytes());</span><br><span class="line">            put.addColumn(&quot;info&quot;.getBytes(), &quot;age&quot;.getBytes(), split[3].getBytes());</span><br><span class="line">            put.addColumn(&quot;info&quot;.getBytes(), &quot;department&quot;.getBytes(), split[4].getBytes());</span><br><span class="line">            </span><br><span class="line">            context.write(NullWritable.get(), put);</span><br><span class="line">        </span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="MapReduce从HBase读取数据计算平均年龄并存储到HDFS中"><a href="#MapReduce从HBase读取数据计算平均年龄并存储到HDFS中" class="headerlink" title="MapReduce从HBase读取数据计算平均年龄并存储到HDFS中"></a>MapReduce从HBase读取数据计算平均年龄并存储到HDFS中</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.conf.Configured;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.hbase.Cell;</span><br><span class="line">import org.apache.hadoop.hbase.CellUtil;</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line">import org.apache.hadoop.hbase.client.Result;</span><br><span class="line">import org.apache.hadoop.hbase.client.Scan;</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableMapper;</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line">import org.apache.hadoop.io.DoubleWritable;</span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line">import org.apache.hadoop.util.Tool;</span><br><span class="line">import org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">public class ReadHbaseDataToHDFS extends Configured implements Tool&#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        </span><br><span class="line">        int run = ToolRunner.run(new ReadHbaseDataToHDFS(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public int run(String[] arg0) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = HBaseConfiguration.create();</span><br><span class="line">        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://myha01/&quot;);</span><br><span class="line">        conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;);</span><br><span class="line">        System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">        FileSystem fs = FileSystem.get(conf);</span><br><span class="line">//        conf.addResource(&quot;config/core-site.xml&quot;);</span><br><span class="line">//        conf.addResource(&quot;config/hdfs-site.xml&quot;);</span><br><span class="line">        </span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line">        </span><br><span class="line">        job.setJarByClass(ReadHbaseDataToHDFS.class);</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        // 取对业务有用的数据 info,age</span><br><span class="line">        Scan scan = new Scan();</span><br><span class="line">        scan.addColumn(&quot;info&quot;.getBytes(), &quot;age&quot;.getBytes());</span><br><span class="line">        </span><br><span class="line">        TableMapReduceUtil.initTableMapperJob(</span><br><span class="line">                &quot;student&quot;.getBytes(), // 指定表名</span><br><span class="line">                scan, // 指定扫描数据的条件</span><br><span class="line">                HbaseToHDFSMapper.class, // 指定mapper class</span><br><span class="line">                Text.class,     // outputKeyClass mapper阶段的输出的key的类型</span><br><span class="line">                IntWritable.class, // outputValueClass mapper阶段的输出的value的类型</span><br><span class="line">                job, // job对象</span><br><span class="line">                false</span><br><span class="line">                );</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">        job.setReducerClass(HbaseToHDFSReducer.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(DoubleWritable.class);</span><br><span class="line">        </span><br><span class="line">        Path outputPath = new Path(&quot;/student/avg/&quot;);</span><br><span class="line">        </span><br><span class="line">        if(fs.exists(outputPath)) &#123;</span><br><span class="line">            fs.delete(outputPath,true);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line">        </span><br><span class="line">        boolean isDone = job.waitForCompletion(true);</span><br><span class="line">        </span><br><span class="line">        return isDone ? 0 : 1;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static class HbaseToHDFSMapper extends TableMapper&lt;Text, IntWritable&gt;&#123;</span><br><span class="line">        </span><br><span class="line">        Text outKey = new Text(&quot;age&quot;);</span><br><span class="line">        IntWritable outValue = new IntWritable();</span><br><span class="line">        // key是hbase中的行键</span><br><span class="line">        // value是hbase中的所行键的所有数据</span><br><span class="line">        @Override</span><br><span class="line">        protected void map(ImmutableBytesWritable key, Result value,Context context)</span><br><span class="line">                throws IOException, InterruptedException &#123;</span><br><span class="line">            </span><br><span class="line">            boolean isContainsColumn = value.containsColumn(&quot;info&quot;.getBytes(), &quot;age&quot;.getBytes());</span><br><span class="line">        </span><br><span class="line">            if(isContainsColumn) &#123;</span><br><span class="line">                </span><br><span class="line">                List&lt;Cell&gt; listCells = value.getColumnCells(&quot;info&quot;.getBytes(), &quot;age&quot;.getBytes());</span><br><span class="line">                System.out.println(&quot;listCells:\t&quot;+listCells);</span><br><span class="line">                Cell cell = listCells.get(0);</span><br><span class="line">                System.out.println(&quot;cells:\t&quot;+cell);</span><br><span class="line">                </span><br><span class="line">                byte[] cloneValue = CellUtil.cloneValue(cell);</span><br><span class="line">                String ageValue = Bytes.toString(cloneValue);</span><br><span class="line">                outValue.set(Integer.parseInt(ageValue));</span><br><span class="line">                </span><br><span class="line">                context.write(outKey,outValue);</span><br><span class="line">                </span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static class HbaseToHDFSReducer extends Reducer&lt;Text, IntWritable, Text, DoubleWritable&gt;&#123;</span><br><span class="line">        </span><br><span class="line">        DoubleWritable outValue = new DoubleWritable();</span><br><span class="line">        </span><br><span class="line">        @Override</span><br><span class="line">        protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context)</span><br><span class="line">                throws IOException, InterruptedException &#123;</span><br><span class="line">            </span><br><span class="line">            int count = 0;</span><br><span class="line">            int sum = 0;</span><br><span class="line">            for(IntWritable value : values) &#123;</span><br><span class="line">                count++;</span><br><span class="line">                sum += value.get();</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            double avgAge = sum * 1.0 / count;</span><br><span class="line">            outValue.set(avgAge);</span><br><span class="line">            context.write(key, outValue);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase学习之路 （三）HBase集群Shell操作</title>
      <link href="/2018-06-03-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%89%EF%BC%89HBase%E9%9B%86%E7%BE%A4Shell%E6%93%8D%E4%BD%9C.html"/>
      <url>/2018-06-03-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%89%EF%BC%89HBase%E9%9B%86%E7%BE%A4Shell%E6%93%8D%E4%BD%9C.html</url>
      
        <content type="html"><![CDATA[<p>** HBase学习之路 （三）HBase集群Shell操作：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        HBase学习之路 （三）HBase集群Shell操作</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="进入HBase命令行"><a href="#进入HBase命令行" class="headerlink" title="进入HBase命令行"></a>进入HBase命令行</h2><p>在你安装的随意台服务器节点上，执行命令：hbase shell，会进入到你的 hbase shell 客 户端</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hbase shell</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">HBase Shell; enter &apos;help&lt;RETURN&gt;&apos; for list of supported commands.</span><br><span class="line">Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell</span><br><span class="line">Version 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017</span><br><span class="line"></span><br><span class="line">hbase(main):001:0&gt;</span><br></pre></td></tr></table></figure><p> 说明，先看一下提示。其实是不是有一句很重要的话：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HBase Shell; enter &apos;help&lt;RETURN&gt;&apos; for list of supported commands.</span><br><span class="line">Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell</span><br></pre></td></tr></table></figure><p>讲述了怎么获得帮助，怎么退出客户端</p><p>help 获取帮助</p><p>　　help：获取所有命令提示</p><p>　　help “dml” ：获取一组命令的提示</p><p>　　help “put” ：获取一个单独命令的提示帮助</p><p>exit 退出 hbase shell 客户端</p><h2 id="HBase表的操作"><a href="#HBase表的操作" class="headerlink" title="HBase表的操作"></a>HBase表的操作</h2><p> 关于表的操作包括（创建create，查看表列表list。查看表的详细信息desc，删除表drop，清空表truncate，修改表的定义alter）</p><h3 id="创建create"><a href="#创建create" class="headerlink" title="创建create"></a>创建create</h3><p>可以输入以下命令进行查看帮助命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; help &apos;create&apos;</span><br></pre></td></tr></table></figure><p><img src="https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif" alt="img"> View Code</p><p>可以看到其中一条提示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase&gt; create &apos;t1&apos;, &#123;NAME =&gt; &apos;f1&apos;&#125;, &#123;NAME =&gt; &apos;f2&apos;&#125;, &#123;NAME =&gt; &apos;f3&apos;&#125;</span><br></pre></td></tr></table></figure><p>其中t1是表名，f1,f2,f3是列簇的名，如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):002:0&gt; create &apos;myHbase&apos;,&#123;NAME =&gt; &apos;myCard&apos;,VERSIONS =&gt; 5&#125;</span><br><span class="line">0 row(s) in 3.1270 seconds</span><br><span class="line"></span><br><span class="line">=&gt; Hbase::Table - myHbase</span><br><span class="line">hbase(main):003:0&gt;</span><br></pre></td></tr></table></figure><p>创建了一个名为myHbase的表，表里面有1个列簇，名为<strong>myCard</strong>，保留5个版本信息</p><h3 id="查看表列表list"><a href="#查看表列表list" class="headerlink" title="查看表列表list"></a>查看表列表list</h3><p>可以输入以下命令进行查看帮助命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):003:0&gt; help &apos;list&apos;</span><br><span class="line">List all tables in hbase. Optional regular expression parameter could</span><br><span class="line">be used to filter the output. Examples:</span><br><span class="line"></span><br><span class="line">  hbase&gt; list</span><br><span class="line">  hbase&gt; list &apos;abc.*&apos;</span><br><span class="line">  hbase&gt; list &apos;ns:abc.*&apos;</span><br><span class="line">  hbase&gt; list &apos;ns:.*&apos;</span><br><span class="line">hbase(main):004:0&gt;</span><br></pre></td></tr></table></figure><p>直接输入list进行查看</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):004:0&gt; list</span><br><span class="line">TABLE                                                                                                           </span><br><span class="line">myHbase                                                                                                         </span><br><span class="line">1 row(s) in 0.0650 seconds</span><br><span class="line"></span><br><span class="line">=&gt; [&quot;myHbase&quot;]</span><br><span class="line">hbase(main):005:0&gt;</span><br></pre></td></tr></table></figure><p>只有一条结果，就是刚刚创建的表myHbase</p><h3 id="查看表的详细信息desc"><a href="#查看表的详细信息desc" class="headerlink" title="查看表的详细信息desc"></a>查看表的详细信息desc</h3><p>一个大括号，就相当于一个列簇。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):006:0&gt; desc &apos;myHbase&apos;</span><br><span class="line">Table myHbase is ENABLED                                                                                        </span><br><span class="line">myHbase                                                                                                         </span><br><span class="line">COLUMN FAMILIES DESCRIPTION                                                                                     </span><br><span class="line">&#123;NAME =&gt; &apos;myCard&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;5&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, D</span><br><span class="line">ATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;</span><br><span class="line">, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;                                                               </span><br><span class="line">1 row(s) in 0.2160 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):007:0&gt;</span><br></pre></td></tr></table></figure><h3 id="修改表的定义alter"><a href="#修改表的定义alter" class="headerlink" title="修改表的定义alter"></a>修改表的定义alter</h3><h4 id="添加一个列簇"><a href="#添加一个列簇" class="headerlink" title="添加一个列簇"></a>添加一个列簇</h4><p>hbase(main):007:0&gt; <strong>alter ‘myHbase’, NAME =&gt; ‘myInfo’</strong><br>Updating all regions with the new schema…<br>1/1 regions updated.<br>Done.<br>0 row(s) in 2.0690 seconds</p><p>hbase(main):008:0&gt; <strong>desc ‘myHbase’</strong><br>Table myHbase is ENABLED<br>myHbase<br>COLUMN FAMILIES DESCRIPTION<br>{NAME =&gt; ‘myCard’, BLOOMFILTER =&gt; ‘ROW’, VERSIONS =&gt; ‘5’, IN_MEMORY =&gt; ‘false’, KEEP_DELETED_CELLS =&gt; ‘FALSE’, D<br>ATA_BLOCK_ENCODING =&gt; ‘NONE’, TTL =&gt; ‘FOREVER’, COMPRESSION =&gt; ‘NONE’, MIN_VERSIONS =&gt; ‘0’, BLOCKCACHE =&gt; ‘true’<br>, BLOCKSIZE =&gt; ‘65536’, REPLICATION_SCOPE =&gt; ‘0’}<br>{NAME =&gt; ‘myInfo’, BLOOMFILTER =&gt; ‘ROW’, VERSIONS =&gt; ‘1’, IN_MEMORY =&gt; ‘false’, KEEP_DELETED_CELLS =&gt; ‘FALSE’, D<br>ATA_BLOCK_ENCODING =&gt; ‘NONE’, TTL =&gt; ‘FOREVER’, COMPRESSION =&gt; ‘NONE’, MIN_VERSIONS =&gt; ‘0’, BLOCKCACHE =&gt; ‘true’<br>, BLOCKSIZE =&gt; ‘65536’, REPLICATION_SCOPE =&gt; ‘0’}<br>2 row(s) in 0.0420 seconds</p><p>hbase(main):009:0&gt;</p><h4 id="删除一个列簇"><a href="#删除一个列簇" class="headerlink" title="删除一个列簇"></a>删除一个列簇</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):009:0&gt; alter &apos;myHbase&apos;, NAME =&gt; &apos;myCard&apos;, METHOD =&gt; &apos;delete&apos;</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1/1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">0 row(s) in 2.1920 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):010:0&gt; desc &apos;myHbase&apos;</span><br><span class="line">Table myHbase is ENABLED                                                                                        </span><br><span class="line">myHbase                                                                                                         </span><br><span class="line">COLUMN FAMILIES DESCRIPTION                                                                                     </span><br><span class="line">&#123;NAME =&gt; &apos;myInfo&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, D</span><br><span class="line">ATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;</span><br><span class="line">, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;                                                               </span><br><span class="line">1 row(s) in 0.0290 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):011:0&gt;</span><br></pre></td></tr></table></figure><p>删除一个列簇也可以执行以下命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter &apos;myHbase&apos;, &apos;delete&apos; =&gt; &apos;myCard&apos;</span><br></pre></td></tr></table></figure><h4 id="添加列簇hehe同时删除列簇myInfo"><a href="#添加列簇hehe同时删除列簇myInfo" class="headerlink" title="添加列簇hehe同时删除列簇myInfo"></a>添加列簇hehe同时删除列簇myInfo</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):011:0&gt; alter &apos;myHbase&apos;, &#123;NAME =&gt; &apos;hehe&apos;&#125;, &#123;NAME =&gt; &apos;myInfo&apos;, METHOD =&gt; &apos;delete&apos;&#125;</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1/1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">Updating all regions with the new schema...</span><br><span class="line">1/1 regions updated.</span><br><span class="line">Done.</span><br><span class="line">0 row(s) in 3.8260 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):012:0&gt; desc &apos;myHbase&apos;</span><br><span class="line">Table myHbase is ENABLED                                                                                        </span><br><span class="line">myHbase                                                                                                         </span><br><span class="line">COLUMN FAMILIES DESCRIPTION                                                                                     </span><br><span class="line">&#123;NAME =&gt; &apos;hehe&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, DAT</span><br><span class="line">A_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, </span><br><span class="line">BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;                                                                 </span><br><span class="line">1 row(s) in 0.0410 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):013:0&gt;</span><br></pre></td></tr></table></figure><h4 id="清空表truncate"><a href="#清空表truncate" class="headerlink" title="清空表truncate"></a>清空表truncate</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):013:0&gt; truncate &apos;myHbase&apos;</span><br><span class="line">Truncating &apos;myHbase&apos; table (it may take a while):</span><br><span class="line"> - Disabling table...</span><br><span class="line"> - Truncating table...</span><br><span class="line">0 row(s) in 3.6760 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):014:0&gt;</span><br></pre></td></tr></table></figure><h4 id="删除表drop"><a href="#删除表drop" class="headerlink" title="删除表drop"></a>删除表drop</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):014:0&gt; drop &apos;myHbase&apos;</span><br><span class="line"></span><br><span class="line">ERROR: Table myHbase is enabled. Disable it first.</span><br><span class="line"></span><br><span class="line">Here is some help for this command:</span><br><span class="line">Drop the named table. Table must first be disabled:</span><br><span class="line">  hbase&gt; drop &apos;t1&apos;</span><br><span class="line">  hbase&gt; drop &apos;ns1:t1&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hbase(main):015:0&gt;</span><br></pre></td></tr></table></figure><p>直接删除表会报错，根据提示需要先停用表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):015:0&gt; disable &apos;myHbase&apos;</span><br><span class="line">0 row(s) in 2.2620 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):016:0&gt; drop &apos;myHbase&apos;</span><br><span class="line">0 row(s) in 1.2970 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):017:0&gt; list</span><br><span class="line">TABLE                                                                                                           </span><br><span class="line">0 row(s) in 0.0110 seconds</span><br><span class="line"></span><br><span class="line">=&gt; []</span><br><span class="line">hbase(main):018:0&gt;</span><br></pre></td></tr></table></figure><h2 id="HBase表中数据的操作"><a href="#HBase表中数据的操作" class="headerlink" title="HBase表中数据的操作"></a>HBase表中数据的操作</h2><p>关于数据的操作（增put，删delete，查get + scan,  改==变相的增加）</p><p>创建 user 表，包含 info、data 两个列簇</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):018:0&gt; create &apos;user_info&apos;,&#123;NAME=&gt;&apos;base_info&apos;,VERSIONS=&gt;3 &#125;,&#123;NAME=&gt;&apos;extra_info&apos;,VERSIONS=&gt;1 &#125; </span><br><span class="line">0 row(s) in 4.2670 seconds</span><br><span class="line"></span><br><span class="line">=&gt; Hbase::Table - user_info</span><br><span class="line">hbase(main):019:0&gt;</span><br></pre></td></tr></table></figure><h3 id="增put"><a href="#增put" class="headerlink" title="增put"></a>增put</h3><p>查看帮助，需要传入表名，rowkey，列簇名、值等</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):019:0&gt; help &apos;put&apos;</span><br><span class="line">Put a cell &apos;value&apos; at specified table/row/column and optionally</span><br><span class="line">timestamp coordinates.  To put a cell value into table &apos;ns1:t1&apos; or &apos;t1&apos;</span><br><span class="line">at row &apos;r1&apos; under column &apos;c1&apos; marked with the time &apos;ts1&apos;, do:</span><br><span class="line"></span><br><span class="line">  hbase&gt; put &apos;ns1:t1&apos;, &apos;r1&apos;, &apos;c1&apos;, &apos;value&apos;</span><br><span class="line">  hbase&gt; put &apos;t1&apos;, &apos;r1&apos;, &apos;c1&apos;, &apos;value&apos;</span><br><span class="line">  hbase&gt; put &apos;t1&apos;, &apos;r1&apos;, &apos;c1&apos;, &apos;value&apos;, ts1</span><br><span class="line">  hbase&gt; put &apos;t1&apos;, &apos;r1&apos;, &apos;c1&apos;, &apos;value&apos;, &#123;ATTRIBUTES=&gt;&#123;&apos;mykey&apos;=&gt;&apos;myvalue&apos;&#125;&#125;</span><br><span class="line">  hbase&gt; put &apos;t1&apos;, &apos;r1&apos;, &apos;c1&apos;, &apos;value&apos;, ts1, &#123;ATTRIBUTES=&gt;&#123;&apos;mykey&apos;=&gt;&apos;myvalue&apos;&#125;&#125;</span><br><span class="line">  hbase&gt; put &apos;t1&apos;, &apos;r1&apos;, &apos;c1&apos;, &apos;value&apos;, ts1, &#123;VISIBILITY=&gt;&apos;PRIVATE|SECRET&apos;&#125;</span><br><span class="line"></span><br><span class="line">The same commands also can be run on a table reference. Suppose you had a reference</span><br><span class="line">t to table &apos;t1&apos;, the corresponding command would be:</span><br><span class="line"></span><br><span class="line">  hbase&gt; t.put &apos;r1&apos;, &apos;c1&apos;, &apos;value&apos;, ts1, &#123;ATTRIBUTES=&gt;&#123;&apos;mykey&apos;=&gt;&apos;myvalue&apos;&#125;&#125;</span><br><span class="line">hbase(main):020:0&gt;</span><br></pre></td></tr></table></figure><p> 向 user 表中插入信息，row key 为 user0001，列簇 base_info 中添加 name 列标示符，值为 zhangsan1</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):020:0&gt; put &apos;user_info&apos;, &apos;user0001&apos;, &apos;base_info:name&apos;, &apos;zhangsan1&apos;</span><br><span class="line">0 row(s) in 0.2900 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):021:0&gt;</span><br></pre></td></tr></table></figure><p>此处可以多添加几条数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0001&apos;, &apos;base_info:name&apos;, &apos;zhangsan1&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0002&apos;, &apos;base_info:name&apos;, &apos;zhangsan2&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0003&apos;, &apos;base_info:name&apos;, &apos;zhangsan3&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0004&apos;, &apos;base_info:name&apos;, &apos;zhangsan4&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0005&apos;, &apos;base_info:name&apos;, &apos;zhangsan5&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0006&apos;, &apos;base_info:name&apos;, &apos;zhangsan6&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0007&apos;, &apos;base_info:name&apos;, &apos;zhangsan7&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0008&apos;, &apos;base_info:name&apos;, &apos;zhangsan8&apos;</span><br><span class="line"></span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0001&apos;, &apos;base_info:age&apos;, &apos;21&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0002&apos;, &apos;base_info:age&apos;, &apos;22&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0003&apos;, &apos;base_info:age&apos;, &apos;23&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0004&apos;, &apos;base_info:age&apos;, &apos;24&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0005&apos;, &apos;base_info:age&apos;, &apos;25&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0006&apos;, &apos;base_info:age&apos;, &apos;26&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0007&apos;, &apos;base_info:age&apos;, &apos;27&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0008&apos;, &apos;base_info:age&apos;, &apos;28&apos;</span><br><span class="line"></span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0001&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0002&apos;, &apos;extra_info:Hobbies&apos;, &apos;sport&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0003&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0004&apos;, &apos;extra_info:Hobbies&apos;, &apos;sport&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0005&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0006&apos;, &apos;extra_info:Hobbies&apos;, &apos;sport&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;zhangsan_20150701_0007&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;</span><br><span class="line"></span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0001&apos;, &apos;base_info:name&apos;, &apos;baiyc1&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0002&apos;, &apos;base_info:name&apos;, &apos;baiyc2&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0003&apos;, &apos;base_info:name&apos;, &apos;baiyc3&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0004&apos;, &apos;base_info:name&apos;, &apos;baiyc4&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0005&apos;, &apos;base_info:name&apos;, &apos;baiyc5&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0006&apos;, &apos;base_info:name&apos;, &apos;baiyc6&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0007&apos;, &apos;base_info:name&apos;, &apos;baiyc7&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0008&apos;, &apos;base_info:name&apos;, &apos;baiyc8&apos;</span><br><span class="line"></span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0001&apos;, &apos;base_info:age&apos;, &apos;21&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0002&apos;, &apos;base_info:age&apos;, &apos;22&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0003&apos;, &apos;base_info:age&apos;, &apos;23&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0004&apos;, &apos;base_info:age&apos;, &apos;24&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0005&apos;, &apos;base_info:age&apos;, &apos;25&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0006&apos;, &apos;base_info:age&apos;, &apos;26&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0007&apos;, &apos;base_info:age&apos;, &apos;27&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0008&apos;, &apos;base_info:age&apos;, &apos;28&apos;</span><br><span class="line"></span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0001&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0002&apos;, &apos;extra_info:Hobbies&apos;, &apos;sport&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0003&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0004&apos;, &apos;extra_info:Hobbies&apos;, &apos;sport&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0005&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0006&apos;, &apos;extra_info:Hobbies&apos;, &apos;sport&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0007&apos;, &apos;extra_info:Hobbies&apos;, &apos;music&apos;</span><br><span class="line">put &apos;user_info&apos;, &apos;baiyc_20150716_0008&apos;, &apos;extra_info:Hobbies&apos;, &apos;sport&apos;</span><br></pre></td></tr></table></figure><h3 id="查get-scan"><a href="#查get-scan" class="headerlink" title="查get + scan"></a>查get + scan</h3><p>获取 user 表中 row key 为 user0001 的所有信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):022:0&gt; get &apos;user_info&apos;, &apos;user0001&apos;</span><br><span class="line">COLUMN                        CELL                                                                              </span><br><span class="line"> base_info:name               timestamp=1522320801670, value=zhangsan1                                          </span><br><span class="line">1 row(s) in 0.1310 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):023:0&gt;</span><br></pre></td></tr></table></figure><p>获取user表中row key为rk0001，info列簇的所有信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):025:0&gt; get &apos;user_info&apos;, &apos;rk0001&apos;, &apos;base_info&apos;</span><br><span class="line">COLUMN                        CELL                                                                              </span><br><span class="line"> base_info:name               timestamp=1522321247732, value=zhangsan                                           </span><br><span class="line">1 row(s) in 0.0320 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):026:0&gt;</span><br></pre></td></tr></table></figure><p>查询user_info表中的所有信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):026:0&gt; scan &apos;user_info&apos;</span><br><span class="line">ROW                           COLUMN+CELL                                                                       </span><br><span class="line"> rk0001                       column=base_info:name, timestamp=1522321247732, value=zhangsan                    </span><br><span class="line"> user0001                     column=base_info:name, timestamp=1522320801670, value=zhangsan1                   </span><br><span class="line">2 row(s) in 0.0970 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):027:0&gt;</span><br></pre></td></tr></table></figure><p>查询user_info表中列簇为base_info的信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):027:0&gt; scan &apos;user_info&apos;, &#123;COLUMNS =&gt; &apos;base_info&apos;&#125;</span><br><span class="line">ROW                           COLUMN+CELL                                                                       </span><br><span class="line"> rk0001                       column=base_info:name, timestamp=1522321247732, value=zhangsan                    </span><br><span class="line"> user0001                     column=base_info:name, timestamp=1522320801670, value=zhangsan1                   </span><br><span class="line">2 row(s) in 0.0620 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):028:0&gt;</span><br></pre></td></tr></table></figure><h3 id="删delete"><a href="#删delete" class="headerlink" title="删delete"></a>删delete</h3><p>删除user_info表row key为rk0001，列标示符为base_info:name的数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):028:0&gt; delete &apos;user_info&apos;, &apos;rk0001&apos;, &apos;base_info:name&apos;</span><br><span class="line">0 row(s) in 0.0780 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):029:0&gt; scan &apos;user_info&apos;, &#123;COLUMNS =&gt; &apos;base_info&apos;&#125;</span><br><span class="line">ROW                           COLUMN+CELL                                                                       </span><br><span class="line"> user0001                     column=base_info:name, timestamp=1522320801670, value=zhangsan1                   </span><br><span class="line">1 row(s) in 0.0530 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):030:0&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase学习之路 （二）HBase集群安装</title>
      <link href="/2018-06-02-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%EF%BC%89HBase%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85.html"/>
      <url>/2018-06-02-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%EF%BC%89HBase%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85.html</url>
      
        <content type="html"><![CDATA[<p>** HBase学习之路 （二）HBase集群安装：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        HBase学习之路 （二）HBase集群安装</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><p>1、HBase 依赖于 HDFS 做底层的数据存储</p><p>2、HBase 依赖于 MapReduce 做数据计算</p><p>3、HBase 依赖于 ZooKeeper 做服务协调</p><p>4、HBase源码是java编写的，安装需要依赖JDK</p><h2 id="版本选择"><a href="#版本选择" class="headerlink" title="版本选择"></a>版本选择</h2><p>打开官方的版本说明<a href="http://hbase.apache.org/1.2/book.html" target="_blank" rel="noopener">http://hbase.apache.org/1.2/book.html</a></p><h3 id="JDK的选择"><a href="#JDK的选择" class="headerlink" title="JDK的选择"></a>JDK的选择</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180329113922497-155780788.png" alt="img"></p><h3 id="Hadoop的选择"><a href="#Hadoop的选择" class="headerlink" title="Hadoop的选择"></a>Hadoop的选择</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180329114135620-1227767001.png" alt="img"></p><p> <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180329114414143-737897699.png" alt="img"></p><p>此处我们的hadoop版本用的的是2.7.5，HBase选择的版本是1.2.6</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="1、zookeeper的安装"><a href="#1、zookeeper的安装" class="headerlink" title="1、zookeeper的安装"></a>1、zookeeper的安装</h3><p>参考<a href="http://www.cnblogs.com/qingyunzong/p/8619184.html" target="_blank" rel="noopener">http://www.cnblogs.com/qingyunzong/p/8619184.html</a></p><h3 id="2、Hadoopd的安装"><a href="#2、Hadoopd的安装" class="headerlink" title="2、Hadoopd的安装"></a>2、Hadoopd的安装</h3><p>参考<a href="http://www.cnblogs.com/qingyunzong/p/8634335.html" target="_blank" rel="noopener">http://www.cnblogs.com/qingyunzong/p/8634335.html</a></p><h3 id="3、下载安装包"><a href="#3、下载安装包" class="headerlink" title="3、下载安装包"></a>3、下载安装包</h3><p>找到官网下载 hbase 安装包 hbase-1.2.6-bin.tar.gz，这里给大家提供一个下载地址： <a href="http://mirrors.hust.edu.cn/apache/hbase/" target="_blank" rel="noopener">http://mirrors.hust.edu.cn/apache/hbase/</a></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180329114720014-1743547970.png" alt="img"></p><h3 id="4、上传服务器并解压缩到指定目录"><a href="#4、上传服务器并解压缩到指定目录" class="headerlink" title="4、上传服务器并解压缩到指定目录"></a>4、上传服务器并解压缩到指定目录</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ ls</span><br><span class="line">apps  data  hbase-1.2.6-bin.tar.gz  hello.txt  log  zookeeper.out</span><br><span class="line">[hadoop@hadoop1 ~]$ tar -zxvf hbase-1.2.6-bin.tar.gz -C apps/</span><br></pre></td></tr></table></figure><h3 id="5、修改配置文件"><a href="#5、修改配置文件" class="headerlink" title="5、修改配置文件"></a>5、修改配置文件</h3><p>配置文件目录在安装包的conf文件夹中</p><h4 id="（1）修改hbase-env-sh"><a href="#（1）修改hbase-env-sh" class="headerlink" title="（1）修改hbase-env.sh"></a>（1）修改hbase-env.sh</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ vi hbase-env.sh</span><br><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_73export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180329115352886-1492779607.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180329115451607-2133828594.png" alt="img"></p><h4 id="（2）修改hbase-site-xml"><a href="#（2）修改hbase-site-xml" class="headerlink" title="（2）修改hbase-site.xml"></a>（2）修改hbase-site.xml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ vi hbase-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;!-- 指定 hbase 在 HDFS 上存储的路径 --&gt;</span><br><span class="line">                &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;hdfs://myha01/hbase126&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;!-- 指定 hbase 是分布式的 --&gt;</span><br><span class="line">                &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;!-- 指定 zk 的地址，多个用“,”分割 --&gt;</span><br><span class="line">                &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;hadoop1:2181,hadoop2:2181,hadoop3:2181,hadoop4:2181&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="（3）修改regionservers"><a href="#（3）修改regionservers" class="headerlink" title="（3）修改regionservers"></a>（3）修改regionservers</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ vi regionservers </span><br><span class="line">hadoop1</span><br><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br><span class="line">hadoop4</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180329125540901-2106027549.png" alt="img"></p><h4 id="（4）修改backup-masters"><a href="#（4）修改backup-masters" class="headerlink" title="（4）修改backup-masters"></a>（4）修改backup-masters</h4><p>该文件是不存在的，先自行创建</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ vi backup-masters</span><br><span class="line">hadoop4</span><br></pre></td></tr></table></figure><h4 id="（5）修改hdfs-site-xml-和-core-site-xml"><a href="#（5）修改hdfs-site-xml-和-core-site-xml" class="headerlink" title="（5）修改hdfs-site.xml 和 core-site.xml"></a>（5）修改<strong>hdfs-site.xml 和 core-site.xml</strong></h4><p><strong>最重要一步，要把 hadoop 的 hdfs-site.xml 和 core-site.xml 放到 hbase-1.2.6/conf 下</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ cd ~/apps/hadoop-2.7.5/etc/hadoop/</span><br><span class="line">[hadoop@hadoop1 hadoop]$ cp core-site.xml hdfs-site.xml ~/apps/hbase-1.2.6/conf/</span><br></pre></td></tr></table></figure><h3 id="6、将HBase安装包分发到其他节点"><a href="#6、将HBase安装包分发到其他节点" class="headerlink" title="6、将HBase安装包分发到其他节点"></a>6、将HBase安装包分发到其他节点</h3><p>分发之前先删除HBase目录下的docs文件夹，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hbase-1.2.6]$ rm -rf docs/</span><br></pre></td></tr></table></figure><p>在进行分发</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 apps]$ scp -r hbase-1.2.6/ hadoop2:$PWD</span><br><span class="line">[hadoop@hadoop1 apps]$ scp -r hbase-1.2.6/ hadoop3:$PWD</span><br><span class="line">[hadoop@hadoop1 apps]$ scp -r hbase-1.2.6/ hadoop4:$PWD</span><br></pre></td></tr></table></figure><h3 id="7、-同步时间"><a href="#7、-同步时间" class="headerlink" title="7、 同步时间"></a>7、 同步时间</h3><p>HBase 集群对于时间的同步要求的比 HDFS 严格，所以，集群启动之前千万记住要进行 时间同步，要求相差不要超过 30s</p><h3 id="8、配置环境变量"><a href="#8、配置环境变量" class="headerlink" title="8、配置环境变量"></a>8、配置环境变量</h3><p>所有服务器都有进行配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 apps]$ vi ~/.bashrc </span><br><span class="line">#HBase</span><br><span class="line">export HBASE_HOME=/home/hadoop/apps/hbase-1.2.6</span><br><span class="line">export PATH=$PATH:$HBASE_HOME/bin</span><br></pre></td></tr></table></figure><p>使环境变量立即生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 apps]$ source ~/.bashrc</span><br></pre></td></tr></table></figure><h2 id="启动HBase集群"><a href="#启动HBase集群" class="headerlink" title="启动HBase集群"></a>启动HBase集群</h2><p>严格按照启动顺序进行</p><h3 id="1、启动zookeeper集群"><a href="#1、启动zookeeper集群" class="headerlink" title="1、启动zookeeper集群"></a>1、启动zookeeper集群</h3><p>每个zookeeper节点都要执行以下命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 apps]$ zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /home/hadoop/apps/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">[hadoop@hadoop1 apps]$</span><br></pre></td></tr></table></figure><h3 id="2、启动HDFS集群及YARN集群"><a href="#2、启动HDFS集群及YARN集群" class="headerlink" title="2、启动HDFS集群及YARN集群"></a>2、启动HDFS集群及YARN集群</h3><p>如果需要运行MapReduce程序则启动yarn集群，否则不需要启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 apps]$ start-dfs.sh</span><br><span class="line">Starting namenodes on [hadoop1 hadoop2]</span><br><span class="line">hadoop2: starting namenode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-namenode-hadoop2.out</span><br><span class="line">hadoop1: starting namenode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-namenode-hadoop1.out</span><br><span class="line">hadoop3: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop3.out</span><br><span class="line">hadoop4: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop4.out</span><br><span class="line">hadoop2: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop2.out</span><br><span class="line">hadoop1: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop1.out</span><br><span class="line">Starting journal nodes [hadoop1 hadoop2 hadoop3]</span><br><span class="line">hadoop3: starting journalnode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-journalnode-hadoop3.out</span><br><span class="line">hadoop2: starting journalnode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-journalnode-hadoop2.out</span><br><span class="line">hadoop1: starting journalnode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-journalnode-hadoop1.out</span><br><span class="line">Starting ZK Failover Controllers on NN hosts [hadoop1 hadoop2]</span><br><span class="line">hadoop2: starting zkfc, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-zkfc-hadoop2.out</span><br><span class="line">hadoop1: starting zkfc, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-zkfc-hadoop1.out</span><br><span class="line">[hadoop@hadoop1 apps]$</span><br></pre></td></tr></table></figure><p>启动完成之后检查以下namenode的状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 apps]$ hdfs haadmin -getServiceState nn1</span><br><span class="line">standby</span><br><span class="line">[hadoop@hadoop1 apps]$ hdfs haadmin -getServiceState nn2</span><br><span class="line">active</span><br><span class="line">[hadoop@hadoop1 apps]$</span><br></pre></td></tr></table></figure><h3 id="3、启动HBase"><a href="#3、启动HBase" class="headerlink" title="3、启动HBase"></a>3、启动HBase</h3><p>保证 ZooKeeper 集群和 HDFS 集群启动正常的情况下启动 HBase 集群 启动命令：start-hbase.sh，在哪台节点上执行此命令，哪个节点就是主节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ start-hbase.sh</span><br><span class="line">starting master, logging to /home/hadoop/apps/hbase-1.2.6/logs/hbase-hadoop-master-hadoop1.out</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0</span><br><span class="line">hadoop3: starting regionserver, logging to /home/hadoop/apps/hbase-1.2.6/logs/hbase-hadoop-regionserver-hadoop3.out</span><br><span class="line">hadoop4: starting regionserver, logging to /home/hadoop/apps/hbase-1.2.6/logs/hbase-hadoop-regionserver-hadoop4.out</span><br><span class="line">hadoop2: starting regionserver, logging to /home/hadoop/apps/hbase-1.2.6/logs/hbase-hadoop-regionserver-hadoop2.out</span><br><span class="line">hadoop3: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0</span><br><span class="line">hadoop3: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0</span><br><span class="line">hadoop4: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0</span><br><span class="line">hadoop4: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0</span><br><span class="line">hadoop2: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0</span><br><span class="line">hadoop2: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0</span><br><span class="line">hadoop1: starting regionserver, logging to /home/hadoop/apps/hbase-1.2.6/logs/hbase-hadoop-regionserver-hadoop1.out</span><br><span class="line">hadoop4: starting master, logging to /home/hadoop/apps/hbase-1.2.6/logs/hbase-hadoop-master-hadoop4.out</span><br><span class="line">[hadoop@hadoop1 conf]$</span><br></pre></td></tr></table></figure><p>观看启动日志可以看到：</p><p>（1）首先在命令执行节点启动 master</p><p>（2）然后分别在 hadoop02,hadoop03,hadoop04,hadoop05 启动 regionserver</p><p>（3）然后在 backup-masters 文件中配置的备节点上再启动一个 master 主进程</p><h2 id="验证启动是否正常"><a href="#验证启动是否正常" class="headerlink" title="验证启动是否正常"></a>验证启动是否正常</h2><h3 id="1、检查各进程是否启动正常"><a href="#1、检查各进程是否启动正常" class="headerlink" title="1、检查各进程是否启动正常"></a>1、检查各进程是否启动正常</h3><p> 主节点和备用节点都启动 hmaster 进程</p><p> 各从节点都启动 hregionserver 进程</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180329132223188-429838759.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180329132240867-541766781.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180329132301074-29966918.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180329132326294-1697748614.png" alt="img"></p><p>按照对应的配置信息各个节点应该要启动的进程如上图所示</p><h3 id="2、通过访问浏览器页面"><a href="#2、通过访问浏览器页面" class="headerlink" title="2、通过访问浏览器页面"></a>2、通过访问浏览器页面</h3><p>hadoop1</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180329132509206-920100773.png" alt="img"></p><p>hadop4</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180329132546255-325710346.png" alt="img"></p><p>从图中可以看出hadoop4是备用节点</p><h3 id="3、验证高可用"><a href="#3、验证高可用" class="headerlink" title="3、验证高可用"></a>3、验证高可用</h3><p>干掉hadoop1上的hbase进程，观察备用节点是否启用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ jps</span><br><span class="line">4960 HMaster</span><br><span class="line">2960 QuorumPeerMain</span><br><span class="line">3169 NameNode</span><br><span class="line">3699 DFSZKFailoverController</span><br><span class="line">3285 DataNode</span><br><span class="line">5098 HRegionServer</span><br><span class="line">5471 Jps</span><br><span class="line">3487 JournalNode</span><br><span class="line">[hadoop@hadoop1 conf]$ kill -9 4960</span><br></pre></td></tr></table></figure><p> hadoop1界面访问不了</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180329132808177-232739231.png" alt="img"></p><p>hadoop4变成主节点</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180329132858305-523100964.png" alt="img"></p><h3 id="4、如果有节点相应的进程没有启动，那么可以手动启动"><a href="#4、如果有节点相应的进程没有启动，那么可以手动启动" class="headerlink" title="4、如果有节点相应的进程没有启动，那么可以手动启动"></a>4、如果有节点相应的进程没有启动，那么可以手动启动</h3><p>启动HMaster进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 conf]$ jps</span><br><span class="line">3360 Jps</span><br><span class="line">2833 JournalNode</span><br><span class="line">2633 QuorumPeerMain</span><br><span class="line">3179 HRegionServer</span><br><span class="line">2732 DataNode</span><br><span class="line">[hadoop@hadoop3 conf]$ hbase-daemon.sh start master</span><br><span class="line">starting master, logging to /home/hadoop/apps/hbase-1.2.6/logs/hbase-hadoop-master-hadoop3.out</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0</span><br><span class="line">[hadoop@hadoop3 conf]$ jps</span><br><span class="line">2833 JournalNode</span><br><span class="line">3510 Jps</span><br><span class="line">3432 HMaster</span><br><span class="line">2633 QuorumPeerMain</span><br><span class="line">3179 HRegionServer</span><br><span class="line">2732 DataNode</span><br><span class="line">[hadoop@hadoop3 conf]$</span><br></pre></td></tr></table></figure><p>启动HRegionServer进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 conf]$ hbase-daemon.sh start regionserver</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase学习之路 （一）HBase基础介绍</title>
      <link href="/2018-06-01-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%80%EF%BC%89HBase%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D.html"/>
      <url>/2018-06-01-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%80%EF%BC%89HBase%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D.html</url>
      
        <content type="html"><![CDATA[<p>** HBase学习之路 （一）HBase基础介绍：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        HBase学习之路 （一）HBase基础介绍</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="产生背景"><a href="#产生背景" class="headerlink" title="产生背景"></a>产生背景</h2><p>自 1970 年以来，关系数据库用于数据存储和维护有关问题的解决方案。大数据的出现后， 好多公司实现处理大数据并从中受益，并开始选择像 Hadoop 的解决方案。Hadoop 使用分 布式文件系统，用于存储大数据，并使用 MapReduce 来处理。Hadoop 擅长于存储各种格式 的庞大的数据，任意的格式甚至非结构化的处理。</p><p>Hadoop 的限制</p><p>Hadoop 只能执行批量处理，并且只以顺序方式访问数据。这意味着必须搜索整个数据集， 即使是最简单的搜索工作。 当处理结果在另一个庞大的数据集，也是按顺序处理一个巨大的数据集。在这一点上，一个 新的解决方案，需要访问数据中的任何点（随机访问）单元。</p><p>Hadoop 随机存取数据库</p><p>应用程序，如 HBase，Cassandra，CouchDB，Dynamo 和 MongoDB 都是一些存储大量数据和 以随机方式访问数据的数据库。</p><p>总结：</p><p>（1）海量数据量存储成为瓶颈，单台机器无法负载大量数据</p><p>（2）单台机器 IO 读写请求成为海量数据存储时候高并发大规模请求的瓶颈</p><p>（3）随着数据规模越来越大，大量业务场景开始考虑数据存储横向水平扩展，使得存储服 务可以增加/删除，而目前的关系型数据库更专注于一台机器</p><h2 id="HBase简介"><a href="#HBase简介" class="headerlink" title="HBase简介"></a>HBase简介</h2><p><strong>HBase 是 BigTable 的开源（源码使用 Java 编写）版本。是 Apache Hadoop 的数据库，是建 立在 HDFS 之上，被设计用来提供高可靠性、高性能、列存储、可伸缩、多版本的 NoSQL 的分布式数据存储系统，实现对大型数据的实时、随机的读写访问。</strong></p><p><strong>HBase 依赖于 HDFS 做底层的数据存储，BigTable 依赖 Google GFS 做数据存储</strong></p><p><strong>HBase 依赖于 MapReduce 做数据计算，BigTable 依赖 Google MapReduce 做数据计算</strong></p><p><strong>HBase 依赖于 ZooKeeper 做服务协调，BigTable 依赖 Google Chubby 做服务协调</strong></p><p>NoSQL = NO SQL</p><p>NoSQL = Not Only SQL：会有一些把 NoSQL 数据的原生查询语句封装成 SQL，比如 HBase 就有 Phoenix 工具</p><h3 id="关系型数据库-和-非关系型数据库的典型代表"><a href="#关系型数据库-和-非关系型数据库的典型代表" class="headerlink" title="关系型数据库 和 非关系型数据库的典型代表"></a>关系型数据库 和 非关系型数据库的典型代表</h3><p>NoSQL：hbase, redis, mongodb</p><p>RDBMS：mysql,oracle,sql server,db2</p><h3 id="HBase-这个-NoSQL-数据库的要点"><a href="#HBase-这个-NoSQL-数据库的要点" class="headerlink" title="HBase 这个 NoSQL 数据库的要点"></a>HBase 这个 NoSQL 数据库的要点</h3><p>① 它介于 NoSQL 和 RDBMS 之间，<strong>仅能通过主键(rowkey)和主键的 range 来检索数据</strong></p><p>② HBase 查询数据功能很简单，<strong>不支持 join 等复杂操作</strong></p><p>③ 不支持复杂的事务，<strong>只支持行级事务</strong>(可通过 hive 支持来实现多表 join 等复杂操作)。</p><p>④ <strong>HBase 中支持的数据类型：byte[]（底层所有数据的存储都是字节数组）</strong></p><p>⑤ <strong>主要用来存储结构化和半结构化的松散数据。</strong></p><h3 id="结构化、半结构化和非结构化"><a href="#结构化、半结构化和非结构化" class="headerlink" title="结构化、半结构化和非结构化"></a>结构化、半结构化和非结构化</h3><p><strong>结构化</strong>：数据结构字段含义确定，清晰，典型的如数据库中的表结构</p><p><strong>半结构化</strong>：具有一定结构，但语义不够确定，典型的如 HTML 网页，有些字段是确定的(title)， 有些不确定(table)</p><p><strong>非结构化</strong>：杂乱无章的数据，很难按照一个概念去进行抽取，无规律性</p><p>与 Hadoop 一样，HBase 目标主要依靠横向扩展，通过不断增加廉价的商用服务器，来增加 计算和存储能力。</p><h3 id="HBase-中的表特点"><a href="#HBase-中的表特点" class="headerlink" title="HBase 中的表特点"></a>HBase 中的表特点</h3><p>1、<strong>大</strong>：一个表可以有上十亿行，上百万列</p><p>2、<strong>面向列</strong>：面向列(族)的存储和权限控制，列(簇)独立检索。</p><p>3、<strong>稀疏</strong>：对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。</p><p>4、<strong>无模式</strong>：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一 张表中不同的行可以有截然不同的列</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180328184005972-1939218640.png" alt="img"></p><h2 id="HBase表结构逻辑视图"><a href="#HBase表结构逻辑视图" class="headerlink" title="HBase表结构逻辑视图"></a>HBase表结构逻辑视图</h2><p>初次接触HBase，可能看到以下描述会懵：<strong>“基于列存储”，“稀疏MAP”，“RowKey”,“ColumnFamily”。</strong></p><p>其实没那么高深，我们需要分两步来理解HBase, 就能够理解为什么HBase能够“快速地”“分布式地”处理“大量数据”了。</p><p>　　<strong>1.内存结构</strong></p><p>　　<strong>2.文件存储结构</strong></p><h3 id="名词概念"><a href="#名词概念" class="headerlink" title="名词概念"></a><strong>名词概念</strong></h3><p>加入我们有如下一张表</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180328185514080-1540820263.png" alt="img"></p><h4 id="Rowkey的概念"><a href="#Rowkey的概念" class="headerlink" title="Rowkey的概念"></a>Rowkey的概念</h4><p><strong>Rowkey的概念和mysql中的主键是完全一样的，Hbase使用Rowkey来唯一的区分某一行的数据。</strong></p><p>由于Hbase只支持3中查询方式：</p><p>1、基于Rowkey的单行查询</p><p>2、基于Rowkey的范围扫描</p><p>3、全表扫描</p><p>因此，Rowkey对Hbase的性能影响非常大，Rowkey的设计就显得尤为的重要。设计的时候要兼顾基于Rowkey的单行查询也要键入Rowkey的范围扫描。具体Rowkey要如何设计后续会整理相关的文章做进一步的描述。这里大家只要有一个概念就是Rowkey的设计极为重要。</p><p>rowkey 行键可以是任意字符串(最大长度是 <strong>64KB</strong>，实际应用中长度一般为 10-100bytes)，最好是 16。在 HBase 内部，rowkey 保存为字节数组。<strong>HBase 会对表中的数据按照 rowkey 排序 (字典顺序)</strong></p><h4 id="Column的概念"><a href="#Column的概念" class="headerlink" title="Column的概念"></a><strong>Column的概念</strong></h4><p>列，可理解成MySQL列。</p><h4 id="ColumnFamily的概念"><a href="#ColumnFamily的概念" class="headerlink" title="ColumnFamily的概念"></a>ColumnFamily的概念</h4><p>列族, HBase引入的概念。</p><p>Hbase通过列族划分数据的存储，列族下面可以包含任意多的列，实现灵活的数据存取。就像是家族的概念，我们知道一个家族是由于很多个的家庭组成的。列族也类似，列族是由一个一个的列组成（任意多）。</p><p>Hbase表的创建的时候就必须指定列族。就像关系型数据库创建的时候必须指定具体的列是一样的。</p><p>Hbase的列族不是越多越好，官方推荐的是列族最好小于或者等于3。我们使用的场景一般是1个列族。</p><h4 id="TimeStamp的概念"><a href="#TimeStamp的概念" class="headerlink" title="TimeStamp的概念"></a>TimeStamp的概念</h4><p>TimeStamp对Hbase来说至关重要，因为它是实现Hbase多版本的关键。在Hbase中使用不同的timestame来标识相同rowkey行对应的不通版本的数据。</p><p>HBase 中通过 rowkey 和 columns 确定的为一个存储单元称为 cell。每个 cell 都保存着同一份 数据的多个版本。版本通过时间戳来索引。时间戳的类型是 64 位整型。时间戳可以由 hbase(在数据写入时</p><p>自动)赋值，此时时间戳是精确到毫秒的当前系统时间。时间戳也可以由 客户显式赋值。如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。 每个 cell 中，不同版本的数据按照时间</p><p>倒序排序，即最新的数据排在最前面。</p><p>为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，hbase 提供了两种数据版 本回收方式：</p><p>　　<strong>保存数据的最后 n 个版本</strong></p><p>　　<strong>保存最近一段时间内的版本（设置数据的生命周期 TTL）。</strong></p><p>用户可以针对每个列簇进行设置。</p><h4 id="单元格（Cell）"><a href="#单元格（Cell）" class="headerlink" title="单元格（Cell）"></a>单元格（Cell）</h4><p>由{rowkey, column( = + ), version} 唯一确定的单元。 Cell 中的数据是没有类型的，全部是字节码形式存贮。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka学习之路 （五）Kafka在zookeeper中的存储</title>
      <link href="/2018-05-08-Kafka%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%94%EF%BC%89Kafka%E5%9C%A8zookeeper%E4%B8%AD%E7%9A%84%E5%AD%98%E5%82%A8.html"/>
      <url>/2018-05-08-Kafka%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%94%EF%BC%89Kafka%E5%9C%A8zookeeper%E4%B8%AD%E7%9A%84%E5%AD%98%E5%82%A8.html</url>
      
        <content type="html"><![CDATA[<p>** Kafka学习之路 （五）Kafka在zookeeper中的存储：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Kafka学习之路 （五）Kafka在zookeeper中的存储</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、Kafka在zookeeper中存储结构图"><a href="#一、Kafka在zookeeper中存储结构图" class="headerlink" title="一、Kafka在zookeeper中存储结构图"></a>一、Kafka在zookeeper中存储结构图</h2><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508101652574-1613892176.png" alt="img"></p><h2 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h2><h3 id="2-1-topic注册信息"><a href="#2-1-topic注册信息" class="headerlink" title="2.1　topic注册信息"></a>2.1　topic注册信息</h3><p>/brokers/topics/[topic] :</p><p>存储某个topic的partitions所有分配信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] get /brokers/topics/topic2</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508103023550-1689525175.png" alt="img"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Schema:</span><br><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;: &quot;版本编号目前固定为数字1&quot;,</span><br><span class="line">    &quot;partitions&quot;: &#123;</span><br><span class="line">        &quot;partitionId编号&quot;: [</span><br><span class="line">            同步副本组brokerId列表</span><br><span class="line">        ],</span><br><span class="line">        &quot;partitionId编号&quot;: [</span><br><span class="line">            同步副本组brokerId列表</span><br><span class="line">        ],</span><br><span class="line">        .......</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">Example:</span><br><span class="line">&#123;</span><br><span class="line">&quot;version&quot;: 1,</span><br><span class="line">&quot;partitions&quot;: &#123;</span><br><span class="line">&quot;2&quot;: [1, 2, 3],</span><br><span class="line">&quot;1&quot;: [0, 1, 2],</span><br><span class="line">&quot;0&quot;: [3, 0, 1],</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-2-partition状态信息"><a href="#2-2-partition状态信息" class="headerlink" title="2.2　partition状态信息"></a>2.2　partition状态信息</h3><p>/brokers/topics/[topic]/partitions/[0…N]  其中[0..N]表示partition索引号</p><p>/brokers/topics/[topic]/partitions/[partitionId]/state</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508103535274-360887579.png" alt="img"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Schema:</span><br><span class="line">&#123;</span><br><span class="line">&quot;controller_epoch&quot;: 表示kafka集群中的中央控制器选举次数,</span><br><span class="line">&quot;leader&quot;: 表示该partition选举leader的brokerId,</span><br><span class="line">&quot;version&quot;: 版本编号默认为1,</span><br><span class="line">&quot;leader_epoch&quot;: 该partition leader选举次数,</span><br><span class="line">&quot;isr&quot;: [同步副本组brokerId列表]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">Example:</span><br><span class="line">&#123;</span><br><span class="line">&quot;controller_epoch&quot;: 1,</span><br><span class="line">&quot;leader&quot;: 3,</span><br><span class="line">&quot;version&quot;: 1,</span><br><span class="line">&quot;leader_epoch&quot;: 0,</span><br><span class="line">&quot;isr&quot;: [3, 0, 1]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-Broker注册信息"><a href="#2-3-Broker注册信息" class="headerlink" title="2.3　Broker注册信息"></a>2.3　Broker注册信息</h3><p>/brokers/ids/[0…N]                 </p><p>每个broker的配置文件中都需要指定一个数字类型的id(全局不可重复),此节点为临时znode(EPHEMERAL)</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508103818455-1476762306.png" alt="img"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Schema:</span><br><span class="line">&#123;</span><br><span class="line">&quot;jmx_port&quot;: jmx端口号,</span><br><span class="line">&quot;timestamp&quot;: kafka broker初始启动时的时间戳,</span><br><span class="line">&quot;host&quot;: 主机名或ip地址,</span><br><span class="line">&quot;version&quot;: 版本编号默认为1,</span><br><span class="line">&quot;port&quot;: kafka broker的服务端端口号,由server.properties中参数port确定</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">Example:</span><br><span class="line">&#123;</span><br><span class="line">&quot;jmx_port&quot;: -1,</span><br><span class="line">&quot;timestamp&quot;:&quot;1525741823119&quot;</span><br><span class="line">&quot;version&quot;: 1,</span><br><span class="line">&quot;host&quot;: &quot;hadoop1&quot;,</span><br><span class="line">&quot;port&quot;: 9092</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-4-Controller-epoch"><a href="#2-4-Controller-epoch" class="headerlink" title="2.4　Controller epoch"></a>2.4　Controller epoch</h3><p>/controller_epoch –&gt;  int (epoch)   </p><p>此值为一个数字,kafka集群中第一个broker第一次启动时为1，以后只要集群中center controller中央控制器所在broker变更或挂掉，就会重新选举新的center controller，每次center controller变更controller_epoch值就会 + 1; </p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508104057913-1808088971.png" alt="img"></p><h3 id="2-5-Controller注册信息"><a href="#2-5-Controller注册信息" class="headerlink" title="2.5　Controller注册信息"></a>2.5　Controller注册信息</h3><p>/controller -&gt; int (broker id of the controller)  存储center controller中央控制器所在kafka broker的信息</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508104206487-400676103.png" alt="img"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Schema:</span><br><span class="line">&#123;</span><br><span class="line">&quot;version&quot;: 版本编号默认为1,</span><br><span class="line">&quot;brokerid&quot;: kafka集群中broker唯一编号,</span><br><span class="line">&quot;timestamp&quot;: kafka broker中央控制器变更时的时间戳</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">Example:</span><br><span class="line">&#123;</span><br><span class="line">&quot;version&quot;: 1,</span><br><span class="line">&quot;brokerid&quot;: 0,</span><br><span class="line">&quot;timestamp&quot;: &quot;1525741822769&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-6-补充Consumer-and-Consumer-group"><a href="#2-6-补充Consumer-and-Consumer-group" class="headerlink" title="2.6　补充Consumer and Consumer group"></a>2.6　补充Consumer and Consumer group</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508104651193-86573907.png" alt="img"></p><p><strong>a.每个consumer客户端被创建时,会向zookeeper注册自己的信息;</strong><br><strong>b.此作用主要是为了”负载均衡”.</strong><br><strong>c.同一个Consumer Group中的Consumers，Kafka将相应Topic中的每个消息只发送给其中一个Consumer。</strong><br><strong>d.Consumer Group中的每个Consumer读取Topic的一个或多个Partitions，并且是唯一的Consumer；</strong><br><strong>e.一个Consumer group的多个consumer的所有线程依次有序地消费一个topic的所有partitions,如果Consumer group中所有consumer总线程大于partitions数量，则会出现空闲情况;</strong></p><blockquote><p><strong>举例说明：</strong></p></blockquote><blockquote><p><strong>kafka集群中创建一个topic为report-log   4 partitions 索引编号为0,1,2,3</strong></p></blockquote><blockquote><p><strong>假如有目前有三个消费者node：注意–&gt;一个consumer中一个消费线程可以消费一个或多个partition.</strong></p></blockquote><blockquote><p><strong>如果每个consumer创建一个consumer thread线程,各个node消费情况如下，node1消费索引编号为0,1分区，node2费索引编号为2,node3费索引编号为3</strong></p></blockquote><blockquote><p><strong>如果每个consumer创建2个consumer thread线程，各个node消费情况如下(是从consumer node先后启动状态来确定的)，node1消费索引编号为0,1分区；node2费索引编号为2,3；node3为空闲状态</strong></p></blockquote><p><strong>总结：</strong><br><strong>从以上可知，Consumer Group中各个consumer是根据先后启动的顺序有序消费一个topic的所有partitions的。</strong></p><p><strong>如果Consumer Group中所有consumer的总线程数大于partitions数量，则可能consumer thread或consumer会出现空闲状态</strong>。</p><h3 id="2-7-Consumer均衡算法"><a href="#2-7-Consumer均衡算法" class="headerlink" title="2.7　Consumer均衡算法"></a>2.7　Consumer均衡算法</h3><p><strong>当一个group中,有consumer加入或者离开时,会触发partitions均衡.均衡的最终目的,是提升topic的并发消费能力.</strong><br><strong>1) 假如topic1,具有如下partitions: P0,P1,P2,P3</strong><br><strong>2) 加入group中,有如下consumer: C0,C1</strong><br><strong>3) 首先根据partition索引号对partitions排序: P0,P1,P2,P3</strong><br><strong>4) 根据(consumer.id + ‘-‘+ thread序号)排序: C0,C1</strong><br><strong>5) 计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整)</strong><br><strong>6) 然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1) * M -1)]</strong></p><h3 id="2-8-Consumer注册信息"><a href="#2-8-Consumer注册信息" class="headerlink" title="2.8　Consumer注册信息"></a>2.8　Consumer注册信息</h3><p>每个consumer都有一个唯一的ID(consumerId可以通过配置文件指定,也可以由系统生成),此id用来标记消费者信息.</p><p>/consumers/[groupId]/ids/[consumerIdString]</p><p>是一个临时的znode,此节点的值为请看consumerIdString产生规则,即表示此consumer目前所消费的topic + partitions列表.</p><p>consumerId产生规则：</p><blockquote><p>StringconsumerUuid = null;<br>    if(config.consumerId!=null &amp;&amp; config.consumerId)<br>      consumerUuid = consumerId;<br>    else {<br>      String uuid = UUID.randomUUID()<br>      consumerUuid = “%s-%d-%s”.format(<br>        InetAddress.getLocalHost.getHostName, System.currentTimeMillis,<br>        uuid.getMostSignificantBits().toHexString.substring(0,8));</p><p>​     }<br>​     String consumerIdString = config.groupId + “_” + consumerUuid;</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 11] get /consumers/console-consumer-2304/ids/console-consumer-2304_hadoop2-1525747915241-6b48ff32</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508105321039-416763241.png" alt="img"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Schema:</span><br><span class="line">&#123;</span><br><span class="line">&quot;version&quot;: 版本编号默认为1,</span><br><span class="line">&quot;subscription&quot;: &#123; //订阅topic列表</span><br><span class="line">&quot;topic名称&quot;: consumer中topic消费者线程数</span><br><span class="line">&#125;,</span><br><span class="line">&quot;pattern&quot;: &quot;static&quot;,</span><br><span class="line">&quot;timestamp&quot;: &quot;consumer启动时的时间戳&quot;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">Example:</span><br><span class="line">&#123;</span><br><span class="line">&quot;version&quot;: 1,</span><br><span class="line">&quot;subscription&quot;: &#123;</span><br><span class="line">&quot;topic2&quot;: 1</span><br><span class="line">&#125;,</span><br><span class="line">&quot;pattern&quot;: &quot;white_list&quot;,</span><br><span class="line">&quot;timestamp&quot;: &quot;1525747915336&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-9-Consumer-owner"><a href="#2-9-Consumer-owner" class="headerlink" title="2.9　Consumer owner"></a>2.9　Consumer owner</h3><p>/consumers/[groupId]/owners/[topic]/[partitionId] -&gt; consumerIdString + threadId索引编号</p><p>a) 首先进行”Consumer Id注册”;</p><p>b) 然后在”Consumer id 注册”节点下注册一个watch用来监听当前group中其他consumer的”退出”和”加入”;只要此znode path下节点列表变更,都会触发此group下consumer的负载均衡.(比如一个consumer失效,那么其他consumer接管partitions).</p><p>c) 在”Broker id 注册”节点下,注册一个watch用来监听broker的存活情况;如果broker列表变更,将会触发所有的groups下的consumer重新balance.</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508105839526-553140898.png" alt="img"></p><h3 id="2-10-Consumer-offset"><a href="#2-10-Consumer-offset" class="headerlink" title="2.10　Consumer offset"></a>2.10　Consumer offset</h3><p>/consumers/[groupId]/offsets/[topic]/[partitionId] -&gt; long (offset)</p><p>用来跟踪每个consumer目前所消费的partition中最大的offset</p><p>此znode为持久节点,可以看出offset跟group_id有关,以表明当消费者组(consumer group)中一个消费者失效,</p><p>重新触发balance,其他consumer可以继续消费.</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508110049469-140963000.png" alt="img"></p><h3 id="2-11-Re-assign-partitions"><a href="#2-11-Re-assign-partitions" class="headerlink" title="2.11　Re-assign partitions"></a>2.11　Re-assign partitions</h3><p>/admin/reassign_partitions</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   &quot;fields&quot;:[</span><br><span class="line">      &#123;</span><br><span class="line">         &quot;name&quot;:&quot;version&quot;,</span><br><span class="line">         &quot;type&quot;:&quot;int&quot;,</span><br><span class="line">         &quot;doc&quot;:&quot;version id&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">         &quot;name&quot;:&quot;partitions&quot;,</span><br><span class="line">         &quot;type&quot;:&#123;</span><br><span class="line">            &quot;type&quot;:&quot;array&quot;,</span><br><span class="line">            &quot;items&quot;:&#123;</span><br><span class="line">               &quot;fields&quot;:[</span><br><span class="line">                  &#123;</span><br><span class="line">                     &quot;name&quot;:&quot;topic&quot;,</span><br><span class="line">                     &quot;type&quot;:&quot;string&quot;,</span><br><span class="line">                     &quot;doc&quot;:&quot;topic of the partition to be reassigned&quot;</span><br><span class="line">                  &#125;,</span><br><span class="line">                  &#123;</span><br><span class="line">                     &quot;name&quot;:&quot;partition&quot;,</span><br><span class="line">                     &quot;type&quot;:&quot;int&quot;,</span><br><span class="line">                     &quot;doc&quot;:&quot;the partition to be reassigned&quot;</span><br><span class="line">                  &#125;,</span><br><span class="line">                  &#123;</span><br><span class="line">                     &quot;name&quot;:&quot;replicas&quot;,</span><br><span class="line">                     &quot;type&quot;:&quot;array&quot;,</span><br><span class="line">                     &quot;items&quot;:&quot;int&quot;,</span><br><span class="line">                     &quot;doc&quot;:&quot;a list of replica ids&quot;</span><br><span class="line">                  &#125;</span><br><span class="line">               ],</span><br><span class="line">            &#125;</span><br><span class="line">            &quot;doc&quot;:&quot;an array of partitions to be reassigned to new replicas&quot;</span><br><span class="line">         &#125;</span><br><span class="line">      &#125;</span><br><span class="line">   ]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">Example:</span><br><span class="line">&#123;</span><br><span class="line">  &quot;version&quot;: 1,</span><br><span class="line">  &quot;partitions&quot;:</span><br><span class="line">     [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;: &quot;Foo&quot;,</span><br><span class="line">            &quot;partition&quot;: 1,</span><br><span class="line">            &quot;replicas&quot;: [0, 1, 3]</span><br><span class="line">        &#125;</span><br><span class="line">     ]            </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-12-Preferred-replication-election"><a href="#2-12-Preferred-replication-election" class="headerlink" title="2.12　Preferred replication election"></a>2.12　Preferred replication election</h3><p>/admin/preferred_replica_election</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   &quot;fields&quot;:[</span><br><span class="line">      &#123;</span><br><span class="line">         &quot;name&quot;:&quot;version&quot;,</span><br><span class="line">         &quot;type&quot;:&quot;int&quot;,</span><br><span class="line">         &quot;doc&quot;:&quot;version id&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">         &quot;name&quot;:&quot;partitions&quot;,</span><br><span class="line">         &quot;type&quot;:&#123;</span><br><span class="line">            &quot;type&quot;:&quot;array&quot;,</span><br><span class="line">            &quot;items&quot;:&#123;</span><br><span class="line">               &quot;fields&quot;:[</span><br><span class="line">                  &#123;</span><br><span class="line">                     &quot;name&quot;:&quot;topic&quot;,</span><br><span class="line">                     &quot;type&quot;:&quot;string&quot;,</span><br><span class="line">                     &quot;doc&quot;:&quot;topic of the partition for which preferred replica election should be triggered&quot;</span><br><span class="line">                  &#125;,</span><br><span class="line">                  &#123;</span><br><span class="line">                     &quot;name&quot;:&quot;partition&quot;,</span><br><span class="line">                     &quot;type&quot;:&quot;int&quot;,</span><br><span class="line">                     &quot;doc&quot;:&quot;the partition for which preferred replica election should be triggered&quot;</span><br><span class="line">                  &#125;</span><br><span class="line">               ],</span><br><span class="line">            &#125;</span><br><span class="line">            &quot;doc&quot;:&quot;an array of partitions for which preferred replica election should be triggered&quot;</span><br><span class="line">         &#125;</span><br><span class="line">      &#125;</span><br><span class="line">   ]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">例子:</span><br><span class="line"> </span><br><span class="line">&#123;</span><br><span class="line">  &quot;version&quot;: 1,</span><br><span class="line">  &quot;partitions&quot;:</span><br><span class="line">     [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;: &quot;Foo&quot;,</span><br><span class="line">            &quot;partition&quot;: 1         </span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;: &quot;Bar&quot;,</span><br><span class="line">            &quot;partition&quot;: 0         </span><br><span class="line">        &#125;</span><br><span class="line">     ]            </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-13-删除topics"><a href="#2-13-删除topics" class="headerlink" title="2.13　删除topics"></a>2.13　删除topics</h3><p>/admin/delete_topics</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508110442336-511702454.png" alt="img"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Schema:</span><br><span class="line">&#123; &quot;fields&quot;:</span><br><span class="line">    [ &#123;&quot;name&quot;: &quot;version&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;version id&quot;&#125;,</span><br><span class="line">      &#123;&quot;name&quot;: &quot;topics&quot;,</span><br><span class="line">       &quot;type&quot;: &#123; &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;string&quot;, &quot;doc&quot;: &quot;an array of topics to be deleted&quot;&#125;</span><br><span class="line">      &#125; ]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">例子:</span><br><span class="line">&#123;</span><br><span class="line">  &quot;version&quot;: 1,</span><br><span class="line">  &quot;topics&quot;: [&quot;foo&quot;, &quot;bar&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-14-Topic配置"><a href="#2-14-Topic配置" class="headerlink" title="2.14　Topic配置"></a>2.14　Topic配置</h3><p>/config/topics/[topic_name]</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508110605357-406763586.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka学习之路 （四）Kafka的安装</title>
      <link href="/2018-05-07-Kafka%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%9B%9B%EF%BC%89Kafka%E7%9A%84%E5%AE%89%E8%A3%85.html"/>
      <url>/2018-05-07-Kafka%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%9B%9B%EF%BC%89Kafka%E7%9A%84%E5%AE%89%E8%A3%85.html</url>
      
        <content type="html"><![CDATA[<p>** Kafka学习之路 （四）Kafka的安装：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Kafka学习之路 （四）Kafka的安装</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、下载"><a href="#一、下载" class="headerlink" title="一、下载"></a>一、下载</h2><p>下载地址：</p><p><a href="http://kafka.apache.org/downloads.html" target="_blank" rel="noopener">http://kafka.apache.org/downloads.html</a></p><p><a href="http://mirrors.hust.edu.cn/apache/" target="_blank" rel="noopener">http://mirrors.hust.edu.cn/apache/</a></p><h2 id="二、安装前提（zookeeper安装）"><a href="#二、安装前提（zookeeper安装）" class="headerlink" title="二、安装前提（zookeeper安装）"></a>二、安装前提（zookeeper安装）</h2><p>参考<a href="http://www.cnblogs.com/qingyunzong/p/8634335.html#_label4_0" target="_blank" rel="noopener">http://www.cnblogs.com/qingyunzong/p/8634335.html#_label4_0</a></p><h2 id="三、安装"><a href="#三、安装" class="headerlink" title="三、安装"></a>三、安装</h2><p>此处使用版本为kafka_2.11-0.8.2.0.tgz</p><h3 id="2-1-上传解压缩"><a href="#2-1-上传解压缩" class="headerlink" title="2.1　上传解压缩"></a>2.1　上传解压缩</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ tar -zxvf kafka_2.11-0.8.2.0.tgz -C apps</span><br><span class="line">[hadoop@hadoop1 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop1 apps]$ ln -s kafka_2.11-0.8.2.0/ kafka</span><br></pre></td></tr></table></figure><h3 id="2-2-修改配置文件"><a href="#2-2-修改配置文件" class="headerlink" title="2.2　修改配置文件"></a>2.2　修改配置文件</h3><p>进入kafka的安装配置目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/kafka/config/</span><br></pre></td></tr></table></figure><p>主要关注：<strong>server.properties</strong> 这个文件即可，我们可以发现在目录下：</p><p>有很多文件，这里可以发现有Zookeeper文件，我们可以根据Kafka内带的zk集群来启动，但是建议使用独立的zk集群</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507205256986-826023478.png" alt="img"></p><p>server.properties（<strong>broker.id和host.name每个节点都不相同</strong>）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">//当前机器在集群中的唯一标识，和zookeeper的myid性质一样</span><br><span class="line">broker.id=0</span><br><span class="line">//当前kafka对外提供服务的端口默认是9092</span><br><span class="line">port=9092</span><br><span class="line">//这个参数默认是关闭的，在0.8.1有个bug，DNS解析问题，失败率的问题。</span><br><span class="line">host.name=hadoop1</span><br><span class="line">//这个是borker进行网络处理的线程数</span><br><span class="line">num.network.threads=3</span><br><span class="line">//这个是borker进行I/O处理的线程数</span><br><span class="line">num.io.threads=8</span><br><span class="line">//发送缓冲区buffer大小，数据不是一下子就发送的，先回存储到缓冲区了到达一定的大小后在发送，能提高性能</span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line">//kafka接收缓冲区大小，当数据到达一定大小后在序列化到磁盘</span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line">//这个参数是向kafka请求消息或者向kafka发送消息的请请求的最大数，这个值不能超过java的堆栈大小</span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line">//消息存放的目录，这个目录可以配置为“，”逗号分割的表达式，上面的num.io.threads要大于这个目录的个数这个目录，</span><br><span class="line">//如果配置多个目录，新创建的topic他把消息持久化的地方是，当前以逗号分割的目录中，那个分区数最少就放那一个</span><br><span class="line">log.dirs=/home/hadoop/log/kafka-logs</span><br><span class="line">//默认的分区数，一个topic默认1个分区数</span><br><span class="line">num.partitions=1</span><br><span class="line">//每个数据目录用来日志恢复的线程数目</span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line">//默认消息的最大持久化时间，168小时，7天</span><br><span class="line">log.retention.hours=168</span><br><span class="line">//这个参数是：因为kafka的消息是以追加的形式落地到文件，当超过这个值的时候，kafka会新起一个文件</span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line">//每隔300000毫秒去检查上面配置的log失效时间</span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line">//是否启用log压缩，一般不用启用，启用的话可以提高性能</span><br><span class="line">log.cleaner.enable=false</span><br><span class="line">//设置zookeeper的连接端口</span><br><span class="line">zookeeper.connect=192.168.123.102:2181,192.168.123.103:2181,192.168.123.104:2181</span><br><span class="line">//设置zookeeper的连接超时时间</span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br></pre></td></tr></table></figure><p>producer.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">metadata.broker.list=192.168.123.102:9092,192.168.123.103:9092,192.168.123.104:9092</span><br></pre></td></tr></table></figure><p>consumer.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zookeeper.connect=192.168.123.102:2181,192.168.123.103:2181,192.168.123.104:2181</span><br></pre></td></tr></table></figure><h3 id="2-3-将kafka的安装包分发到其他节点"><a href="#2-3-将kafka的安装包分发到其他节点" class="headerlink" title="2.3　将kafka的安装包分发到其他节点"></a>2.3　将kafka的安装包分发到其他节点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 apps]$ scp -r kafka_2.11-0.8.2.0/ hadoop2:$PWD</span><br><span class="line">[hadoop@hadoop1 apps]$ scp -r kafka_2.11-0.8.2.0/ hadoop3:$PWD</span><br><span class="line">[hadoop@hadoop1 apps]$ scp -r kafka_2.11-0.8.2.0/ hadoop4:$PWD</span><br></pre></td></tr></table></figure><h3 id="2-4-创建软连接"><a href="#2-4-创建软连接" class="headerlink" title="2.4　创建软连接"></a>2.4　创建软连接</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 apps]$ ln -s kafka_2.11-0.8.2.0/ kafka</span><br></pre></td></tr></table></figure><h3 id="2-5-修改环境变量"><a href="#2-5-修改环境变量" class="headerlink" title="2.5　修改环境变量"></a>2.5　修改环境变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ vi .bashrc </span><br><span class="line">#Kafka</span><br><span class="line">export KAFKA_HOME=/home/hadoop/apps/kafka</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br></pre></td></tr></table></figure><p>保存使其立即生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ source ~/.bashrc</span><br></pre></td></tr></table></figure><h2 id="三、启动"><a href="#三、启动" class="headerlink" title="三、启动"></a>三、启动</h2><h3 id="3-1-首先启动zookeeper集群"><a href="#3-1-首先启动zookeeper集群" class="headerlink" title="3.1　首先启动zookeeper集群"></a>3.1　首先启动zookeeper集群</h3><p>所有zookeeper节点都需要执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ zkServer.sh start</span><br></pre></td></tr></table></figure><h3 id="3-2-启动Kafka集群服务"><a href="#3-2-启动Kafka集群服务" class="headerlink" title="3.2　启动Kafka集群服务"></a>3.2　启动Kafka集群服务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 kafka]$ bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure><p>hadoop1</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508091330478-1618680698.png" alt="img"></p><p>Hadoop2</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508091357167-630027883.png" alt="img"></p><p>hadoop3</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508091419194-1962654932.png" alt="img"></p><p>hadoop4</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508091439631-71784584.png" alt="img"></p><h3 id="3-3-创建的topic"><a href="#3-3-创建的topic" class="headerlink" title="3.3　创建的topic"></a>3.3　创建的topic</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 kafka]$ bin/kafka-topics.sh --create --zookeeper hadoop1:2181 --replication-factor 3 --partitions 3 --topic topic2</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508093450833-853043206.png" alt="img"></p><h3 id="3-4-查看topic副本信息"><a href="#3-4-查看topic副本信息" class="headerlink" title="3.4　查看topic副本信息"></a>3.4　查看topic副本信息</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 kafka]$ bin/kafka-topics.sh --describe --zookeeper hadoop1:2181 --topic topic2</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508093738851-372829720.png" alt="img"></p><h3 id="3-5-查看已经创建的topic信息"><a href="#3-5-查看已经创建的topic信息" class="headerlink" title="3.5　查看已经创建的topic信息"></a>3.5　查看已经创建的topic信息</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 kafka]$ bin/kafka-topics.sh --list --zookeeper hadoop1:2181</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508094000763-519973668.png" alt="img"></p><h3 id="3-6-生产者发送消息"><a href="#3-6-生产者发送消息" class="headerlink" title="3.6　生产者发送消息"></a>3.6　生产者发送消息</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop1:9092 --topic topic2</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508094536515-251235516.png" alt="img"></p><p>hadoop1显示接收到消息</p><h3 id="3-7-消费者消费消息"><a href="#3-7-消费者消费消息" class="headerlink" title="3.7　消费者消费消息"></a>3.7　消费者消费消息</h3><p>在hadoop2上消费消息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop1:2181 --from-beginning --topic topic2</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508094916762-1544456891.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka学习之路 （三）Kafka的高可用</title>
      <link href="/2018-05-06-Kafka%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%89%EF%BC%89Kafka%E7%9A%84%E9%AB%98%E5%8F%AF%E7%94%A8.html"/>
      <url>/2018-05-06-Kafka%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%89%EF%BC%89Kafka%E7%9A%84%E9%AB%98%E5%8F%AF%E7%94%A8.html</url>
      
        <content type="html"><![CDATA[<p>** Kafka学习之路 （三）Kafka的高可用：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Kafka学习之路 （三）Kafka的高可用</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、高可用的由来"><a href="#一、高可用的由来" class="headerlink" title="一、高可用的由来"></a>一、高可用的由来</h2><h3 id="1-1-为何需要Replication"><a href="#1-1-为何需要Replication" class="headerlink" title="1.1　为何需要Replication"></a>1.1　为何需要Replication</h3><p>　　在Kafka在0.8以前的版本中，是没有Replication的，一旦某一个Broker宕机，则其上所有的Partition数据都不可被消费，这与Kafka数据持久性及Delivery Guarantee的设计目标相悖。同时Producer都不能再将数据存于这些Partition中。</p><p>　　如果Producer使用同步模式则Producer会在尝试重新发送message.send.max.retries（默认值为3）次后抛出Exception，用户可以选择停止发送后续数据也可选择继续选择发送。而前者会造成数据的阻塞，后者会造成本应发往该Broker的数据的丢失。</p><p>　　如果Producer使用异步模式，则Producer会尝试重新发送message.send.max.retries（默认值为3）次后记录该异常并继续发送后续数据，这会造成数据丢失并且用户只能通过日志发现该问题。同时，Kafka的Producer并未对异步模式提供callback接口。</p><p>　　由此可见，在没有Replication的情况下，一旦某机器宕机或者某个Broker停止工作则会造成整个系统的可用性降低。随着集群规模的增加，整个集群中出现该类异常的几率大大增加，因此对于生产系统而言Replication机制的引入非常重要。</p><h3 id="1-2-Leader-Election"><a href="#1-2-Leader-Election" class="headerlink" title="1.2　Leader Election"></a>1.2　Leader Election</h3><p>　　引入Replication之后，同一个Partition可能会有多个Replica，而这时需要在这些Replication之间选出一个Leader，Producer和Consumer只与这个Leader交互，其它Replica作为Follower从Leader中复制数据。</p><p>　　因为需要保证同一个Partition的多个Replica之间的数据一致性（其中一个宕机后其它Replica必须要能继续服务并且即不能造成数据重复也不能造成数据丢失）。如果没有一个Leader，所有Replica都可同时读/写数据，那就需要保证多个Replica之间互相（N×N条通路）同步数据，数据的一致性和有序性非常难保证，大大增加了Replication实现的复杂性，同时也增加了出现异常的几率。而引入Leader后，只有Leader负责数据读写，Follower只向Leader顺序Fetch数据（N条通路），系统更加简单且高效。</p><h2 id="二、Kafka-HA设计解析"><a href="#二、Kafka-HA设计解析" class="headerlink" title="二、Kafka HA设计解析"></a>二、Kafka HA设计解析</h2><h3 id="2-1-如何将所有Replica均匀分布到整个集群"><a href="#2-1-如何将所有Replica均匀分布到整个集群" class="headerlink" title="2.1　如何将所有Replica均匀分布到整个集群"></a>2.1　如何将所有Replica均匀分布到整个集群</h3><p>为了更好的做负载均衡，Kafka尽量将所有的Partition均匀分配到整个集群上。一个典型的部署方式是一个Topic的Partition数量大于Broker的数量。同时为了提高Kafka的容错能力，也需要将同一个Partition的Replica尽量分散到不同的机器。实际上，如果所有的Replica都在同一个Broker上，那一旦该Broker宕机，该Partition的所有Replica都无法工作，也就达不到HA的效果。同时，如果某个Broker宕机了，需要保证它上面的负载可以被均匀的分配到其它幸存的所有Broker上。</p><p>Kafka分配Replica的算法如下：</p><p>1.将所有Broker（假设共n个Broker）和待分配的Partition排序</p><p>2.将第i个Partition分配到第（i mod n）个Broker上</p><p>3.将第i个Partition的第j个Replica分配到第（(i + j) mode n）个Broker上</p><h3 id="2-2-Data-Replication（副本策略）"><a href="#2-2-Data-Replication（副本策略）" class="headerlink" title="2.2　Data Replication（副本策略）"></a>2.2　Data Replication（副本策略）</h3><p>Kafka的高可靠性的保障来源于其健壮的副本（replication）策略。</p><h4 id="2-2-1-消息传递同步策略"><a href="#2-2-1-消息传递同步策略" class="headerlink" title="2.2.1　消息传递同步策略"></a>2.2.1　消息传递同步策略</h4><p>Producer在发布消息到某个Partition时，先通过ZooKeeper找到该Partition的Leader，然后无论该Topic的Replication Factor为多少，Producer只将该消息发送到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。一旦Leader收到了ISR中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW并且向Producer发送ACK。</p><p>为了提高性能，每个Follower在接收到数据后就立马向Leader发送ACK，而非等到数据写入Log中。因此，对于已经commit的消息，Kafka只能保证它被存于多个Replica的内存中，而不能保证它们被持久化到磁盘中，也就不能完全保证异常发生后该条消息一定能被Consumer消费。</p><p>Consumer读消息也是从Leader读取，只有被commit过的消息才会暴露给Consumer。</p><p>Kafka Replication的数据流如下图所示：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507194612622-1788087919.png" alt="img"></p><h4 id="2-2-2-ACK前需要保证有多少个备份"><a href="#2-2-2-ACK前需要保证有多少个备份" class="headerlink" title="2.2.2　ACK前需要保证有多少个备份"></a>2.2.2　ACK前需要保证有多少个备份</h4><p>对于Kafka而言，定义一个Broker是否“活着”包含两个条件：</p><ul><li>一是它必须维护与ZooKeeper的session（这个通过ZooKeeper的Heartbeat机制来实现）。</li><li>二是Follower必须能够及时将Leader的消息复制过来，不能“落后太多”。</li></ul><p>Leader会跟踪与其保持同步的Replica列表，该列表称为ISR（即in-sync Replica）。如果一个Follower宕机，或者落后太多，Leader将把它从ISR中移除。这里所描述的“落后太多”指Follower复制的消息落后于Leader后的条数超过预定值（该值可在$KAFKA_HOME/config/server.properties中通过replica.lag.max.messages配置，其默认值是4000）或者Follower超过一定时间（该值可在$KAFKA_HOME/config/server.properties中通过replica.lag.time.max.ms来配置，其默认值是10000）未向Leader发送fetch请求。</p><p>Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，完全同步复制要求所有能工作的Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率（高吞吐率是Kafka非常重要的一个特性）。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下如果Follower都复制完都落后于Leader，而如果Leader突然宕机，则会丢失数据。而Kafka的这种使用ISR的方式则很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，这样极大的提高复制性能（批量写磁盘），极大减少了Follower与Leader的差距。</p><p>需要说明的是，Kafka只解决fail/recover，不处理“Byzantine”（“拜占庭”）问题。一条消息只有被ISR里的所有Follower都从Leader复制过去才会被认为已提交。这样就避免了部分数据被写进了Leader，还没来得及被任何Follower复制就宕机了，而造成数据丢失（Consumer无法消费这些数据）。而对于Producer而言，它可以选择是否等待消息commit，这可以通过request.required.acks来设置。这种机制确保了只要ISR有一个或以上的Follower，一条被commit的消息就不会丢失。</p><h4 id="2-2-3-Leader-Election算法"><a href="#2-2-3-Leader-Election算法" class="headerlink" title="2.2.3　Leader Election算法"></a>2.2.3　Leader Election算法</h4><p>Leader选举本质上是一个分布式锁，有两种方式实现基于ZooKeeper的分布式锁：</p><ul><li>节点名称唯一性：多个客户端创建一个节点，只有成功创建节点的客户端才能获得锁</li><li>临时顺序节点：所有客户端在某个目录下创建自己的临时顺序节点，只有序号最小的才获得锁</li></ul><p>一种非常常用的选举leader的方式是“Majority Vote”（“少数服从多数”），但Kafka并未采用这种方式。这种模式下，如果我们有2f+1个Replica（包含Leader和Follower），那在commit之前必须保证有f+1个Replica复制完消息，为了保证正确选出新的Leader，fail的Replica不能超过f个。因为在剩下的任意f+1个Replica里，至少有一个Replica包含有最新的所有消息。这种方式有个很大的优势，系统的latency只取决于最快的几个Broker，而非最慢那个。Majority Vote也有一些劣势，为了保证Leader Election的正常进行，它所能容忍的fail的follower个数比较少。如果要容忍1个follower挂掉，必须要有3个以上的Replica，如果要容忍2个Follower挂掉，必须要有5个以上的Replica。也就是说，在生产环境下为了保证较高的容错程度，必须要有大量的Replica，而大量的Replica又会在大数据量下导致性能的急剧下降。这就是这种算法更多用在ZooKeeper这种共享集群配置的系统中而很少在需要存储大量数据的系统中使用的原因。例如HDFS的HA Feature是基于majority-vote-based journal，但是它的数据存储并没有使用这种方式。</p><p>Kafka在ZooKeeper中动态维护了一个ISR（in-sync replicas），这个ISR里的所有Replica都跟上了leader，只有ISR里的成员才有被选为Leader的可能。在这种模式下，对于f+1个Replica，一个Partition能在保证不丢失已经commit的消息的前提下容忍f个Replica的失败。在大多数使用场景中，这种模式是非常有利的。事实上，为了容忍f个Replica的失败，Majority Vote和ISR在commit前需要等待的Replica数量是一样的，但是ISR需要的总的Replica的个数几乎是Majority Vote的一半。</p><p>虽然Majority Vote与ISR相比有不需等待最慢的Broker这一优势，但是Kafka作者认为Kafka可以通过Producer选择是否被commit阻塞来改善这一问题，并且节省下来的Replica和磁盘使得ISR模式仍然值得。</p><h4 id="2-2-4-如何处理所有Replica都不工作"><a href="#2-2-4-如何处理所有Replica都不工作" class="headerlink" title="2.2.4　如何处理所有Replica都不工作"></a>2.2.4　如何处理所有Replica都不工作</h4><p>在ISR中至少有一个follower时，Kafka可以确保已经commit的数据不丢失，但如果某个Partition的所有Replica都宕机了，就无法保证数据不丢失了。这种情况下有两种可行的方案：</p><p>1.等待ISR中的任一个Replica“活”过来，并且选它作为Leader</p><p>2.选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader</p><p>这就需要在可用性和一致性当中作出一个简单的折衷。如果一定要等待ISR中的Replica“活”过来，那不可用的时间就可能会相对较长。而且如果ISR中的所有Replica都无法“活”过来了，或者数据都丢失了，这个Partition将永远不可用。选择第一个“活”过来的Replica作为Leader，而这个Replica不是ISR中的Replica，那即使它并不保证已经包含了所有已commit的消息，它也会成为Leader而作为consumer的数据源（前文有说明，所有读写都由Leader完成）。Kafka0.8.*使用了第二种方式。根据Kafka的文档，在以后的版本中，Kafka支持用户通过配置选择这两种方式中的一种，从而根据不同的使用场景选择高可用性还是强一致性。</p><h4 id="2-2-5-选举Leader"><a href="#2-2-5-选举Leader" class="headerlink" title="2.2.5　选举Leader"></a>2.2.5　选举Leader</h4><p>最简单最直观的方案是，所有Follower都在ZooKeeper上设置一个Watch，一旦Leader宕机，其对应的ephemeral znode会自动删除，此时所有Follower都尝试创建该节点，而创建成功者（ZooKeeper保证只有一个能创建成功）即是新的Leader，其它Replica即为Follower。</p><p>但是该方法会有3个问题：</p><p>1.split-brain 这是由ZooKeeper的特性引起的，虽然ZooKeeper能保证所有Watch按顺序触发，但并不能保证同一时刻所有Replica“看”到的状态是一样的，这就可能造成不同Replica的响应不一致</p><p>2.herd effect 如果宕机的那个Broker上的Partition比较多，会造成多个Watch被触发，造成集群内大量的调整</p><p>3.ZooKeeper负载过重 每个Replica都要为此在ZooKeeper上注册一个Watch，当集群规模增加到几千个Partition时ZooKeeper负载会过重。</p><p>Kafka 0.8.*的Leader Election方案解决了上述问题，它在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式（比ZooKeeper Queue的方式更高效）通知需为为此作为响应的Broker。同时controller也负责增删Topic以及Replica的重新分配。</p><h2 id="三、HA相关ZooKeeper结构"><a href="#三、HA相关ZooKeeper结构" class="headerlink" title="三、HA相关ZooKeeper结构"></a>三、HA相关ZooKeeper结构</h2><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507195223218-1719228508.png" alt="img"></p><h3 id="3-1-admin"><a href="#3-1-admin" class="headerlink" title="3.1　admin"></a>3.1　admin</h3><p>该目录下znode只有在有相关操作时才会存在，操作结束时会将其删除</p><p>/admin/reassign_partitions用于将一些Partition分配到不同的broker集合上。对于每个待重新分配的Partition，Kafka会在该znode上存储其所有的Replica和相应的Broker id。该znode由管理进程创建并且一旦重新分配成功它将会被自动移除。</p><h3 id="3-2-broker"><a href="#3-2-broker" class="headerlink" title="3.2　broker"></a>3.2　broker</h3><p>即/brokers/ids/[brokerId]）存储“活着”的broker信息。</p><p>topic注册信息（/brokers/topics/[topic]），存储该topic的所有partition的所有replica所在的broker id，第一个replica即为preferred replica，对一个给定的partition，它在同一个broker上最多只有一个replica,因此broker id可作为replica id。</p><h3 id="3-3-controller"><a href="#3-3-controller" class="headerlink" title="3.3　controller"></a>3.3　controller</h3><p>/controller -&gt; int (broker id of the controller)存储当前controller的信息</p><p>/controller_epoch -&gt; int (epoch)直接以整数形式存储controller epoch，而非像其它znode一样以JSON字符串形式存储。</p><h2 id="四、producer发布消息"><a href="#四、producer发布消息" class="headerlink" title="四、producer发布消息"></a>四、<strong>producer发布消息</strong></h2><h3 id="4-1-写入方式"><a href="#4-1-写入方式" class="headerlink" title="4.1　写入方式"></a>4.1　写入方式</h3><p>producer 采用 push 模式将消息发布到 broker，每条消息都被 append 到 patition 中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障 kafka 吞吐率）。</p><h3 id="4-2-消息路由"><a href="#4-2-消息路由" class="headerlink" title="4.2　消息路由"></a>4.2　消息路由</h3><p>producer 发送消息到 broker 时，会根据分区算法选择将其存储到哪一个 partition。其路由机制为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、 指定了 patition，则直接使用；</span><br><span class="line">2、 未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition</span><br><span class="line">3、 patition 和 key 都未指定，使用轮询选出一个 patition。</span><br></pre></td></tr></table></figure><h3 id="4-3-写入流程"><a href="#4-3-写入流程" class="headerlink" title="4.3　写入流程"></a>4.3　写入流程</h3><p>producer 写入消息序列图如下所示：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507200019142-182025107.png" alt="img"></p><p>流程说明：</p><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; 1、 producer 先从 zookeeper 的 &quot;/brokers/.../state&quot; 节点找到该 partition 的 leader </span><br><span class="line">&gt; 2、 producer 将消息发送给该 leader </span><br><span class="line">&gt; 3、 leader 将消息写入本地 log </span><br><span class="line">&gt; 4、 followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK </span><br><span class="line">&gt; 5、 leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h2 id="五、broker保存消息"><a href="#五、broker保存消息" class="headerlink" title="五、broker保存消息"></a>五、broker保存消息</h2><h3 id="5-1-存储方式"><a href="#5-1-存储方式" class="headerlink" title="5.1　存储方式"></a>5.1　存储方式</h3><p>物理上把 topic 分成一个或多个 patition（对应 server.properties 中的 num.partitions=3 配置），每个 patition 物理上对应一个文件夹（该文件夹存储该 patition 的所有消息和索引文件），如下：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507200226759-1617322728.png" alt="img"></p><h3 id="5-2-存储策略"><a href="#5-2-存储策略" class="headerlink" title="5.2　存储策略"></a>5.2　存储策略</h3><p>无论消息是否被消费，kafka 都会保留所有消息。有两种策略可以删除旧数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1、 基于时间：log.retention.hours=168 </span><br><span class="line">2、 基于大小：log.retention.bytes=1073741824</span><br></pre></td></tr></table></figure><h2 id="六、Topic的创建和删除"><a href="#六、Topic的创建和删除" class="headerlink" title="六、Topic的创建和删除"></a>六、Topic的创建和删除</h2><h3 id="6-1-创建topic"><a href="#6-1-创建topic" class="headerlink" title="6.1　创建topic"></a>6.1　创建topic</h3><p>创建 topic 的序列图如下所示：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507200343317-1340406332.png" alt="img"></p><p>流程说明：</p><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; 1、 controller 在 ZooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被创建，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。</span><br><span class="line">&gt; 2、 controller从 /brokers/ids 读取当前所有可用的 broker 列表，对于 set_p 中的每一个 partition：</span><br><span class="line">&gt;      2.1、 从分配给该 partition 的所有 replica（称为AR）中任选一个可用的 broker 作为新的 leader，并将AR设置为新的 ISR </span><br><span class="line">&gt;      2.2、 将新的 leader 和 ISR 写入 /brokers/topics/[topic]/partitions/[partition]/state </span><br><span class="line">&gt; 3、 controller 通过 RPC 向相关的 broker 发送 LeaderAndISRRequest。</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h3 id="6-2-删除topic"><a href="#6-2-删除topic" class="headerlink" title="6.2　删除topic"></a>6.2　删除topic</h3><p>删除 topic 的序列图如下所示：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507200533571-310409492.png" alt="img"></p><p>流程说明：</p><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; 1、 controller 在 zooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被删除，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。 </span><br><span class="line">&gt; 2、 若 delete.topic.enable=false，结束；否则 controller 注册在 /admin/delete_topics 上的 watch 被 fire，controller 通过回调向对应的 broker 发送 StopReplicaRequest。</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h2 id="七、broker-failover"><a href="#七、broker-failover" class="headerlink" title="七、broker failover"></a>七、<strong>broker failover</strong></h2><p>kafka broker failover 序列图如下所示：</p><p><strong><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507200729833-108400321.png" alt="img"></strong></p><p>流程说明：</p><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; 1、 controller 在 zookeeper 的 /brokers/ids/[brokerId] 节点注册 Watcher，当 broker 宕机时 zookeeper 会 fire watch</span><br><span class="line">&gt; 2、 controller 从 /brokers/ids 节点读取可用broker </span><br><span class="line">&gt; 3、 controller决定set_p，该集合包含宕机 broker 上的所有 partition </span><br><span class="line">&gt; 4、 对 set_p 中的每一个 partition </span><br><span class="line">&gt;     4.1、 从/brokers/topics/[topic]/partitions/[partition]/state 节点读取 ISR </span><br><span class="line">&gt;     4.2、 决定新 leader </span><br><span class="line">&gt;     4.3、 将新 leader、ISR、controller_epoch 和 leader_epoch 等信息写入 state 节点</span><br><span class="line">&gt; 5、 通过 RPC 向相关 broker 发送 leaderAndISRRequest 命令</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h2 id="八、controller-failover"><a href="#八、controller-failover" class="headerlink" title="八、controller failover"></a>八、<strong>controller failover</strong></h2><p>当 controller 宕机时会触发 controller failover。每个 broker 都会在 zookeeper 的 “/controller” 节点注册 watcher，当 controller 宕机时 zookeeper 中的临时节点消失，所有存活的 broker 收到 fire 的通知，每个 broker 都尝试创建新的 controller path，只有一个竞选成功并当选为 controller。</p><p>当新的 controller 当选时，会触发 KafkaController.onControllerFailover 方法，在该方法中完成如下操作：</p><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt; 1、 读取并增加 Controller Epoch。 </span><br><span class="line">&gt; 2、 在 reassignedPartitions Patch(/admin/reassign_partitions) 上注册 watcher。 </span><br><span class="line">&gt; 3、 在 preferredReplicaElection Path(/admin/preferred_replica_election) 上注册 watcher。 </span><br><span class="line">&gt; 4、 通过 partitionStateMachine 在 broker Topics Patch(/brokers/topics) 上注册 watcher。 </span><br><span class="line">&gt; 5、 若 delete.topic.enable=true（默认值是 false），则 partitionStateMachine 在 Delete Topic Patch(/admin/delete_topics) 上注册 watcher。 </span><br><span class="line">&gt; 6、 通过 replicaStateMachine在 Broker Ids Patch(/brokers/ids)上注册Watch。 </span><br><span class="line">&gt; 7、 初始化 ControllerContext 对象，设置当前所有 topic，“活”着的 broker 列表，所有 partition 的 leader 及 ISR等。 </span><br><span class="line">&gt; 8、 启动 replicaStateMachine 和 partitionStateMachine。 </span><br><span class="line">&gt; 9、 将 brokerState 状态设置为 RunningAsController。 </span><br><span class="line">&gt; 10、 将每个 partition 的 Leadership 信息发送给所有“活”着的 broker。 </span><br><span class="line">&gt; 11、 若 auto.leader.rebalance.enable=true（默认值是true），则启动 partition-rebalance 线程。 </span><br><span class="line">&gt; 12、 若 delete.topic.enable=true 且Delete Topic Patch(/admin/delete_topics)中有值，则删除相应的Topic。</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka学习之路 （二）Kafka的架构</title>
      <link href="/2018-05-05-Kafka%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%EF%BC%89Kafka%E7%9A%84%E6%9E%B6%E6%9E%84.html"/>
      <url>/2018-05-05-Kafka%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%EF%BC%89Kafka%E7%9A%84%E6%9E%B6%E6%9E%84.html</url>
      
        <content type="html"><![CDATA[<p>** Kafka学习之路 （二）Kafka的架构：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Kafka学习之路 （二）Kafka的架构</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、Kafka的架构"><a href="#一、Kafka的架构" class="headerlink" title="一、Kafka的架构"></a>一、Kafka的架构</h2><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507192145249-1414897650.png" alt="img"></p><p>如上图所示，一个典型的Kafka集群中包含若干Producer（可以是web前端产生的Page View，或者是服务器日志，系统CPU、Memory等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。</p><h2 id="二、Topics和Partition"><a href="#二、Topics和Partition" class="headerlink" title="二、Topics和Partition"></a>二、Topics和Partition</h2><p>Topic在逻辑上可以被认为是一个queue，每条消费都必须指定它的Topic，可以简单理解为必须指明把这条消息放进哪个queue里。为了使得Kafka的吞吐率可以线性提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件。创建一个topic时，同时可以指定分区数目，分区数越多，其吞吐量也越大，但是需要的资源也越多，同时也会导致更高的不可用性，kafka在接收到生产者发送的消息之后，会根据均衡策略将消息存储到不同的分区中。因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507192840409-1435311830.png" alt="img"></p><p>对于传统的message queue而言，一般会删除已经被消费的消息，而Kafka集群会保留所有的消息，无论其被消费与否。当然，因为磁盘限制，不可能永久保留所有数据（实际上也没必要），因此Kafka提供两种策略删除旧数据。一是基于时间，二是基于Partition文件大小。例如可以通过配置$KAFKA_HOME/config/server.properties，让Kafka删除一周前的数据，也可在Partition文件超过1GB时删除旧数据，配置如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># The minimum age of a log file to be eligible for deletion</span><br><span class="line">log.retention.hours=168</span><br><span class="line"># The maximum size of a log segment file. When this size is reached a new log segment will be created.</span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line"># The interval at which log segments are checked to see if they can be deleted according to the retention policies</span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line"># If log.cleaner.enable=true is set the cleaner will be enabled and individual logs can then be marked for log compaction.</span><br><span class="line">log.cleaner.enable=false</span><br></pre></td></tr></table></figure><p>因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高Kafka性能无关。选择怎样的删除策略只与磁盘以及具体的需求有关。另外，Kafka会为每一个Consumer Group保留一些metadata信息——当前消费的消息的position，也即offset。这个offset由Consumer控制。正常情况下Consumer会在消费完一条消息后递增该offset。当然，Consumer也可将offset设成一个较小的值，重新消费一些消息。因为offet由Consumer控制，所以Kafka broker是无状态的，它不需要标记哪些消息被哪些消费过，也不需要通过broker去保证同一个Consumer Group只有一个Consumer能消费某一条消息，因此也就不需要锁机制，这也为Kafka的高吞吐率提供了有力保障。</p><h2 id="三、Producer消息路由"><a href="#三、Producer消息路由" class="headerlink" title="三、Producer消息路由"></a>三、Producer消息路由</h2><p>Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。如果Partition机制设置合理，所有消息可以均匀分布到不同的Partition里，这样就实现了负载均衡。如果一个Topic对应一个文件，那这个文件所在的机器I/O将会成为这个Topic的性能瓶颈，而有了Partition后，不同的消息可以并行写入不同broker的不同Partition里，极大的提高了吞吐率。可以在$KAFKA_HOME/config/server.properties中通过配置项num.partitions来指定新建Topic的默认Partition数量，也可在创建Topic时通过参数指定，同时也可以在Topic创建之后通过Kafka提供的工具修改。</p><p>在发送一条消息时，可以指定这条消息的key，Producer根据这个key和Partition机制来判断应该将这条消息发送到哪个Parition。Paritition机制可以通过指定Producer的paritition. class这一参数来指定，该class必须实现kafka.producer.Partitioner接口。</p><h2 id="四、Consumer-Group"><a href="#四、Consumer-Group" class="headerlink" title="四、Consumer Group"></a>四、Consumer Group</h2><p>使用Consumer high level API时，同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507193553697-2141118410.png" alt="img"></p><p>这是Kafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。一个Topic可以对应多个Consumer Group。如果需要实现广播，只要每个Consumer有一个独立的Group就可以了。要实现单播只要所有的Consumer在同一个Group里。用Consumer Group还可以将Consumer进行自由的分组而不需要多次发送消息到不同的Topic。</p><p>实际上，Kafka的设计理念之一就是同时提供离线处理和实时处理。根据这一特性，可以使用Storm这种实时流处理系统对消息进行实时在线处理，同时使用Hadoop这种批处理系统进行离线处理，还可以同时将数据实时备份到另一个数据中心，只需要保证这三个操作所使用的Consumer属于不同的Consumer Group即可。</p><h2 id="五、Push-vs-Pull"><a href="#五、Push-vs-Pull" class="headerlink" title="五、Push vs. Pull"></a>五、Push vs. Pull</h2><p>作为一个消息系统，Kafka遵循了传统的方式，选择由Producer向broker push消息并由Consumer从broker pull消息。一些logging-centric system，比如Facebook的Scribe和Cloudera的Flume，采用push模式。事实上，push模式和pull模式各有优劣。</p><p>push模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。push模式的目标是尽可能以最快速度传递消息，但是这样很容易造成Consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据Consumer的消费能力以适当的速率消费消息。</p><p>对于Kafka而言，pull模式更合适。pull模式可简化broker的设计，Consumer可自主控制消费消息的速率，同时Consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。</p><h2 id="六、Kafka-delivery-guarantee"><a href="#六、Kafka-delivery-guarantee" class="headerlink" title="六、Kafka delivery guarantee"></a>六、Kafka delivery guarantee</h2><p>有这么几种可能的delivery guarantee：</p><blockquote><p>At most once 　　消息可能会丢，但绝不会重复传输</p><p>At least one 　　  消息绝不会丢，但可能会重复传输</p><p>Exactly once 　　 每条消息肯定会被传输一次且仅传输一次，很多时候这是用户所想要的。</p></blockquote><p>当Producer向broker发送消息时，一旦这条消息被commit，因数replication的存在，它就不会丢。但是如果Producer发送数据给broker后，遇到网络问题而造成通信中断，那Producer就无法判断该条消息是否已经commit。虽然Kafka无法确定网络故障期间发生了什么，但是Producer可以生成一种类似于主键的东西，发生故障时幂等性的重试多次，这样就做到了Exactly once。</p><p>接下来讨论的是消息从broker到Consumer的delivery guarantee语义。（仅针对Kafka consumer high level API）。Consumer在从broker读取消息后，可以选择commit，该操作会在Zookeeper中保存该Consumer在该Partition中读取的消息的offset。该Consumer下一次再读该Partition时会从下一条开始读取。如未commit，下一次读取的开始位置会跟上一次commit之后的开始位置相同。当然可以将Consumer设置为autocommit，即Consumer一旦读到数据立即自动commit。如果只讨论这一读取消息的过程，那Kafka是确保了Exactly once。但实际使用中应用程序并非在Consumer读取完数据就结束了，而是要进行进一步处理，而数据处理与commit的顺序在很大程度上决定了消息从broker和consumer的delivery guarantee semantic。</p><p><strong>Kafka默认保证At least once</strong>，并且允许通过设置Producer异步提交来实现At most once。而Exactly once要求与外部存储系统协作，幸运的是Kafka提供的offset可以非常直接非常容易得使用这种方式。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka学习之路 （一）Kafka的简介</title>
      <link href="/2018-05-04-Kafka%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%80%EF%BC%89Kafka%E7%9A%84%E7%AE%80%E4%BB%8B.html"/>
      <url>/2018-05-04-Kafka%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%80%EF%BC%89Kafka%E7%9A%84%E7%AE%80%E4%BB%8B.html</url>
      
        <content type="html"><![CDATA[<p>** Kafka学习之路 （一）Kafka的简介：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Kafka学习之路 （一）Kafka的简介</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507184834852-1994140834.png" alt="img"></p><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><h3 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1　概述"></a>1.1　概述</h3><p>Kafka是最初由Linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当做MQ系统），常见可以用于web/nginx日志、访问日志，消息服务等等，Linkedin于2010年贡献给了Apache基金会并成为顶级开源项目。</p><p>主要应用场景是：日志收集系统和消息系统。</p><p>Kafka主要设计目标如下：</p><ul><li>以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间的访问性能。</li><li>高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条消息的传输。</li><li>支持Kafka Server间的消息分区，及分布式消费，同时保证每个partition内的消息顺序传输。</li><li>同时支持离线数据处理和实时数据处理。</li><li>Scale out:支持在线水平扩展</li></ul><h3 id="1-2-消息系统介绍"><a href="#1-2-消息系统介绍" class="headerlink" title="1.2　消息系统介绍"></a>1.2　消息系统介绍</h3><p>一个消息系统负责将数据从一个应用传递到另外一个应用，应用只需关注于数据，无需关注数据在两个或多个应用间是如何传递的。分布式消息传递基于可靠的消息队列，在客户端应用和消息系统之间异步传递消息。有两种主要的消息传递模式：<strong>点对点传递模式、发布-订阅模式</strong>。大部分的消息系统选用发布-订阅模式。<strong>Kafka就是一种发布-订阅模式</strong>。</p><h3 id="1-3-点对点消息传递模式"><a href="#1-3-点对点消息传递模式" class="headerlink" title="1.3　点对点消息传递模式"></a>1.3　点对点消息传递模式</h3><p>在点对点消息系统中，消息持久化到一个队列中。此时，将有一个或多个消费者消费队列中的数据。但是一条消息只能被消费一次。当一个消费者消费了队列中的某条数据之后，该条数据则从消息队列中删除。该模式即使有多个消费者同时消费数据，也能保证数据处理的顺序。这种架构描述示意图如下：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507190326476-771565746.png" alt="img"></p><p><strong>生产者发送一条消息到queue，只有一个消费者能收到</strong>。</p><h3 id="1-4-发布-订阅消息传递模式"><a href="#1-4-发布-订阅消息传递模式" class="headerlink" title="1.4　发布-订阅消息传递模式"></a>1.4　发布-订阅消息传递模式</h3><p>在发布-订阅消息系统中，消息被持久化到一个topic中。与点对点消息系统不同的是，消费者可以订阅一个或多个topic，消费者可以消费该topic中所有的数据，同一条数据可以被多个消费者消费，数据被消费后不会立马删除。在发布-订阅消息系统中，消息的生产者称为发布者，消费者称为订阅者。该模式的示例图如下：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507190443404-1266011458.png" alt="img"></p><p><strong>发布者发送到topic的消息，只有订阅了topic的订阅者才会收到消息</strong>。</p><h2 id="二、Kafka的优点"><a href="#二、Kafka的优点" class="headerlink" title="二、Kafka的优点"></a>二、Kafka的优点</h2><h3 id="2-1-解耦"><a href="#2-1-解耦" class="headerlink" title="2.1　解耦"></a>2.1　解耦</h3><p>在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。消息系统在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口。这允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p><h3 id="2-2-冗余（副本）"><a href="#2-2-冗余（副本）" class="headerlink" title="2.2　冗余（副本）"></a>2.2　冗余（副本）</h3><p>有些情况下，处理数据的过程会失败。除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。</p><h3 id="2-3-扩展性"><a href="#2-3-扩展性" class="headerlink" title="2.3　扩展性"></a>2.3　扩展性</h3><p>因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。不需要改变代码、不需要调节参数。扩展就像调大电力按钮一样简单。</p><h3 id="2-4-灵活性-amp-峰值处理能力"><a href="#2-4-灵活性-amp-峰值处理能力" class="headerlink" title="2.4　灵活性&amp;峰值处理能力"></a>2.4　灵活性&amp;峰值处理能力</h3><p>在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见；如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。</p><h3 id="2-5-可恢复性"><a href="#2-5-可恢复性" class="headerlink" title="2.5　可恢复性"></a>2.5　可恢复性</h3><p>系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。</p><h3 id="2-6-顺序保证"><a href="#2-6-顺序保证" class="headerlink" title="2.6　顺序保证"></a>2.6　顺序保证</h3><p>在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。Kafka保证一个Partition内的消息的有序性。</p><h3 id="2-7-缓冲"><a href="#2-7-缓冲" class="headerlink" title="2.7　缓冲"></a>2.7　缓冲</h3><p>在任何重要的系统中，都会有需要不同的处理时间的元素。例如，加载一张图片比应用过滤器花费更少的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行———写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。</p><h3 id="2-8-异步通信"><a href="#2-8-异步通信" class="headerlink" title="2.8　异步通信"></a>2.8　异步通信</h3><p>很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</p><h2 id="三、常用Message-Queue对比"><a href="#三、常用Message-Queue对比" class="headerlink" title="三、常用Message Queue对比"></a>三、常用Message Queue对比</h2><h3 id="3-1-RabbitMQ"><a href="#3-1-RabbitMQ" class="headerlink" title="3.1　RabbitMQ"></a>3.1　RabbitMQ</h3><p>RabbitMQ是使用Erlang编写的一个开源的消息队列，本身支持很多的协议：AMQP，XMPP, SMTP, STOMP，也正因如此，它非常重量级，更适合于企业级的开发。同时实现了Broker构架，这意味着消息在发送给客户端时先在中心队列排队。对路由，负载均衡或者数据持久化都有很好的支持。</p><h3 id="3-2-Redis"><a href="#3-2-Redis" class="headerlink" title="3.2　Redis"></a>3.2　Redis</h3><p>Redis是一个基于Key-Value对的NoSQL数据库，开发维护很活跃。虽然它是一个Key-Value数据库存储系统，但它本身支持MQ功能，所以完全可以当做一个轻量级的队列服务来使用。对于RabbitMQ和Redis的入队和出队操作，各执行100万次，每10万次记录一次执行时间。测试数据分为128Bytes、512Bytes、1K和10K四个不同大小的数据。实验表明：入队时，当数据比较小时Redis的性能要高于RabbitMQ，而如果数据大小超过了10K，Redis则慢的无法忍受；出队时，无论数据大小，Redis都表现出非常好的性能，而RabbitMQ的出队性能则远低于Redis。</p><h3 id="3-3-ZeroMQ"><a href="#3-3-ZeroMQ" class="headerlink" title="3.3　ZeroMQ"></a>3.3　ZeroMQ</h3><p>ZeroMQ号称最快的消息队列系统，尤其针对大吞吐量的需求场景。ZeroMQ能够实现RabbitMQ不擅长的高级/复杂的队列，但是开发人员需要自己组合多种技术框架，技术上的复杂度是对这MQ能够应用成功的挑战。ZeroMQ具有一个独特的非中间件的模式，你不需要安装和运行一个消息服务器或中间件，因为你的应用程序将扮演这个服务器角色。你只需要简单的引用ZeroMQ程序库，可以使用NuGet安装，然后你就可以愉快的在应用程序之间发送消息了。但是ZeroMQ仅提供非持久性的队列，也就是说如果宕机，数据将会丢失。其中，Twitter的Storm 0.9.0以前的版本中默认使用ZeroMQ作为数据流的传输（Storm从0.9版本开始同时支持ZeroMQ和Netty作为传输模块）。</p><h3 id="3-4-ActiveMQ"><a href="#3-4-ActiveMQ" class="headerlink" title="3.4　ActiveMQ"></a>3.4　ActiveMQ</h3><p>ActiveMQ是Apache下的一个子项目。 类似于ZeroMQ，它能够以代理人和点对点的技术实现队列。同时类似于RabbitMQ，它少量代码就可以高效地实现高级应用场景。</p><h3 id="3-5-Kafka-Jafka"><a href="#3-5-Kafka-Jafka" class="headerlink" title="3.5　Kafka/Jafka"></a>3.5　Kafka/Jafka</h3><p>Kafka是Apache下的一个子项目，是一个高性能跨语言分布式发布/订阅消息队列系统，而Jafka是在Kafka之上孵化而来的，即Kafka的一个升级版。具有以下特性：快速持久化，可以在O(1)的系统开销下进行消息持久化；高吞吐，在一台普通的服务器上既可以达到10W/s的吞吐速率；完全的分布式系统，Broker、Producer、Consumer都原生自动支持分布式，自动实现负载均衡；支持Hadoop数据并行加载，对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka通过Hadoop的并行加载机制统一了在线和离线的消息处理。Apache Kafka相对于ActiveMQ是一个非常轻量级的消息系统，除了性能非常好之外，还是一个工作良好的分布式系统。</p><h2 id="四、Kafka中的术语解释"><a href="#四、Kafka中的术语解释" class="headerlink" title="四、Kafka中的术语解释"></a>四、Kafka中的术语解释</h2><h3 id="4-1-概述"><a href="#4-1-概述" class="headerlink" title="4.1　概述"></a>4.1　概述</h3><p>在深入理解Kafka之前，先介绍一下Kafka中的术语。下图展示了Kafka的相关术语以及之间的关系：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180507190731172-1317551019.png" alt="img"></p><p>上图中一个topic配置了3个partition。Partition1有两个offset：0和1。Partition2有4个offset。Partition3有1个offset。副本的id和副本所在的机器的id恰好相同。</p><p>如果一个topic的副本数为3，那么Kafka将在集群中为每个partition创建3个相同的副本。集群中的每个broker存储一个或多个partition。多个producer和consumer可同时生产和消费数据。</p><h3 id="4-2-broker"><a href="#4-2-broker" class="headerlink" title="4.2　broker"></a>4.2　broker</h3><p>Kafka 集群包含一个或多个服务器，服务器节点称为broker。</p><p>broker存储topic的数据。如果某topic有N个partition，集群有N个broker，那么每个broker存储该topic的一个partition。</p><p>如果某topic有N个partition，集群有(N+M)个broker，那么其中有N个broker存储该topic的一个partition，剩下的M个broker不存储该topic的partition数据。</p><p>如果某topic有N个partition，集群中broker数目少于N个，那么一个broker存储该topic的一个或多个partition。在实际生产环境中，尽量避免这种情况的发生，这种情况容易导致Kafka集群数据不均衡。</p><h3 id="4-3-Topic"><a href="#4-3-Topic" class="headerlink" title="4.3　Topic"></a>4.3　Topic</h3><p>每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）</p><p>类似于数据库的表名</p><h3 id="4-3-Partition"><a href="#4-3-Partition" class="headerlink" title="4.3　Partition"></a>4.3　<strong>Partition</strong></h3><p>topic中的数据分割为一个或多个partition。每个topic至少有一个partition。每个partition中的数据使用多个segment文件存储。partition中的数据是有序的，不同partition间的数据丢失了数据的顺序。如果topic有多个partition，消费数据时就不能保证数据的顺序。在需要严格保证消息的消费顺序的场景下，需要将partition数目设为1。</p><h3 id="4-4-Producer"><a href="#4-4-Producer" class="headerlink" title="4.4　Producer"></a>4.4　Producer</h3><p>生产者即数据的发布者，该角色将消息发布到Kafka的topic中。broker接收到生产者发送的消息后，broker将该消息<strong>追加</strong>到当前用于追加数据的segment文件中。生产者发送的消息，存储到一个partition中，生产者也可以指定数据存储的partition。</p><h3 id="4-5-Consumer"><a href="#4-5-Consumer" class="headerlink" title="4.5　Consumer"></a>4.5　Consumer</h3><p>消费者可以从broker中读取数据。消费者可以消费多个topic中的数据。</p><h3 id="4-6-Consumer-Group"><a href="#4-6-Consumer-Group" class="headerlink" title="4.6　Consumer Group"></a>4.6　Consumer Group</h3><p>每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。</p><h3 id="4-7-Leader"><a href="#4-7-Leader" class="headerlink" title="4.7　Leader"></a>4.7　Leader</h3><p>每个partition有多个副本，其中有且仅有一个作为Leader，Leader是当前负责数据的读写的partition。</p><h3 id="4-8-Follower"><a href="#4-8-Follower" class="headerlink" title="4.8　Follower"></a>4.8　Follower</h3><p>Follower跟随Leader，所有写请求都通过Leader路由，数据变更会广播给所有Follower，Follower与Leader保持数据同步。如果Leader失效，则从Follower中选举出一个新的Leader。当Follower与Leader挂掉、卡住或者同步太慢，leader会把这个follower从“in sync replicas”（ISR）列表中删除，重新创建一个Follower。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume学习之路 （三）Flume的配置方式</title>
      <link href="/2018-05-03-Flume%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%89%EF%BC%89Flume%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%B9%E5%BC%8F.html"/>
      <url>/2018-05-03-Flume%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%89%EF%BC%89Flume%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%B9%E5%BC%8F.html</url>
      
        <content type="html"><![CDATA[<p>** Flume学习之路 （三）Flume的配置方式：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Flume学习之路 （三）Flume的配置方式</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、单一代理流配置"><a href="#一、单一代理流配置" class="headerlink" title="一、单一代理流配置"></a>一、单一代理流配置</h2><h3 id="1-1-官网介绍"><a href="#1-1-官网介绍" class="headerlink" title="1.1　官网介绍"></a>1.1　官网介绍</h3><p><a href="http://flume.apache.org/FlumeUserGuide.html#avro-source" target="_blank" rel="noopener">http://flume.apache.org/FlumeUserGuide.html#avro-source</a></p><p>通过一个通道将来源和接收器链接。需要列出源，接收器和通道，为给定的代理，然后指向源和接收器及通道。一个源的实例可以指定多个通道，但只能指定一个接收器实例。格式如下：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505194245456-451959321.png" alt="img"></p><p>实例解析：一个代理名为agent_foo，外部通过avro客户端，并且发送数据通过内存通道给hdfs。在配置文件foo.config的可能看起来像这样：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505194704534-537614658.png" alt="img"></p><p>案例说明：这将使事件流从avro-appserver-src-1到hdfs-sink-1通过内存通道mem-channel-1。当代理开始foo.config作为其配置文件，它会实例化流。</p><p><strong>配置单个组件</strong></p><p>定义流之后，需要设置每个源，接收器和通道的属性。可以分别设定组件的属性值。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505200346302-895681544.png" alt="img"></p><p>“type”属性必须为每个组件设置，以了解它需要什么样的对象。每个源，接收器和通道类型有其自己的一套，它所需的性能，以实现预期的功能。所有这些，必须根据需要设置。在前面的例子中，从hdfs-sink-1中的流到HDFS，通过内存通道mem-channel-1的avro-appserver-src-1源。下面是 一个例子，显示了这些组件的配置。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505200538929-1864653830.png" alt="img"></p><h3 id="1-2-测试示例（一）"><a href="#1-2-测试示例（一）" class="headerlink" title="1.2　测试示例（一）"></a>1.2　测试示例（一）</h3><p>通过flume来监控一个目录，当目录中有新文件时，将文件内容输出到控制台。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#配置一个agent，agent的名称可以自定义（如a1）</span><br><span class="line">#指定agent的sources（如s1）、sinks（如k1）、channels（如c1）</span><br><span class="line">#分别指定agent的sources，sinks,channels的名称 名称可以自定义</span><br><span class="line">a1.sources = s1  </span><br><span class="line">a1.sinks = k1  </span><br><span class="line">a1.channels = c1  </span><br><span class="line">   </span><br><span class="line">#描述source</span><br><span class="line">#配置目录scource</span><br><span class="line">a1.sources.s1.type =spooldir  </span><br><span class="line">a1.sources.s1.spoolDir =/home/hadoop/logs  </span><br><span class="line">a1.sources.s1.fileHeader= true  </span><br><span class="line">a1.sources.s1.channels =c1  </span><br><span class="line">   </span><br><span class="line">#配置sink </span><br><span class="line">a1.sinks.k1.type = logger  </span><br><span class="line">a1.sinks.k1.channel = c1  </span><br><span class="line">   </span><br><span class="line">#配置channel(内存做缓存)</span><br><span class="line">a1.channels.c1.type = memory</span><br></pre></td></tr></table></figure><p>启动命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ flume-ng agent --conf conf --conf-file /home/hadoop/apps/flume/examples/case_spool.properties --name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506194342069-922154308.png" alt="img"></p><p>将123.log移动到logs目录</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506194412852-76543455.png" alt="img"></p><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506194423110-835413271.png" alt="img"></p><h3 id="1-3-测试案例（二）"><a href="#1-3-测试案例（二）" class="headerlink" title="1.3　测试案例（二）"></a>1.3　测试案例（二）</h3><p>案例2：实时模拟从web服务器中读取数据到hdfs中</p><p>此处使用exec source详细参考<a href="http://www.cnblogs.com/qingyunzong/p/8995554.html" target="_blank" rel="noopener">http://www.cnblogs.com/qingyunzong/p/8995554.html</a></p><p>里面的2.3Exec Source介绍</p><p><a href="https://www.cnblogs.com/qingyunzong/p/8996155.html#_labelTop" target="_blank" rel="noopener">回到顶部</a></p><h2 id="二、单代理多流配置"><a href="#二、单代理多流配置" class="headerlink" title="二、单代理多流配置"></a>二、单代理多流配置</h2><p>单个Flume代理可以包含几个独立的流。你可以在一个配置文件中列出多个源，接收器和通道。这些组件可以连接形成多个流。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505194245456-451959321.png" alt="img"></p><p>可以连接源和接收器到其相应的通道，设置两个不同的流。例如，如果需要设置一个agent_foo代理两个流，一个从外部Avro客户端到HDFS，另外一个是tail的输出到Avro接收器，然后在这里是做一个配置</p><h3 id="2-1-官方案例"><a href="#2-1-官方案例" class="headerlink" title="2.1　官方案例"></a>2.1　官方案例</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506195038031-773927667.png" alt="img"></p><h2 id="三、配置多代理流程"><a href="#三、配置多代理流程" class="headerlink" title="三、配置多代理流程"></a>三、配置多代理流程</h2><p>设置一个多层的流，需要有一个指向下一跳avro源的第一跳的avro 接收器。这将导致第一Flume代理转发事件到下一个Flume代理。例如，如果定期发送的文件，每个事件（1文件）AVRO客户端使用本地Flume 代理，那么这个当地的代理可以转发到另一个有存储的代理。</p><p>配置如下</p><h3 id="3-1-官方案例"><a href="#3-1-官方案例" class="headerlink" title="3.1　官方案例"></a>3.1　官方案例</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506195144875-828067342.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506195203532-676409754.png" alt="img"></p><p>这里连接从weblog-agent的avro-forward-sink 到hdfs-agent的avro-collection-source收集源。最终结果从外部源的appserver最终存储在HDFS的事件。</p><h3 id="3-2-测试案例"><a href="#3-2-测试案例" class="headerlink" title="3.2　测试案例"></a>3.2　测试案例</h3><p>case_avro.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = s1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">a1.sources.s1.type = avro</span><br><span class="line">a1.sources.s1.channels = c1</span><br><span class="line">a1.sources.s1.bind = 192.168.123.102</span><br><span class="line">a1.sources.s1.port = 22222</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>case_avro_sink.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a2.sources = s1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"> </span><br><span class="line">a2.sources.s1.type = syslogtcp</span><br><span class="line">a2.sources.s1.channels = c1</span><br><span class="line">a2.sources.s1.host = 192.168.123.102</span><br><span class="line">a2.sources.s1.port = 33333</span><br><span class="line"> </span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"> </span><br><span class="line">a2.sinks.k1.type = avro</span><br><span class="line">a2.sinks.k1.hostname = 192.168.123.102</span><br><span class="line">a2.sinks.k1.port = 22222</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>说明：case_avro_sink.properties是前面的Agent，case_avro.properties是后面的Agent</p><p><strong>#**</strong>先启动Avro<strong><strong>的Source,</strong></strong>监听端口**</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ flume-ng agent --conf conf --conf-file ~/apps/flume/examples/case_avro.properties --name a1 -Dflume.root.logger=DEBUG,console -Dorg.apache.flume.log.printconfig=true -Dorg.apache.flume.log.rawdata=true</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506200609968-1667465574.png" alt="img"></p><p><strong>#**</strong>再启动Avro<strong>**的Sink</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent --conf conf --conf-file ~/apps/flume/examples/case_avro_sink.properties --name a2 -Dflume.root.logger=DEBUG,console -Dorg.apache.flume.log.printconfig=true -Dorg.apache.flume.log.rawdata=true</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506201628366-1736224639.png" alt="img"></p><p>可以看到已经建立连接</p><p><strong><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506201510416-1301773498.png" alt="img"></strong></p><p><strong>#**</strong>在Avro Sink<strong>**上生成测试log</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ echo &quot;hello flume avro sink&quot; | nc 192.168.123.102 33333</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506201816067-1646840986.png" alt="img"></p><p>查看其它结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506202008780-1105774708.png" alt="img"></p><h2 id="四、多路复用流"><a href="#四、多路复用流" class="headerlink" title="四、多路复用流"></a>四、多路复用流</h2><p>Flume支持扇出流从一个源到多个通道。有两种模式的扇出，复制和复用。在复制流的事件被发送到所有的配置通道。在复用的情况下，事件被发送到合格的渠 道只有一个子集。扇出流，需要指定源和扇出通道的规则。这是通过添加一个通道“选择”，可以复制或复用。再进一步指定选择的规则，如果它是一个多路。如果你 不指定一个选择，则默认情况下它复制。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505194245456-451959321.png" alt="img"></p><p>复用的选择集的属性进一步分叉。这需要指定一个事件属性映射到一组通道。选择配置属性中的每个事件头检查。如果指定的值相匹配，那么该事件被发送到所有的通道映射到该值。如果没有匹配，那么该事件被发送到设置为默认配置的通道。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506202356393-170234215.png" alt="img"></p><p>映射允许每个值通道可以重叠。默认值可以包含任意数量的通道。下面的示例中有一个单一的流复用两条路径。代理有一个单一的avro源和连接道两个接收器的两个通道。</p><h3 id="4-1-官方案例"><a href="#4-1-官方案例" class="headerlink" title="4.1　官方案例"></a>4.1　官方案例</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506202430234-980795967.png" alt="img"></p><p>“State”作为Header的选择检查。如果值是“CA”，然后将其发送到mem-channel-1，如果它的“AZ”的，那么jdbc- channel-2，如果它的“NY”那么发到这两个。如果“State”头未设置或不匹配的任何三个，然后去默认的mem-channel-1通道。</p><h3 id="4-2-测试案例（一）复制"><a href="#4-2-测试案例（一）复制" class="headerlink" title="4.2　测试案例（一）复制"></a>4.2　测试案例（一）复制</h3><p>case_replicate_sink.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = s1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"></span><br><span class="line">a1.sources.s1.type = syslogtcp</span><br><span class="line">a1.sources.s1.channels = c1 c2</span><br><span class="line">a1.sources.s1.host = 192.168.123.102</span><br><span class="line">a1.sources.s1.port = 6666</span><br><span class="line">a1.sources.s1.selector.type = replicating</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = 192.168.123.102</span><br><span class="line">a1.sinks.k1.port = 7777</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = 192.168.123.102</span><br><span class="line">a1.sinks.k1.port = 7777</span><br><span class="line">a1.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure><p>case_replicate_s1.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a2.sources = s1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"> </span><br><span class="line">a2.sources.s1.type = avro</span><br><span class="line">a2.sources.s1.channels = c1</span><br><span class="line">a2.sources.s1.host = 192.168.123.102</span><br><span class="line">a2.sources.s1.port = 7777</span><br><span class="line"> </span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"> </span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>case_replicate_s2.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a3.sources = s1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"></span><br><span class="line">a3.sources.s1.type = avro</span><br><span class="line">a3.sources.s1.channels = c1</span><br><span class="line">a3.sources.s1.host = 192.168.123.102</span><br><span class="line">a3.sources.s1.port = 7777</span><br><span class="line"></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p><strong>#**</strong>先启动Avro<strong><strong>的Source,</strong></strong>监听端口**</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent --conf conf --conf-file ~/apps/flume/examples/case_replicate_s1.properties --name a2 -Dflume.root.logger=DEBUG,console -Dorg.apache.flume.log.printconfig=true -Dorg.apache.flume.log.rawdata=true</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent --conf conf --conf-file ~/apps/flume/examples/case_replicate_s2.properties --name a3 -Dflume.root.logger=DEBUG,console -Dorg.apache.flume.log.printconfig=true -Dorg.apache.flume.log.rawdata=true</span><br></pre></td></tr></table></figure><p><strong>#**</strong>再启动Avro<strong>**的Sink</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent --conf conf --conf-file ~/apps/flume/examples/case_replicate_sink.properties --name a1 -Dflume.root.logger=DEBUG,console -Dorg.apache.flume.log.printconfig=true -Dorg.apache.flume.log.rawdata=true</span><br></pre></td></tr></table></figure><p><strong>#**</strong>生成测试log**</p><p>echo “hello via channel selector” | nc 192.168.123.102 6666</p><h3 id="4-3-测试案例（二）复用"><a href="#4-3-测试案例（二）复用" class="headerlink" title="4.3　测试案例（二）复用"></a>4.3　测试案例（二）复用</h3><p>case_multi_sink.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">#2个channel和2个sink的配置文件</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"> </span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = org.apache.flume.source.http.HTTPSource</span><br><span class="line">a1.sources.r1.port = 5140</span><br><span class="line">a1.sources.r1.host = 0.0.0.0</span><br><span class="line">a1.sources.r1.selector.type = multiplexing</span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line"> </span><br><span class="line">a1.sources.r1.selector.header = state</span><br><span class="line">a1.sources.r1.selector.mapping.CZ = c1</span><br><span class="line">a1.sources.r1.selector.mapping.US = c2</span><br><span class="line">a1.sources.r1.selector.default = c1</span><br><span class="line"> </span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k1.hostname = 172.25.4.23</span><br><span class="line">a1.sinks.k1.port = 4545</span><br><span class="line"> </span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.channel = c2</span><br><span class="line">a1.sinks.k2.hostname = 172.25.4.33</span><br><span class="line">a1.sinks.k2.port = 4545</span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"> </span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br></pre></td></tr></table></figure><p>case_ multi _s1.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"> </span><br><span class="line"># Describe/configure the source</span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sources.r1.bind = 172.25.4.23</span><br><span class="line">a2.sources.r1.port = 4545</span><br><span class="line"> </span><br><span class="line"># Describe the sink</span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"> a2.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br></pre></td></tr></table></figure><p>case_ multi _s2.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"> </span><br><span class="line"># Describe/configure the source</span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sources.r1.bind = 172.25.4.33</span><br><span class="line">a3.sources.r1.port = 4545</span><br><span class="line"> </span><br><span class="line"># Describe the sink</span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"> a3.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br></pre></td></tr></table></figure><p><strong>#**</strong>先启动Avro<strong><strong>的Source,</strong></strong>监听端口**</p><p>flume-ng agent -c . -f case_ multi _s1.conf -n a2 -Dflume.root.logger=INFO,console</p><p>flume-ng agent -c . -f case_ multi _s2.conf -n a3 -Dflume.root.logger=INFO,console</p><p><strong>#**</strong>再启动Avro<strong>**的Sink</strong></p><p>flume-ng agent -c . -f case_multi_sink.conf -n a1-Dflume.root.logger=INFO,console</p><p><strong>#**</strong>根据配置文件生成测试的header** <strong>为state**</strong>的POST<strong>**请求</strong></p><p>curl -X POST -d ‘[{ “headers” :{“state” : “CZ”},”body” : “TEST1”}]’ <a href="http://localhost:5140" target="_blank" rel="noopener">http://localhost:5140</a></p><p>curl -X POST -d ‘[{ “headers” :{“state” : “US”},”body” : “TEST2”}]’ <a href="http://localhost:5140" target="_blank" rel="noopener">http://localhost:5140</a></p><p>curl -X POST -d ‘[{ “headers” :{“state” : “SH”},”body” : “TEST3”}]’ <a href="http://localhost:5140" target="_blank" rel="noopener">http://localhost:5140</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume学习之路 （二）Flume的Source类型</title>
      <link href="/2018-05-02-Flume%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%EF%BC%89Flume%E7%9A%84Source%E7%B1%BB%E5%9E%8B.html"/>
      <url>/2018-05-02-Flume%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%EF%BC%89Flume%E7%9A%84Source%E7%B1%BB%E5%9E%8B.html</url>
      
        <content type="html"><![CDATA[<p>** Flume学习之路 （二）Flume的Source类型：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Flume学习之路 （二）Flume的Source类型</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>官方文档介绍：<a href="http://flume.apache.org/FlumeUserGuide.html#flume-sources" target="_blank" rel="noopener">http://flume.apache.org/FlumeUserGuide.html#flume-sources</a></p><h2 id="二、Flume-Sources-描述"><a href="#二、Flume-Sources-描述" class="headerlink" title="二、Flume Sources 描述"></a>二、Flume Sources 描述</h2><h3 id="2-1-Avro-Source"><a href="#2-1-Avro-Source" class="headerlink" title="2.1　Avro Source"></a>2.1　Avro Source</h3><h4 id="2-1-1-介绍"><a href="#2-1-1-介绍" class="headerlink" title="2.1.1　介绍"></a>2.1.1　介绍</h4><p>监听Avro端口，从Avro client streams接收events。当与另一个（前一跳）Flume agent内置的Avro Sink配对时，它可以创建分层收集拓扑。<strong>字体加粗的属性必须进行设置</strong>。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505184843030-1971765582.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506141436097-274079212.png" alt="img"></p><h4 id="2-1-2-示例"><a href="#2-1-2-示例" class="headerlink" title="2.1.2　示例"></a>2.1.2　示例</h4><p>示例一：示例请参考官方文档</p><p>示例二：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#配置一个agent，agent的名称可以自定义（如a1）</span><br><span class="line">#指定agent的sources（如s1）、sinks（如k1）、channels（如c1）</span><br><span class="line">#分别指定agent的sources，sinks,channels的名称 名称可以自定义</span><br><span class="line">a1.sources = s1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置source</span><br><span class="line">a1.sources.s1.channels = c1</span><br><span class="line">a1.sources.s1.type = avro</span><br><span class="line">a1.sources.s1.bind = 192.168.123.102</span><br><span class="line">a1.sources.s1.port = 6666</span><br><span class="line"></span><br><span class="line">#配置channels</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"></span><br><span class="line">#配置sinks</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line">#为sources和sinks绑定channels</span><br><span class="line">a1.sources.s1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>启动flume</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ flume-ng agent --conf conf --conf-file ~/apps/flume/examples/single_avro.properties --name a1 -Dflume.root.logger=DEBUG,console -Dorg.apache.flume.log.printconfig=true -Dorg.apache.flume.log.rawdata=true</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506145745861-959604080.png" alt="img"></p><p>通过flume提供的avro客户端向指定机器指定端口发送日志信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ flume-ng avro-client -c ~/apps/flume/conf -H 192.168.123.102 -p 6666 -F 666.txt</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506145952465-49933213.png" alt="img"></p><p>接收到的信息</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506150037233-1093975937.png" alt="img"></p><h3 id="2-2-Thrift-Source"><a href="#2-2-Thrift-Source" class="headerlink" title="2.2　Thrift Source"></a>2.2　Thrift Source</h3><h4 id="2-2-1-介绍"><a href="#2-2-1-介绍" class="headerlink" title="2.2.1　介绍"></a>2.2.1　介绍</h4><p>ThriftSource 与Avro Source 基本一致。只要把source的类型改成thrift即可，例如a1.sources.r1.type = thrift，比较简单，不做赘述。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506150242469-1712924280.png" alt="img"></p><h3 id="2-3-Exec-Source"><a href="#2-3-Exec-Source" class="headerlink" title="2.3　Exec Source"></a>2.3　Exec Source</h3><h4 id="2-3-1-介绍"><a href="#2-3-1-介绍" class="headerlink" title="2.3.1　介绍"></a>2.3.1　介绍</h4><p>ExecSource的配置就是设定一个Unix(linux)命令，然后通过这个命令不断输出数据。如果进程退出，Exec Source也一起退出，不会产生进一步的数据。</p><p>下面是官网给出的source的配置，加粗的参数是必选，描述就不解释了。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506150910809-733881050.png" alt="img"></p><h4 id="2-3-2-示例"><a href="#2-3-2-示例" class="headerlink" title="2.3.2　示例"></a>2.3.2　示例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#配置文件</span><br><span class="line">#Name the components on this agent  </span><br><span class="line">a1.sources= s1  </span><br><span class="line">a1.sinks= k1  </span><br><span class="line">a1.channels= c1  </span><br><span class="line">   </span><br><span class="line">#配置sources</span><br><span class="line">a1.sources.s1.type = exec  </span><br><span class="line">a1.sources.s1.command = tail -F /home/hadoop/logs/test.log  </span><br><span class="line">a1.sources.s1.channels = c1  </span><br><span class="line">   </span><br><span class="line">#配置sinks </span><br><span class="line">a1.sinks.k1.type= logger  </span><br><span class="line">a1.sinks.k1.channel= c1  </span><br><span class="line">   </span><br><span class="line">#配置channel</span><br><span class="line">a1.channels.c1.type= memory</span><br></pre></td></tr></table></figure><p>启动命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ flume-ng agent --conf conf --conf-file ~/apps/flume/examples/case_exec.properties --name a1 -Dflume.root.logger=DEBUG,console -Dorg.apache.flume.log.printconfig=true -Dorg.apache.flume.log.rawdata=true</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506152209264-144025853.png" alt="img"></p><p>继续往日志里添加数据</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506152321240-2131595143.png" alt="img"></p><p>接收到的信息</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506152357088-2003036234.png" alt="img"></p><h3 id="2-4-JMS-Source"><a href="#2-4-JMS-Source" class="headerlink" title="2.4　JMS Source"></a>2.4　JMS Source</h3><h4 id="2-4-1-介绍"><a href="#2-4-1-介绍" class="headerlink" title="2.4.1　介绍"></a>2.4.1　介绍</h4><p>从JMS系统（消息、主题）中读取数据，ActiveMQ已经测试过</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506152618649-1113553854.png" alt="img"></p><h4 id="2-4-2-官网示例"><a href="#2-4-2-官网示例" class="headerlink" title="2.4.2　官网示例"></a>2.4.2　官网示例</h4><p> <img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506152721617-417195784.png" alt="img"></p><h3 id="2-5-Spooling-Directory-Source"><a href="#2-5-Spooling-Directory-Source" class="headerlink" title="2.5　Spooling Directory Source"></a>2.5　Spooling Directory Source</h3><h4 id="2-5-1-介绍"><a href="#2-5-1-介绍" class="headerlink" title="2.5.1　介绍"></a>2.5.1　介绍</h4><p>Spooling Directory Source监测配置的目录下新增的文件，并将文件中的数据读取出来。其中，Spool Source有2个注意地方，<strong>第一个是拷贝到spool目录下的文件不可以再打开编辑，第二个是spool目录下不可包含相应的子目录。这个主要用途作为对日志的准实时监控</strong>。</p><p>下面是官网给出的source的配置，加粗的参数是必选。可选项太多，这边就介绍一个<strong>fileSuffix</strong>，即文件读取后添加的后缀名，这个是可以更改。</p><p> <img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506153121836-501580850.png" alt="img"></p><h4 id="2-5-2-示例"><a href="#2-5-2-示例" class="headerlink" title="2.5.2　示例"></a>2.5.2　示例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = s1  </span><br><span class="line">a1.sinks = k1  </span><br><span class="line">a1.channels = c1  </span><br><span class="line">   </span><br><span class="line"># Describe/configure the source  </span><br><span class="line">a1.sources.s1.type =spooldir  </span><br><span class="line">a1.sources.s1.spoolDir =/home/hadoop/logs  </span><br><span class="line">a1.sources.s1.fileHeader= true  </span><br><span class="line">a1.sources.s1.channels =c1  </span><br><span class="line">   </span><br><span class="line"># Describe the sink  </span><br><span class="line">a1.sinks.k1.type = logger  </span><br><span class="line">a1.sinks.k1.channel = c1  </span><br><span class="line">   </span><br><span class="line"># Use a channel which buffers events inmemory  </span><br><span class="line">a1.channels.c1.type = memory</span><br></pre></td></tr></table></figure><p>启动命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ flume-ng agent --conf conf --conf-file /home/hadoop/apps/flume/examples/case_spool.properties --name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506154727697-1088669530.png" alt="img"></p><p>讲123.log移动到logs目录</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506154936448-1678686443.png" alt="img"></p><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180506154905986-1159084596.png" alt="img"></p><h3 id="2-6-其他"><a href="#2-6-其他" class="headerlink" title="2.6　其他"></a>2.6　其他</h3><p>参考<a href="https://blog.csdn.net/looklook5/article/details/40400885" target="_blank" rel="noopener">https://blog.csdn.net/looklook5/article/details/40400885</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume学习之路 （一）Flume的基础介绍</title>
      <link href="/2018-05-01-Flume%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%80%EF%BC%89Flume%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D.html"/>
      <url>/2018-05-01-Flume%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%80%EF%BC%89Flume%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D.html</url>
      
        <content type="html"><![CDATA[<p>** Flume学习之路 （一）Flume的基础介绍：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Flume学习之路 （一）Flume的基础介绍</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>Hadoop业务的整体开发流程：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505130552077-1235640783.png" alt="img"></p><p>　　从Hadoop的业务开发流程图中可以看出，在大数据的业务处理过程中，对于数据的采集是十分重要的一步，也是不可避免的一步.</p><p>许多公司的平台每天会产生大量的日志（一般为流式数据，如，搜索引擎的pv，查询等），处理这些日志需要特定的日志系统，一般而言，这些系统需要具有以下特征：</p><p>（1） 构建应用系统和分析系统的桥梁，并将它们之间的关联解耦；</p><p>（2） 支持近实时的在线分析系统和类似于Hadoop之类的离线分析系统；</p><p>（3） 具有高可扩展性。即：当数据量增加时，可以通过增加节点进行水平扩展。</p><p>开源的日志系统，包括facebook的scribe，apache的chukwa，linkedin的kafka和cloudera的flume等。</p><h2 id="二、Flume的简介"><a href="#二、Flume的简介" class="headerlink" title="二、Flume的简介"></a>二、Flume的简介</h2><p>　　flume 作为 cloudera 开发的实时日志收集系统，受到了业界的认可与广泛应用。Flume 初始的发行版本目前被统称为 Flume OG（original generation），属于 cloudera。</p><p>　　但随着 FLume 功能的扩展，Flume OG 代码工程臃肿、核心组件设计不合理、核心配置不标准等缺点暴露出来，尤其是在 Flume OG 的最后一个发行版本 0.9.4. 中，日</p><p>志传输不稳定的现象尤为严重，为了解决这些问题，2011 年 10 月 22 号，cloudera 完成了 Flume-728，对 Flume 进行了里程碑式的改动：重构核心组件、核心配置以</p><p>及代码架构，重构后的版本统称为 Flume NG（next generation）；改动的另一原因是将 Flume 纳入 apache 旗下，cloudera Flume 改名为 Apache Flume。</p><p>　　Flume是Apache的顶级项目，官方网站：<a href="http://flume.apache.org/" target="_blank" rel="noopener">http://flume.apache.org/</a></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505130937239-1763974355.png" alt="img"></p><p>　　Flume是一个分布式、可靠、高可用的海量日志聚合系统，支持在系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据的简单处理，并写到各种数据接收方的能力。</p><p>Flume 在0.9.x and 1.x之间有较大的架构调整，1.x版本之后的改称Flume NG，0.9.x的称为Flume OG。</p><p>　　Flume目前只有Linux系统的启动脚本，没有Windows环境的启动脚本。</p><h2 id="三、Flume-NG的介绍"><a href="#三、Flume-NG的介绍" class="headerlink" title="三、Flume NG的介绍"></a>三、Flume NG的介绍</h2><h3 id="3-1-Flume特点"><a href="#3-1-Flume特点" class="headerlink" title="3.1　Flume特点"></a>3.1　Flume特点</h3><p>flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。支持在日志系统中定制各类数据发送方，用于收集数据;同时，Flume提供对数据进行简单处理，并写到各种数据接受方(比如文本、HDFS、Hbase等)的能力 。</p><p>　　flume的数据流由事件(Event)贯穿始终。事件是Flume的基本数据单位，它携带日志数据(字节数组形式)并且携带有头信息，这些Event由Agent外部的Source生成，当Source捕获事件后会进行特定的格式化，然后Source会把事件推入(单个或多个)Channel中。你可以把Channel看作是一个缓冲区，它将保存事件直到Sink处理完该事件。Sink负责持久化日志或者把事件推向另一个Source。</p><p>　（1）flume的可靠性<br>　　当节点出现故障时，日志能够被传送到其他节点上而不会丢失。Flume提供了三种级别的可靠性保障，从强到弱依次分别为：end-to-end（收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送。），Store on failure（这也是scribe采用的策略，当数据接收方crash时，将数据写到本地，待恢复后，继续发送），Besteffort（数据发送到接收方后，不会进行确认）。</p><p>　（2）flume的可恢复性<br>　　还是靠Channel。推荐使用FileChannel，事件持久化在本地文件系统里(性能较差)。</p><h3 id="3-2-Flume的一些核心概念"><a href="#3-2-Flume的一些核心概念" class="headerlink" title="3.2　Flume的一些核心概念"></a>3.2　Flume的一些核心概念</h3><p>　　Client：Client生产数据，运行在一个独立的线程。</p><p>　　Event： 一个数据单元，消息头和消息体组成。（Events可以是日志记录、 avro 对象等。）<br>　　Flow： Event从源点到达目的点的迁移的抽象。<br>　　Agent： 一个独立的Flume进程，包含组件Source、 Channel、 Sink。（Agent使用JVM 运行Flume。每台机器运行一个agent，但是可以在一个agent中包含</p><p>　　　　　　多个sources和sinks。）<br>　　Source： 数据收集组件。（source从Client收集数据，传递给Channel）<br>　　Channel： 中转Event的一个临时存储，保存由Source组件传递过来的Event。（Channel连接 sources 和 sinks ，这个有点像一个队列。）<br>　　Sink： 从Channel中读取并移除Event， 将Event传递到FlowPipeline中的下一个Agent（如果有的话）（Sink从Channel收集数据，运行在一个独立线程。）</p><h3 id="3-3-Flume-NG的体系结构"><a href="#3-3-Flume-NG的体系结构" class="headerlink" title="3.3　Flume NG的体系结构"></a>3.3　Flume NG的体系结构</h3><p>　Flume 运行的核心是 Agent。Flume以agent为最小的独立运行单位。一个agent就是一个JVM。它是一个完整的数据收集工具，含有三个核心组件，分别是</p><p>　source、 channel、 sink。通过这些组件， Event 可以从一个地方流向另一个地方，如下图所示。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505131617177-926828141.png" alt="img"></p><h3 id="3-4-Source"><a href="#3-4-Source" class="headerlink" title="3.4　Source"></a>3.4　Source</h3><p>　　Source是数据的收集端，负责将数据捕获后进行特殊的格式化，将数据封装到事件（event） 里，然后将事件推入Channel中。</p><p>　　Flume提供了各种source的实现，包括Avro Source、Exce Source、Spooling Directory Source、NetCat Source、Syslog Source、Syslog TCP Source、Syslog UDP Source、HTTP Source、HDFS Source，etc。如果内置的Source无法满足需要， Flume还支持自定义Source。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505131939853-800980370.png" alt="img"></p><h3 id="3-5-Channel"><a href="#3-5-Channel" class="headerlink" title="3.5　Channel"></a>3.5　Channel</h3><p>　　Channel是连接Source和Sink的组件，大家可以将它看做一个数据的缓冲区（数据队列），它可以将事件暂存到内存中也可以持久化到本地磁盘上， 直到Sink处理完该事件。</p><p>Flume对于Channel，则提供了Memory Channel、JDBC Chanel、File Channel，etc。</p><p>　　MemoryChannel可以实现高速的吞吐，但是无法保证数据的完整性。</p><p>　　MemoryRecoverChannel在官方文档的建议上已经建义使用FileChannel来替换。</p><p>　　FileChannel保证数据的完整性与一致性。在具体配置不现的FileChannel时，建议FileChannel设置的目录和程序日志文件保存的目录设成不同的磁盘，以便提高效率。</p><h3 id="3-6-Sink"><a href="#3-6-Sink" class="headerlink" title="3.6　Sink"></a>3.6　Sink</h3><p>　　Flume Sink取出Channel中的数据，进行相应的存储文件系统，数据库，或者提交到远程服务器。</p><p>　　Flume也提供了各种sink的实现，包括HDFS sink、Logger sink、Avro sink、File Roll sink、Null sink、HBase sink，etc。</p><p>　　Flume Sink在设置存储数据时，可以向文件系统中，数据库中，hadoop中储数据，在日志数据较少时，可以将数据存储在文件系中，并且设定一定的时间间隔保存数据。在日志数据较多时，可以将相应的日志数据存储到Hadoop中，便于日后进行相应的数据分析。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505132525329-1581308997.png" alt="img"></p><h2 id="四、Flume的部署类型"><a href="#四、Flume的部署类型" class="headerlink" title="四、Flume的部署类型"></a>四、Flume的部署类型</h2><h3 id="4-1-单一流程"><a href="#4-1-单一流程" class="headerlink" title="4.1　单一流程"></a>4.1　单一流程</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505133009480-1735773489.png" alt="img"></p><h3 id="4-2-多代理流程（多个agent顺序连接）"><a href="#4-2-多代理流程（多个agent顺序连接）" class="headerlink" title="4.2　多代理流程（多个agent顺序连接）"></a>4.2　多代理流程（多个agent顺序连接）</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505133038658-581187076.png" alt="img">　　</p><p>　　可以将多个Agent顺序连接起来，将最初的数据源经过收集，存储到最终的存储系统中。这是最简单的情况，一般情况下，应该控制这种顺序连接的Agent 的数量，因为数据流经的路径变长了，如果不考虑failover的话，出现故障将影响整个Flow上的Agent收集服务。 </p><h3 id="4-3-流的合并（多个Agent的数据汇聚到同一个Agent-）"><a href="#4-3-流的合并（多个Agent的数据汇聚到同一个Agent-）" class="headerlink" title="4.3　流的合并（多个Agent的数据汇聚到同一个Agent ）"></a>4.3　流的合并（多个Agent的数据汇聚到同一个Agent ）</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505133200273-2038146111.png" alt="img"></p><p>　　这种情况应用的场景比较多，比如要收集Web网站的用户行为日志， Web网站为了可用性使用的负载集群模式，每个节点都产生用户行为日志，可以为每 个节点都配置一个Agent来单独收集日志数据，然后多个Agent将数据最终汇聚到一个用来存储数据存储系统，如HDFS上。</p><h3 id="4-4-多路复用流（多级流）"><a href="#4-4-多路复用流（多级流）" class="headerlink" title="4.4　多路复用流（多级流）"></a>4.4　多路复用流（多级流）</h3><p>　　Flume还支持多级流，什么多级流？来举个例子，当syslog， java， nginx、 tomcat等混合在一起的日志流开始流入一个agent后，可以agent中将混杂的日志流分开，然后给每种日志建立一个自己的传输通道。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505133415618-1087849488.png" alt="img"></p><h3 id="4-5-load-balance功能"><a href="#4-5-load-balance功能" class="headerlink" title="4.5　load balance功能"></a>4.5　load balance功能</h3><p>　　下图Agent1是一个路由节点，负责将Channel暂存的Event均衡到对应的多个Sink组件上，而每个Sink组件分别连接到一个独立的Agent上 。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505133509376-482336945.png" alt="img"></p><h2 id="五、Flume的安装"><a href="#五、Flume的安装" class="headerlink" title="五、Flume的安装"></a>五、Flume的安装</h2><h3 id="5-1-Flume的下载"><a href="#5-1-Flume的下载" class="headerlink" title="5.1　Flume的下载"></a>5.1　Flume的下载</h3><p>下载地址：</p><p><a href="http://mirrors.hust.edu.cn/apache/" target="_blank" rel="noopener">http://mirrors.hust.edu.cn/apache/</a></p><p><a href="http://flume.apache.org/download.html" target="_blank" rel="noopener">http://flume.apache.org/download.html</a></p><h3 id="5-2-Flume的安装"><a href="#5-2-Flume的安装" class="headerlink" title="5.2　Flume的安装"></a>5.2　Flume的安装</h3><p>　　Flume框架对hadoop和zookeeper的依赖只是在jar包上，并不要求flume启动时必须将hadoop和zookeeper服务也启动。</p><h4 id="（1）将安装包上传到服务器并解压"><a href="#（1）将安装包上传到服务器并解压" class="headerlink" title="（1）将安装包上传到服务器并解压"></a>（1）将安装包上传到服务器并解压</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ tar -zxvf apache-flume-1.8.0-bin.tar.gz -C apps/</span><br></pre></td></tr></table></figure><h4 id="（2）创建软连接"><a href="#（2）创建软连接" class="headerlink" title="（2）创建软连接"></a>（2）创建软连接</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ ln -s apache-flume-1.8.0-bin/ flume</span><br></pre></td></tr></table></figure><h4 id="（3）修改配置文件"><a href="#（3）修改配置文件" class="headerlink" title="（3）修改配置文件"></a>（3）修改配置文件</h4><p>/home/hadoop/apps/apache-flume-1.8.0-bin/conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ cp flume-env.sh.template flume-env.sh</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505141146383-24188676.png" alt="img"></p><h4 id="（4）配置环境变量"><a href="#（4）配置环境变量" class="headerlink" title="（4）配置环境变量"></a>（4）配置环境变量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ vi ~/.bashrc </span><br><span class="line">#FLUME</span><br><span class="line">export FLUME_HOME=/home/hadoop/apps/flume</span><br><span class="line">export PATH=$PATH:$FLUME_HOME/bin</span><br></pre></td></tr></table></figure><p>保存使其立即生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ source ~/.bashrc</span><br></pre></td></tr></table></figure><h4 id="（5）查看版本"><a href="#（5）查看版本" class="headerlink" title="（5）查看版本"></a>（5）查看版本</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ flume-ng version</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180505150126118-2040189318.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase学习之路 （四）HBase的API操作</title>
      <link href="/2018-06-04-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%9B%9B%EF%BC%89HBase%E7%9A%84API%E6%93%8D%E4%BD%9C.html"/>
      <url>/2018-06-04-HBase%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%9B%9B%EF%BC%89HBase%E7%9A%84API%E6%93%8D%E4%BD%9C.html</url>
      
        <content type="html"><![CDATA[<p>** HBase学习之路 （四）HBase的API操作：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        HBase学习之路 （四）HBase的API操作</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="Eclipse环境搭建"><a href="#Eclipse环境搭建" class="headerlink" title="Eclipse环境搭建"></a>Eclipse环境搭建</h2><p>具体的jar的引入方式可以参考<a href="http://www.cnblogs.com/qingyunzong/p/8623309.html" target="_blank" rel="noopener">http://www.cnblogs.com/qingyunzong/p/8623309.html</a></p><h2 id="HBase-API操作表和数据"><a href="#HBase-API操作表和数据" class="headerlink" title="HBase API操作表和数据"></a>HBase API操作表和数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br></pre></td><td class="code"><pre><span class="line">  1 import java.io.IOException;</span><br><span class="line">  2 import java.util.Date;</span><br><span class="line">  3 </span><br><span class="line">  4 import org.apache.hadoop.conf.Configuration;</span><br><span class="line">  5 import org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line">  6 import org.apache.hadoop.hbase.HColumnDescriptor;</span><br><span class="line">  7 import org.apache.hadoop.hbase.HTableDescriptor;</span><br><span class="line">  8 import org.apache.hadoop.hbase.TableName;</span><br><span class="line">  9 import org.apache.hadoop.hbase.client.Admin;</span><br><span class="line"> 10 import org.apache.hadoop.hbase.client.Connection;</span><br><span class="line"> 11 import org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line"> 12 import org.apache.hadoop.hbase.client.Delete;</span><br><span class="line"> 13 import org.apache.hadoop.hbase.client.Get;</span><br><span class="line"> 14 import org.apache.hadoop.hbase.client.Put;</span><br><span class="line"> 15 import org.apache.hadoop.hbase.client.Result;</span><br><span class="line"> 16 import org.apache.hadoop.hbase.client.ResultScanner;</span><br><span class="line"> 17 import org.apache.hadoop.hbase.client.Scan;</span><br><span class="line"> 18 import org.apache.hadoop.hbase.client.Table;</span><br><span class="line"> 19 </span><br><span class="line"> 20 import com.study.hbase.service.HBaseUtils;</span><br><span class="line"> 21 </span><br><span class="line"> 22 public class HBaseUtilsImpl implements HBaseUtils &#123;</span><br><span class="line"> 23 </span><br><span class="line"> 24     private static final String ZK_CONNECT_KEY = &quot;hbase.zookeeper.quorum&quot;;</span><br><span class="line"> 25     private static final String ZK_CONNECT_VALUE = &quot;hadoop1:2181,hadoop2:2181,hadoop3:2181&quot;;</span><br><span class="line"> 26 </span><br><span class="line"> 27     private static Connection conn = null;</span><br><span class="line"> 28     private static Admin admin = null;</span><br><span class="line"> 29 </span><br><span class="line"> 30     public static void main(String[] args) throws Exception &#123;</span><br><span class="line"> 31         </span><br><span class="line"> 32         getConnection();</span><br><span class="line"> 33         getAdmin();</span><br><span class="line"> 34         </span><br><span class="line"> 35         HBaseUtilsImpl hbu = new HBaseUtilsImpl();</span><br><span class="line"> 36         </span><br><span class="line"> 37         </span><br><span class="line"> 38         //hbu.getAllTables();</span><br><span class="line"> 39         </span><br><span class="line"> 40         //hbu.descTable(&quot;people&quot;);</span><br><span class="line"> 41         </span><br><span class="line"> 42         //String[] infos = &#123;&quot;info&quot;,&quot;family&quot;&#125;;</span><br><span class="line"> 43         //hbu.createTable(&quot;people&quot;, infos);</span><br><span class="line"> 44         </span><br><span class="line"> 45         //String[] add = &#123;&quot;cs1&quot;,&quot;cs2&quot;&#125;;</span><br><span class="line"> 46         //String[] remove = &#123;&quot;cf1&quot;,&quot;cf2&quot;&#125;;</span><br><span class="line"> 47         </span><br><span class="line"> 48         //HColumnDescriptor hc = new HColumnDescriptor(&quot;sixsixsix&quot;);</span><br><span class="line"> 49         </span><br><span class="line"> 50         //hbu.modifyTable(&quot;stu&quot;,hc);</span><br><span class="line"> 51         //hbu.getAllTables();</span><br><span class="line"> 52 </span><br><span class="line"> 53         </span><br><span class="line"> 54         hbu.putData(&quot;huoying&quot;, &quot;rk001&quot;, &quot;cs2&quot;, &quot;name&quot;, &quot;aobama&quot;,new Date().getTime());</span><br><span class="line"> 55         hbu.getAllTables();</span><br><span class="line"> 56         </span><br><span class="line"> 57         conn.close();</span><br><span class="line"> 58     &#125;</span><br><span class="line"> 59 </span><br><span class="line"> 60     // 获取连接</span><br><span class="line"> 61     public static Connection getConnection() &#123;</span><br><span class="line"> 62         // 创建一个可以用来管理hbase配置信息的conf对象</span><br><span class="line"> 63         Configuration conf = HBaseConfiguration.create();</span><br><span class="line"> 64         // 设置当前的程序去寻找的hbase在哪里</span><br><span class="line"> 65         conf.set(ZK_CONNECT_KEY, ZK_CONNECT_VALUE);</span><br><span class="line"> 66         try &#123;</span><br><span class="line"> 67             conn = ConnectionFactory.createConnection(conf);</span><br><span class="line"> 68         &#125; catch (IOException e) &#123;</span><br><span class="line"> 69             e.printStackTrace();</span><br><span class="line"> 70         &#125;</span><br><span class="line"> 71         return conn;</span><br><span class="line"> 72     &#125;</span><br><span class="line"> 73 </span><br><span class="line"> 74     // 获取管理员对象</span><br><span class="line"> 75     public static Admin getAdmin() &#123;</span><br><span class="line"> 76         try &#123;</span><br><span class="line"> 77             admin = conn.getAdmin();</span><br><span class="line"> 78         &#125; catch (IOException e) &#123;</span><br><span class="line"> 79             e.printStackTrace();</span><br><span class="line"> 80         &#125;</span><br><span class="line"> 81         return admin;</span><br><span class="line"> 82     &#125;</span><br><span class="line"> 83 </span><br><span class="line"> 84     // 查询所有表</span><br><span class="line"> 85     @Override</span><br><span class="line"> 86     public void getAllTables() throws Exception &#123;</span><br><span class="line"> 87         //获取列簇的描述信息</span><br><span class="line"> 88         HTableDescriptor[] listTables = admin.listTables();</span><br><span class="line"> 89         for (HTableDescriptor listTable : listTables) &#123;</span><br><span class="line"> 90             //转化为表名</span><br><span class="line"> 91             String tbName = listTable.getNameAsString();</span><br><span class="line"> 92             //获取列的描述信息</span><br><span class="line"> 93             HColumnDescriptor[] columnFamilies = listTable.getColumnFamilies();</span><br><span class="line"> 94             System.out.println(&quot;tableName:&quot;+tbName);</span><br><span class="line"> 95             for(HColumnDescriptor columnFamilie : columnFamilies) &#123;</span><br><span class="line"> 96                 //获取列簇的名字</span><br><span class="line"> 97                 String columnFamilyName = columnFamilie.getNameAsString();</span><br><span class="line"> 98                 System.out.print(&quot;\t&quot;+&quot;columnFamilyName:&quot;+columnFamilyName);</span><br><span class="line"> 99             &#125;</span><br><span class="line">100             System.out.println();</span><br><span class="line">101         &#125;</span><br><span class="line">102 </span><br><span class="line">103     &#125;</span><br><span class="line">104 </span><br><span class="line">105     // 创建表，传参，表名和列簇的名字</span><br><span class="line">106     @Override</span><br><span class="line">107     public void createTable(String tableName, String[] family) throws Exception &#123;</span><br><span class="line">108         </span><br><span class="line">109         TableName name = TableName.valueOf(tableName);</span><br><span class="line">110         //判断表是否存在</span><br><span class="line">111         if(admin.tableExists(name)) &#123;</span><br><span class="line">112             System.out.println(&quot;table已经存在！&quot;);</span><br><span class="line">113         &#125;else &#123;</span><br><span class="line">114             //表的列簇示例</span><br><span class="line">115             HTableDescriptor htd = new HTableDescriptor(name);</span><br><span class="line">116             //向列簇中添加列的信息</span><br><span class="line">117             for(String str : family) &#123;</span><br><span class="line">118                 HColumnDescriptor hcd = new HColumnDescriptor(str);</span><br><span class="line">119                 htd.addFamily(hcd);</span><br><span class="line">120             &#125;</span><br><span class="line">121             //创建表</span><br><span class="line">122             admin.createTable(htd);</span><br><span class="line">123             //判断表是否创建成功</span><br><span class="line">124             if(admin.tableExists(name)) &#123;</span><br><span class="line">125                 System.out.println(&quot;table创建成功&quot;);</span><br><span class="line">126             &#125;else &#123;</span><br><span class="line">127                 System.out.println(&quot;table创建失败&quot;);</span><br><span class="line">128             &#125;</span><br><span class="line">129         &#125;    </span><br><span class="line">130         </span><br><span class="line">131     &#125;</span><br><span class="line">132 </span><br><span class="line">133     // 创建表，传参:封装好的多个列簇</span><br><span class="line">134     @Override</span><br><span class="line">135     public void createTable(HTableDescriptor htds) throws Exception &#123;</span><br><span class="line">136         //获得表的名字</span><br><span class="line">137         String tbName = htds.getNameAsString();</span><br><span class="line">138         </span><br><span class="line">139         admin.createTable(htds);</span><br><span class="line">140     &#125;</span><br><span class="line">141 </span><br><span class="line">142     // 创建表，传参，表名和封装好的多个列簇</span><br><span class="line">143     @Override</span><br><span class="line">144     public void createTable(String tableName, HTableDescriptor htds) throws Exception &#123;</span><br><span class="line">145 </span><br><span class="line">146         TableName name = TableName.valueOf(tableName);</span><br><span class="line">147         </span><br><span class="line">148         if(admin.tableExists(name)) &#123;</span><br><span class="line">149             System.out.println(&quot;table已经存在！&quot;);</span><br><span class="line">150         &#125;else &#123;</span><br><span class="line">151             admin.createTable(htds);</span><br><span class="line">152             boolean flag = admin.tableExists(name);</span><br><span class="line">153             System.out.println(flag ? &quot;创建成功&quot; : &quot;创建失败&quot;);</span><br><span class="line">154         &#125;</span><br><span class="line">155         </span><br><span class="line">156     &#125;</span><br><span class="line">157 </span><br><span class="line">158     </span><br><span class="line">159     // 查看表的列簇属性</span><br><span class="line">160     @Override</span><br><span class="line">161     public void descTable(String tableName) throws Exception &#123;</span><br><span class="line">162         //转化为表名</span><br><span class="line">163         TableName name = TableName.valueOf(tableName);</span><br><span class="line">164         //判断表是否存在</span><br><span class="line">165         if(admin.tableExists(name)) &#123;</span><br><span class="line">166             //获取表中列簇的描述信息</span><br><span class="line">167             HTableDescriptor tableDescriptor = admin.getTableDescriptor(name);</span><br><span class="line">168             //获取列簇中列的信息</span><br><span class="line">169             HColumnDescriptor[] columnFamilies = tableDescriptor.getColumnFamilies();</span><br><span class="line">170             for(HColumnDescriptor columnFamily : columnFamilies) &#123;</span><br><span class="line">171                 System.out.println(columnFamily);</span><br><span class="line">172             &#125;</span><br><span class="line">173             </span><br><span class="line">174         &#125;else &#123;</span><br><span class="line">175             System.out.println(&quot;table不存在&quot;);</span><br><span class="line">176         &#125;</span><br><span class="line">177         </span><br><span class="line">178     &#125;</span><br><span class="line">179 </span><br><span class="line">180     // 判断表存在不存在</span><br><span class="line">181     @Override</span><br><span class="line">182     public boolean existTable(String tableName) throws Exception &#123;</span><br><span class="line">183         TableName name = TableName.valueOf(tableName);</span><br><span class="line">184         return admin.tableExists(name);</span><br><span class="line">185     &#125;</span><br><span class="line">186 </span><br><span class="line">187     // disable表</span><br><span class="line">188     @Override</span><br><span class="line">189     public void disableTable(String tableName) throws Exception &#123;</span><br><span class="line">190         </span><br><span class="line">191         TableName name = TableName.valueOf(tableName);</span><br><span class="line">192         </span><br><span class="line">193         if(admin.tableExists(name)) &#123;</span><br><span class="line">194             if(admin.isTableEnabled(name)) &#123;</span><br><span class="line">195                 admin.disableTable(name);</span><br><span class="line">196             &#125;else &#123;</span><br><span class="line">197                 System.out.println(&quot;table不是活动状态&quot;);</span><br><span class="line">198             &#125;</span><br><span class="line">199         &#125;else &#123;</span><br><span class="line">200             System.out.println(&quot;table不存在&quot;);</span><br><span class="line">201         &#125;</span><br><span class="line">202             </span><br><span class="line">203     &#125;</span><br><span class="line">204 </span><br><span class="line">205     // drop表</span><br><span class="line">206     @Override</span><br><span class="line">207     public void dropTable(String tableName) throws Exception &#123;</span><br><span class="line">208         //转化为表名</span><br><span class="line">209         TableName name = TableName.valueOf(tableName);</span><br><span class="line">210         //判断表是否存在</span><br><span class="line">211         if(admin.tableExists(name)) &#123;</span><br><span class="line">212             //判断表是否处于可用状态</span><br><span class="line">213             boolean tableEnabled = admin.isTableEnabled(name);</span><br><span class="line">214             </span><br><span class="line">215             if(tableEnabled) &#123;</span><br><span class="line">216                 //使表变成不可用状态</span><br><span class="line">217                 admin.disableTable(name);</span><br><span class="line">218             &#125;</span><br><span class="line">219             //删除表</span><br><span class="line">220             admin.deleteTable(name);</span><br><span class="line">221             //判断表是否存在</span><br><span class="line">222             if(admin.tableExists(name)) &#123;</span><br><span class="line">223                 System.out.println(&quot;删除失败&quot;);</span><br><span class="line">224             &#125;else &#123;</span><br><span class="line">225                 System.out.println(&quot;删除成功&quot;);</span><br><span class="line">226             &#125;</span><br><span class="line">227             </span><br><span class="line">228         &#125;else &#123;</span><br><span class="line">229             System.out.println(&quot;table不存在&quot;);</span><br><span class="line">230         &#125; </span><br><span class="line">231         </span><br><span class="line">232         </span><br><span class="line">233     &#125;</span><br><span class="line">234     </span><br><span class="line">235     // 修改表(增加和删除)</span><br><span class="line">236     @Override</span><br><span class="line">237     public void modifyTable(String tableName) throws Exception &#123;</span><br><span class="line">238         //转化为表名</span><br><span class="line">239         TableName name = TableName.valueOf(tableName);</span><br><span class="line">240         //判断表是否存在</span><br><span class="line">241         if(admin.tableExists(name)) &#123;</span><br><span class="line">242             //判断表是否可用状态</span><br><span class="line">243             boolean tableEnabled = admin.isTableEnabled(name);</span><br><span class="line">244             </span><br><span class="line">245             if(tableEnabled) &#123;</span><br><span class="line">246                 //使表变成不可用</span><br><span class="line">247                 admin.disableTable(name);</span><br><span class="line">248             &#125;</span><br><span class="line">249             //根据表名得到表</span><br><span class="line">250             HTableDescriptor tableDescriptor = admin.getTableDescriptor(name);</span><br><span class="line">251             //创建列簇结构对象</span><br><span class="line">252             HColumnDescriptor columnFamily1 = new HColumnDescriptor(&quot;cf1&quot;.getBytes());</span><br><span class="line">253             HColumnDescriptor columnFamily2 = new HColumnDescriptor(&quot;cf2&quot;.getBytes());</span><br><span class="line">254             </span><br><span class="line">255             tableDescriptor.addFamily(columnFamily1);</span><br><span class="line">256             tableDescriptor.addFamily(columnFamily2);</span><br><span class="line">257             //替换该表所有的列簇</span><br><span class="line">258             admin.modifyTable(name, tableDescriptor);</span><br><span class="line">259             </span><br><span class="line">260         &#125;else &#123;</span><br><span class="line">261             System.out.println(&quot;table不存在&quot;);</span><br><span class="line">262         &#125; </span><br><span class="line">263     &#125;</span><br><span class="line">264 </span><br><span class="line">265     // 修改表(增加和删除)</span><br><span class="line">266     @Override</span><br><span class="line">267     public void modifyTable(String tableName, String[] addColumn, String[] removeColumn) throws Exception &#123;</span><br><span class="line">268         //转化为表名</span><br><span class="line">269         TableName name = TableName.valueOf(tableName);</span><br><span class="line">270         //判断表是否存在</span><br><span class="line">271         if(admin.tableExists(name)) &#123;</span><br><span class="line">272             //判断表是否可用状态</span><br><span class="line">273             boolean tableEnabled = admin.isTableEnabled(name);</span><br><span class="line">274             </span><br><span class="line">275             if(tableEnabled) &#123;</span><br><span class="line">276                 //使表变成不可用</span><br><span class="line">277                 admin.disableTable(name);</span><br><span class="line">278             &#125;</span><br><span class="line">279             //根据表名得到表</span><br><span class="line">280             HTableDescriptor tableDescriptor = admin.getTableDescriptor(name);</span><br><span class="line">281             //创建列簇结构对象，添加列</span><br><span class="line">282             for(String add : addColumn) &#123;</span><br><span class="line">283                 HColumnDescriptor addColumnDescriptor = new HColumnDescriptor(add);</span><br><span class="line">284                 tableDescriptor.addFamily(addColumnDescriptor);</span><br><span class="line">285             &#125;</span><br><span class="line">286             //创建列簇结构对象，删除列</span><br><span class="line">287             for(String remove : removeColumn) &#123;</span><br><span class="line">288                 HColumnDescriptor removeColumnDescriptor = new HColumnDescriptor(remove);</span><br><span class="line">289                 tableDescriptor.removeFamily(removeColumnDescriptor.getName());</span><br><span class="line">290             &#125;</span><br><span class="line">291             </span><br><span class="line">292             admin.modifyTable(name, tableDescriptor);</span><br><span class="line">293             </span><br><span class="line">294             </span><br><span class="line">295         &#125;else &#123;</span><br><span class="line">296             System.out.println(&quot;table不存在&quot;);</span><br><span class="line">297         &#125; </span><br><span class="line">298         </span><br><span class="line">299     &#125;</span><br><span class="line">300 </span><br><span class="line">301     @Override</span><br><span class="line">302     public void modifyTable(String tableName, HColumnDescriptor hcds) throws Exception &#123;</span><br><span class="line">303         //转化为表名</span><br><span class="line">304         TableName name = TableName.valueOf(tableName);</span><br><span class="line">305         //根据表名得到表</span><br><span class="line">306         HTableDescriptor tableDescriptor = admin.getTableDescriptor(name);</span><br><span class="line">307         //获取表中所有的列簇信息</span><br><span class="line">308         HColumnDescriptor[] columnFamilies = tableDescriptor.getColumnFamilies();</span><br><span class="line">309         </span><br><span class="line">310         boolean flag = false;</span><br><span class="line">311         //判断参数中传入的列簇是否已经在表中存在</span><br><span class="line">312         for(HColumnDescriptor columnFamily : columnFamilies) &#123;</span><br><span class="line">313             if(columnFamily.equals(hcds)) &#123;</span><br><span class="line">314                 flag = true;</span><br><span class="line">315             &#125;</span><br><span class="line">316         &#125;    </span><br><span class="line">317         //存在提示，不存在直接添加该列簇信息</span><br><span class="line">318         if(flag) &#123;</span><br><span class="line">319             System.out.println(&quot;该列簇已经存在&quot;);</span><br><span class="line">320         &#125;else &#123;</span><br><span class="line">321             tableDescriptor.addFamily(hcds);</span><br><span class="line">322             admin.modifyTable(name, tableDescriptor);</span><br><span class="line">323         &#125;</span><br><span class="line">324         </span><br><span class="line">325     &#125;</span><br><span class="line">326 </span><br><span class="line">327     </span><br><span class="line">328     /**添加数据</span><br><span class="line">329     *tableName:    表明</span><br><span class="line">330     *rowKey:    行键</span><br><span class="line">331     *familyName:列簇</span><br><span class="line">332     *columnName:列名</span><br><span class="line">333     *value:        值</span><br><span class="line">334     */</span><br><span class="line">335     @Override</span><br><span class="line">336     public void putData(String tableName, String rowKey, String familyName, String columnName, String value)</span><br><span class="line">337             throws Exception &#123;</span><br><span class="line">338         //转化为表名</span><br><span class="line">339         TableName name = TableName.valueOf(tableName);</span><br><span class="line">340         //添加数据之前先判断表是否存在，不存在的话先创建表</span><br><span class="line">341         if(admin.tableExists(name)) &#123;</span><br><span class="line">342         </span><br><span class="line">343         &#125;else &#123;</span><br><span class="line">344             //根据表明创建表结构</span><br><span class="line">345             HTableDescriptor tableDescriptor = new HTableDescriptor(name);</span><br><span class="line">346             //定义列簇的名字</span><br><span class="line">347             HColumnDescriptor columnFamilyName = new HColumnDescriptor(familyName);</span><br><span class="line">348             tableDescriptor.addFamily(columnFamilyName);</span><br><span class="line">349             admin.createTable(tableDescriptor);</span><br><span class="line">350         </span><br><span class="line">351         &#125;</span><br><span class="line">352 </span><br><span class="line">353         Table table = conn.getTable(name);</span><br><span class="line">354         Put put = new Put(rowKey.getBytes());</span><br><span class="line">355         </span><br><span class="line">356         put.addColumn(familyName.getBytes(), columnName.getBytes(), value.getBytes());</span><br><span class="line">357         table.put(put);</span><br><span class="line">358 </span><br><span class="line">359     &#125;</span><br><span class="line">360 </span><br><span class="line">361     @Override</span><br><span class="line">362     public void putData(String tableName, String rowKey, String familyName, String columnName, String value,</span><br><span class="line">363             long timestamp) throws Exception &#123;</span><br><span class="line">364 </span><br><span class="line">365         // 转化为表名</span><br><span class="line">366         TableName name = TableName.valueOf(tableName);</span><br><span class="line">367         // 添加数据之前先判断表是否存在，不存在的话先创建表</span><br><span class="line">368         if (admin.tableExists(name)) &#123;</span><br><span class="line">369 </span><br><span class="line">370         &#125; else &#123;</span><br><span class="line">371             // 根据表明创建表结构</span><br><span class="line">372             HTableDescriptor tableDescriptor = new HTableDescriptor(name);</span><br><span class="line">373             // 定义列簇的名字</span><br><span class="line">374             HColumnDescriptor columnFamilyName = new HColumnDescriptor(familyName);</span><br><span class="line">375             tableDescriptor.addFamily(columnFamilyName);</span><br><span class="line">376             admin.createTable(tableDescriptor);</span><br><span class="line">377 </span><br><span class="line">378         &#125;</span><br><span class="line">379 </span><br><span class="line">380         Table table = conn.getTable(name);</span><br><span class="line">381         Put put = new Put(rowKey.getBytes());</span><br><span class="line">382 </span><br><span class="line">383         //put.addColumn(familyName.getBytes(), columnName.getBytes(), value.getBytes());</span><br><span class="line">384         put.addImmutable(familyName.getBytes(), columnName.getBytes(), timestamp, value.getBytes());</span><br><span class="line">385         table.put(put);</span><br><span class="line">386 </span><br><span class="line">387     &#125;</span><br><span class="line">388 </span><br><span class="line">389 </span><br><span class="line">390     // 根据rowkey查询数据</span><br><span class="line">391     @Override</span><br><span class="line">392     public Result getResult(String tableName, String rowKey) throws Exception &#123;</span><br><span class="line">393         </span><br><span class="line">394         Result result;</span><br><span class="line">395         TableName name = TableName.valueOf(tableName);</span><br><span class="line">396         if(admin.tableExists(name)) &#123;</span><br><span class="line">397             Table table = conn.getTable(name);</span><br><span class="line">398             Get get = new Get(rowKey.getBytes());</span><br><span class="line">399             result = table.get(get);</span><br><span class="line">400             </span><br><span class="line">401         &#125;else &#123;</span><br><span class="line">402             result = null;</span><br><span class="line">403         &#125;</span><br><span class="line">404         </span><br><span class="line">405         return result;</span><br><span class="line">406     &#125;</span><br><span class="line">407 </span><br><span class="line">408     // 根据rowkey查询数据</span><br><span class="line">409     @Override</span><br><span class="line">410     public Result getResult(String tableName, String rowKey, String familyName) throws Exception &#123;</span><br><span class="line">411         Result result;</span><br><span class="line">412         TableName name = TableName.valueOf(tableName);</span><br><span class="line">413         if(admin.tableExists(name)) &#123;</span><br><span class="line">414             Table table = conn.getTable(name);</span><br><span class="line">415             Get get = new Get(rowKey.getBytes());</span><br><span class="line">416             get.addFamily(familyName.getBytes());</span><br><span class="line">417             result = table.get(get);</span><br><span class="line">418             </span><br><span class="line">419         &#125;else &#123;</span><br><span class="line">420             result = null;</span><br><span class="line">421         &#125;</span><br><span class="line">422         </span><br><span class="line">423         return result;</span><br><span class="line">424     &#125;</span><br><span class="line">425 </span><br><span class="line">426     // 根据rowkey查询数据</span><br><span class="line">427     @Override</span><br><span class="line">428     public Result getResult(String tableName, String rowKey, String familyName, String columnName) throws Exception &#123;</span><br><span class="line">429         </span><br><span class="line">430         Result result;</span><br><span class="line">431         TableName name = TableName.valueOf(tableName);</span><br><span class="line">432         if(admin.tableExists(name)) &#123;</span><br><span class="line">433             Table table = conn.getTable(name);</span><br><span class="line">434             Get get = new Get(rowKey.getBytes());</span><br><span class="line">435             get.addColumn(familyName.getBytes(), columnName.getBytes());</span><br><span class="line">436             result = table.get(get);</span><br><span class="line">437             </span><br><span class="line">438         &#125;else &#123;</span><br><span class="line">439             result = null;</span><br><span class="line">440         &#125;</span><br><span class="line">441         </span><br><span class="line">442         return result;</span><br><span class="line">443     &#125;</span><br><span class="line">444 </span><br><span class="line">445     // 查询指定version</span><br><span class="line">446     @Override</span><br><span class="line">447     public Result getResultByVersion(String tableName, String rowKey, String familyName, String columnName,</span><br><span class="line">448             int versions) throws Exception &#123;</span><br><span class="line">449         </span><br><span class="line">450         Result result;</span><br><span class="line">451         TableName name = TableName.valueOf(tableName);</span><br><span class="line">452         if(admin.tableExists(name)) &#123;</span><br><span class="line">453             Table table = conn.getTable(name);</span><br><span class="line">454             Get get = new Get(rowKey.getBytes());</span><br><span class="line">455             get.addColumn(familyName.getBytes(), columnName.getBytes());</span><br><span class="line">456             get.setMaxVersions(versions);</span><br><span class="line">457             result = table.get(get);</span><br><span class="line">458             </span><br><span class="line">459         &#125;else &#123;</span><br><span class="line">460             result = null;</span><br><span class="line">461         &#125;</span><br><span class="line">462         </span><br><span class="line">463         return result;</span><br><span class="line">464     &#125;</span><br><span class="line">465 </span><br><span class="line">466     // scan全表数据</span><br><span class="line">467     @Override</span><br><span class="line">468     public ResultScanner getResultScann(String tableName) throws Exception &#123;</span><br><span class="line">469         </span><br><span class="line">470         ResultScanner result;</span><br><span class="line">471         TableName name = TableName.valueOf(tableName);</span><br><span class="line">472         if(admin.tableExists(name)) &#123;</span><br><span class="line">473             Table table = conn.getTable(name);</span><br><span class="line">474             Scan scan = new Scan();</span><br><span class="line">475             result = table.getScanner(scan);</span><br><span class="line">476             </span><br><span class="line">477         &#125;else &#123;</span><br><span class="line">478             result = null;</span><br><span class="line">479         &#125;</span><br><span class="line">480         </span><br><span class="line">481         return result;</span><br><span class="line">482     &#125;</span><br><span class="line">483 </span><br><span class="line">484     // scan全表数据</span><br><span class="line">485     @Override</span><br><span class="line">486     public ResultScanner getResultScann(String tableName, Scan scan) throws Exception &#123;</span><br><span class="line">487         </span><br><span class="line">488         ResultScanner result;</span><br><span class="line">489         TableName name = TableName.valueOf(tableName);</span><br><span class="line">490         if(admin.tableExists(name)) &#123;</span><br><span class="line">491             Table table = conn.getTable(name);</span><br><span class="line">492             result = table.getScanner(scan);</span><br><span class="line">493             </span><br><span class="line">494         &#125;else &#123;</span><br><span class="line">495             result = null;</span><br><span class="line">496         &#125;</span><br><span class="line">497         </span><br><span class="line">498         return result;</span><br><span class="line">499     &#125;</span><br><span class="line">500 </span><br><span class="line">501     // 删除数据（指定的列）</span><br><span class="line">502     @Override</span><br><span class="line">503     public void deleteColumn(String tableName, String rowKey) throws Exception &#123;</span><br><span class="line">504         </span><br><span class="line">505         TableName name = TableName.valueOf(tableName);</span><br><span class="line">506         if(admin.tableExists(name)) &#123;</span><br><span class="line">507             Table table = conn.getTable(name);</span><br><span class="line">508             Delete delete = new Delete(rowKey.getBytes());</span><br><span class="line">509             table.delete(delete);</span><br><span class="line">510         </span><br><span class="line">511         &#125;else &#123;</span><br><span class="line">512             System.out.println(&quot;table不存在&quot;);</span><br><span class="line">513         &#125;</span><br><span class="line">514         </span><br><span class="line">515         </span><br><span class="line">516     &#125;</span><br><span class="line">517 </span><br><span class="line">518     // 删除数据（指定的列）</span><br><span class="line">519     @Override</span><br><span class="line">520     public void deleteColumn(String tableName, String rowKey, String falilyName) throws Exception &#123;</span><br><span class="line">521         </span><br><span class="line">522         TableName name = TableName.valueOf(tableName);</span><br><span class="line">523         if(admin.tableExists(name)) &#123;</span><br><span class="line">524             Table table = conn.getTable(name);</span><br><span class="line">525             Delete delete = new Delete(rowKey.getBytes());</span><br><span class="line">526             delete.addFamily(falilyName.getBytes());</span><br><span class="line">527             table.delete(delete);</span><br><span class="line">528         </span><br><span class="line">529         &#125;else &#123;</span><br><span class="line">530             System.out.println(&quot;table不存在&quot;);</span><br><span class="line">531         &#125;</span><br><span class="line">532         </span><br><span class="line">533     &#125;</span><br><span class="line">534 </span><br><span class="line">535     // 删除数据（指定的列）</span><br><span class="line">536     @Override</span><br><span class="line">537     public void deleteColumn(String tableName, String rowKey, String falilyName, String columnName) throws Exception &#123;</span><br><span class="line">538         TableName name = TableName.valueOf(tableName);</span><br><span class="line">539         if(admin.tableExists(name)) &#123;</span><br><span class="line">540             Table table = conn.getTable(name);</span><br><span class="line">541             Delete delete = new Delete(rowKey.getBytes());</span><br><span class="line">542             delete.addColumn(falilyName.getBytes(), columnName.getBytes());</span><br><span class="line">543             table.delete(delete);</span><br><span class="line">544         </span><br><span class="line">545         &#125;else &#123;</span><br><span class="line">546             System.out.println(&quot;table不存在&quot;);</span><br><span class="line">547         &#125;</span><br><span class="line">548     &#125;</span><br><span class="line">549 </span><br><span class="line">550 &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（二十八）MapReduce的API使用（五）</title>
      <link href="/2018-04-28-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E5%85%AB%EF%BC%89MapReduce%E7%9A%84API%E4%BD%BF%E7%94%A8%EF%BC%88%E4%BA%94%EF%BC%89.html"/>
      <url>/2018-04-28-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E5%85%AB%EF%BC%89MapReduce%E7%9A%84API%E4%BD%BF%E7%94%A8%EF%BC%88%E4%BA%94%EF%BC%89.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（二十八）MapReduce的API使用（五）：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（二十八）MapReduce的API使用（五）</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>求所有两两用户之间的共同好友</p><p>数据格式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">A:B,C,D,F,E,O</span><br><span class="line">B:A,C,E,K</span><br><span class="line">C:F,A,D,I</span><br><span class="line">D:A,E,F,L</span><br><span class="line">E:B,C,D,M,L</span><br><span class="line">F:A,B,C,D,E,O,M</span><br><span class="line">G:A,C,D,E,F</span><br><span class="line">H:A,C,D,E,O</span><br><span class="line">I:A,O</span><br><span class="line">J:B,O</span><br><span class="line">K:A,C,D</span><br><span class="line">L:D,E,F</span><br><span class="line">M:E,F,G</span><br><span class="line">O:A,H,I,J,K</span><br></pre></td></tr></table></figure><p>以上是数据：<br>A:B,C,D,F,E,O<br>表示：B,C,D,E,F,O是A用户的好友。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line">  1 public class SharedFriend &#123;</span><br><span class="line">  2     /*</span><br><span class="line">  3      第一阶段的map函数主要完成以下任务</span><br><span class="line">  4      1.遍历原始文件中每行&lt;所有朋友&gt;信息</span><br><span class="line">  5      2.遍历“朋友”集合，以每个“朋友”为键，原来的“人”为值  即输出&lt;朋友,人&gt;</span><br><span class="line">  6      */</span><br><span class="line">  7     static class SharedFriendMapper01 extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123;</span><br><span class="line">  8         @Override</span><br><span class="line">  9         protected void map(LongWritable key, Text value,Context context)</span><br><span class="line"> 10                 throws IOException, InterruptedException &#123;</span><br><span class="line"> 11             String line = value.toString();</span><br><span class="line"> 12             String[] person_friends = line.split(&quot;:&quot;);</span><br><span class="line"> 13             String person = person_friends[0];</span><br><span class="line"> 14             String[] friends = person_friends[1].split(&quot;,&quot;);</span><br><span class="line"> 15             </span><br><span class="line"> 16             for(String friend : friends)&#123;</span><br><span class="line"> 17                 context.write(new Text(friend), new Text(person));</span><br><span class="line"> 18             &#125;</span><br><span class="line"> 19         &#125;</span><br><span class="line"> 20     &#125;</span><br><span class="line"> 21     </span><br><span class="line"> 22     /*</span><br><span class="line"> 23       第一阶段的reduce函数主要完成以下任务</span><br><span class="line"> 24       1.对所有传过来的&lt;朋友，list(人)&gt;进行拼接，输出&lt;朋友,拥有这名朋友的所有人&gt;</span><br><span class="line"> 25      */</span><br><span class="line"> 26     static class SharedFriendReducer01 extends Reducer&lt;Text, Text, Text, Text&gt;&#123;</span><br><span class="line"> 27         @Override</span><br><span class="line"> 28         protected void reduce(Text key, Iterable&lt;Text&gt; values,Context context)</span><br><span class="line"> 29                 throws IOException, InterruptedException &#123;</span><br><span class="line"> 30             StringBuffer sb = new StringBuffer();</span><br><span class="line"> 31             for(Text friend : values)&#123;</span><br><span class="line"> 32                 sb.append(friend.toString()).append(&quot;,&quot;);</span><br><span class="line"> 33             &#125;</span><br><span class="line"> 34             sb.deleteCharAt(sb.length()-1);</span><br><span class="line"> 35             context.write(key, new Text(sb.toString()));</span><br><span class="line"> 36         &#125;</span><br><span class="line"> 37     &#125;</span><br><span class="line"> 38     </span><br><span class="line"> 39     /*</span><br><span class="line"> 40     第二阶段的map函数主要完成以下任务</span><br><span class="line"> 41     1.将上一阶段reduce输出的&lt;朋友,拥有这名朋友的所有人&gt;信息中的 “拥有这名朋友的所有人”进行排序 ，以防出现B-C C-B这样的重复</span><br><span class="line"> 42     2.将 “拥有这名朋友的所有人”进行两两配对，并将配对后的字符串当做键，“朋友”当做值输出，即输出&lt;人-人，共同朋友&gt;</span><br><span class="line"> 43      */</span><br><span class="line"> 44     static class SharedFriendMapper02 extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123;</span><br><span class="line"> 45         @Override</span><br><span class="line"> 46         protected void map(LongWritable key, Text value,Context context)</span><br><span class="line"> 47                 throws IOException, InterruptedException &#123;</span><br><span class="line"> 48             String line = value.toString();</span><br><span class="line"> 49             String[] friend_persons = line.split(&quot;\t&quot;);</span><br><span class="line"> 50             String friend = friend_persons[0];</span><br><span class="line"> 51             String[] persons = friend_persons[1].split(&quot;,&quot;);</span><br><span class="line"> 52             Arrays.sort(persons); //排序</span><br><span class="line"> 53             </span><br><span class="line"> 54             //两两配对</span><br><span class="line"> 55             for(int i=0;i&lt;persons.length-1;i++)&#123;</span><br><span class="line"> 56                 for(int j=i+1;j&lt;persons.length;j++)&#123;</span><br><span class="line"> 57                     context.write(new Text(persons[i]+&quot;-&quot;+persons[j]+&quot;:&quot;), new Text(friend));</span><br><span class="line"> 58                 &#125;</span><br><span class="line"> 59             &#125;</span><br><span class="line"> 60         &#125;</span><br><span class="line"> 61     &#125;</span><br><span class="line"> 62     </span><br><span class="line"> 63     /*</span><br><span class="line"> 64     第二阶段的reduce函数主要完成以下任务</span><br><span class="line"> 65     1.&lt;人-人，list(共同朋友)&gt; 中的“共同好友”进行拼接 最后输出&lt;人-人，两人的所有共同好友&gt;</span><br><span class="line"> 66      */</span><br><span class="line"> 67     static class SharedFriendReducer02 extends Reducer&lt;Text, Text, Text, Text&gt;&#123;</span><br><span class="line"> 68         @Override</span><br><span class="line"> 69         protected void reduce(Text key, Iterable&lt;Text&gt; values,Context context)</span><br><span class="line"> 70                 throws IOException, InterruptedException &#123;</span><br><span class="line"> 71             StringBuffer sb = new StringBuffer();</span><br><span class="line"> 72             Set&lt;String&gt; set = new HashSet&lt;String&gt;();</span><br><span class="line"> 73             for(Text friend : values)&#123;</span><br><span class="line"> 74                 if(!set.contains(friend.toString()))</span><br><span class="line"> 75                     set.add(friend.toString());</span><br><span class="line"> 76             &#125;</span><br><span class="line"> 77             for(String friend : set)&#123;</span><br><span class="line"> 78                 sb.append(friend.toString()).append(&quot;,&quot;);</span><br><span class="line"> 79             &#125;</span><br><span class="line"> 80             sb.deleteCharAt(sb.length()-1);</span><br><span class="line"> 81             </span><br><span class="line"> 82             context.write(key, new Text(sb.toString()));</span><br><span class="line"> 83         &#125;</span><br><span class="line"> 84     &#125;</span><br><span class="line"> 85     </span><br><span class="line"> 86     public static void main(String[] args)throws Exception &#123;</span><br><span class="line"> 87         Configuration conf = new Configuration();</span><br><span class="line"> 88 </span><br><span class="line"> 89         //第一阶段</span><br><span class="line"> 90         Job job1 = Job.getInstance(conf);</span><br><span class="line"> 91         job1.setJarByClass(SharedFriend.class);</span><br><span class="line"> 92         job1.setMapperClass(SharedFriendMapper01.class);</span><br><span class="line"> 93         job1.setReducerClass(SharedFriendReducer01.class);</span><br><span class="line"> 94         </span><br><span class="line"> 95         job1.setOutputKeyClass(Text.class);</span><br><span class="line"> 96         job1.setOutputValueClass(Text.class);</span><br><span class="line"> 97         </span><br><span class="line"> 98         FileInputFormat.setInputPaths(job1, new Path(&quot;H:/大数据/mapreduce/sharedfriend/input&quot;));</span><br><span class="line"> 99         FileOutputFormat.setOutputPath(job1, new Path(&quot;H:/大数据/mapreduce/sharedfriend/output&quot;));</span><br><span class="line">100         </span><br><span class="line">101         boolean res1 = job1.waitForCompletion(true);</span><br><span class="line">102         </span><br><span class="line">103         //第二阶段</span><br><span class="line">104         Job job2 = Job.getInstance(conf);</span><br><span class="line">105         job2.setJarByClass(SharedFriend.class);</span><br><span class="line">106         job2.setMapperClass(SharedFriendMapper02.class);</span><br><span class="line">107         job2.setReducerClass(SharedFriendReducer02.class);</span><br><span class="line">108         </span><br><span class="line">109         job2.setOutputKeyClass(Text.class);</span><br><span class="line">110         job2.setOutputValueClass(Text.class);</span><br><span class="line">111         </span><br><span class="line">112         FileInputFormat.setInputPaths(job2, new Path(&quot;H:/大数据/mapreduce/sharedfriend/output&quot;));</span><br><span class="line">113         FileOutputFormat.setOutputPath(job2, new Path(&quot;H:/大数据/mapreduce/sharedfriend/output01&quot;));</span><br><span class="line">114         </span><br><span class="line">115         boolean res2 = job2.waitForCompletion(true);</span><br><span class="line">116         </span><br><span class="line">117         System.exit(res1?0:1);</span><br><span class="line">118     &#125;</span><br><span class="line">119 &#125;</span><br></pre></td></tr></table></figure><p>第一阶段输出结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> 1 A    F,I,O,K,G,D,C,H,B</span><br><span class="line"> 2 B    E,J,F,A</span><br><span class="line"> 3 C    B,E,K,A,H,G,F</span><br><span class="line"> 4 D    H,C,G,F,E,A,K,L</span><br><span class="line"> 5 E    A,B,L,G,M,F,D,H</span><br><span class="line"> 6 F    C,M,L,A,D,G</span><br><span class="line"> 7 G    M</span><br><span class="line"> 8 H    O</span><br><span class="line"> 9 I    O,C</span><br><span class="line">10 J    O</span><br><span class="line">11 K    O,B</span><br><span class="line">12 L    D,E</span><br><span class="line">13 M    E,F</span><br><span class="line">14 O    A,H,I,J,F</span><br></pre></td></tr></table></figure><p>第二阶段输出结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"> 1 A-B    C,E</span><br><span class="line"> 2 A-C    D,F</span><br><span class="line"> 3 A-D    E,F</span><br><span class="line"> 4 A-E    C,B,D</span><br><span class="line"> 5 A-F    E,O,C,D,B</span><br><span class="line"> 6 A-G    F,C,E,D</span><br><span class="line"> 7 A-H    D,O,C,E</span><br><span class="line"> 8 A-I    O</span><br><span class="line"> 9 A-J    B,O</span><br><span class="line">10 A-K    C,D</span><br><span class="line">11 A-L    D,E,F</span><br><span class="line">12 A-M    E,F</span><br><span class="line">13 B-C    A</span><br><span class="line">14 B-D    A,E</span><br><span class="line">15 B-E    C</span><br><span class="line">16 B-F    A,C,E</span><br><span class="line">17 B-G    E,C,A</span><br><span class="line">18 B-H    A,E,C</span><br><span class="line">19 B-I    A</span><br><span class="line">20 B-K    A,C</span><br><span class="line">21 B-L    E</span><br><span class="line">22 B-M    E</span><br><span class="line">23 B-O    K,A</span><br><span class="line">24 C-D    F,A</span><br><span class="line">25 C-E    D</span><br><span class="line">26 C-F    D,A</span><br><span class="line">27 C-G    D,F,A</span><br><span class="line">28 C-H    D,A</span><br><span class="line">29 C-I    A</span><br><span class="line">30 C-K    A,D</span><br><span class="line">31 C-L    D,F</span><br><span class="line">32 C-M    F</span><br><span class="line">33 C-O    I,A</span><br><span class="line">34 D-E    L</span><br><span class="line">35 D-F    A,E</span><br><span class="line">36 D-G    F,A,E</span><br><span class="line">37 D-H    A,E</span><br><span class="line">38 D-I    A</span><br><span class="line">39 D-K    A</span><br><span class="line">40 D-L    F,E</span><br><span class="line">41 D-M    F,E</span><br><span class="line">42 D-O    A</span><br><span class="line">43 E-F    C,D,M,B</span><br><span class="line">44 E-G    C,D</span><br><span class="line">45 E-H    C,D</span><br><span class="line">46 E-J    B</span><br><span class="line">47 E-K    D,C</span><br><span class="line">48 E-L    D</span><br><span class="line">49 F-G    C,E,D,A</span><br><span class="line">50 F-H    D,O,A,E,C</span><br><span class="line">51 F-I    A,O</span><br><span class="line">52 F-J    O,B</span><br><span class="line">53 F-K    D,C,A</span><br><span class="line">54 F-L    D,E</span><br><span class="line">55 F-M    E</span><br><span class="line">56 F-O    A</span><br><span class="line">57 G-H    E,C,D,A</span><br><span class="line">58 G-I    A</span><br><span class="line">59 G-K    D,A,C</span><br><span class="line">60 G-L    F,E,D</span><br><span class="line">61 G-M    E,F</span><br><span class="line">62 G-O    A</span><br><span class="line">63 H-I    A,O</span><br><span class="line">64 H-J    O</span><br><span class="line">65 H-K    C,D,A</span><br><span class="line">66 H-L    D,E</span><br><span class="line">67 H-M    E</span><br><span class="line">68 H-O    A</span><br><span class="line">69 I-J    O</span><br><span class="line">70 I-K    A</span><br><span class="line">71 I-O    A</span><br><span class="line">72 K-L    D</span><br><span class="line">73 K-O    A</span><br><span class="line">74 L-M    F,E</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（二十七）MapReduce的API使用（四）</title>
      <link href="/2018-04-27-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%83%EF%BC%89MapReduce%E7%9A%84API%E4%BD%BF%E7%94%A8%EF%BC%88%E5%9B%9B%EF%BC%89.html"/>
      <url>/2018-04-27-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%83%EF%BC%89MapReduce%E7%9A%84API%E4%BD%BF%E7%94%A8%EF%BC%88%E5%9B%9B%EF%BC%89.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（二十七）MapReduce的API使用（四）：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（二十七）MapReduce的API使用（四）</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>第一题</p><p>下面是三种商品的销售数据 </p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180324160210598-546517385.png" alt="img"></p><p>要求：根据以上数据，用 MapReduce 统计出如下数据：</p><p>1、每种商品的销售总金额，并降序排序</p><p>2、每种商品销售额最多的三周</p><p>第二题：MapReduce 题</p><p>现有如下数据文件需要处理:</p><p>格式：CSV</p><p>数据样例：</p><p>user_a,location_a,2018-01-01 08:00:00,60</p><p>user_a,location_a,2018-01-01 09:00:00,60</p><p>user_a,location_b,2018-01-01 10:00:00,60</p><p>user_a,location_a,2018-01-01 11:00:00,60</p><p>字段：用户 ID，位置 ID，开始时间，停留时长（分钟）</p><p>数据意义：某个用户在某个位置从某个时刻开始停留了多长时间</p><p>处理逻辑： 对同一个用户，在同一个位置，连续的多条记录进行合并</p><p>合并原则：开始时间取最早的，停留时长加和</p><p>要求：请编写 MapReduce 程序实现</p><p>其他：只有数据样例，没有数据。</p><p>UserLocationMR.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line">  1 /**</span><br><span class="line">  2 测试数据：</span><br><span class="line">  3 user_a    location_a    2018-01-01 08:00:00    60</span><br><span class="line">  4 user_a    location_a    2018-01-01 09:00:00    60</span><br><span class="line">  5 user_a    location_a    2018-01-01 11:00:00    60</span><br><span class="line">  6 user_a    location_a    2018-01-01 12:00:00    60</span><br><span class="line">  7 user_a    location_b    2018-01-01 10:00:00    60</span><br><span class="line">  8 user_a    location_c    2018-01-01 08:00:00    60</span><br><span class="line">  9 user_a    location_c    2018-01-01 09:00:00    60</span><br><span class="line"> 10 user_a    location_c    2018-01-01 10:00:00    60</span><br><span class="line"> 11 user_b    location_a    2018-01-01 15:00:00    60</span><br><span class="line"> 12 user_b    location_a    2018-01-01 16:00:00    60</span><br><span class="line"> 13 user_b    location_a    2018-01-01 18:00:00    60</span><br><span class="line"> 14 </span><br><span class="line"> 15 </span><br><span class="line"> 16 结果数据：</span><br><span class="line"> 17 user_a    location_a    2018-01-01 08:00:00    120</span><br><span class="line"> 18 user_a    location_a    2018-01-01 11:00:00    120</span><br><span class="line"> 19 user_a    location_b    2018-01-01 10:00:00    60</span><br><span class="line"> 20 user_a    location_c    2018-01-01 08:00:00    180</span><br><span class="line"> 21 user_b    location_a    2018-01-01 15:00:00    120</span><br><span class="line"> 22 user_b    location_a    2018-01-01 18:00:00    60</span><br><span class="line"> 23 </span><br><span class="line"> 24 </span><br><span class="line"> 25  */</span><br><span class="line"> 26 public class UserLocationMR &#123;</span><br><span class="line"> 27 </span><br><span class="line"> 28     public static void main(String[] args) throws Exception &#123;</span><br><span class="line"> 29         // 指定hdfs相关的参数</span><br><span class="line"> 30         Configuration conf = new Configuration();</span><br><span class="line"> 31         //        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop02:9000&quot;);</span><br><span class="line"> 32         //        System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line"> 33 </span><br><span class="line"> 34         Job job = Job.getInstance(conf);</span><br><span class="line"> 35         // 设置jar包所在路径</span><br><span class="line"> 36         job.setJarByClass(UserLocationMR.class);</span><br><span class="line"> 37 </span><br><span class="line"> 38         // 指定mapper类和reducer类</span><br><span class="line"> 39         job.setMapperClass(UserLocationMRMapper.class);</span><br><span class="line"> 40         job.setReducerClass(UserLocationMRReducer.class);</span><br><span class="line"> 41 </span><br><span class="line"> 42         // 指定maptask的输出类型</span><br><span class="line"> 43         job.setMapOutputKeyClass(UserLocation.class);</span><br><span class="line"> 44         job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"> 45         // 指定reducetask的输出类型</span><br><span class="line"> 46         job.setOutputKeyClass(UserLocation.class);</span><br><span class="line"> 47         job.setOutputValueClass(NullWritable.class);</span><br><span class="line"> 48 </span><br><span class="line"> 49         job.setGroupingComparatorClass(UserLocationGC.class);</span><br><span class="line"> 50 </span><br><span class="line"> 51         // 指定该mapreduce程序数据的输入和输出路径</span><br><span class="line"> 52         Path inputPath = new Path(&quot;D:\\武文\\second\\input&quot;);</span><br><span class="line"> 53         Path outputPath = new Path(&quot;D:\\武文\\second\\output2&quot;);</span><br><span class="line"> 54         FileSystem fs = FileSystem.get(conf);</span><br><span class="line"> 55         if (fs.exists(outputPath)) &#123;</span><br><span class="line"> 56             fs.delete(outputPath, true);</span><br><span class="line"> 57         &#125;</span><br><span class="line"> 58         FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line"> 59         FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line"> 60 </span><br><span class="line"> 61         // 最后提交任务</span><br><span class="line"> 62         boolean waitForCompletion = job.waitForCompletion(true);</span><br><span class="line"> 63         System.exit(waitForCompletion ? 0 : 1);</span><br><span class="line"> 64     &#125;</span><br><span class="line"> 65 </span><br><span class="line"> 66     private static class UserLocationMRMapper extends Mapper&lt;LongWritable, Text, UserLocation, NullWritable&gt; &#123;</span><br><span class="line"> 67 </span><br><span class="line"> 68         UserLocation outKey = new UserLocation();</span><br><span class="line"> 69 </span><br><span class="line"> 70         /**</span><br><span class="line"> 71          * value = user_a,location_a,2018-01-01 12:00:00,60</span><br><span class="line"> 72          */</span><br><span class="line"> 73         @Override</span><br><span class="line"> 74         protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line"> 75 </span><br><span class="line"> 76             String[] split = value.toString().split(&quot;,&quot;);</span><br><span class="line"> 77 </span><br><span class="line"> 78             outKey.set(split);</span><br><span class="line"> 79 </span><br><span class="line"> 80             context.write(outKey, NullWritable.get());</span><br><span class="line"> 81         &#125;</span><br><span class="line"> 82     &#125;</span><br><span class="line"> 83 </span><br><span class="line"> 84     private static class UserLocationMRReducer extends Reducer&lt;UserLocation, NullWritable, UserLocation, NullWritable&gt; &#123;</span><br><span class="line"> 85 </span><br><span class="line"> 86         SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line"> 87 </span><br><span class="line"> 88         UserLocation outKey = new UserLocation();</span><br><span class="line"> 89 </span><br><span class="line"> 90         /**</span><br><span class="line"> 91          * user_a    location_a    2018-01-01 08:00:00    60</span><br><span class="line"> 92          * user_a    location_a    2018-01-01 09:00:00    60</span><br><span class="line"> 93          * user_a    location_a    2018-01-01 11:00:00    60</span><br><span class="line"> 94          * user_a    location_a    2018-01-01 12:00:00    60</span><br><span class="line"> 95          */</span><br><span class="line"> 96         @Override</span><br><span class="line"> 97         protected void reduce(UserLocation key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line"> 98 </span><br><span class="line"> 99             int count = 0;</span><br><span class="line">100             for (NullWritable nvl : values) &#123;</span><br><span class="line">101                 count++;</span><br><span class="line">102                 // 如果是这一组key-value中的第一个元素时，直接赋值给outKey对象。基础对象</span><br><span class="line">103                 if (count == 1) &#123;</span><br><span class="line">104                     // 复制值</span><br><span class="line">105                     outKey.set(key);</span><br><span class="line">106                 &#125; else &#123;</span><br><span class="line">107 </span><br><span class="line">108                     // 有可能连续，有可能不连续，  连续则继续变量， 否则输出</span><br><span class="line">109                     long current_timestamp = 0;</span><br><span class="line">110                     long last_timestamp = 0;</span><br><span class="line">111                     try &#123;</span><br><span class="line">112                         // 这是新遍历出来的记录的时间戳</span><br><span class="line">113                         current_timestamp = sdf.parse(key.getTime()).getTime();</span><br><span class="line">114                         // 这是上一条记录的时间戳 和 停留时间之和</span><br><span class="line">115                         last_timestamp = sdf.parse(outKey.getTime()).getTime() + outKey.getDuration() * 60 * 1000;</span><br><span class="line">116                     &#125; catch (ParseException e) &#123;</span><br><span class="line">117                         e.printStackTrace();</span><br><span class="line">118                     &#125;</span><br><span class="line">119 </span><br><span class="line">120                     // 如果相等，证明是连续记录，所以合并</span><br><span class="line">121                     if (current_timestamp == last_timestamp) &#123;</span><br><span class="line">122 </span><br><span class="line">123                         outKey.setDuration(outKey.getDuration() + key.getDuration());</span><br><span class="line">124 </span><br><span class="line">125                     &#125; else &#123;</span><br><span class="line">126 </span><br><span class="line">127                         // 先输出上一条记录</span><br><span class="line">128                         context.write(outKey, nvl);</span><br><span class="line">129 </span><br><span class="line">130                         // 然后再次记录当前遍历到的这一条记录</span><br><span class="line">131                         outKey.set(key);</span><br><span class="line">132                     &#125;</span><br><span class="line">133                 &#125;</span><br><span class="line">134             &#125;</span><br><span class="line">135             // 最后无论如何，还得输出最后一次</span><br><span class="line">136             context.write(outKey, NullWritable.get());</span><br><span class="line">137         &#125;</span><br><span class="line">138     &#125;</span><br><span class="line">139 &#125;</span><br></pre></td></tr></table></figure><p>UserLocation.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line">  1 public class UserLocation implements WritableComparable&lt;UserLocation&gt; &#123;</span><br><span class="line">  2 </span><br><span class="line">  3     private String userid;</span><br><span class="line">  4     private String locationid;</span><br><span class="line">  5     private String time;</span><br><span class="line">  6     private long duration;</span><br><span class="line">  7 </span><br><span class="line">  8     @Override</span><br><span class="line">  9     public String toString() &#123;</span><br><span class="line"> 10         return userid + &quot;\t&quot; + locationid + &quot;\t&quot; + time + &quot;\t&quot; + duration;</span><br><span class="line"> 11     &#125;</span><br><span class="line"> 12 </span><br><span class="line"> 13     public UserLocation() &#123;</span><br><span class="line"> 14         super();</span><br><span class="line"> 15     &#125;</span><br><span class="line"> 16     </span><br><span class="line"> 17     public void set(String[] split)&#123;</span><br><span class="line"> 18         this.setUserid(split[0]);</span><br><span class="line"> 19         this.setLocationid(split[1]);</span><br><span class="line"> 20         this.setTime(split[2]);</span><br><span class="line"> 21         this.setDuration(Long.parseLong(split[3]));</span><br><span class="line"> 22     &#125;</span><br><span class="line"> 23     </span><br><span class="line"> 24     public void set(UserLocation ul)&#123;</span><br><span class="line"> 25         this.setUserid(ul.getUserid());</span><br><span class="line"> 26         this.setLocationid(ul.getLocationid());</span><br><span class="line"> 27         this.setTime(ul.getTime());</span><br><span class="line"> 28         this.setDuration(ul.getDuration());</span><br><span class="line"> 29     &#125;</span><br><span class="line"> 30 </span><br><span class="line"> 31     public UserLocation(String userid, String locationid, String time, long duration) &#123;</span><br><span class="line"> 32         super();</span><br><span class="line"> 33         this.userid = userid;</span><br><span class="line"> 34         this.locationid = locationid;</span><br><span class="line"> 35         this.time = time;</span><br><span class="line"> 36         this.duration = duration;</span><br><span class="line"> 37     &#125;</span><br><span class="line"> 38 </span><br><span class="line"> 39     public String getUserid() &#123;</span><br><span class="line"> 40         return userid;</span><br><span class="line"> 41     &#125;</span><br><span class="line"> 42 </span><br><span class="line"> 43     public void setUserid(String userid) &#123;</span><br><span class="line"> 44         this.userid = userid;</span><br><span class="line"> 45     &#125;</span><br><span class="line"> 46 </span><br><span class="line"> 47     public String getLocationid() &#123;</span><br><span class="line"> 48         return locationid;</span><br><span class="line"> 49     &#125;</span><br><span class="line"> 50 </span><br><span class="line"> 51     public void setLocationid(String locationid) &#123;</span><br><span class="line"> 52         this.locationid = locationid;</span><br><span class="line"> 53     &#125;</span><br><span class="line"> 54 </span><br><span class="line"> 55     public String getTime() &#123;</span><br><span class="line"> 56         return time;</span><br><span class="line"> 57     &#125;</span><br><span class="line"> 58 </span><br><span class="line"> 59     public void setTime(String time) &#123;</span><br><span class="line"> 60         this.time = time;</span><br><span class="line"> 61     &#125;</span><br><span class="line"> 62 </span><br><span class="line"> 63     public long getDuration() &#123;</span><br><span class="line"> 64         return duration;</span><br><span class="line"> 65     &#125;</span><br><span class="line"> 66 </span><br><span class="line"> 67     public void setDuration(long duration) &#123;</span><br><span class="line"> 68         this.duration = duration;</span><br><span class="line"> 69     &#125;</span><br><span class="line"> 70 </span><br><span class="line"> 71     @Override</span><br><span class="line"> 72     public void write(DataOutput out) throws IOException &#123;</span><br><span class="line"> 73         // TODO Auto-generated method stub</span><br><span class="line"> 74         out.writeUTF(userid);</span><br><span class="line"> 75         out.writeUTF(locationid);</span><br><span class="line"> 76         out.writeUTF(time);</span><br><span class="line"> 77         out.writeLong(duration);</span><br><span class="line"> 78     &#125;</span><br><span class="line"> 79 </span><br><span class="line"> 80     @Override</span><br><span class="line"> 81     public void readFields(DataInput in) throws IOException &#123;</span><br><span class="line"> 82         // TODO Auto-generated method stub</span><br><span class="line"> 83         this.userid = in.readUTF();</span><br><span class="line"> 84         this.locationid = in.readUTF();</span><br><span class="line"> 85         this.time = in.readUTF();</span><br><span class="line"> 86         this.duration = in.readLong();</span><br><span class="line"> 87     &#125;</span><br><span class="line"> 88 </span><br><span class="line"> 89     /**</span><br><span class="line"> 90      * 排序规则</span><br><span class="line"> 91      * </span><br><span class="line"> 92      * 按照 userid  locationid  和  time 排序  都是 升序</span><br><span class="line"> 93      */</span><br><span class="line"> 94     @Override</span><br><span class="line"> 95     public int compareTo(UserLocation o) &#123;</span><br><span class="line"> 96 </span><br><span class="line"> 97         int diff_userid = o.getUserid().compareTo(this.getUserid());</span><br><span class="line"> 98         if(diff_userid == 0)&#123;</span><br><span class="line"> 99             </span><br><span class="line">100             int diff_location = o.getLocationid().compareTo(this.getLocationid());</span><br><span class="line">101             if(diff_location == 0)&#123;</span><br><span class="line">102                 </span><br><span class="line">103                 int diff_time = o.getTime().compareTo(this.getTime());</span><br><span class="line">104                 if(diff_time == 0)&#123;</span><br><span class="line">105                     return 0;</span><br><span class="line">106                 &#125;else&#123;</span><br><span class="line">107                     return diff_time &gt; 0 ? -1 : 1;</span><br><span class="line">108                 &#125;</span><br><span class="line">109                 </span><br><span class="line">110             &#125;else&#123;</span><br><span class="line">111                 return diff_location &gt; 0 ? -1 : 1;</span><br><span class="line">112             &#125;</span><br><span class="line">113             </span><br><span class="line">114         &#125;else&#123;</span><br><span class="line">115             return diff_userid &gt; 0 ? -1 : 1;</span><br><span class="line">116         &#125;</span><br><span class="line">117     &#125;</span><br><span class="line">118 &#125;</span><br></pre></td></tr></table></figure><p>UserLocationGC.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class UserLocationGC extends WritableComparator&#123;</span><br><span class="line"> 2     </span><br><span class="line"> 3     public UserLocationGC()&#123;</span><br><span class="line"> 4         super(UserLocation.class, true);</span><br><span class="line"> 5     &#125;</span><br><span class="line"> 6 </span><br><span class="line"> 7     @Override</span><br><span class="line"> 8     public int compare(WritableComparable a, WritableComparable b) &#123;</span><br><span class="line"> 9 </span><br><span class="line">10         UserLocation ul_a = (UserLocation)a;</span><br><span class="line">11         UserLocation ul_b = (UserLocation)b;</span><br><span class="line">12 </span><br><span class="line">13         int diff_userid = ul_a.getUserid().compareTo(ul_b.getUserid());</span><br><span class="line">14         if(diff_userid == 0)&#123;</span><br><span class="line">15             </span><br><span class="line">16             int diff_location = ul_a.getLocationid().compareTo(ul_b.getLocationid());</span><br><span class="line">17             if(diff_location == 0)&#123;</span><br><span class="line">18                 </span><br><span class="line">19                 return 0;</span><br><span class="line">20                 </span><br><span class="line">21             &#125;else&#123;</span><br><span class="line">22                 return diff_location &gt; 0 ? -1 : 1;</span><br><span class="line">23             &#125;</span><br><span class="line">24             </span><br><span class="line">25         &#125;else&#123;</span><br><span class="line">26             return diff_userid &gt; 0 ? -1 : 1;</span><br><span class="line">27         &#125;</span><br><span class="line">28     &#125;</span><br><span class="line">29 &#125;</span><br></pre></td></tr></table></figure><p>第三题：MapReduce 题–倒排索引</p><p>概念： 倒排索引（Inverted Index），也常被称为反向索引、置入档案或反向档案，是一种索引方法， 被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档 检索系统中最常用的数据结构。了解详情可自行百度</p><p>有两份数据：</p><p>mapreduce-4-1.txt</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">huangbo love xuzheng</span><br><span class="line">huangxiaoming love baby huangxiaoming love yangmi</span><br><span class="line">liangchaowei love liujialing</span><br><span class="line">huangxiaoming xuzheng huangbo wangbaoqiang</span><br></pre></td></tr></table></figure><p>mapreduce-4-2.txt</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hello huangbo</span><br><span class="line">hello xuzheng</span><br><span class="line">hello huangxiaoming</span><br></pre></td></tr></table></figure><p>题目一：编写 MapReduce 求出以下格式的结果数据：统计每个关键词在每个文档中当中的 第几行出现了多少次 例如，huangxiaoming 关键词的格式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huangixaoming mapreduce-4-1.txt:2,2; mapreduce-4-1.txt:4,1;mapreduce-4-2.txt:3,1</span><br></pre></td></tr></table></figure><p>以上答案的意义：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">关键词 huangxiaoming 在第一份文档 mapreduce-4-1.txt 中的第 2 行出现了 2 次</span><br><span class="line">关键词 huangxiaoming 在第一份文档 mapreduce-4-1.txt 中的第 4 行出现了 1 次</span><br><span class="line">关键词 huangxiaoming 在第二份文档 mapreduce-4-2.txt 中的第 3 行出现了 1 次</span><br></pre></td></tr></table></figure><p>题目二：编写 MapReduce 程序求出每个关键词在每个文档出现了多少次，并且按照出现次 数降序排序</p><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huangixaoming mapreduce-4-1.txt,3;mapreduce-4-2.txt,1</span><br></pre></td></tr></table></figure><p>以上答案的含义： 表示关键词 huangxiaoming 在第一份文档 mapreduce-4-1.txt 中出现了 3 次，在第二份文档mapreduce-4-2.txt 中出现了 1 次</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（二十六）MapReduce的API使用（三）</title>
      <link href="/2018-04-26-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E5%85%AD%EF%BC%89MapReduce%E7%9A%84API%E4%BD%BF%E7%94%A8%EF%BC%88%E4%B8%89%EF%BC%89.html"/>
      <url>/2018-04-26-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E5%85%AD%EF%BC%89MapReduce%E7%9A%84API%E4%BD%BF%E7%94%A8%EF%BC%88%E4%B8%89%EF%BC%89.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（二十六）MapReduce的API使用（三）：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（二十六）MapReduce的API使用（三）</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="数据及需求"><a href="#数据及需求" class="headerlink" title="数据及需求"></a>数据及需求</h2><h3 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h3><p>movies.dat　　3884条数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1::Toy Story (1995)::Animation|Children&apos;s|Comedy</span><br><span class="line">2::Jumanji (1995)::Adventure|Children&apos;s|Fantasy</span><br><span class="line">3::Grumpier Old Men (1995)::Comedy|Romance</span><br><span class="line">4::Waiting to Exhale (1995)::Comedy|Drama</span><br><span class="line">5::Father of the Bride Part II (1995)::Comedy</span><br><span class="line">6::Heat (1995)::Action|Crime|Thriller</span><br><span class="line">7::Sabrina (1995)::Comedy|Romance</span><br><span class="line">8::Tom and Huck (1995)::Adventure|Children&apos;s</span><br><span class="line">9::Sudden Death (1995)::Action</span><br><span class="line">10::GoldenEye (1995)::Action|Adventure|Thriller</span><br></pre></td></tr></table></figure><p>users.dat　　6041条数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1::F::1::10::48067</span><br><span class="line">2::M::56::16::70072</span><br><span class="line">3::M::25::15::55117</span><br><span class="line">4::M::45::7::02460</span><br><span class="line">5::M::25::20::55455</span><br><span class="line">6::F::50::9::55117</span><br><span class="line">7::M::35::1::06810</span><br><span class="line">8::M::25::12::11413</span><br><span class="line">9::M::25::17::61614</span><br><span class="line">10::F::35::1::95370</span><br></pre></td></tr></table></figure><p>ratings.dat　　1000210条数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1::1193::5::978300760</span><br><span class="line">1::661::3::978302109</span><br><span class="line">1::914::3::978301968</span><br><span class="line">1::3408::4::978300275</span><br><span class="line">1::2355::5::978824291</span><br><span class="line">1::1197::3::978302268</span><br><span class="line">1::1287::5::978302039</span><br><span class="line">1::2804::5::978300719</span><br><span class="line">1::594::4::978302268</span><br><span class="line">1::919::4::978301368</span><br></pre></td></tr></table></figure><h3 id="数据解释"><a href="#数据解释" class="headerlink" title="数据解释"></a>数据解释</h3><p>1、users.dat 数据格式为： 2::M::56::16::70072<br>对应字段为：UserID BigInt, Gender String, Age Int, Occupation String, Zipcode String<br>对应字段中文解释：用户id，性别，年龄，职业，邮政编码</p><p>2、movies.dat    数据格式为： 2::Jumanji (1995)::Adventure|Children’s|Fantasy<br>对应字段为：MovieID BigInt, Title String, Genres String<br>对应字段中文解释：电影ID，电影名字，电影类型</p><p>3、ratings.dat    数据格式为： 1::1193::5::978300760<br>对应字段为：UserID BigInt, MovieID BigInt, Rating Double, Timestamped String<br>对应字段中文解释：用户ID，电影ID，评分，评分时间戳</p><p>用户ID，电影ID，评分，评分时间戳，性别，年龄，职业，邮政编码，电影名字，电影类型<br>userid, movieId, rate, ts, gender, age, occupation, zipcode, movieName, movieType</p><h3 id="需求统计"><a href="#需求统计" class="headerlink" title="需求统计"></a>需求统计</h3><p>（1）求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）<br>（2）分别求男性，女性当中评分最高的10部电影（性别，电影名，评分）<br>（3）求movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，评分）<br>（4）求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（人，电影名，影评）<br>（5）求好片（评分&gt;=4.0）最多的那个年份的最好看的10部电影<br>（6）求1997年上映的电影中，评分最高的10部Comedy类电影<br>（7）该影评库中各种类型电影中评价最高的5部电影（类型，电影名，平均影评分）<br>（8）各年评分最高的电影类型（年份，类型，影评分）<br>（9）每个地区最高评分的电影名，把结果存入HDFS（地区，电影名，电影评分）</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="1、求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）"><a href="#1、求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）" class="headerlink" title="1、求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）"></a>1、求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）</h3><p>分析：此问题涉及到2个文件，ratings.dat和movies.dat，2个文件数据量倾斜比较严重，此处应该使用mapjoin方法，先将数据量较小的文件预先加载到内存中</p><p>MovieMR1_1.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line">  1 public class MovieMR1_1 &#123;</span><br><span class="line">  2 </span><br><span class="line">  3     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">  4         </span><br><span class="line">  5         if(args.length &lt; 4) &#123;</span><br><span class="line">  6             args = new String[4];</span><br><span class="line">  7             args[0] = &quot;/movie/input/&quot;;</span><br><span class="line">  8             args[1] = &quot;/movie/output/&quot;;</span><br><span class="line">  9             args[2] = &quot;/movie/cache/movies.dat&quot;;</span><br><span class="line"> 10             args[3] = &quot;/movie/output_last/&quot;;</span><br><span class="line"> 11         &#125;</span><br><span class="line"> 12         </span><br><span class="line"> 13         </span><br><span class="line"> 14         Configuration conf1 = new Configuration();</span><br><span class="line"> 15         conf1.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000/&quot;);</span><br><span class="line"> 16         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line"> 17         FileSystem fs1 = FileSystem.get(conf1);</span><br><span class="line"> 18         </span><br><span class="line"> 19         </span><br><span class="line"> 20         Job job1 = Job.getInstance(conf1);</span><br><span class="line"> 21         </span><br><span class="line"> 22         job1.setJarByClass(MovieMR1_1.class);</span><br><span class="line"> 23         </span><br><span class="line"> 24         job1.setMapperClass(MoviesMapJoinRatingsMapper1.class);</span><br><span class="line"> 25         job1.setReducerClass(MovieMR1Reducer1.class);</span><br><span class="line"> 26         </span><br><span class="line"> 27         job1.setMapOutputKeyClass(Text.class);</span><br><span class="line"> 28         job1.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"> 29         </span><br><span class="line"> 30         job1.setOutputKeyClass(Text.class);</span><br><span class="line"> 31         job1.setOutputValueClass(IntWritable.class);</span><br><span class="line"> 32         </span><br><span class="line"> 33         </span><br><span class="line"> 34         </span><br><span class="line"> 35         //缓存普通文件到task运行节点的工作目录</span><br><span class="line"> 36         URI uri = new URI(&quot;hdfs://hadoop1:9000&quot;+args[2]);</span><br><span class="line"> 37         System.out.println(uri);</span><br><span class="line"> 38         job1.addCacheFile(uri);</span><br><span class="line"> 39         </span><br><span class="line"> 40         </span><br><span class="line"> 41         Path inputPath1 = new Path(args[0]);</span><br><span class="line"> 42         Path outputPath1 = new Path(args[1]);</span><br><span class="line"> 43         if(fs1.exists(outputPath1)) &#123;</span><br><span class="line"> 44             fs1.delete(outputPath1, true);</span><br><span class="line"> 45         &#125;</span><br><span class="line"> 46         FileInputFormat.setInputPaths(job1, inputPath1);</span><br><span class="line"> 47         FileOutputFormat.setOutputPath(job1, outputPath1);</span><br><span class="line"> 48         </span><br><span class="line"> 49         boolean isDone = job1.waitForCompletion(true);</span><br><span class="line"> 50         System.exit(isDone ? 0 : 1);</span><br><span class="line"> 51        </span><br><span class="line"> 52     &#125;</span><br><span class="line"> 53     </span><br><span class="line"> 54     public static class MoviesMapJoinRatingsMapper1 extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123;</span><br><span class="line"> 55         </span><br><span class="line"> 56         //用了存放加载到内存中的movies.dat数据</span><br><span class="line"> 57         private static Map&lt;String,String&gt; movieMap =  new HashMap&lt;&gt;();</span><br><span class="line"> 58         //key：电影ID</span><br><span class="line"> 59         Text outKey = new Text();</span><br><span class="line"> 60         //value：电影名+电影类型</span><br><span class="line"> 61         IntWritable outValue = new IntWritable();</span><br><span class="line"> 62         </span><br><span class="line"> 63         </span><br><span class="line"> 64         /**</span><br><span class="line"> 65          * movies.dat:    1::Toy Story (1995)::Animation|Children&apos;s|Comedy</span><br><span class="line"> 66          * </span><br><span class="line"> 67          * </span><br><span class="line"> 68          * 将小表(movies.dat)中的数据预先加载到内存中去</span><br><span class="line"> 69          * */</span><br><span class="line"> 70         @Override</span><br><span class="line"> 71         protected void setup(Context context) throws IOException, InterruptedException &#123;</span><br><span class="line"> 72             </span><br><span class="line"> 73             Path[] localCacheFiles = context.getLocalCacheFiles();</span><br><span class="line"> 74             </span><br><span class="line"> 75             </span><br><span class="line"> 76             String strPath = localCacheFiles[0].toUri().toString();</span><br><span class="line"> 77             </span><br><span class="line"> 78             BufferedReader br = new BufferedReader(new FileReader(strPath));</span><br><span class="line"> 79             String readLine;</span><br><span class="line"> 80             while((readLine = br.readLine()) != null) &#123;</span><br><span class="line"> 81                 </span><br><span class="line"> 82                 String[] split = readLine.split(&quot;::&quot;);</span><br><span class="line"> 83                 String movieId = split[0];</span><br><span class="line"> 84                 String movieName = split[1];</span><br><span class="line"> 85                 String movieType = split[2];</span><br><span class="line"> 86                 </span><br><span class="line"> 87                 movieMap.put(movieId, movieName+&quot;\t&quot;+movieType);</span><br><span class="line"> 88             &#125;</span><br><span class="line"> 89             </span><br><span class="line"> 90             br.close();</span><br><span class="line"> 91         &#125;</span><br><span class="line"> 92         </span><br><span class="line"> 93         </span><br><span class="line"> 94         /**</span><br><span class="line"> 95          * movies.dat:    1    ::    Toy Story (1995)    ::    Animation|Children&apos;s|Comedy    </span><br><span class="line"> 96          *                 电影ID    电影名字                    电影类型</span><br><span class="line"> 97          * </span><br><span class="line"> 98          * ratings.dat:    1    ::    1193    ::    5    ::    978300760</span><br><span class="line"> 99          *                 用户ID    电影ID        评分        评分时间戳</span><br><span class="line">100          * </span><br><span class="line">101          * value:    ratings.dat读取的数据</span><br><span class="line">102          * */</span><br><span class="line">103         @Override</span><br><span class="line">104         protected void map(LongWritable key, Text value, Context context)</span><br><span class="line">105                 throws IOException, InterruptedException &#123;</span><br><span class="line">106             </span><br><span class="line">107             String[] split = value.toString().split(&quot;::&quot;);</span><br><span class="line">108             </span><br><span class="line">109             String userId = split[0];</span><br><span class="line">110             String movieId = split[1];</span><br><span class="line">111             String movieRate = split[2];</span><br><span class="line">112             </span><br><span class="line">113             //根据movieId从内存中获取电影名和类型</span><br><span class="line">114             String movieNameAndType = movieMap.get(movieId);</span><br><span class="line">115             String movieName = movieNameAndType.split(&quot;\t&quot;)[0];</span><br><span class="line">116             String movieType = movieNameAndType.split(&quot;\t&quot;)[1];</span><br><span class="line">117             </span><br><span class="line">118             outKey.set(movieName);</span><br><span class="line">119             outValue.set(Integer.parseInt(movieRate));</span><br><span class="line">120             </span><br><span class="line">121             context.write(outKey, outValue);</span><br><span class="line">122             </span><br><span class="line">123         &#125;</span><br><span class="line">124             </span><br><span class="line">125     &#125;</span><br><span class="line">126 </span><br><span class="line">127     </span><br><span class="line">128     public static class MovieMR1Reducer1 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;</span><br><span class="line">129         //每部电影评论的次数</span><br><span class="line">130         int count;</span><br><span class="line">131         //评分次数</span><br><span class="line">132         IntWritable outValue = new IntWritable();</span><br><span class="line">133         </span><br><span class="line">134         @Override</span><br><span class="line">135         protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">136             </span><br><span class="line">137             count = 0;</span><br><span class="line">138             </span><br><span class="line">139             for(IntWritable value : values) &#123;</span><br><span class="line">140                 count++;</span><br><span class="line">141             &#125;</span><br><span class="line">142             </span><br><span class="line">143             outValue.set(count);</span><br><span class="line">144             </span><br><span class="line">145             context.write(key, outValue);</span><br><span class="line">146         &#125;</span><br><span class="line">147         </span><br><span class="line">148     &#125;</span><br><span class="line">149     </span><br><span class="line">150     </span><br><span class="line">151 &#125;</span><br></pre></td></tr></table></figure><p>MovieMR1_2.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class MovieMR1_2 &#123;</span><br><span class="line"> 2 </span><br><span class="line"> 3     public static void main(String[] args) throws Exception &#123;</span><br><span class="line"> 4         if(args.length &lt; 2) &#123;</span><br><span class="line"> 5             args = new String[2];</span><br><span class="line"> 6             args[0] = &quot;/movie/output/&quot;;</span><br><span class="line"> 7             args[1] = &quot;/movie/output_last/&quot;;</span><br><span class="line"> 8         &#125;</span><br><span class="line"> 9         </span><br><span class="line">10         </span><br><span class="line">11         Configuration conf1 = new Configuration();</span><br><span class="line">12         conf1.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000/&quot;);</span><br><span class="line">13         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">14         FileSystem fs1 = FileSystem.get(conf1);</span><br><span class="line">15         </span><br><span class="line">16         </span><br><span class="line">17         Job job = Job.getInstance(conf1);</span><br><span class="line">18         </span><br><span class="line">19         job.setJarByClass(MovieMR1_2.class);</span><br><span class="line">20         </span><br><span class="line">21         job.setMapperClass(MoviesMapJoinRatingsMapper2.class);</span><br><span class="line">22         job.setReducerClass(MovieMR1Reducer2.class);</span><br><span class="line">23 </span><br><span class="line">24         </span><br><span class="line">25         job.setMapOutputKeyClass(MovieRating.class);</span><br><span class="line">26         job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">27         </span><br><span class="line">28         job.setOutputKeyClass(MovieRating.class);</span><br><span class="line">29         job.setOutputValueClass(NullWritable.class);</span><br><span class="line">30         </span><br><span class="line">31         </span><br><span class="line">32         Path inputPath1 = new Path(args[0]);</span><br><span class="line">33         Path outputPath1 = new Path(args[1]);</span><br><span class="line">34         if(fs1.exists(outputPath1)) &#123;</span><br><span class="line">35             fs1.delete(outputPath1, true);</span><br><span class="line">36         &#125;</span><br><span class="line">37         //对第一步的输出结果进行降序排序</span><br><span class="line">38         FileInputFormat.setInputPaths(job, inputPath1);</span><br><span class="line">39         FileOutputFormat.setOutputPath(job, outputPath1);</span><br><span class="line">40         </span><br><span class="line">41         boolean isDone = job.waitForCompletion(true);</span><br><span class="line">42         System.exit(isDone ? 0 : 1);</span><br><span class="line">43         </span><br><span class="line">44 </span><br><span class="line">45     &#125;</span><br><span class="line">46     </span><br><span class="line">47     //注意输出类型为自定义对象MovieRating，MovieRating按照降序排序</span><br><span class="line">48     public static class MoviesMapJoinRatingsMapper2 extends Mapper&lt;LongWritable, Text, MovieRating, NullWritable&gt;&#123;</span><br><span class="line">49         </span><br><span class="line">50         MovieRating outKey = new MovieRating();</span><br><span class="line">51         </span><br><span class="line">52         @Override</span><br><span class="line">53         protected void map(LongWritable key, Text value, Context context)</span><br><span class="line">54                 throws IOException, InterruptedException &#123;</span><br><span class="line">55             //&apos;Night Mother (1986)         70</span><br><span class="line">56             String[] split = value.toString().split(&quot;\t&quot;);</span><br><span class="line">57             </span><br><span class="line">58             outKey.setCount(Integer.parseInt(split[1]));;</span><br><span class="line">59             outKey.setMovieName(split[0]);</span><br><span class="line">60             </span><br><span class="line">61             context.write(outKey, NullWritable.get());</span><br><span class="line">62                         </span><br><span class="line">63         &#125;</span><br><span class="line">64                 </span><br><span class="line">65     &#125;</span><br><span class="line">66     </span><br><span class="line">67     //排序之后自然输出，只取前10部电影</span><br><span class="line">68     public static class MovieMR1Reducer2 extends Reducer&lt;MovieRating, NullWritable, MovieRating, NullWritable&gt;&#123;</span><br><span class="line">69         </span><br><span class="line">70         Text outKey = new Text();</span><br><span class="line">71         int count = 0;</span><br><span class="line">72         </span><br><span class="line">73         @Override</span><br><span class="line">74         protected void reduce(MovieRating key, Iterable&lt;NullWritable&gt; values,Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">75 </span><br><span class="line">76             for(NullWritable value : values) &#123;</span><br><span class="line">77                 count++;</span><br><span class="line">78                 if(count &gt; 10) &#123;</span><br><span class="line">79                     return;</span><br><span class="line">80                 &#125;</span><br><span class="line">81                 context.write(key, value);</span><br><span class="line">82                 </span><br><span class="line">83             &#125;</span><br><span class="line">84         </span><br><span class="line">85         &#125;</span><br><span class="line">86         </span><br><span class="line">87     &#125;</span><br><span class="line">88 &#125;</span><br></pre></td></tr></table></figure><p>MovieRating.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class MovieRating implements WritableComparable&lt;MovieRating&gt;&#123;</span><br><span class="line"> 2     private String movieName;</span><br><span class="line"> 3     private int count;</span><br><span class="line"> 4     </span><br><span class="line"> 5     public String getMovieName() &#123;</span><br><span class="line"> 6         return movieName;</span><br><span class="line"> 7     &#125;</span><br><span class="line"> 8     public void setMovieName(String movieName) &#123;</span><br><span class="line"> 9         this.movieName = movieName;</span><br><span class="line">10     &#125;</span><br><span class="line">11     public int getCount() &#123;</span><br><span class="line">12         return count;</span><br><span class="line">13     &#125;</span><br><span class="line">14     public void setCount(int count) &#123;</span><br><span class="line">15         this.count = count;</span><br><span class="line">16     &#125;</span><br><span class="line">17     </span><br><span class="line">18     public MovieRating() &#123;&#125;</span><br><span class="line">19     </span><br><span class="line">20     public MovieRating(String movieName, int count) &#123;</span><br><span class="line">21         super();</span><br><span class="line">22         this.movieName = movieName;</span><br><span class="line">23         this.count = count;</span><br><span class="line">24     &#125;</span><br><span class="line">25     </span><br><span class="line">26     </span><br><span class="line">27     @Override</span><br><span class="line">28     public String toString() &#123;</span><br><span class="line">29         return  movieName + &quot;\t&quot; + count;</span><br><span class="line">30     &#125;</span><br><span class="line">31     @Override</span><br><span class="line">32     public void readFields(DataInput in) throws IOException &#123;</span><br><span class="line">33         movieName = in.readUTF();</span><br><span class="line">34         count = in.readInt();</span><br><span class="line">35     &#125;</span><br><span class="line">36     @Override</span><br><span class="line">37     public void write(DataOutput out) throws IOException &#123;</span><br><span class="line">38         out.writeUTF(movieName);</span><br><span class="line">39         out.writeInt(count);</span><br><span class="line">40     &#125;</span><br><span class="line">41     @Override</span><br><span class="line">42     public int compareTo(MovieRating o) &#123;</span><br><span class="line">43         return o.count - this.count ;</span><br><span class="line">44     &#125;</span><br><span class="line">45     </span><br><span class="line">46 &#125;</span><br></pre></td></tr></table></figure><h3 id="2、分别求男性，女性当中评分最高的10部电影（性别，电影名，评分）"><a href="#2、分别求男性，女性当中评分最高的10部电影（性别，电影名，评分）" class="headerlink" title="2、分别求男性，女性当中评分最高的10部电影（性别，电影名，评分）"></a>2、分别求男性，女性当中评分最高的10部电影（性别，电影名，评分）</h3><p>分析：此问题涉及到3个表的联合查询，需要先将2个小表的数据预先加载到内存中，再进行查询</p><p>对三表进行联合</p><p>MoviesThreeTableJoin.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line">  1 /**</span><br><span class="line">  2  * 进行3表的联合查询</span><br><span class="line">  3  * </span><br><span class="line">  4  * */</span><br><span class="line">  5 public class MoviesThreeTableJoin &#123;</span><br><span class="line">  6 </span><br><span class="line">  7     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">  8         </span><br><span class="line">  9         if(args.length &lt; 4) &#123;</span><br><span class="line"> 10             args = new String[4];</span><br><span class="line"> 11             args[0] = &quot;/movie/input/&quot;;</span><br><span class="line"> 12             args[1] = &quot;/movie/output2/&quot;;</span><br><span class="line"> 13             args[2] = &quot;/movie/cache/movies.dat&quot;;</span><br><span class="line"> 14             args[3] = &quot;/movie/cache/users.dat&quot;;</span><br><span class="line"> 15         &#125;</span><br><span class="line"> 16         </span><br><span class="line"> 17         Configuration conf = new Configuration();</span><br><span class="line"> 18         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000/&quot;);</span><br><span class="line"> 19         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line"> 20         FileSystem fs = FileSystem.get(conf);</span><br><span class="line"> 21         Job job = Job.getInstance(conf);</span><br><span class="line"> 22         </span><br><span class="line"> 23         job.setJarByClass(MoviesThreeTableJoin.class);</span><br><span class="line"> 24         job.setMapperClass(ThreeTableMapper.class);</span><br><span class="line"> 25         </span><br><span class="line"> 26         job.setOutputKeyClass(Text.class);</span><br><span class="line"> 27         job.setOutputValueClass(NullWritable.class);</span><br><span class="line"> 28         </span><br><span class="line"> 29         URI uriUsers = new URI(&quot;hdfs://hadoop1:9000&quot;+args[3]);</span><br><span class="line"> 30         URI uriMovies = new URI(&quot;hdfs://hadoop1:9000&quot;+args[2]);</span><br><span class="line"> 31         job.addCacheFile(uriUsers);</span><br><span class="line"> 32         job.addCacheFile(uriMovies);</span><br><span class="line"> 33         </span><br><span class="line"> 34         Path inputPath = new Path(args[0]);</span><br><span class="line"> 35         Path outputPath = new Path(args[1]);</span><br><span class="line"> 36         </span><br><span class="line"> 37         if(fs.exists(outputPath)) &#123;</span><br><span class="line"> 38             fs.delete(outputPath,true);</span><br><span class="line"> 39         &#125;</span><br><span class="line"> 40         </span><br><span class="line"> 41         FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line"> 42         FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line"> 43         </span><br><span class="line"> 44         boolean isDone = job.waitForCompletion(true);</span><br><span class="line"> 45         System.exit(isDone ? 0 : 1);</span><br><span class="line"> 46         </span><br><span class="line"> 47     &#125;</span><br><span class="line"> 48 </span><br><span class="line"> 49     </span><br><span class="line"> 50     public static class ThreeTableMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123;</span><br><span class="line"> 51         </span><br><span class="line"> 52         </span><br><span class="line"> 53         //用于缓存movies和users中数据</span><br><span class="line"> 54         private Map&lt;String,String&gt; moviesMap = new HashMap&lt;&gt;();</span><br><span class="line"> 55         private Map&lt;String,String&gt; usersMap = new HashMap&lt;&gt;();</span><br><span class="line"> 56         //用来存放读取的ratings.dat中的一行数据</span><br><span class="line"> 57         String[] ratings;</span><br><span class="line"> 58         </span><br><span class="line"> 59         </span><br><span class="line"> 60         Text outKey = new Text();</span><br><span class="line"> 61         </span><br><span class="line"> 62         @Override</span><br><span class="line"> 63         protected void setup(Context context) throws IOException, InterruptedException &#123;</span><br><span class="line"> 64             </span><br><span class="line"> 65             BufferedReader br = null;</span><br><span class="line"> 66             </span><br><span class="line"> 67             Path[] paths = context.getLocalCacheFiles();</span><br><span class="line"> 68             String usersLine = null;</span><br><span class="line"> 69             String moviesLine = null;</span><br><span class="line"> 70             </span><br><span class="line"> 71             for(Path path : paths) &#123;</span><br><span class="line"> 72                 String name = path.toUri().getPath();</span><br><span class="line"> 73                 if(name.contains(&quot;movies.dat&quot;)) &#123;</span><br><span class="line"> 74                     //读取movies.dat文件中的一行数据</span><br><span class="line"> 75                     br = new BufferedReader(new FileReader(name));</span><br><span class="line"> 76                     while((moviesLine = br.readLine()) != null) &#123;</span><br><span class="line"> 77                         /**对读取的这行数据按照：：进行切分</span><br><span class="line"> 78                         *    2::Jumanji (1995)::Adventure|Children&apos;s|Fantasy</span><br><span class="line"> 79                         *    电影ID，电影名字，电影类型</span><br><span class="line"> 80                         *</span><br><span class="line"> 81                         *电影ID作为key，其余作为value</span><br><span class="line"> 82                         */</span><br><span class="line"> 83                         String[] split = moviesLine.split(&quot;::&quot;);</span><br><span class="line"> 84                         moviesMap.put(split[0], split[1]+&quot;::&quot;+split[2]);</span><br><span class="line"> 85                     &#125;        </span><br><span class="line"> 86                 &#125;else if(name.contains(&quot;users.dat&quot;)) &#123;</span><br><span class="line"> 87                     //读取users.dat文件中的一行数据</span><br><span class="line"> 88                     br = new BufferedReader(new FileReader(name));</span><br><span class="line"> 89                     while((usersLine = br.readLine()) != null) &#123;</span><br><span class="line"> 90                         /**</span><br><span class="line"> 91                          * 对读取的这行数据按照：：进行切分</span><br><span class="line"> 92                          * 2::M::56::16::70072</span><br><span class="line"> 93                          * 用户id，性别，年龄，职业，邮政编码</span><br><span class="line"> 94                          * </span><br><span class="line"> 95                          * 用户ID作为key，其他的作为value</span><br><span class="line"> 96                          * */</span><br><span class="line"> 97                         String[] split = usersLine.split(&quot;::&quot;);</span><br><span class="line"> 98                         System.out.println(split[0]+&quot;----&quot;+split[1]);</span><br><span class="line"> 99                         usersMap.put(split[0], split[1]+&quot;::&quot;+split[2]+&quot;::&quot;+split[3]+&quot;::&quot;+split[4]);</span><br><span class="line">100                     &#125;</span><br><span class="line">101                 &#125;</span><br><span class="line">102             </span><br><span class="line">103             &#125;</span><br><span class="line">104         </span><br><span class="line">105         &#125;</span><br><span class="line">106         </span><br><span class="line">107         </span><br><span class="line">108         @Override</span><br><span class="line">109         protected void map(LongWritable key, Text value, Context context)</span><br><span class="line">110                 throws IOException, InterruptedException &#123;</span><br><span class="line">111             </span><br><span class="line">112             ratings = value.toString().split(&quot;::&quot;);</span><br><span class="line">113             //通过电影ID和用户ID获取用户表和电影表中的其他信息</span><br><span class="line">114             String movies = moviesMap.get(ratings[1]);</span><br><span class="line">115             String users = usersMap.get(ratings[0]);</span><br><span class="line">116             </span><br><span class="line">117             //三表信息的联合</span><br><span class="line">118             String threeTables = value.toString()+&quot;::&quot;+movies+&quot;::&quot;+users;</span><br><span class="line">119             outKey.set(threeTables);</span><br><span class="line">120             </span><br><span class="line">121             context.write(outKey, NullWritable.get());</span><br><span class="line">122         &#125;</span><br><span class="line">123     &#125;</span><br><span class="line">124     </span><br><span class="line">125     </span><br><span class="line">126 &#125;</span><br></pre></td></tr></table></figure><p>三表联合之后的数据为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1000::1023::5::975041651::Winnie the Pooh and the Blustery Day (1968)::Animation|Children&apos;s::F::25::6::90027</span><br><span class="line">1000::1029::3::975041859::Dumbo (1941)::Animation|Children&apos;s|Musical::F::25::6::90027</span><br><span class="line">1000::1036::4::975040964::Die Hard (1988)::Action|Thriller::F::25::6::90027</span><br><span class="line">1000::1104::5::975042421::Streetcar Named Desire, A (1951)::Drama::F::25::6::90027</span><br><span class="line">1000::110::5::975040841::Braveheart (1995)::Action|Drama|War::F::25::6::90027</span><br><span class="line">1000::1196::3::975040841::Star Wars: Episode V - The Empire Strikes Back (1980)::Action|Adventure|Drama|Sci-Fi|War::F::25::6::90027</span><br><span class="line">1000::1198::5::975040841::Raiders of the Lost Ark (1981)::Action|Adventure::F::25::6::90027</span><br><span class="line">1000::1200::4::975041125::Aliens (1986)::Action|Sci-Fi|Thriller|War::F::25::6::90027</span><br><span class="line">1000::1201::5::975041025::Good, The Bad and The Ugly, The (1966)::Action|Western::F::25::6::90027</span><br><span class="line">1000::1210::5::975040629::Star Wars: Episode VI - Return of the Jedi (1983)::Action|Adventure|Romance|Sci-Fi|War::F::25::6::90027</span><br></pre></td></tr></table></figure><p>字段解释</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1000    ::    1036    ::    4    ::    975040964    ::    Die Hard (1988)    ::    Action|Thriller    ::    F    ::    25    ::    6    ::    90027</span><br><span class="line"></span><br><span class="line">用户ID        电影ID        评分     　　 评分时间戳             电影名字                  电影类型                性别        年龄        职业        邮政编码0　　　　　　　　1　　　　　　　　2　　　　　　　　3　　　　　　　　　　　　4　　　　　　　　　　　　　　5　　　　　　　　　　　　6　　　　　　7　　　　　　8　　　　　　   9</span><br></pre></td></tr></table></figure><p> 要分别求男性，女性当中评分最高的10部电影（性别，电影名，评分）</p><p>1、以性别和电影名分组，以电影名+性别为key，以评分为value进行计算；</p><p>2、以性别+电影名+评分作为对象，以性别分组，以评分降序进行输出TOP10</p><p>业务逻辑：MoviesDemo2.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line">  1 public class MoviesDemo2 &#123;</span><br><span class="line">  2 </span><br><span class="line">  3     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">  4         </span><br><span class="line">  5         Configuration conf1 = new Configuration();</span><br><span class="line">  6         Configuration conf2 = new Configuration();</span><br><span class="line">  7         FileSystem fs1 = FileSystem.get(conf1);</span><br><span class="line">  8         FileSystem fs2 = FileSystem.get(conf2);</span><br><span class="line">  9         Job job1 = Job.getInstance(conf1);</span><br><span class="line"> 10         Job job2 = Job.getInstance(conf2);</span><br><span class="line"> 11         </span><br><span class="line"> 12         job1.setJarByClass(MoviesDemo2.class);</span><br><span class="line"> 13         job1.setMapperClass(MoviesDemo2Mapper1.class);</span><br><span class="line"> 14         job2.setMapperClass(MoviesDemo2Mapper2.class);</span><br><span class="line"> 15         job1.setReducerClass(MoviesDemo2Reducer1.class);</span><br><span class="line"> 16         job2.setReducerClass(MoviesDemo2Reducer2.class);</span><br><span class="line"> 17         </span><br><span class="line"> 18         job1.setOutputKeyClass(Text.class);</span><br><span class="line"> 19         job1.setOutputValueClass(DoubleWritable.class);</span><br><span class="line"> 20         </span><br><span class="line"> 21         job2.setOutputKeyClass(MoviesSexBean.class);</span><br><span class="line"> 22         job2.setOutputValueClass(NullWritable.class);</span><br><span class="line"> 23         </span><br><span class="line"> 24         job2.setGroupingComparatorClass(MoviesSexGC.class);</span><br><span class="line"> 25         </span><br><span class="line"> 26         Path inputPath1 = new Path(&quot;D:\\MR\\hw\\movie\\output3he1&quot;);</span><br><span class="line"> 27         Path outputPath1 = new Path(&quot;D:\\MR\\hw\\movie\\output2_1&quot;);</span><br><span class="line"> 28         Path inputPath2 = new Path(&quot;D:\\MR\\hw\\movie\\output2_1&quot;);</span><br><span class="line"> 29         Path outputPath2 = new Path(&quot;D:\\MR\\hw\\movie\\output2_end&quot;);</span><br><span class="line"> 30         </span><br><span class="line"> 31         if(fs1.exists(outputPath1)) &#123;</span><br><span class="line"> 32             fs1.delete(outputPath1,true);</span><br><span class="line"> 33         &#125;</span><br><span class="line"> 34         if(fs2.exists(outputPath2)) &#123;</span><br><span class="line"> 35             fs2.delete(outputPath2,true);</span><br><span class="line"> 36         &#125;</span><br><span class="line"> 37         </span><br><span class="line"> 38         </span><br><span class="line"> 39         FileInputFormat.setInputPaths(job1, inputPath1);</span><br><span class="line"> 40         FileOutputFormat.setOutputPath(job1, outputPath1);</span><br><span class="line"> 41         </span><br><span class="line"> 42         FileInputFormat.setInputPaths(job2, inputPath2);</span><br><span class="line"> 43         FileOutputFormat.setOutputPath(job2, outputPath2);</span><br><span class="line"> 44         </span><br><span class="line"> 45         JobControl control = new JobControl(&quot;MoviesDemo2&quot;);</span><br><span class="line"> 46         </span><br><span class="line"> 47         ControlledJob aJob = new ControlledJob(job1.getConfiguration());</span><br><span class="line"> 48         ControlledJob bJob = new ControlledJob(job2.getConfiguration());</span><br><span class="line"> 49         </span><br><span class="line"> 50         bJob.addDependingJob(aJob);</span><br><span class="line"> 51         </span><br><span class="line"> 52         control.addJob(aJob);</span><br><span class="line"> 53         control.addJob(bJob);</span><br><span class="line"> 54         </span><br><span class="line"> 55         Thread thread = new Thread(control);</span><br><span class="line"> 56         thread.start();</span><br><span class="line"> 57         </span><br><span class="line"> 58         while(!control.allFinished()) &#123;</span><br><span class="line"> 59             thread.sleep(1000);</span><br><span class="line"> 60         &#125;</span><br><span class="line"> 61         System.exit(0);</span><br><span class="line"> 62         </span><br><span class="line"> 63         </span><br><span class="line"> 64     &#125;</span><br><span class="line"> 65     </span><br><span class="line"> 66     </span><br><span class="line"> 67     /**</span><br><span class="line"> 68      * 数据来源：3个文件关联之后的输出文件</span><br><span class="line"> 69      * 以电影名+性别为key，以评分为value进行输出</span><br><span class="line"> 70      * </span><br><span class="line"> 71      * 1000::1036::4::975040964::Die Hard (1988)::Action|Thriller::F::25::6::90027</span><br><span class="line"> 72      * </span><br><span class="line"> 73      * 用户ID::电影ID::评分::评分时间戳::电影名字::电影类型::性别::年龄::职业::邮政编码</span><br><span class="line"> 74      * </span><br><span class="line"> 75      * */</span><br><span class="line"> 76     public static class MoviesDemo2Mapper1 extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt;&#123;</span><br><span class="line"> 77         </span><br><span class="line"> 78         Text outKey = new Text();</span><br><span class="line"> 79         DoubleWritable outValue = new DoubleWritable();</span><br><span class="line"> 80         </span><br><span class="line"> 81         @Override</span><br><span class="line"> 82         protected void map(LongWritable key, Text value,Context context)</span><br><span class="line"> 83                 throws IOException, InterruptedException &#123;</span><br><span class="line"> 84 </span><br><span class="line"> 85             String[] split = value.toString().split(&quot;::&quot;);</span><br><span class="line"> 86             String strKey = split[4]+&quot;\t&quot;+split[6];</span><br><span class="line"> 87             String strValue = split[2];</span><br><span class="line"> 88             </span><br><span class="line"> 89             outKey.set(strKey);</span><br><span class="line"> 90             outValue.set(Double.parseDouble(strValue));</span><br><span class="line"> 91             </span><br><span class="line"> 92             context.write(outKey, outValue);</span><br><span class="line"> 93         &#125;</span><br><span class="line"> 94         </span><br><span class="line"> 95     &#125;</span><br><span class="line"> 96     </span><br><span class="line"> 97     /**</span><br><span class="line"> 98      * 以电影名+性别为key，计算平均分</span><br><span class="line"> 99      * */</span><br><span class="line">100     public static class MoviesDemo2Reducer1 extends Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt;&#123;</span><br><span class="line">101         </span><br><span class="line">102         DoubleWritable outValue = new DoubleWritable();</span><br><span class="line">103         </span><br><span class="line">104         @Override</span><br><span class="line">105         protected void reduce(Text key, Iterable&lt;DoubleWritable&gt; values,Context context)</span><br><span class="line">106                 throws IOException, InterruptedException &#123;</span><br><span class="line">107             </span><br><span class="line">108             int count = 0;</span><br><span class="line">109             double sum = 0;</span><br><span class="line">110             for(DoubleWritable value : values) &#123;</span><br><span class="line">111                 count++;</span><br><span class="line">112                 sum += Double.parseDouble(value.toString());</span><br><span class="line">113             &#125;</span><br><span class="line">114             double avg = sum / count;</span><br><span class="line">115             </span><br><span class="line">116             outValue.set(avg);</span><br><span class="line">117             context.write(key, outValue);</span><br><span class="line">118         &#125;</span><br><span class="line">119     &#125;</span><br><span class="line">120     </span><br><span class="line">121     /**</span><br><span class="line">122      * 以电影名+性别+评分作为对象，以性别分组，以评分降序排序</span><br><span class="line">123      * */</span><br><span class="line">124     public static class MoviesDemo2Mapper2 extends Mapper&lt;LongWritable, Text, MoviesSexBean, NullWritable&gt;&#123;</span><br><span class="line">125         </span><br><span class="line">126         MoviesSexBean outKey = new MoviesSexBean();</span><br><span class="line">127         </span><br><span class="line">128         @Override</span><br><span class="line">129         protected void map(LongWritable key, Text value,Context context)</span><br><span class="line">130                 throws IOException, InterruptedException &#123;</span><br><span class="line">131             </span><br><span class="line">132             String[] split = value.toString().split(&quot;\t&quot;);</span><br><span class="line">133             outKey.setMovieName(split[0]);</span><br><span class="line">134             outKey.setSex(split[1]);</span><br><span class="line">135             outKey.setScore(Double.parseDouble(split[2]));</span><br><span class="line">136             </span><br><span class="line">137             context.write(outKey, NullWritable.get());</span><br><span class="line">138         </span><br><span class="line">139         &#125;</span><br><span class="line">140     &#125;</span><br><span class="line">141     </span><br><span class="line">142     /**</span><br><span class="line">143      * 取性别男女各前10名评分最好的电影</span><br><span class="line">144      * */</span><br><span class="line">145     public static class MoviesDemo2Reducer2 extends Reducer&lt;MoviesSexBean, NullWritable, MoviesSexBean, NullWritable&gt;&#123;</span><br><span class="line">146         </span><br><span class="line">147         @Override</span><br><span class="line">148         protected void reduce(MoviesSexBean key, Iterable&lt;NullWritable&gt; values,Context context)</span><br><span class="line">149                 throws IOException, InterruptedException &#123;</span><br><span class="line">150             </span><br><span class="line">151             int count = 0;</span><br><span class="line">152             for(NullWritable nvl : values) &#123;</span><br><span class="line">153                 count++;</span><br><span class="line">154                 context.write(key, NullWritable.get());</span><br><span class="line">155                 if(count == 10) &#123;</span><br><span class="line">156                     return;</span><br><span class="line">157                 &#125;        </span><br><span class="line">158             &#125;</span><br><span class="line">159         </span><br><span class="line">160         &#125;</span><br><span class="line">161     &#125;</span><br><span class="line">162 &#125;</span><br></pre></td></tr></table></figure><p>对象：MoviesSexBean.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class MoviesSexBean implements WritableComparable&lt;MoviesSexBean&gt;&#123;</span><br><span class="line"> 2     </span><br><span class="line"> 3     private String movieName;</span><br><span class="line"> 4     private String sex;</span><br><span class="line"> 5     private double score;</span><br><span class="line"> 6     </span><br><span class="line"> 7     public MoviesSexBean() &#123;</span><br><span class="line"> 8         super();</span><br><span class="line"> 9     &#125;</span><br><span class="line">10     public MoviesSexBean(String movieName, String sex, double score) &#123;</span><br><span class="line">11         super();</span><br><span class="line">12         this.movieName = movieName;</span><br><span class="line">13         this.sex = sex;</span><br><span class="line">14         this.score = score;</span><br><span class="line">15     &#125;</span><br><span class="line">16     public String getMovieName() &#123;</span><br><span class="line">17         return movieName;</span><br><span class="line">18     &#125;</span><br><span class="line">19     public void setMovieName(String movieName) &#123;</span><br><span class="line">20         this.movieName = movieName;</span><br><span class="line">21     &#125;</span><br><span class="line">22     public String getSex() &#123;</span><br><span class="line">23         return sex;</span><br><span class="line">24     &#125;</span><br><span class="line">25     public void setSex(String sex) &#123;</span><br><span class="line">26         this.sex = sex;</span><br><span class="line">27     &#125;</span><br><span class="line">28     public double getScore() &#123;</span><br><span class="line">29         return score;</span><br><span class="line">30     &#125;</span><br><span class="line">31     public void setScore(double score) &#123;</span><br><span class="line">32         this.score = score;</span><br><span class="line">33     &#125;</span><br><span class="line">34     @Override</span><br><span class="line">35     public String toString() &#123;</span><br><span class="line">36         return movieName + &quot;\t&quot; + sex + &quot;\t&quot; + score ;</span><br><span class="line">37     &#125;</span><br><span class="line">38     @Override</span><br><span class="line">39     public void readFields(DataInput in) throws IOException &#123;</span><br><span class="line">40         movieName = in.readUTF();</span><br><span class="line">41         sex = in.readUTF();</span><br><span class="line">42         score = in.readDouble();</span><br><span class="line">43     &#125;</span><br><span class="line">44     @Override</span><br><span class="line">45     public void write(DataOutput out) throws IOException &#123;</span><br><span class="line">46         out.writeUTF(movieName);</span><br><span class="line">47         out.writeUTF(sex);</span><br><span class="line">48         out.writeDouble(score);</span><br><span class="line">49     &#125;</span><br><span class="line">50     @Override</span><br><span class="line">51     public int compareTo(MoviesSexBean o) &#123;</span><br><span class="line">52         </span><br><span class="line">53         int result = this.getSex().compareTo(o.getSex());</span><br><span class="line">54         if(result == 0) &#123;</span><br><span class="line">55             double diff = this.getScore() - o.getScore();</span><br><span class="line">56             </span><br><span class="line">57             if(diff == 0) &#123;</span><br><span class="line">58                 return 0;</span><br><span class="line">59             &#125;else &#123;</span><br><span class="line">60                 return diff &gt; 0 ? -1 : 1;</span><br><span class="line">61             &#125;</span><br><span class="line">62             </span><br><span class="line">63         &#125;else &#123;</span><br><span class="line">64             return result &gt; 0 ? -1 : 1;</span><br><span class="line">65         &#125;</span><br><span class="line">66         </span><br><span class="line">67     &#125;</span><br><span class="line">68     </span><br><span class="line">69     </span><br><span class="line">70     </span><br><span class="line">71 &#125;</span><br></pre></td></tr></table></figure><p>分组：MoviesSexGC.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class MoviesSexGC extends WritableComparator&#123;</span><br><span class="line"> 2     </span><br><span class="line"> 3     public MoviesSexGC() &#123;</span><br><span class="line"> 4         super(MoviesSexBean.class,true);</span><br><span class="line"> 5     &#125;</span><br><span class="line"> 6     </span><br><span class="line"> 7     @Override</span><br><span class="line"> 8     public int compare(WritableComparable a, WritableComparable b) &#123;</span><br><span class="line"> 9         </span><br><span class="line">10         MoviesSexBean msb1 = (MoviesSexBean)a;</span><br><span class="line">11         MoviesSexBean msb2 = (MoviesSexBean)b;</span><br><span class="line">12 </span><br><span class="line">13         return msb1.getSex().compareTo(msb2.getSex());</span><br><span class="line">14     &#125;</span><br><span class="line">15     </span><br><span class="line">16 &#125;</span><br></pre></td></tr></table></figure><h3 id="3、求movieid-2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，评分）"><a href="#3、求movieid-2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，评分）" class="headerlink" title="3、求movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，评分）"></a>3、求movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，评分）</h3><p>以第二部三表联合之后的文件进行操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class MovieDemo3 &#123;</span><br><span class="line"> 2 </span><br><span class="line"> 3     public static void main(String[] args) throws Exception &#123;</span><br><span class="line"> 4         </span><br><span class="line"> 5         Configuration conf = new Configuration();</span><br><span class="line"> 6         FileSystem fs = FileSystem.get(conf);</span><br><span class="line"> 7         Job job = Job.getInstance(conf);</span><br><span class="line"> 8         </span><br><span class="line"> 9         job.setJarByClass(MovieDemo3.class);</span><br><span class="line">10         job.setMapperClass(MovieDemo3Mapper.class);</span><br><span class="line">11         job.setReducerClass(MovieDemo3Reducer.class);</span><br><span class="line">12         </span><br><span class="line">13         job.setOutputKeyClass(Text.class);</span><br><span class="line">14         job.setOutputValueClass(DoubleWritable.class);</span><br><span class="line">15         </span><br><span class="line">16         Path inputPath = new Path(&quot;D:\\MR\\hw\\movie\\3he1&quot;);</span><br><span class="line">17         Path outputPath = new Path(&quot;D:\\MR\\hw\\movie\\outpu3&quot;);</span><br><span class="line">18         </span><br><span class="line">19         if(fs.exists(outputPath)) &#123;</span><br><span class="line">20             fs.delete(outputPath,true);</span><br><span class="line">21         &#125;</span><br><span class="line">22         </span><br><span class="line">23         FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">24         FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line">25         </span><br><span class="line">26         boolean isDone = job.waitForCompletion(true);</span><br><span class="line">27         System.exit(isDone ? 0 : 1);</span><br><span class="line">28         </span><br><span class="line">29     &#125;</span><br><span class="line">30     </span><br><span class="line">31     </span><br><span class="line">32     /**</span><br><span class="line">33      * 1000::1036::4::975040964::Die Hard (1988)::Action|Thriller::F::25::6::90027</span><br><span class="line">34      * </span><br><span class="line">35      * 用户ID::电影ID::评分::评分时间戳::电影名字::电影类型::性别::年龄::职业::邮政编码</span><br><span class="line">36      * 0        1     2      3        4       5     6   7    8    9</span><br><span class="line">37      * </span><br><span class="line">38      * key:电影ID+电影名字+年龄段</span><br><span class="line">39      * value:评分</span><br><span class="line">40      * 求movieid = 2116这部电影各年龄段</span><br><span class="line">41      * */</span><br><span class="line">42     public static class MovieDemo3Mapper extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt;&#123;</span><br><span class="line">43         </span><br><span class="line">44         Text outKey = new Text();</span><br><span class="line">45         DoubleWritable outValue = new DoubleWritable();</span><br><span class="line">46         </span><br><span class="line">47         @Override</span><br><span class="line">48         protected void map(LongWritable key, Text value, Context context)</span><br><span class="line">49                 throws IOException, InterruptedException &#123;</span><br><span class="line">50 </span><br><span class="line">51             String[] split = value.toString().split(&quot;::&quot;);</span><br><span class="line">52             int movieID = Integer.parseInt(split[1]);</span><br><span class="line">53             </span><br><span class="line">54             if(movieID == 2116) &#123;</span><br><span class="line">55                 String strKey = split[1]+&quot;\t&quot;+split[4]+&quot;\t&quot;+split[7];</span><br><span class="line">56                 String strValue = split[2];</span><br><span class="line">57                 </span><br><span class="line">58                 outKey.set(strKey);</span><br><span class="line">59                 outValue.set(Double.parseDouble(strValue));</span><br><span class="line">60                 </span><br><span class="line">61                 context.write(outKey, outValue);</span><br><span class="line">62             &#125;</span><br><span class="line">63             </span><br><span class="line">64         &#125;</span><br><span class="line">65     &#125;</span><br><span class="line">66     </span><br><span class="line">67     </span><br><span class="line">68     </span><br><span class="line">69     /**</span><br><span class="line">70      * 对map的输出结果求平均评分</span><br><span class="line">71      * */</span><br><span class="line">72     public static class MovieDemo3Reducer extends Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt;&#123;</span><br><span class="line">73 </span><br><span class="line">74         DoubleWritable outValue = new DoubleWritable();</span><br><span class="line">75         </span><br><span class="line">76         @Override</span><br><span class="line">77         protected void reduce(Text key, Iterable&lt;DoubleWritable&gt; values, Context context)</span><br><span class="line">78                 throws IOException, InterruptedException &#123;</span><br><span class="line">79 </span><br><span class="line">80             int count = 0;</span><br><span class="line">81             double sum = 0;</span><br><span class="line">82             </span><br><span class="line">83             for(DoubleWritable value : values) &#123;</span><br><span class="line">84                 count++;</span><br><span class="line">85                 sum += Double.parseDouble(value.toString()); </span><br><span class="line">86             &#125;</span><br><span class="line">87             </span><br><span class="line">88             double avg = sum / count;</span><br><span class="line">89             </span><br><span class="line">90             outValue.set(avg);</span><br><span class="line">91             </span><br><span class="line">92             context.write(key, outValue);</span><br><span class="line">93             </span><br><span class="line">94         &#125;</span><br><span class="line">95         </span><br><span class="line">96     &#125;</span><br><span class="line">97     </span><br><span class="line">98 &#125;</span><br></pre></td></tr></table></figure><h3 id="4、求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（人，电影名，影评）"><a href="#4、求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（人，电影名，影评）" class="headerlink" title="4、求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（人，电影名，影评）"></a>4、求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分（人，电影名，影评）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1000    ::    1036    ::    4    ::    975040964    ::    Die Hard (1988)    ::    Action|Thriller    ::    F    ::    25    ::    6    ::    90027</span><br><span class="line"></span><br><span class="line">用户ID        电影ID        评分     　　 评分时间戳             电影名字                  电影类型                性别        年龄        职业        邮政编码</span><br><span class="line"></span><br><span class="line">0　　　　　　　　1　　　　　　　　2　　　　　　　　3　　　　　　　　　　　　4　　　　　　　　　　　　　　5　　　　　　　　　　　　6　　　　　　7　　　　　　8　　　　　　   9</span><br></pre></td></tr></table></figure><p>（1）求出评论次数最多的女性ID</p><p>　　MoviesDemo4_1.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br></pre></td><td class="code"><pre><span class="line">  1 public class MoviesDemo4 &#123;</span><br><span class="line">  2 </span><br><span class="line">  3     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">  4         </span><br><span class="line">  5         Configuration conf1 = new Configuration();</span><br><span class="line">  6         FileSystem fs1 = FileSystem.get(conf1);</span><br><span class="line">  7         Job job1 = Job.getInstance(conf1);</span><br><span class="line">  8         </span><br><span class="line">  9         job1.setJarByClass(MoviesDemo4.class);</span><br><span class="line"> 10         job1.setMapperClass(MoviesDemo4Mapper1.class);</span><br><span class="line"> 11         job1.setReducerClass(MoviesDemo4Reducer1.class);</span><br><span class="line"> 12         </span><br><span class="line"> 13         </span><br><span class="line"> 14         job1.setMapOutputKeyClass(Text.class);</span><br><span class="line"> 15         job1.setMapOutputValueClass(Text.class);</span><br><span class="line"> 16         job1.setOutputKeyClass(Text.class);</span><br><span class="line"> 17         job1.setOutputValueClass(DoubleWritable.class);</span><br><span class="line"> 18         </span><br><span class="line"> 19         </span><br><span class="line"> 20         Configuration conf2 = new Configuration();</span><br><span class="line"> 21         FileSystem fs2 = FileSystem.get(conf2);</span><br><span class="line"> 22         Job job2 = Job.getInstance(conf2);</span><br><span class="line"> 23         </span><br><span class="line"> 24         job2.setJarByClass(MoviesDemo4.class);</span><br><span class="line"> 25         job2.setMapperClass(MoviesDemo4Mapper2.class);</span><br><span class="line"> 26         job2.setReducerClass(MoviesDemo4Reducer2.class);</span><br><span class="line"> 27         </span><br><span class="line"> 28         job2.setMapOutputKeyClass(Moviegoers.class);</span><br><span class="line"> 29         job2.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"> 30         job2.setOutputKeyClass(Moviegoers.class);</span><br><span class="line"> 31         job2.setOutputValueClass(NullWritable.class);</span><br><span class="line"> 32         </span><br><span class="line"> 33         Path inputPath1 = new Path(&quot;D:\\MR\\hw\\movie\\3he1&quot;);</span><br><span class="line"> 34         Path outputPath1 = new Path(&quot;D:\\MR\\hw\\movie\\outpu4_1&quot;);</span><br><span class="line"> 35         </span><br><span class="line"> 36         if(fs1.exists(outputPath1)) &#123;</span><br><span class="line"> 37             fs1.delete(outputPath1,true);</span><br><span class="line"> 38         &#125;</span><br><span class="line"> 39         </span><br><span class="line"> 40         FileInputFormat.setInputPaths(job1, inputPath1);</span><br><span class="line"> 41         FileOutputFormat.setOutputPath(job1, outputPath1);</span><br><span class="line"> 42         </span><br><span class="line"> 43         </span><br><span class="line"> 44         Path inputPath2 = new Path(&quot;D:\\MR\\hw\\movie\\outpu4_1&quot;);</span><br><span class="line"> 45         Path outputPath2 = new Path(&quot;D:\\MR\\hw\\movie\\outpu4_2&quot;);</span><br><span class="line"> 46         </span><br><span class="line"> 47         if(fs2.exists(outputPath2)) &#123;</span><br><span class="line"> 48             fs2.delete(outputPath2,true);</span><br><span class="line"> 49         &#125;</span><br><span class="line"> 50         </span><br><span class="line"> 51         FileInputFormat.setInputPaths(job2, inputPath2);</span><br><span class="line"> 52         FileOutputFormat.setOutputPath(job2, outputPath2);</span><br><span class="line"> 53         </span><br><span class="line"> 54         JobControl control = new JobControl(&quot;MoviesDemo4&quot;);</span><br><span class="line"> 55         </span><br><span class="line"> 56         ControlledJob ajob = new ControlledJob(job1.getConfiguration());</span><br><span class="line"> 57         ControlledJob bjob = new ControlledJob(job2.getConfiguration());</span><br><span class="line"> 58         </span><br><span class="line"> 59         bjob.addDependingJob(ajob);</span><br><span class="line"> 60         </span><br><span class="line"> 61         control.addJob(ajob);</span><br><span class="line"> 62         control.addJob(bjob);</span><br><span class="line"> 63         </span><br><span class="line"> 64         Thread thread = new Thread(control);</span><br><span class="line"> 65         thread.start();</span><br><span class="line"> 66         </span><br><span class="line"> 67         while(!control.allFinished()) &#123;</span><br><span class="line"> 68             thread.sleep(1000);</span><br><span class="line"> 69         &#125;</span><br><span class="line"> 70         System.exit(0);</span><br><span class="line"> 71     &#125;</span><br><span class="line"> 72     </span><br><span class="line"> 73     /**</span><br><span class="line"> 74      * 1000::1036::4::975040964::Die Hard (1988)::Action|Thriller::F::25::6::90027</span><br><span class="line"> 75      * </span><br><span class="line"> 76      * 用户ID::电影ID::评分::评分时间戳::电影名字::电影类型::性别::年龄::职业::邮政编码</span><br><span class="line"> 77      * 0        1     2      3        4       5     6   7    8    9</span><br><span class="line"> 78      * </span><br><span class="line"> 79      * 1、key:用户ID</span><br><span class="line"> 80       * 2、value：电影名+评分</span><br><span class="line"> 81      * </span><br><span class="line"> 82      * */</span><br><span class="line"> 83     public static class MoviesDemo4Mapper1 extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123;</span><br><span class="line"> 84         </span><br><span class="line"> 85         Text outKey = new Text();</span><br><span class="line"> 86         Text outValue = new Text();</span><br><span class="line"> 87         </span><br><span class="line"> 88         @Override</span><br><span class="line"> 89         protected void map(LongWritable key, Text value, Context context)</span><br><span class="line"> 90                 throws IOException, InterruptedException &#123;</span><br><span class="line"> 91             </span><br><span class="line"> 92             String[] split = value.toString().split(&quot;::&quot;);</span><br><span class="line"> 93             </span><br><span class="line"> 94             String strKey = split[0];</span><br><span class="line"> 95             String strValue = split[4]+&quot;\t&quot;+split[2];</span><br><span class="line"> 96             </span><br><span class="line"> 97             if(split[6].equals(&quot;F&quot;)) &#123;</span><br><span class="line"> 98                 outKey.set(strKey);</span><br><span class="line"> 99                 outValue.set(strValue);</span><br><span class="line">100                 context.write(outKey, outValue);</span><br><span class="line">101             &#125;</span><br><span class="line">102             </span><br><span class="line">103         &#125;</span><br><span class="line">104         </span><br><span class="line">105     &#125;</span><br><span class="line">106     </span><br><span class="line">107     //统计每位女性的评论总数</span><br><span class="line">108     public static class MoviesDemo4Reducer1 extends Reducer&lt;Text, Text, Text, IntWritable&gt;&#123;</span><br><span class="line">109         </span><br><span class="line">110         IntWritable outValue = new IntWritable();</span><br><span class="line">111         </span><br><span class="line">112         @Override</span><br><span class="line">113         protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context)</span><br><span class="line">114                 throws IOException, InterruptedException &#123;</span><br><span class="line">115 </span><br><span class="line">116             int count = 0;</span><br><span class="line">117             for(Text value : values) &#123;</span><br><span class="line">118                 count++;</span><br><span class="line">119             &#125;</span><br><span class="line">120             outValue.set(count);</span><br><span class="line">121             context.write(key, outValue);</span><br><span class="line">122         &#125;</span><br><span class="line">123         </span><br><span class="line">124     &#125;</span><br><span class="line">125     </span><br><span class="line">126     //对第一次MapReduce的输出结果进行降序排序</span><br><span class="line">127     public static class MoviesDemo4Mapper2 extends Mapper&lt;LongWritable, Text,Moviegoers,NullWritable&gt;&#123;</span><br><span class="line">128         </span><br><span class="line">129         Moviegoers outKey = new Moviegoers();</span><br><span class="line">130         </span><br><span class="line">131         @Override</span><br><span class="line">132         protected void map(LongWritable key, Text value, Context context)</span><br><span class="line">133                 throws IOException, InterruptedException &#123;</span><br><span class="line">134             </span><br><span class="line">135             String[] split = value.toString().split(&quot;\t&quot;);</span><br><span class="line">136             </span><br><span class="line">137             outKey.setName(split[0]);</span><br><span class="line">138             outKey.setCount(Integer.parseInt(split[1]));</span><br><span class="line">139             context.write(outKey, NullWritable.get());</span><br><span class="line">140         &#125;</span><br><span class="line">141         </span><br><span class="line">142     &#125;</span><br><span class="line">143     </span><br><span class="line">144     //排序之后取第一个值（评论最多的女性ID和评论次数）</span><br><span class="line">145     public static class MoviesDemo4Reducer2 extends Reducer&lt;Moviegoers,NullWritable, Moviegoers,NullWritable&gt;&#123;</span><br><span class="line">146         </span><br><span class="line">147         int count = 0;</span><br><span class="line">148         </span><br><span class="line">149         @Override</span><br><span class="line">150         protected void reduce(Moviegoers key, Iterable&lt;NullWritable&gt; values,Context context)</span><br><span class="line">151                 throws IOException, InterruptedException &#123;</span><br><span class="line">152 </span><br><span class="line">153             for(NullWritable nvl : values) &#123;</span><br><span class="line">154                 count++;</span><br><span class="line">155                 if(count &gt; 1) &#123;</span><br><span class="line">156                     return;</span><br><span class="line">157                 &#125;</span><br><span class="line">158                 context.write(key, nvl);    </span><br><span class="line">159             &#125;</span><br><span class="line">160         </span><br><span class="line">161         &#125;</span><br><span class="line">162         </span><br><span class="line">163     &#125;</span><br><span class="line">164     </span><br><span class="line">165     </span><br><span class="line">166 &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（二十四）YARN的资源调度</title>
      <link href="/2018-04-24-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E5%9B%9B%EF%BC%89YARN%E7%9A%84%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6.html"/>
      <url>/2018-04-24-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E5%9B%9B%EF%BC%89YARN%E7%9A%84%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（二十四）YARN的资源调度：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（二十四）YARN的资源调度</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1、YARN-概述-YARN（Yet-Another-Resource-Negotiator）"><a href="#1-1、YARN-概述-YARN（Yet-Another-Resource-Negotiator）" class="headerlink" title="1.1、YARN 概述 YARN（Yet Another Resource Negotiator）"></a>1.1、YARN 概述 YARN（Yet Another Resource Negotiator）</h2><p>　　YARN 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操 作系统平台，而 MapReduce 等运算程序则相当于运行于操作系统之上的应用程序</p><p>　　YARN 是 Hadoop2.x 版本中的一个新特性。它的出现其实是为了解决第一代 MapReduce 编程 框架的不足，提高集群环境下的资源利用率，这些资源包括内存，磁盘，网络，IO等。Hadoop2.X 版本中重新设计的这个 YARN 集群，具有更好的扩展性，可用性，可靠性，向后兼容性，以 及能支持除 MapReduce 以外的更多分布式计算程序</p><p>　　1、YARN 并不清楚用户提交的程序的运行机制</p><p>　　2、YARN 只提供运算资源的调度（用户程序向 YARN 申请资源，YARN 就负责分配资源）</p><p>　　3、YARN 中的主管角色叫 ResourceManager</p><p>　　4、YARN 中具体提供运算资源的角色叫 NodeManager</p><p>　　5、这样一来，YARN 其实就与运行的用户程序完全解耦，就意味着 YARN 上可以运行各种类 型的分布式运算程序（MapReduce 只是其中的一种），比如 MapReduce、Storm 程序，Spark 程序，Tez ……</p><p>　　6、所以，Spark、Storm 等运算框架都可以整合在 YARN 上运行，只要他们各自的框架中有 符合 YARN 规范的资源请求机制即可</p><p>　　7、yarn 就成为一个通用的资源调度平台，从此，企业中以前存在的各种运算集群都可以整 合在一个物理集群上，提高资源利用率，方便数据共享</p><h3 id="1-2、原-MapReduce-框架的不足"><a href="#1-2、原-MapReduce-框架的不足" class="headerlink" title="1.2、原 MapReduce 框架的不足"></a>1.2、原 MapReduce 框架的不足</h3><p>　　1、JobTracker 是集群事务的集中处理点，存在单点故障</p><p>　　2、JobTracker 需要完成的任务太多，既要维护 job 的状态又要维护 job 的 task 的状态，造成 过多的资源消耗</p><p>　　3、在 TaskTracker 端，用 Map/Reduce Task 作为资源的表示过于简单，没有考虑到 CPU、内 存等资源情况，当把两个需要消耗大内存的 Task 调度到一起，很容易出现 OOM</p><p>　　4、把资源强制划分为 Map/Reduce Slot，当只有 MapTask 时，TeduceSlot 不能用；当只有 Reduce Task 时，MapSlot 不能用，容易造成资源利用不足。</p><p>　　总结起来就是：</p><p>　　　　1、扩展性差</p><p>　　　　2、可靠性低</p><p>　　　　3、资源利用率低</p><p>　　　　4、不支持多种计算框架</p><h3 id="1-3、新版-YARN-架构的优点"><a href="#1-3、新版-YARN-架构的优点" class="headerlink" title="1.3、新版 YARN 架构的优点"></a>1.3、新版 YARN 架构的优点</h3><p>　　YARN/MRv2 最基本的想法是将原 JobTracker 主要的资源管理和 Job 调度/监视功能分开作为 两个单独的守护进程。有一个全局的 ResourceManager(RM)和每个 Application 有一个 ApplicationMaster(AM)，Application 相当于 MapReduce Job 或者 DAG jobs。ResourceManager 和 NodeManager(NM)组成了基本的数据计算框架。ResourceManager 协调集群的资源利用， 任何 Client 或者运行着的 applicatitonMaster 想要运行 Job 或者 Task 都得向 RM 申请一定的资 源。ApplicatonMaster 是一个框架特殊的库，对于 MapReduce 框架而言有它自己的 AM 实现， 用户也可以实现自己的 AM，在运行的时候，AM 会与 NM 一起来启动和监视 Tasks。</p><h3 id="1-4、YARN-的重要概念"><a href="#1-4、YARN-的重要概念" class="headerlink" title="1.4、YARN 的重要概念"></a>1.4、YARN 的重要概念</h3><h4 id="1-4-1、ResourceManager"><a href="#1-4-1、ResourceManager" class="headerlink" title="1.4.1、ResourceManager"></a>1.4.1、ResourceManager</h4><p>　　ResourceManager 是基于应用程序对集群资源的需求进行调度的 YARN 集群主控节点，负责 协调和管理整个集群（所有 NodeManager）的资源，响应用户提交的不同类型应用程序的 解析，调度，监控等工作。ResourceManager 会为每一个 Application 启动一个 MRAppMaster， 并且 MRAppMaster 分散在各个 NodeManager 节点</p><p>　　它主要由两个组件构成：<strong>调度器（Scheduler）和应用程序管理器（ApplicationsManager， ASM）</strong></p><p>　　YARN 集群的主节点 ResourceManager 的职责：</p><p>　　　　1、处理客户端请求</p><p>　　　　2、启动或监控 MRAppMaster</p><p>　　　　3、监控 NodeManager</p><p>　　　　4、资源的分配与调度</p><h4 id="1-4-2、NodeManager"><a href="#1-4-2、NodeManager" class="headerlink" title="1.4.2、NodeManager"></a>1.4.2、NodeManager</h4><p>　　NodeManager 是 YARN 集群当中真正资源的提供者，是真正执行应用程序的容器的提供者， 监控应用程序的资源使用情况（CPU，内存，硬盘，网络），并通过心跳向集群资源调度器 ResourceManager 进行汇报以更新自己的健康状态。同时其也会监督 Container 的生命周期 管理，监控每个 Container 的资源使用（内存、CPU 等）情况，追踪节点健康状况，管理日 志和不同应用程序用到的附属服务（auxiliary service）。</p><p>　　YARN 集群的从节点 NodeManager 的职责：</p><p>　　　　1、管理单个节点上的资源</p><p>　　　　2、处理来自 ResourceManager 的命令</p><p>　　　　3、处理来自 MRAppMaster 的命令</p><h4 id="1-4-3、MRAppMaster"><a href="#1-4-3、MRAppMaster" class="headerlink" title="1.4.3、MRAppMaster"></a>1.4.3、MRAppMaster</h4><p>　　MRAppMaster 对应一个应用程序，职责是：向资源调度器申请执行任务的资源容器，运行 任务，监控整个任务的执行，跟踪整个任务的状态，处理任务失败以异常情况</p><h4 id="1-4-4、Container"><a href="#1-4-4、Container" class="headerlink" title="1.4.4、Container"></a>1.4.4、Container</h4><p>　　Container 容器是一个抽象出来的逻辑资源单位。容器是由 ResourceManager Scheduler 服务 动态分配的资源构成，它包括了该节点上的一定量 CPU，内存，磁盘，网络等信息，MapReduce 程序的所有 Task 都是在一个容器里执行完成的，容器的大小是可以动态调整的</p><h4 id="1-4-5、ASM"><a href="#1-4-5、ASM" class="headerlink" title="1.4.5、ASM"></a>1.4.5、ASM</h4><p>　　应用程序管理器 ASM 负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协 商资源以启动 MRAppMaster、监控 MRAppMaster 运行状态并在失败时重新启动它等</p><h4 id="1-4-6、Scheduler"><a href="#1-4-6、Scheduler" class="headerlink" title="1.4.6、Scheduler"></a>1.4.6、Scheduler</h4><p>　　调度器根据应用程序的资源需求进行资源分配，不参与应用程序具体的执行和监控等工作 资源分配的单位就是 Container，调度器是一个可插拔的组件，用户可以根据自己的需求实 现自己的调度器。YARN 本身为我们提供了多种直接可用的调度器，比如 FIFO，Fair Scheduler 和 Capacity Scheduler 等</p><h3 id="1-5、YARN-架构及各角色职责"><a href="#1-5、YARN-架构及各角色职责" class="headerlink" title="1.5、YARN 架构及各角色职责"></a>1.5、YARN 架构及各角色职责</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180324155640892-527759119.png" alt="img"></p><h3 id="1-6、YARN-作业执行流程"><a href="#1-6、YARN-作业执行流程" class="headerlink" title="1.6、YARN 作业执行流程"></a>1.6、YARN 作业执行流程</h3><p>　　YARN 作业执行流程：</p><p>　　　　1、用户向 YARN 中提交应用程序，其中包括 MRAppMaster 程序，启动 MRAppMaster 的命令， 用户程序等。</p><p>　　　　2、ResourceManager 为该程序分配第一个 Container，并与对应的 NodeManager 通讯，要求 它在这个 Container 中启动应用程序 MRAppMaster。</p><p>　　　　3、MRAppMaster 首先向 ResourceManager 注册，这样用户可以直接通过 ResourceManager 查看应用程序的运行状态，然后将为各个任务申请资源，并监控它的运行状态，直到运行结束，重复 4 到 7 的步骤。</p><p>　　　　4、MRAppMaster 采用轮询的方式通过 RPC 协议向 ResourceManager 申请和领取资源。</p><p>　　　　5、一旦 MRAppMaster 申请到资源后，便与对应的 NodeManager 通讯，要求它启动任务。</p><p>　　　　6、NodeManager 为任务设置好运行环境（包括环境变量、JAR 包、二进制程序等）后，将 任务启动命令写到一个脚本中，并通过运行该脚本启动任务。</p><p>　　　　7、各个任务通过某个 RPC 协议向 MRAppMaster 汇报自己的状态和进度，以让 MRAppMaster 随时掌握各个任务的运行状态，从而可以在任务败的时候重新启动任务。</p><p>　　　　8、应用程序运行完成后，MRAppMaster 向 ResourceManager 注销并关闭自己。</p><p> <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180324155754097-1779386657.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（二十五）MapReduce的API使用（二）</title>
      <link href="/2018-04-25-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E4%BA%94%EF%BC%89MapReduce%E7%9A%84API%E4%BD%BF%E7%94%A8%EF%BC%88%E4%BA%8C%EF%BC%89.html"/>
      <url>/2018-04-25-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E4%BA%94%EF%BC%89MapReduce%E7%9A%84API%E4%BD%BF%E7%94%A8%EF%BC%88%E4%BA%8C%EF%BC%89.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（二十五）MapReduce的API使用（二）：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（二十五）MapReduce的API使用（二）</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="学生成绩—增强版"><a href="#学生成绩—增强版" class="headerlink" title="学生成绩—增强版"></a>学生成绩—增强版</h2><h3 id="数据信息"><a href="#数据信息" class="headerlink" title="数据信息"></a>数据信息</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"> 1 computer,huangxiaoming,85,86,41,75,93,42,85</span><br><span class="line"> 2 computer,xuzheng,54,52,86,91,42</span><br><span class="line"> 3 computer,huangbo,85,42,96,38</span><br><span class="line"> 4 english,zhaobenshan,54,52,86,91,42,85,75</span><br><span class="line"> 5 english,liuyifei,85,41,75,21,85,96,14</span><br><span class="line"> 6 algorithm,liuyifei,75,85,62,48,54,96,15</span><br><span class="line"> 7 computer,huangjiaju,85,75,86,85,85</span><br><span class="line"> 8 english,liuyifei,76,95,86,74,68,74,48</span><br><span class="line"> 9 english,huangdatou,48,58,67,86,15,33,85</span><br><span class="line">10 algorithm,huanglei,76,95,86,74,68,74,48</span><br><span class="line">11 algorithm,huangjiaju,85,75,86,85,85,74,86</span><br><span class="line">12 computer,huangdatou,48,58,67,86,15,33,85</span><br><span class="line">13 english,zhouqi,85,86,41,75,93,42,85,75,55,47,22</span><br><span class="line">14 english,huangbo,85,42,96,38,55,47,22</span><br><span class="line">15 algorithm,liutao,85,75,85,99,66</span><br><span class="line">16 computer,huangzitao,85,86,41,75,93,42,85</span><br><span class="line">17 math,wangbaoqiang,85,86,41,75,93,42,85</span><br><span class="line">18 computer,liujialing,85,41,75,21,85,96,14,74,86</span><br><span class="line">19 computer,liuyifei,75,85,62,48,54,96,15</span><br><span class="line">20 computer,liutao,85,75,85,99,66,88,75,91</span><br><span class="line">21 computer,huanglei,76,95,86,74,68,74,48</span><br><span class="line">22 english,liujialing,75,85,62,48,54,96,15</span><br><span class="line">23 math,huanglei,76,95,86,74,68,74,48</span><br><span class="line">24 math,huangjiaju,85,75,86,85,85,74,86</span><br><span class="line">25 math,liutao,48,58,67,86,15,33,85</span><br><span class="line">26 english,huanglei,85,75,85,99,66,88,75,91</span><br><span class="line">27 math,xuzheng,54,52,86,91,42,85,75</span><br><span class="line">28 math,huangxiaoming,85,75,85,99,66,88,75,91</span><br><span class="line">29 math,liujialing,85,86,41,75,93,42,85,75</span><br><span class="line">30 english,huangxiaoming,85,86,41,75,93,42,85</span><br><span class="line">31 algorithm,huangdatou,48,58,67,86,15,33,85</span><br><span class="line">32 algorithm,huangzitao,85,86,41,75,93,42,85,75</span><br></pre></td></tr></table></figure><h3 id="数据解释"><a href="#数据解释" class="headerlink" title="数据解释"></a>数据解释</h3><p>数据字段个数不固定：<br>第一个是课程名称，总共四个课程，computer，math，english，algorithm，<br>第二个是学生姓名，后面是每次考试的分数</p><h3 id="统计需求"><a href="#统计需求" class="headerlink" title="统计需求"></a>统计需求</h3><p>1、统计每门课程的参考人数和课程平均分</p><p>2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件，并且按平均分从高到低排序，分数保留一位小数</p><p>3、求出每门课程参考学生成绩最高的学生的信息：课程，姓名和平均分</p><p>第一题</p><p>MRAvgScore1.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line">  1 /**</span><br><span class="line">  2  * 需求：统计每门课程的参考人数和课程平均分</span><br><span class="line">  3  * */</span><br><span class="line">  4 public class MRAvgScore1 &#123;</span><br><span class="line">  5 </span><br><span class="line">  6     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">  7         </span><br><span class="line">  8         Configuration conf1 = new Configuration();</span><br><span class="line">  9         Configuration conf2 = new Configuration();</span><br><span class="line"> 10         </span><br><span class="line"> 11         Job job1 = Job.getInstance(conf1);</span><br><span class="line"> 12         Job job2 = Job.getInstance(conf2);</span><br><span class="line"> 13             </span><br><span class="line"> 14         job1.setJarByClass(MRAvgScore1.class);</span><br><span class="line"> 15         job1.setMapperClass(AvgScoreMapper1.class);</span><br><span class="line"> 16         //job.setReducerClass(MFReducer.class);</span><br><span class="line"> 17             </span><br><span class="line"> 18         job1.setOutputKeyClass(Text.class);</span><br><span class="line"> 19         job1.setOutputValueClass(DoubleWritable.class);</span><br><span class="line"> 20         </span><br><span class="line"> 21         Path inputPath1 = new Path(&quot;D:\\MR\\hw\\work3\\input&quot;);</span><br><span class="line"> 22         Path outputPath1 = new Path(&quot;D:\\MR\\hw\\work3\\output_hw1_1&quot;);</span><br><span class="line"> 23             </span><br><span class="line"> 24         FileInputFormat.setInputPaths(job1, inputPath1);</span><br><span class="line"> 25         FileOutputFormat.setOutputPath(job1, outputPath1);</span><br><span class="line"> 26         </span><br><span class="line"> 27         </span><br><span class="line"> 28         job2.setMapperClass(AvgScoreMapper2.class);</span><br><span class="line"> 29         job2.setReducerClass(AvgScoreReducer2.class);</span><br><span class="line"> 30             </span><br><span class="line"> 31         job2.setOutputKeyClass(Text.class);</span><br><span class="line"> 32         job2.setOutputValueClass(DoubleWritable.class);</span><br><span class="line"> 33         </span><br><span class="line"> 34         Path inputPath2 = new Path(&quot;D:\\MR\\hw\\work3\\output_hw1_1&quot;);</span><br><span class="line"> 35         Path outputPath2 = new Path(&quot;D:\\MR\\hw\\work3\\output_hw1_end&quot;);</span><br><span class="line"> 36             </span><br><span class="line"> 37         FileInputFormat.setInputPaths(job2, inputPath2);</span><br><span class="line"> 38         FileOutputFormat.setOutputPath(job2, outputPath2);</span><br><span class="line"> 39         </span><br><span class="line"> 40         JobControl control = new JobControl(&quot;AvgScore&quot;);</span><br><span class="line"> 41         </span><br><span class="line"> 42         ControlledJob aJob = new ControlledJob(job1.getConfiguration());</span><br><span class="line"> 43         ControlledJob bJob = new ControlledJob(job2.getConfiguration());</span><br><span class="line"> 44         </span><br><span class="line"> 45         bJob.addDependingJob(aJob);</span><br><span class="line"> 46         </span><br><span class="line"> 47         control.addJob(aJob);</span><br><span class="line"> 48         control.addJob(bJob);</span><br><span class="line"> 49         </span><br><span class="line"> 50         Thread thread = new Thread(control);</span><br><span class="line"> 51         thread.start();</span><br><span class="line"> 52         </span><br><span class="line"> 53         while(!control.allFinished()) &#123;</span><br><span class="line"> 54             thread.sleep(1000);</span><br><span class="line"> 55         &#125;</span><br><span class="line"> 56         System.exit(0);</span><br><span class="line"> 57         </span><br><span class="line"> 58     &#125;</span><br><span class="line"> 59     </span><br><span class="line"> 60     </span><br><span class="line"> 61     </span><br><span class="line"> 62     /**</span><br><span class="line"> 63      * 数据类型：computer,huangxiaoming,85,86,41,75,93,42,85</span><br><span class="line"> 64      * </span><br><span class="line"> 65      * 需求：统计每门课程的参考人数和课程平均分</span><br><span class="line"> 66      * </span><br><span class="line"> 67      * 分析：以课程名称+姓名作为key，以平均分数作为value</span><br><span class="line"> 68      * */</span><br><span class="line"> 69     public static class AvgScoreMapper1 extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt;&#123;</span><br><span class="line"> 70         </span><br><span class="line"> 71         @Override</span><br><span class="line"> 72         protected void map(LongWritable key, Text value,Context context)</span><br><span class="line"> 73                 throws IOException, InterruptedException &#123;</span><br><span class="line"> 74             </span><br><span class="line"> 75             String[] splits = value.toString().split(&quot;,&quot;);</span><br><span class="line"> 76             //拼接成要输出的key</span><br><span class="line"> 77             String outKey = splits[0]+&quot;\t&quot;+splits[1];</span><br><span class="line"> 78             int length = splits.length;</span><br><span class="line"> 79             int sum = 0;</span><br><span class="line"> 80             //求出成绩的总和</span><br><span class="line"> 81             for(int i=2;i&lt;length;i++) &#123;</span><br><span class="line"> 82                 sum += Integer.parseInt(splits[i]);</span><br><span class="line"> 83             &#125;</span><br><span class="line"> 84             //求出平均分</span><br><span class="line"> 85             double outValue = sum / (length - 2);</span><br><span class="line"> 86             </span><br><span class="line"> 87             context.write(new Text(outKey), new DoubleWritable(outValue));</span><br><span class="line"> 88             </span><br><span class="line"> 89         &#125;</span><br><span class="line"> 90         </span><br><span class="line"> 91     &#125;</span><br><span class="line"> 92     </span><br><span class="line"> 93     /**</span><br><span class="line"> 94      * 对第一次MapReduce输出的结果进一步计算，第一步输出结果样式为</span><br><span class="line"> 95      *  math    huangjiaju    82.0</span><br><span class="line"> 96      *  math    huanglei    74.0</span><br><span class="line"> 97      *    math    huangxiaoming    83.0</span><br><span class="line"> 98      *    math    liujialing    72.0</span><br><span class="line"> 99      *    math    liutao    56.0</span><br><span class="line">100      *    math    wangbaoqiang    72.0</span><br><span class="line">101      *    math    xuzheng    69.0</span><br><span class="line">102      * </span><br><span class="line">103      *    需求：统计每门课程的参考人数和课程平均分 </span><br><span class="line">104      *    分析：以课程名称作为key，以分数作为value进行 输出</span><br><span class="line">105      * </span><br><span class="line">106      * */</span><br><span class="line">107     public static class AvgScoreMapper2 extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt;&#123;</span><br><span class="line">108         </span><br><span class="line">109         @Override</span><br><span class="line">110         protected void map(LongWritable key, Text value,Context context)</span><br><span class="line">111                 throws IOException, InterruptedException &#123;</span><br><span class="line">112             </span><br><span class="line">113             String[] splits = value.toString().split(&quot;\t&quot;);</span><br><span class="line">114             String outKey = splits[0];</span><br><span class="line">115             String outValue = splits[2];</span><br><span class="line">116             </span><br><span class="line">117             context.write(new Text(outKey), new DoubleWritable(Double.parseDouble(outValue)));</span><br><span class="line">118         &#125;</span><br><span class="line">119         </span><br><span class="line">120     &#125;</span><br><span class="line">121     </span><br><span class="line">122     /**</span><br><span class="line">123      * 针对同一门课程，对values进行遍历计数，看看有多少人参加了考试，并计算出平均成绩</span><br><span class="line">124      * */</span><br><span class="line">125     public static class AvgScoreReducer2 extends Reducer&lt;Text, DoubleWritable, Text, Text&gt;&#123;</span><br><span class="line">126         </span><br><span class="line">127         @Override</span><br><span class="line">128         protected void reduce(Text key, Iterable&lt;DoubleWritable&gt; values,</span><br><span class="line">129                 Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">130             </span><br><span class="line">131             int count = 0;</span><br><span class="line">132             double sum = 0;</span><br><span class="line">133             for(DoubleWritable value : values) &#123;</span><br><span class="line">134                 count++;</span><br><span class="line">135                 sum += value.get();</span><br><span class="line">136             &#125;</span><br><span class="line">137             </span><br><span class="line">138             double avg = sum / count;</span><br><span class="line">139             String outValue = count + &quot;\t&quot; + avg;</span><br><span class="line">140             context.write(key, new Text(outValue));</span><br><span class="line">141         &#125;</span><br><span class="line">142         </span><br><span class="line">143     &#125;</span><br><span class="line">144     </span><br><span class="line">145     </span><br><span class="line">146 &#125;</span><br></pre></td></tr></table></figure><p>第二题</p><p>MRAvgScore2.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class MRAvgScore2 &#123;</span><br><span class="line"> 2 </span><br><span class="line"> 3     public static void main(String[] args) throws Exception &#123;</span><br><span class="line"> 4         </span><br><span class="line"> 5         Configuration conf = new Configuration();</span><br><span class="line"> 6         </span><br><span class="line"> 7         Job job = Job.getInstance(conf);</span><br><span class="line"> 8             </span><br><span class="line"> 9         job.setJarByClass(MRAvgScore2.class);</span><br><span class="line">10         job.setMapperClass(ScoreMapper3.class);</span><br><span class="line">11         job.setReducerClass(ScoreReducer3.class);</span><br><span class="line">12             </span><br><span class="line">13         job.setOutputKeyClass(StudentBean.class);</span><br><span class="line">14         job.setOutputValueClass(NullWritable.class);</span><br><span class="line">15         </span><br><span class="line">16         job.setPartitionerClass(CoursePartitioner.class);</span><br><span class="line">17         job.setNumReduceTasks(4);</span><br><span class="line">18         </span><br><span class="line">19         Path inputPath = new Path(&quot;D:\\MR\\hw\\work3\\output_hw1_1&quot;);</span><br><span class="line">20         Path outputPath = new Path(&quot;D:\\MR\\hw\\work3\\output_hw2_1&quot;);</span><br><span class="line">21             </span><br><span class="line">22         FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">23         FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line">24         boolean isDone = job.waitForCompletion(true);</span><br><span class="line">25         System.exit(isDone ? 0 : 1);</span><br><span class="line">26     &#125;</span><br><span class="line">27     </span><br><span class="line">28     </span><br><span class="line">29     public static class ScoreMapper3 extends Mapper&lt;LongWritable, Text, StudentBean, NullWritable&gt;&#123;</span><br><span class="line">30         </span><br><span class="line">31         @Override</span><br><span class="line">32         protected void map(LongWritable key, Text value,Context context)</span><br><span class="line">33                 throws IOException, InterruptedException &#123;</span><br><span class="line">34             </span><br><span class="line">35             String[] splits = value.toString().split(&quot;\t&quot;);</span><br><span class="line">36             </span><br><span class="line">37             double score = Double.parseDouble(splits[2]);</span><br><span class="line">38             DecimalFormat df = new DecimalFormat(&quot;#.0&quot;);</span><br><span class="line">39             df.format(score);</span><br><span class="line">40             </span><br><span class="line">41             StudentBean student = new StudentBean(splits[0],splits[1],score);</span><br><span class="line">42             </span><br><span class="line">43             context.write(student, NullWritable.get());</span><br><span class="line">44             </span><br><span class="line">45         &#125;</span><br><span class="line">46         </span><br><span class="line">47     &#125;</span><br><span class="line">48     </span><br><span class="line">49     public static class ScoreReducer3 extends Reducer&lt;StudentBean, NullWritable, StudentBean, NullWritable&gt;&#123;</span><br><span class="line">50         </span><br><span class="line">51         @Override</span><br><span class="line">52         protected void reduce(StudentBean key, Iterable&lt;NullWritable&gt; values,Context context)</span><br><span class="line">53                 throws IOException, InterruptedException &#123;</span><br><span class="line">54 </span><br><span class="line">55             for(NullWritable nvl : values)&#123;</span><br><span class="line">56                 context.write(key, nvl);</span><br><span class="line">57             &#125;</span><br><span class="line">58             </span><br><span class="line">59         &#125;</span><br><span class="line">60     &#125;</span><br><span class="line">61 &#125;</span><br></pre></td></tr></table></figure><p>StudentBean.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class StudentBean implements WritableComparable&lt;StudentBean&gt;&#123;</span><br><span class="line"> 2     private String course;</span><br><span class="line"> 3     private String name;</span><br><span class="line"> 4     private double avgScore;</span><br><span class="line"> 5     </span><br><span class="line"> 6     public String getCourse() &#123;</span><br><span class="line"> 7         return course;</span><br><span class="line"> 8     &#125;</span><br><span class="line"> 9     public void setCourse(String course) &#123;</span><br><span class="line">10         this.course = course;</span><br><span class="line">11     &#125;</span><br><span class="line">12     public String getName() &#123;</span><br><span class="line">13         return name;</span><br><span class="line">14     &#125;</span><br><span class="line">15     public void setName(String name) &#123;</span><br><span class="line">16         this.name = name;</span><br><span class="line">17     &#125;</span><br><span class="line">18     public double getavgScore() &#123;</span><br><span class="line">19         return avgScore;</span><br><span class="line">20     &#125;</span><br><span class="line">21     public void setavgScore(double avgScore) &#123;</span><br><span class="line">22         this.avgScore = avgScore;</span><br><span class="line">23     &#125;</span><br><span class="line">24     public StudentBean(String course, String name, double avgScore) &#123;</span><br><span class="line">25         super();</span><br><span class="line">26         this.course = course;</span><br><span class="line">27         this.name = name;</span><br><span class="line">28         this.avgScore = avgScore;</span><br><span class="line">29     &#125;</span><br><span class="line">30     public StudentBean() &#123;</span><br><span class="line">31         super();</span><br><span class="line">32     &#125;</span><br><span class="line">33     </span><br><span class="line">34     @Override</span><br><span class="line">35     public String toString() &#123;</span><br><span class="line">36         return course + &quot;\t&quot; + name + &quot;\t&quot; + avgScore;</span><br><span class="line">37     &#125;</span><br><span class="line">38     @Override</span><br><span class="line">39     public void readFields(DataInput in) throws IOException &#123;</span><br><span class="line">40         course = in.readUTF();</span><br><span class="line">41         name = in.readUTF();</span><br><span class="line">42         avgScore = in.readDouble();</span><br><span class="line">43     &#125;</span><br><span class="line">44     @Override</span><br><span class="line">45     public void write(DataOutput out) throws IOException &#123;</span><br><span class="line">46         out.writeUTF(course);</span><br><span class="line">47         out.writeUTF(name);</span><br><span class="line">48         out.writeDouble(avgScore);</span><br><span class="line">49     &#125;</span><br><span class="line">50     @Override</span><br><span class="line">51     public int compareTo(StudentBean stu) &#123;</span><br><span class="line">52         double diffent =  this.avgScore - stu.avgScore;</span><br><span class="line">53         if(diffent == 0) &#123;</span><br><span class="line">54             return 0;</span><br><span class="line">55         &#125;else &#123;</span><br><span class="line">56             return diffent &gt; 0 ? -1 : 1;</span><br><span class="line">57         &#125;</span><br><span class="line">58     &#125;</span><br><span class="line">59     </span><br><span class="line">60     </span><br><span class="line">61 &#125;</span><br></pre></td></tr></table></figure><p>第三题</p><p>MRScore3.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line">  1 public class MRScore3 &#123;</span><br><span class="line">  2 </span><br><span class="line">  3     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">  4         </span><br><span class="line">  5         Configuration conf1 = new Configuration();</span><br><span class="line">  6         Configuration conf2 = new Configuration();</span><br><span class="line">  7         </span><br><span class="line">  8         Job job1 = Job.getInstance(conf1);</span><br><span class="line">  9         Job job2 = Job.getInstance(conf2);</span><br><span class="line"> 10             </span><br><span class="line"> 11         job1.setJarByClass(MRScore3.class);</span><br><span class="line"> 12         job1.setMapperClass(MRMapper3_1.class);</span><br><span class="line"> 13         //job.setReducerClass(ScoreReducer3.class);</span><br><span class="line"> 14         </span><br><span class="line"> 15         </span><br><span class="line"> 16         job1.setMapOutputKeyClass(IntWritable.class);</span><br><span class="line"> 17         job1.setMapOutputValueClass(StudentBean.class);</span><br><span class="line"> 18         job1.setOutputKeyClass(IntWritable.class);</span><br><span class="line"> 19         job1.setOutputValueClass(StudentBean.class);</span><br><span class="line"> 20         </span><br><span class="line"> 21         job1.setPartitionerClass(CoursePartitioner2.class);</span><br><span class="line"> 22         </span><br><span class="line"> 23         job1.setNumReduceTasks(4);</span><br><span class="line"> 24         </span><br><span class="line"> 25         Path inputPath = new Path(&quot;D:\\MR\\hw\\work3\\input&quot;);</span><br><span class="line"> 26         Path outputPath = new Path(&quot;D:\\MR\\hw\\work3\\output_hw3_1&quot;);</span><br><span class="line"> 27             </span><br><span class="line"> 28         FileInputFormat.setInputPaths(job1, inputPath);</span><br><span class="line"> 29         FileOutputFormat.setOutputPath(job1, outputPath);</span><br><span class="line"> 30         </span><br><span class="line"> 31         job2.setMapperClass(MRMapper3_2.class);</span><br><span class="line"> 32         job2.setReducerClass(MRReducer3_2.class);</span><br><span class="line"> 33         </span><br><span class="line"> 34         job2.setMapOutputKeyClass(IntWritable.class);</span><br><span class="line"> 35         job2.setMapOutputValueClass(StudentBean.class);</span><br><span class="line"> 36         job2.setOutputKeyClass(StudentBean.class);</span><br><span class="line"> 37         job2.setOutputValueClass(NullWritable.class);</span><br><span class="line"> 38         </span><br><span class="line"> 39         Path inputPath2 = new Path(&quot;D:\\MR\\hw\\work3\\output_hw3_1&quot;);</span><br><span class="line"> 40         Path outputPath2 = new Path(&quot;D:\\MR\\hw\\work3\\output_hw3_end&quot;);</span><br><span class="line"> 41             </span><br><span class="line"> 42         FileInputFormat.setInputPaths(job2, inputPath2);</span><br><span class="line"> 43         FileOutputFormat.setOutputPath(job2, outputPath2);</span><br><span class="line"> 44         </span><br><span class="line"> 45         JobControl control = new JobControl(&quot;Score3&quot;);</span><br><span class="line"> 46         </span><br><span class="line"> 47         ControlledJob aJob = new ControlledJob(job1.getConfiguration());</span><br><span class="line"> 48         ControlledJob bJob = new ControlledJob(job2.getConfiguration());</span><br><span class="line"> 49         </span><br><span class="line"> 50         bJob.addDependingJob(aJob);</span><br><span class="line"> 51         </span><br><span class="line"> 52         control.addJob(aJob);</span><br><span class="line"> 53         control.addJob(bJob);</span><br><span class="line"> 54         </span><br><span class="line"> 55         Thread thread = new Thread(control);</span><br><span class="line"> 56         thread.start();</span><br><span class="line"> 57         </span><br><span class="line"> 58         while(!control.allFinished()) &#123;</span><br><span class="line"> 59             thread.sleep(1000);</span><br><span class="line"> 60         &#125;</span><br><span class="line"> 61         System.exit(0);</span><br><span class="line"> 62         </span><br><span class="line"> 63 </span><br><span class="line"> 64     &#125;</span><br><span class="line"> 65     </span><br><span class="line"> 66     </span><br><span class="line"> 67     </span><br><span class="line"> 68     </span><br><span class="line"> 69     public static class MRMapper3_1 extends Mapper&lt;LongWritable, Text, IntWritable, StudentBean&gt;&#123;</span><br><span class="line"> 70         </span><br><span class="line"> 71         StudentBean outKey = new StudentBean();</span><br><span class="line"> 72         IntWritable outValue = new IntWritable();</span><br><span class="line"> 73         List&lt;String&gt; scoreList = new ArrayList&lt;&gt;();</span><br><span class="line"> 74         </span><br><span class="line"> 75         protected void map(LongWritable key, Text value, Context context) throws java.io.IOException ,InterruptedException &#123;</span><br><span class="line"> 76             </span><br><span class="line"> 77             scoreList.clear();</span><br><span class="line"> 78             String[] splits = value.toString().split(&quot;,&quot;);</span><br><span class="line"> 79             long sum = 0;</span><br><span class="line"> 80             </span><br><span class="line"> 81             for(int i=2;i&lt;splits.length;i++) &#123;</span><br><span class="line"> 82                 scoreList.add(splits[i]);</span><br><span class="line"> 83                 sum += Long.parseLong(splits[i]);</span><br><span class="line"> 84             &#125;</span><br><span class="line"> 85             </span><br><span class="line"> 86             Collections.sort(scoreList);</span><br><span class="line"> 87             outValue.set(Integer.parseInt(scoreList.get(scoreList.size()-1)));</span><br><span class="line"> 88             </span><br><span class="line"> 89             double avg = sum * 1.0/(splits.length-2);</span><br><span class="line"> 90             outKey.setCourse(splits[0]);</span><br><span class="line"> 91             outKey.setName(splits[1]);</span><br><span class="line"> 92             outKey.setavgScore(avg);</span><br><span class="line"> 93             </span><br><span class="line"> 94             context.write(outValue, outKey);</span><br><span class="line"> 95                                 </span><br><span class="line"> 96         &#125;;</span><br><span class="line"> 97     &#125;</span><br><span class="line"> 98     </span><br><span class="line"> 99     </span><br><span class="line">100     </span><br><span class="line">101     public static class MRMapper3_2 extends Mapper&lt;LongWritable, Text,IntWritable, StudentBean &gt;&#123;</span><br><span class="line">102         </span><br><span class="line">103         StudentBean outValue = new StudentBean();</span><br><span class="line">104         IntWritable outKey = new IntWritable();</span><br><span class="line">105         </span><br><span class="line">106         protected void map(LongWritable key, Text value, Context context) throws java.io.IOException ,InterruptedException &#123;</span><br><span class="line">107             </span><br><span class="line">108             String[] splits = value.toString().split(&quot;\t&quot;);</span><br><span class="line">109             outKey.set(Integer.parseInt(splits[0]));</span><br><span class="line">110             </span><br><span class="line">111             outValue.setCourse(splits[1]);</span><br><span class="line">112             outValue.setName(splits[2]);</span><br><span class="line">113             outValue.setavgScore(Double.parseDouble(splits[3]));</span><br><span class="line">114             </span><br><span class="line">115             context.write(outKey, outValue);</span><br><span class="line">116             </span><br><span class="line">117             </span><br><span class="line">118         &#125;;</span><br><span class="line">119     &#125;</span><br><span class="line">120     </span><br><span class="line">121     </span><br><span class="line">122     public static class MRReducer3_2 extends Reducer&lt;IntWritable, StudentBean, StudentBean, NullWritable&gt;&#123;</span><br><span class="line">123         </span><br><span class="line">124         StudentBean outKey = new StudentBean();</span><br><span class="line">125         </span><br><span class="line">126         @Override</span><br><span class="line">127         protected void reduce(IntWritable key, Iterable&lt;StudentBean&gt; values,Context context)</span><br><span class="line">128                 throws IOException, InterruptedException &#123;</span><br><span class="line">129             </span><br><span class="line">130             int length = values.toString().length();</span><br><span class="line">131             </span><br><span class="line">132             for(StudentBean value : values) &#123;</span><br><span class="line">133                 outKey = value;</span><br><span class="line">134             &#125;</span><br><span class="line">135             </span><br><span class="line">136             context.write(outKey, NullWritable.get());</span><br><span class="line">137             </span><br><span class="line">138         &#125;</span><br><span class="line">139     &#125;</span><br><span class="line">140     </span><br><span class="line">141     </span><br><span class="line">142 &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop源码学习－脚本命令（hadoop fs -ls)执行细节</title>
      <link href="/2018-04-29-Hadoop%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%EF%BC%8D%E8%84%9A%E6%9C%AC%E5%91%BD%E4%BB%A4%EF%BC%88hadoop%20fs%20-ls)%E6%89%A7%E8%A1%8C%E7%BB%86%E8%8A%82.html"/>
      <url>/2018-04-29-Hadoop%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%EF%BC%8D%E8%84%9A%E6%9C%AC%E5%91%BD%E4%BB%A4%EF%BC%88hadoop%20fs%20-ls)%E6%89%A7%E8%A1%8C%E7%BB%86%E8%8A%82.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop源码学习－脚本命令（hadoop fs -ls)执行细节：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （一）Spark初识</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>Hadoop有提供一些脚本命令，以便于我们对HDFS进行管理，可以通过命令hadoop fs进行查看： </p><p>通过以上使用说明可以发现，里面提供了大多数和我们在本地操作文件系统相类似的命令，例如，cat查看文件内容，chgrp改变用户群组权限，chmod改变用户权限，chown改变用户拥有者权限，还有创建目录，查看目录，移动文件，重命名等等。</p><p>hadoop fs -ls<br>这里，我们来看看命令hadoop fs -ls： </p><p>这个命令大家肯定非常熟悉，在Linux下使用超级频繁，功能就是列出指定目录下的文件及文件夹。那接下来，我们来看看它是怎样查找到此目录下的文件及文件夹的。</p><p>在运行hadoop fs -ls /命令之后，真正的命令只是hadoop，后面只是参数，那么，这个hadoop命令到底是哪呢？如果说集群是自己手动搭配的话，那大家肯定知道，这个命令就在${HADOOP_HOME}/bin目录下，但如果集群是自动化部署的时候，你可能一下子找不到这个命令在哪，此时，可以使用以下命令查找： </p><p>可以看到，这个命令应该是在目录/usr/bin/之下，使用vim /usr/bin/hadoop查看命令详细：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=$&#123;HADOOP_HOME:-/usr/hdp/2.5.0.0-1245/hadoop&#125;</span><br><span class="line">export HADOOP_MAPRED_HOME=$&#123;HADOOP_MAPRED_HOME:-/usr/hdp/2.5.0.0-1245/hadoop-mapreduce&#125;</span><br><span class="line">export HADOOP_YARN_HOME=$&#123;HADOOP_YARN_HOME:-/usr/hdp/2.5.0.0-1245/hadoop-yarn&#125;</span><br><span class="line">export HADOOP_LIBEXEC_DIR=$&#123;HADOOP_HOME&#125;/libexec</span><br><span class="line">export HDP_VERSION=$&#123;HDP_VERSION:-2.5.0.0-1245&#125;</span><br><span class="line">export HADOOP_OPTS="$&#123;HADOOP_OPTS&#125; -Dhdp.version=$&#123;HDP_VERSION&#125;"</span><br><span class="line"></span><br><span class="line">exec /usr/hdp/2.5.0.0-1245//hadoop/bin/hadoop.distro "$@"</span><br></pre></td></tr></table></figure><p>这个脚本做了两件事，一是export了一些环境变量，使得之后运行的子程序都可以共享这些变量的值；二是执行了命令hadoop.distro命令，并传上了所有参数。</p><p>现在，我们来看下命令hadoop.distro做了哪些事，由于代码有点小多，我就不全部贴了，只贴与执行命令hadoop fs -ls /相关的代码。</p><p>命令hadoop.distro做的事情是：根据之前传入的参数，然后做一些判断，确定一些变量的值，最后执行的是以下命令：</p><p>exec “$JAVA” $JAVA_HEAP_MAX $HADOOP_OPTS $CLASS “$@”<br>1<br>这里，我们看到了一堆变量，其中<br>$JAVA：java<br>$JAVA_HEAP_MAX：<br>$HADOOP_OPTS：</p><pre><code># Always respect HADOOP_OPTS and HADOOP_CLIENT_OPTSHADOOP_OPTS=&quot;$HADOOP_OPTS $HADOOP_CLIENT_OPTS&quot;#make sure security appender is turned offHADOOP_OPTS=&quot;$HADOOP_OPTS -Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,NullAppender}&quot;</code></pre><p>$CLASS：org.apache.hadoop.fs.FsShell<br>$@ ： -ls / 注意：这里已经没有参数fs了</p><p>因此，命令hadoop.distro也就转换成执行一个JAVA类了，然后继续带上参数。</p><p>打开hadoop的源代码，找到类org.apache.hadoop.fs.FsShell，它的main方法如下：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String argv[]) throws Exception &#123;</span><br><span class="line">     FsShell shell = newShellInstance(); //创建FsShell实例</span><br><span class="line">    Configuration conf = new Configuration();  //配置类，</span><br><span class="line">    conf.setQuietMode(false); //设置成“非安静模式”，默认为“安静模式”，在安静模式下，error和information的信息不会被记录。</span><br><span class="line"></span><br><span class="line">    shell.setConf(conf);</span><br><span class="line">    int res;</span><br><span class="line">    try &#123;</span><br><span class="line">      res = ToolRunner.run(shell, argv); //ToolRunner就是一个工具类，用于执行实现了接口Tool的类</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      shell.close();</span><br><span class="line">    &#125;</span><br><span class="line">    System.exit(res);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>ToolRunner类结合GenericOptionsParser类来解析命令行参数，<br>在运行上述ToolRunner.run(shell, argv)代码之后，经过一番解释之后，最后真正执行的仍然是类FsShell的run方法，而且对其参数进行了解析，run方法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">  @Override</span><br><span class="line">  public int run(String argv[]) throws Exception &#123;</span><br><span class="line">    // initialize FsShell 包括注册命令类</span><br><span class="line">    init();</span><br><span class="line">int exitCode = -1;</span><br><span class="line">if (argv.length &lt; 1) &#123;</span><br><span class="line">  printUsage(System.err); //打印使用方法</span><br><span class="line">&#125; else &#123;</span><br><span class="line">  String cmd = argv[0]; //取到第一个参数，即 ls</span><br><span class="line">  Command instance = null;</span><br><span class="line">  try &#123;</span><br><span class="line">    // 取得实现了该命令（ls）的命令实例,并且此类已经通过类CommandFactory的addClass方法进行了注册</span><br><span class="line">    instance = commandFactory.getInstance(cmd);</span><br><span class="line">    if (instance == null) &#123;</span><br><span class="line">      throw new UnknownCommandException();</span><br><span class="line">    &#125;</span><br><span class="line">    exitCode = instance.run(Arrays.copyOfRange(argv, 1, argv.length));</span><br><span class="line">  &#125; catch (IllegalArgumentException e) &#123;</span><br><span class="line">    displayError(cmd, e.getLocalizedMessage());</span><br><span class="line">    if (instance != null) &#123;</span><br><span class="line">      printInstanceUsage(System.err, instance);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; catch (Exception e) &#123;</span><br><span class="line">    // instance.run catches IOE, so something is REALLY wrong if here</span><br><span class="line">    LOG.debug(&quot;Error&quot;, e);</span><br><span class="line">    displayError(cmd, &quot;Fatal internal error&quot;);</span><br><span class="line">    e.printStackTrace(System.err);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">return exitCode;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：通过查看类Ls的代码，我们可以发现，它有一个静态方法registerCommands，这个方法就是对类Ls进行注册，但是，这只是一个静态方法，那么，到底是在哪进行了此方法的调用呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class Ls extends FsCommand &#123;</span><br><span class="line">  public static void registerCommands(CommandFactory factory) &#123;</span><br><span class="line">    factory.addClass(Ls.class, &quot;-ls&quot;);</span><br><span class="line">    factory.addClass(Lsr.class, &quot;-lsr&quot;);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>细心的朋友可能已经发现，就在类FsShell的run方法中，调用了一个init方法，而就在此方法中，有一行注册命令的代码，如下：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">protected void init() throws IOException &#123;</span><br><span class="line">    getConf().setQuietMode(true);</span><br><span class="line">    if (commandFactory == null) &#123;</span><br><span class="line">      commandFactory = new CommandFactory(getConf());</span><br><span class="line">      commandFactory.addObject(new Help(), &quot;-help&quot;);</span><br><span class="line">      commandFactory.addObject(new Usage(), &quot;-usage&quot;);</span><br><span class="line">      // 注册，调用registerCommands方法</span><br><span class="line">      registerCommands(commandFactory);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  protected void registerCommands(CommandFactory factory) &#123;</span><br><span class="line">    // TODO: DFSAdmin subclasses FsShell so need to protect the command</span><br><span class="line">    // registration.  This class should morph into a base class for</span><br><span class="line">    // commands, and then this method can be abstract</span><br><span class="line">    if (this.getClass().equals(FsShell.class)) &#123;</span><br><span class="line">      // 调用CommandFactory类的registerCommands方法</span><br><span class="line">      // 注意，这里传的参数是类FsCommand</span><br><span class="line">      factory.registerCommands(FsCommand.class);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>CommandFactory类的registerCommands方法如下：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public void registerCommands(Class&lt;?&gt; registrarClass) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      // 这里触发的是类CommandFactory的registerCommands方法</span><br><span class="line">      registrarClass.getMethod(</span><br><span class="line">          &quot;registerCommands&quot;, CommandFactory.class</span><br><span class="line">      ).invoke(null, this);</span><br><span class="line">    &#125; catch (Exception e) &#123;</span><br><span class="line">      throw new RuntimeException(StringUtils.stringifyException(e));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>接下来，我拉看看类CommandFactory的registerCommands方法，代码如下：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public static void registerCommands(CommandFactory factory) &#123;</span><br><span class="line">   factory.registerCommands(AclCommands.class);</span><br><span class="line">   factory.registerCommands(CopyCommands.class);</span><br><span class="line">   factory.registerCommands(Count.class);</span><br><span class="line">   factory.registerCommands(Delete.class);</span><br><span class="line">   factory.registerCommands(Display.class);</span><br><span class="line">   factory.registerCommands(Find.class);</span><br><span class="line">   factory.registerCommands(FsShellPermissions.class);</span><br><span class="line">   factory.registerCommands(FsUsage.class);</span><br><span class="line">   // 我们会用到的就是这个类Ls</span><br><span class="line">   factory.registerCommands(Ls.class);</span><br><span class="line">   factory.registerCommands(Mkdir.class);</span><br><span class="line">   factory.registerCommands(MoveCommands.class);</span><br><span class="line">   factory.registerCommands(SetReplication.class);</span><br><span class="line">   factory.registerCommands(Stat.class);</span><br><span class="line">   factory.registerCommands(Tail.class);</span><br><span class="line">   factory.registerCommands(Test.class);</span><br><span class="line">   factory.registerCommands(Touch.class);</span><br><span class="line">   factory.registerCommands(Truncate.class);</span><br><span class="line">   factory.registerCommands(SnapshotCommands.class);</span><br><span class="line">   factory.registerCommands(XAttrCommands.class);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>我们再来看看Ls类</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class Ls extends FsCommand &#123;</span><br><span class="line">  public static void registerCommands(CommandFactory factory) &#123;</span><br><span class="line">    factory.addClass(Ls.class, &quot;-ls&quot;);</span><br><span class="line">    factory.addClass(Lsr.class, &quot;-lsr&quot;);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>也就是，在调用init方法的时候，对这些命令类进行了注册。<br>因此，上面的那个instance，在这里的话，其实就是类Ls的实例。类Ls继承类FsCommand，而类FsCommand是继承类Command,前面instance调用的run方法其实是父类Command的run方法，此方法主要做了两件事，一是处理配置选项，如-d,-R,-h，二是处理参数，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public int run(String...argv) &#123;</span><br><span class="line">    LinkedList&lt;String&gt; args = new LinkedList&lt;String&gt;(Arrays.asList(argv));</span><br><span class="line">    try &#123;</span><br><span class="line">      if (isDeprecated()) &#123;</span><br><span class="line">        displayWarning(</span><br><span class="line">            &quot;DEPRECATED: Please use &apos;&quot;+ getReplacementCommand() + &quot;&apos; instead.&quot;);</span><br><span class="line">      &#125;</span><br><span class="line">      processOptions(args);</span><br><span class="line">      processRawArguments(args);</span><br><span class="line">    &#125; catch (IOException e) &#123;</span><br><span class="line">      displayError(e);</span><br><span class="line">    &#125;</span><br><span class="line">return (numErrors == 0) ? exitCode : exitCodeForError();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>方法processRawArguments的调用层次关系如下：</p><pre><code>\-&gt; processRawArguments(LinkedList)     |-&gt; expandArguments(LinkedList)     |   \-&gt; expandArgument(String)*     \-&gt; processArguments(LinkedList)         |-&gt; processArgument(PathData)*         |   |-&gt; processPathArgument(PathData)         |   \-&gt; processPaths(PathData, PathData...)         |        \-&gt; processPath(PathData)*         \-&gt; processNonexistentPath(PathData)</code></pre><p>从这个层次关系中可以看出，整个方法是先进行展开参数，传入的参数是LinkedList<string>，展开后的参数是LinkedList<pathdata>，PathData类中包含了Path，FileStatus，FileSystem。其实，当程序运行到这里的时候，命令ls的结果就已经可以通过类PathData中的相关方法获取了。</pathdata></string></p><p>展开参数后，开始进行处理参数，此时的参数就是LinkedList<pathdata>,然后循环处理此List，先是判断目录是否存在，是否需要递归查找，是否只是列出本目录（就是看有没有-R和-d参数），我们来看一下到底是如何输出结果的：</pathdata></p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">  protected void processPaths(PathData parent, PathData ... items)</span><br><span class="line">  throws IOException &#123;</span><br><span class="line">    if (parent != null &amp;&amp; !isRecursive() &amp;&amp; items.length != 0) &#123;</span><br><span class="line">      out.println(&quot;Found &quot; + items.length + &quot; items&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    adjustColumnWidths(items); // 计算列宽，重新构建格式字符串</span><br><span class="line">    super.processPaths(parent, items);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>看到这里，大家是不是觉得很面熟？没想起来？我们上个图： </p><p>这下看到了吧，最是输出结果的第一行，找到11项。</p><p>接下来重新调整了一下列宽，最后调用了父类的processPaths方法，我们继续来看父类的这个方法，它做了哪些事：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">protected void processPaths(PathData parent, PathData ... items)</span><br><span class="line">  throws IOException &#123;</span><br><span class="line">    // TODO: this really should be iterative</span><br><span class="line">    for (PathData item : items) &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        processPath(item); // 真正处理每一项，然后打印出来</span><br><span class="line">        if (recursive &amp;&amp; isPathRecursable(item)) &#123;</span><br><span class="line">          recursePath(item); // 如果有指定参数 -R，则需要进行递归</span><br><span class="line">        &#125;</span><br><span class="line">        postProcessPath(item); // 这个没理解，DFS还有后序DFS么？有知情者，请告知，谢谢。</span><br><span class="line">      &#125; catch (IOException e) &#123;</span><br><span class="line">        displayError(e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>我们来看一下打印具体每行信息的代码：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">  protected void processPath(PathData item) throws IOException &#123;</span><br><span class="line">    FileStatus stat = item.stat;</span><br><span class="line">    String line = String.format(lineFormat,</span><br><span class="line">        (stat.isDirectory() ? &quot;d&quot; : &quot;-&quot;),  // 文件夹显示d，文件显示-</span><br><span class="line">        stat.getPermission() + (stat.getPermission().getAclBit() ? &quot;+&quot; : &quot; &quot;), // 获取权限</span><br><span class="line">        (stat.isFile() ? stat.getReplication() : &quot;-&quot;),</span><br><span class="line">        stat.getOwner(), // 获取拥有者</span><br><span class="line">        stat.getGroup(),  // 获取组</span><br><span class="line">        formatSize(stat.getLen()),  // 获取大小</span><br><span class="line">        dateFormat.format(new  Date(stat.getModificationTime())),  // 日期</span><br><span class="line">        item  // 项，即路径</span><br><span class="line">    );</span><br><span class="line">    out.println(line); // 打印行</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>到这里，命令hadoop fs -ls /的执行过程基本已经结束(关于文件系统内部细节，后续再讲），这就是整个命令执行的过程。最后，我们来总结一下：</p><p>执行shell。执行命令hadoop fs -ls /，首先执行的是shell命令，然后转换成执行Java类。<br>执行Java。在执行Java类的时候，使用工具类对其进行配置项解析，并使用反射机制对命令进行了转换，于是后面变成了调用类Ls的run方法。<br>调用类Ls的相关方法。类Ls负责处理路径，并打印详情。</p><p>原文链接：<a href="https://blog.csdn.net/strongyoung88/article/details/68952248" target="_blank" rel="noopener">https://blog.csdn.net/strongyoung88/article/details/68952248</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（二十三）MapReduce中的shuffle详解</title>
      <link href="/2018-04-23-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%89%EF%BC%89MapReduce%E4%B8%AD%E7%9A%84shuffle%E8%AF%A6%E8%A7%A3.html"/>
      <url>/2018-04-23-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%89%EF%BC%89MapReduce%E4%B8%AD%E7%9A%84shuffle%E8%AF%A6%E8%A7%A3.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（二十三）MapReduce中的shuffle详解：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（二十三）MapReduce中的shuffle详解</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>1、MapReduce 中，mapper 阶段处理的数据如何传递给 reducer 阶段，是 MapReduce 框架中 最关键的一个流程，这个流程就叫 Shuffle</p><p>2、Shuffle: 数据混洗 ——（核心机制：数据分区，排序，局部聚合，缓存，拉取，再合并 排序）</p><p>3、具体来说：就是将 MapTask 输出的处理结果数据，按照 Partitioner 组件制定的规则分发 给 ReduceTask，并在分发的过程中，对数据按 key 进行了分区和排序</p><p><a href="https://www.cnblogs.com/qingyunzong/p/8615024.html#_labelTop" target="_blank" rel="noopener">回到顶部</a></p><h2 id="MapReduce的Shuffle过程介绍"><a href="#MapReduce的Shuffle过程介绍" class="headerlink" title="MapReduce的Shuffle过程介绍"></a>MapReduce的Shuffle过程介绍</h2><p>Shuffle的本义是洗牌、混洗，把一组有一定规则的数据尽量转换成一组无规则的数据，越随机越好。MapReduce中的Shuffle更像是洗牌的逆过程，把一组无规则的数据尽量转换成一组具有一定规则的数据。</p><p>为什么MapReduce计算模型需要Shuffle过程？我们都知道MapReduce计算模型一般包括两个重要的阶段：Map是映射，负责数据的过滤分发；Reduce是规约，负责数据的计算归并。Reduce的数据来源于Map，Map的输出即是Reduce的输入，Reduce需要通过Shuffle来获取数据。</p><p>从Map输出到Reduce输入的整个过程可以广义地称为Shuffle。Shuffle横跨Map端和Reduce端，在Map端包括Spill过程，在Reduce端包括copy和sort过程，如图所示：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180321083504916-1942630366.png" alt="img"></p><h3 id="Spill过程"><a href="#Spill过程" class="headerlink" title="Spill过程"></a>Spill过程</h3><p>Spill过程包括输出、排序、溢写、合并等步骤，如图所示：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180321083605120-770489646.png" alt="img"></p><p>Collect</p><p>每个Map任务不断地以对的形式把数据输出到在内存中构造的一个环形数据结构中。使用环形数据结构是为了更有效地使用内存空间，在内存中放置尽可能多的数据。</p><p>这个数据结构其实就是个字节数组，叫Kvbuffer，名如其义，但是这里面不光放置了数据，还放置了一些索引数据，给放置索引数据的区域起了一个Kvmeta的别名，在Kvbuffer的一块区域上穿了一个IntBuffer（字节序采用的是平台自身的字节序）的马甲。数据区域和索引数据区域在Kvbuffer中是相邻不重叠的两个区域，用一个分界点来划分两者，分界点不是亘古不变的，而是每次Spill之后都会更新一次。初始的分界点是0，数据的存储方向是向上增长，索引数据的存储方向是向下增长，如图所示：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180321083814970-49385953.png" alt="img"></p><p>Kvbuffer的存放指针bufindex是一直闷着头地向上增长，比如bufindex初始值为0，一个Int型的key写完之后，bufindex增长为4，一个Int型的value写完之后，bufindex增长为8。</p><p>索引是对在kvbuffer中的索引，是个四元组，包括：value的起始位置、key的起始位置、partition值、value的长度，占用四个Int长度，Kvmeta的存放指针Kvindex每次都是向下跳四个“格子”，然后再向上一个格子一个格子地填充四元组的数据。比如Kvindex初始位置是-4，当第一个写完之后，(Kvindex+0)的位置存放value的起始位置、(Kvindex+1)的位置存放key的起始位置、(Kvindex+2)的位置存放partition的值、(Kvindex+3)的位置存放value的长度，然后Kvindex跳到-8位置，等第二个和索引写完之后，Kvindex跳到-32位置。</p><p>Kvbuffer的大小虽然可以通过参数设置，但是总共就那么大，和索引不断地增加，加着加着，Kvbuffer总有不够用的那天，那怎么办？把数据从内存刷到磁盘上再接着往内存写数据，把Kvbuffer中的数据刷到磁盘上的过程就叫Spill，多么明了的叫法，内存中的数据满了就自动地spill到具有更大空间的磁盘。</p><p>关于Spill触发的条件，也就是Kvbuffer用到什么程度开始Spill，还是要讲究一下的。如果把Kvbuffer用得死死得，一点缝都不剩的时候再开始Spill，那Map任务就需要等Spill完成腾出空间之后才能继续写数据；如果Kvbuffer只是满到一定程度，比如80%的时候就开始Spill，那在Spill的同时，Map任务还能继续写数据，如果Spill够快，Map可能都不需要为空闲空间而发愁。两利相衡取其大，一般选择后者。</p><p>Spill这个重要的过程是由Spill线程承担，Spill线程从Map任务接到“命令”之后就开始正式干活，干的活叫SortAndSpill，原来不仅仅是Spill，在Spill之前还有个颇具争议性的Sort。</p><h3 id="Sort"><a href="#Sort" class="headerlink" title="Sort"></a>Sort</h3><p>先把Kvbuffer中的数据按照partition值和key两个关键字升序排序，移动的只是索引数据，排序结果是Kvmeta中数据按照partition为单位聚集在一起，同一partition内的按照key有序。</p><h3 id="Spill"><a href="#Spill" class="headerlink" title="Spill"></a>Spill</h3><p>Spill线程为这次Spill过程创建一个磁盘文件：从所有的本地目录中轮训查找能存储这么大空间的目录，找到之后在其中创建一个类似于“spill12.out”的文件。Spill线程根据排过序的Kvmeta挨个partition的把数据吐到这个文件中，一个partition对应的数据吐完之后顺序地吐下个partition，直到把所有的partition遍历完。一个partition在文件中对应的数据也叫段(segment)。</p><p>所有的partition对应的数据都放在这个文件里，虽然是顺序存放的，但是怎么直接知道某个partition在这个文件中存放的起始位置呢？强大的索引又出场了。有一个三元组记录某个partition对应的数据在这个文件中的索引：起始位置、原始数据长度、压缩之后的数据长度，一个partition对应一个三元组。然后把这些索引信息存放在内存中，如果内存中放不下了，后续的索引信息就需要写到磁盘文件中了：从所有的本地目录中轮训查找能存储这么大空间的目录，找到之后在其中创建一个类似于“spill12.out.index”的文件，文件中不光存储了索引数据，还存储了crc32的校验数据。(spill12.out.index不一定在磁盘上创建，如果内存（默认1M空间）中能放得下就放在内存中，即使在磁盘上创建了，和spill12.out文件也不一定在同一个目录下。)</p><p>每一次Spill过程就会最少生成一个out文件，有时还会生成index文件，Spill的次数也烙印在文件名中。索引文件和数据文件的对应关系如下图所示：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180321083927742-1030906351.png" alt="img"></p><p>在Spill线程如火如荼的进行SortAndSpill工作的同时，Map任务不会因此而停歇，而是一无既往地进行着数据输出。Map还是把数据写到kvbuffer中，那问题就来了：只顾着闷头按照bufindex指针向上增长，kvmeta只顾着按照Kvindex向下增长，是保持指针起始位置不变继续跑呢，还是另谋它路？如果保持指针起始位置不变，很快bufindex和Kvindex就碰头了，碰头之后再重新开始或者移动内存都比较麻烦，不可取。Map取kvbuffer中剩余空间的中间位置，用这个位置设置为新的分界点，bufindex指针移动到这个分界点，Kvindex移动到这个分界点的-16位置，然后两者就可以和谐地按照自己既定的轨迹放置数据了，当Spill完成，空间腾出之后，不需要做任何改动继续前进。分界点的转换如下图所示：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180321083949570-945173243.png" alt="img"></p><p>Map任务总要把输出的数据写到磁盘上，即使输出数据量很小在内存中全部能装得下，在最后也会把数据刷到磁盘上。</p><h3 id="Merge"><a href="#Merge" class="headerlink" title="Merge"></a><em>Merge</em></h3><p>Map任务如果输出数据量很大，可能会进行好几次Spill，out文件和Index文件会产生很多，分布在不同的磁盘上。最后把这些文件进行合并的merge过程闪亮登场。</p><p>Merge过程怎么知道产生的Spill文件都在哪了呢？从所有的本地目录上扫描得到产生的Spill文件，然后把路径存储在一个数组里。Merge过程又怎么知道Spill的索引信息呢？没错，也是从所有的本地目录上扫描得到Index文件，然后把索引信息存储在一个列表里。到这里，又遇到了一个值得纳闷的地方。在之前Spill过程中的时候为什么不直接把这些信息存储在内存中呢，何必又多了这步扫描的操作？特别是Spill的索引数据，之前当内存超限之后就把数据写到磁盘，现在又要从磁盘把这些数据读出来，还是需要装到更多的内存中。之所以多此一举，是因为这时kvbuffer这个内存大户已经不再使用可以回收，有内存空间来装这些数据了。（对于内存空间较大的土豪来说，用内存来省却这两个io步骤还是值得考虑的。）</p><p>然后为merge过程创建一个叫file.out的文件和一个叫file.out.Index的文件用来存储最终的输出和索引。</p><p>一个partition一个partition的进行合并输出。对于某个partition来说，从索引列表中查询这个partition对应的所有索引信息，每个对应一个段插入到段列表中。也就是这个partition对应一个段列表，记录所有的Spill文件中对应的这个partition那段数据的文件名、起始位置、长度等等。</p><p>然后对这个partition对应的所有的segment进行合并，目标是合并成一个segment。当这个partition对应很多个segment时，会分批地进行合并：先从segment列表中把第一批取出来，以key为关键字放置成最小堆，然后从最小堆中每次取出最小的输出到一个临时文件中，这样就把这一批段合并成一个临时的段，把它加回到segment列表中；再从segment列表中把第二批取出来合并输出到一个临时segment，把其加入到列表中；这样往复执行，直到剩下的段是一批，输出到最终的文件中。</p><p>最终的索引数据仍然输出到Index文件中。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180321084015633-2083307488.png" alt="img"></p><p>Map端的Shuffle过程到此结束。</p><h3 id="Copy"><a href="#Copy" class="headerlink" title="Copy"></a>Copy</h3><p>Reduce任务通过HTTP向各个Map任务拖取它所需要的数据。每个节点都会启动一个常驻的HTTP server，其中一项服务就是响应Reduce拖取Map数据。当有MapOutput的HTTP请求过来的时候，HTTP server就读取相应的Map输出文件中对应这个Reduce部分的数据通过网络流输出给Reduce。</p><p>Reduce任务拖取某个Map对应的数据，如果在内存中能放得下这次数据的话就直接把数据写到内存中。Reduce要向每个Map去拖取数据，在内存中每个Map对应一块数据，当内存中存储的Map数据占用空间达到一定程度的时候，开始启动内存中merge，把内存中的数据merge输出到磁盘上一个文件中。</p><p>如果在内存中不能放得下这个Map的数据的话，直接把Map数据写到磁盘上，在本地目录创建一个文件，从HTTP流中读取数据然后写到磁盘，使用的缓存区大小是64K。拖一个Map数据过来就会创建一个文件，当文件数量达到一定阈值时，开始启动磁盘文件merge，把这些文件合并输出到一个文件。</p><p>有些Map的数据较小是可以放在内存中的，有些Map的数据较大需要放在磁盘上，这样最后Reduce任务拖过来的数据有些放在内存中了有些放在磁盘上，最后会对这些来一个全局合并。</p><h3 id="Merge-Sort"><a href="#Merge-Sort" class="headerlink" title="Merge Sort"></a><em>Merge Sort</em></h3><p>这里使用的Merge和Map端使用的Merge过程一样。Map的输出数据已经是有序的，Merge进行一次合并排序，所谓Reduce端的sort过程就是这个合并的过程。一般Reduce是一边copy一边sort，即copy和sort两个阶段是重叠而不是完全分开的。</p><p>Reduce端的Shuffle过程至此结束。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（二十二）MapReduce的输入和输出</title>
      <link href="/2018-04-22-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E4%BA%8C%EF%BC%89MapReduce%E7%9A%84%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA.html"/>
      <url>/2018-04-22-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E4%BA%8C%EF%BC%89MapReduce%E7%9A%84%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（二十二）MapReduce的输入和输出：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（二十二）MapReduce的输入和输出</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>MapReduce的输入</p><p>作为一个会编写MR程序的人来说，知道map方法的参数是默认的数据读取组件读取到的一行数据</p><p>1、是谁在读取？ 是谁在调用这个map方法?</p><p>查看源码Mapper.java知道是run方法在调用map方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"> 1 /**</span><br><span class="line"> 2      * </span><br><span class="line"> 3      * 找出谁在调用Run方法</span><br><span class="line"> 4      * </span><br><span class="line"> 5      * </span><br><span class="line"> 6      * 有一个组件叫做：MapTask</span><br><span class="line"> 7      * </span><br><span class="line"> 8      * 就会有对应的方法在调用mapper.run(context);</span><br><span class="line"> 9      * </span><br><span class="line">10      * </span><br><span class="line">11      * context.nextKeyValue() ====== lineRecordReader.nextKeyValue();</span><br><span class="line">12      */</span><br><span class="line">13     public void run(Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">14 </span><br><span class="line">15         /**</span><br><span class="line">16          * 在每一个mapTask被初始化出来的时候，就会被调用一次</span><br><span class="line">17          */</span><br><span class="line">18         setup(context);</span><br><span class="line">19         try &#123;</span><br><span class="line">20 </span><br><span class="line">21             /**</span><br><span class="line">22              * 数据读取组件每次读取到一行，都交给map方法执行一次</span><br><span class="line">23              * </span><br><span class="line">24              * </span><br><span class="line">25              * context.nextKeyValue()的意义有连点：</span><br><span class="line">26              * </span><br><span class="line">27              * 1、读取一个key-value到该context对象中的两个属性中：key-value</span><br><span class="line">28              * 2、方法的返回值并不是读取到的key-value，是标志有没有读取到key_value的布尔值</span><br><span class="line">29              * </span><br><span class="line">30              * </span><br><span class="line">31              * context.getCurrentKey() ==== key</span><br><span class="line">32              * context.getCurrentValue() ==== value</span><br><span class="line">33              * </span><br><span class="line">34              * </span><br><span class="line">35              * </span><br><span class="line">36              * 依赖于最底层的 LineRecordReader的实现</span><br><span class="line">37              * </span><br><span class="line">38              * 你的nextKeyValue方法的返回结果中，一定要包含 false</span><br><span class="line">39              */</span><br><span class="line">40             while (context.nextKeyValue()) &#123;</span><br><span class="line">41                 map(context.getCurrentKey(), context.getCurrentValue(), context);</span><br><span class="line">42             &#125;</span><br><span class="line">43 </span><br><span class="line">44         &#125; finally &#123;</span><br><span class="line">45 </span><br><span class="line">46             /**</span><br><span class="line">47              * 当前这个mapTask在执行完毕所有的该切片数据之后，会执行</span><br><span class="line">48              */</span><br><span class="line">49             cleanup(context);</span><br><span class="line">50         &#125;</span><br><span class="line">51     &#125;</span><br></pre></td></tr></table></figure><p>此处map方法中有四个重要的方法：</p><blockquote><p>1、context.nextKeyValue(); //负责读取数据，但是方法的返回值却不是读取到的key-value，而是返回了一个标识有没有读取到数据的布尔值</p><p>2、context.getCurrentKey(); //负责获取context.nextKeyValue() 读取到的key</p><p>3、context.getCurrentValue(); //负责获取context.nextKeyValue() 读取到的value</p><p>4、context.write(key,value); //负责输出mapper阶段输出的数据</p></blockquote><p>2、谁在调用run方法？context参数怎么来的，是什么？</p><p>共同答案：找到了谁在调用run方法，那么就能知道这个谁就会给run方法传入一个参数叫做：context</p><p>最开始，mapper.run(context)是由mapTask实例对象进行调用</p><p>查看源码MapTask.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"> 1 @Override</span><br><span class="line"> 2     public void run(final JobConf job, final TaskUmbilicalProtocol umbilical)</span><br><span class="line"> 3             throws IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line"> 4         this.umbilical = umbilical;</span><br><span class="line"> 5 </span><br><span class="line"> 6         if (isMapTask()) &#123;</span><br><span class="line"> 7             // If there are no reducers then there won&apos;t be any sort. Hence the</span><br><span class="line"> 8             // map</span><br><span class="line"> 9             // phase will govern the entire attempt&apos;s progress.</span><br><span class="line">10             if (conf.getNumReduceTasks() == 0) &#123;</span><br><span class="line">11                 mapPhase = getProgress().addPhase(&quot;map&quot;, 1.0f);</span><br><span class="line">12             &#125; else &#123;</span><br><span class="line">13                 // If there are reducers then the entire attempt&apos;s progress will</span><br><span class="line">14                 // be</span><br><span class="line">15                 // split between the map phase (67%) and the sort phase (33%).</span><br><span class="line">16                 mapPhase = getProgress().addPhase(&quot;map&quot;, 0.667f);</span><br><span class="line">17                 sortPhase = getProgress().addPhase(&quot;sort&quot;, 0.333f);</span><br><span class="line">18             &#125;</span><br><span class="line">19         &#125;</span><br><span class="line">20         TaskReporter reporter = startReporter(umbilical);</span><br><span class="line">21 </span><br><span class="line">22         boolean useNewApi = job.getUseNewMapper();</span><br><span class="line">23         initialize(job, getJobID(), reporter, useNewApi);</span><br><span class="line">24 </span><br><span class="line">25         // check if it is a cleanupJobTask</span><br><span class="line">26         if (jobCleanup) &#123;</span><br><span class="line">27             runJobCleanupTask(umbilical, reporter);</span><br><span class="line">28             return;</span><br><span class="line">29         &#125;</span><br><span class="line">30         if (jobSetup) &#123;</span><br><span class="line">31             runJobSetupTask(umbilical, reporter);</span><br><span class="line">32             return;</span><br><span class="line">33         &#125;</span><br><span class="line">34         if (taskCleanup) &#123;</span><br><span class="line">35             runTaskCleanupTask(umbilical, reporter);</span><br><span class="line">36             return;</span><br><span class="line">37         &#125;</span><br><span class="line">38 </span><br><span class="line">39         /**</span><br><span class="line">40          * run方法的核心：</span><br><span class="line">41          * </span><br><span class="line">42          * 新的API</span><br><span class="line">43          */</span><br><span class="line">44 </span><br><span class="line">45         if (useNewApi) &#123;</span><br><span class="line">46             /**</span><br><span class="line">47              * jobConf对象， splitMetaInfo 切片信息 umbilical 通信协议</span><br><span class="line">48              * reporter就是包含了各种计数器的一个对象</span><br><span class="line">49              */</span><br><span class="line">50             runNewMapper(job, splitMetaInfo, umbilical, reporter);</span><br><span class="line">51         &#125; else &#123;</span><br><span class="line">52             runOldMapper(job, splitMetaInfo, umbilical, reporter);</span><br><span class="line">53         &#125;</span><br><span class="line">54 </span><br><span class="line">55         done(umbilical, reporter);</span><br><span class="line">56     &#125;</span><br></pre></td></tr></table></figure><p>得出伪代码调动新的API</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1　　　　　　　 mapTask.run()&#123;</span><br><span class="line">2                 runNewMapper()&#123;</span><br><span class="line">3                     mapper.run(mapperContext);</span><br><span class="line">4                 &#125;</span><br><span class="line">5             &#125;</span><br></pre></td></tr></table></figure><p>3、查看runNewMapper方法</p><p>发现此方法还是在MapTask.java中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br></pre></td><td class="code"><pre><span class="line">  1 /**</span><br><span class="line">  2      * 这就是具体的调用逻辑的核心；</span><br><span class="line">  3      * </span><br><span class="line">  4      * </span><br><span class="line">  5      * mapper.run(context);</span><br><span class="line">  6      * </span><br><span class="line">  7      * </span><br><span class="line">  8      * </span><br><span class="line">  9      * @param job</span><br><span class="line"> 10      * @param splitIndex</span><br><span class="line"> 11      * @param umbilical</span><br><span class="line"> 12      * @param reporter</span><br><span class="line"> 13      * @throws IOException</span><br><span class="line"> 14      * @throws ClassNotFoundException</span><br><span class="line"> 15      * @throws InterruptedException</span><br><span class="line"> 16      */</span><br><span class="line"> 17     @SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line"> 18     private &lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt; void runNewMapper(final JobConf job, final TaskSplitIndex splitIndex,</span><br><span class="line"> 19             final TaskUmbilicalProtocol umbilical, TaskReporter reporter)</span><br><span class="line"> 20             throws IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line"> 21         // make a task context so we can get the classes</span><br><span class="line"> 22         org.apache.hadoop.mapreduce.TaskAttemptContext taskContext = new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(</span><br><span class="line"> 23                 job, getTaskID(), reporter);</span><br><span class="line"> 24         // make a mapper</span><br><span class="line"> 25         org.apache.hadoop.mapreduce.Mapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt; mapper = (org.apache.hadoop.mapreduce.Mapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;) ReflectionUtils</span><br><span class="line"> 26                 .newInstance(taskContext.getMapperClass(), job);</span><br><span class="line"> 27         </span><br><span class="line"> 28         </span><br><span class="line"> 29         </span><br><span class="line"> 30         </span><br><span class="line"> 31         /**</span><br><span class="line"> 32          * inputFormat.createRecordReader() === RecordReader real</span><br><span class="line"> 33          * </span><br><span class="line"> 34          * </span><br><span class="line"> 35          * inputFormat就是TextInputFormat类的实例对象</span><br><span class="line"> 36          * </span><br><span class="line"> 37          * TextInputFormat组件中的createRecordReader方法的返回值就是  LineRecordReader的实例对象</span><br><span class="line"> 38          */</span><br><span class="line"> 39         // make the input format</span><br><span class="line"> 40         org.apache.hadoop.mapreduce.InputFormat&lt;INKEY, INVALUE&gt; inputFormat = </span><br><span class="line"> 41                 (org.apache.hadoop.mapreduce.InputFormat&lt;INKEY, INVALUE&gt;) </span><br><span class="line"> 42                 ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);</span><br><span class="line"> 43         </span><br><span class="line"> 44         </span><br><span class="line"> 45         </span><br><span class="line"> 46         </span><br><span class="line"> 47         </span><br><span class="line"> 48         // rebuild the input split</span><br><span class="line"> 49         org.apache.hadoop.mapreduce.InputSplit split = null;</span><br><span class="line"> 50         split = getSplitDetails(new Path(splitIndex.getSplitLocation()), splitIndex.getStartOffset());</span><br><span class="line"> 51         LOG.info(&quot;Processing split: &quot; + split);</span><br><span class="line"> 52 </span><br><span class="line"> 53         /**</span><br><span class="line"> 54          * NewTrackingRecordReader这个类中一定有三个方法：</span><br><span class="line"> 55          * </span><br><span class="line"> 56          * nextKeyValue</span><br><span class="line"> 57          * getCurrentKey</span><br><span class="line"> 58          * getCurrentValue</span><br><span class="line"> 59          * </span><br><span class="line"> 60          * NewTrackingRecordReader的里面的三个方法的实现</span><br><span class="line"> 61          * 其实是依赖于于inputFormat对象的createRecordReader方法的返回值的  三个方法的实现</span><br><span class="line"> 62          * </span><br><span class="line"> 63          * 默认的InputFormat： TextInputFormat</span><br><span class="line"> 64          * 默认的RecordReader：LineRecordReader</span><br><span class="line"> 65          * </span><br><span class="line"> 66          * </span><br><span class="line"> 67          * 最终：NewTrackingRecordReader的三个方法的实现是依赖于：LineRecordReader这个类中的三个同名方法的实现</span><br><span class="line"> 68          */</span><br><span class="line"> 69         org.apache.hadoop.mapreduce.RecordReader&lt;INKEY, INVALUE&gt; input = </span><br><span class="line"> 70                 new NewTrackingRecordReader&lt;INKEY, INVALUE&gt;(</span><br><span class="line"> 71                 split, inputFormat, reporter, taskContext);</span><br><span class="line"> 72 </span><br><span class="line"> 73         job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());</span><br><span class="line"> 74         </span><br><span class="line"> 75         </span><br><span class="line"> 76         </span><br><span class="line"> 77         </span><br><span class="line"> 78         </span><br><span class="line"> 79         /**</span><br><span class="line"> 80          * 声明一个Output对象用来给mapper的key-value进行输出</span><br><span class="line"> 81          */</span><br><span class="line"> 82         org.apache.hadoop.mapreduce.RecordWriter output = null;</span><br><span class="line"> 83         // get an output object</span><br><span class="line"> 84         if (job.getNumReduceTasks() == 0) &#123;</span><br><span class="line"> 85             </span><br><span class="line"> 86             /**</span><br><span class="line"> 87              * NewDirectOutputCollector  直接输出的一个收集器，  这个类中一定有一个方法 叫做  write</span><br><span class="line"> 88              */</span><br><span class="line"> 89             output = new NewDirectOutputCollector(taskContext, job, umbilical, reporter);</span><br><span class="line"> 90         &#125; else &#123;</span><br><span class="line"> 91             </span><br><span class="line"> 92             </span><br><span class="line"> 93             /**</span><br><span class="line"> 94              * 有reducer阶段了。</span><br><span class="line"> 95              * </span><br><span class="line"> 96              *         1、能确定，一定会排序</span><br><span class="line"> 97              * </span><br><span class="line"> 98              *         2、能否确定一定会使用Parititioner,  不一定。     在逻辑上可以任务没有起作用。</span><br><span class="line"> 99              * </span><br><span class="line">100              * NewOutputCollector 这个类当中，一定有一个方法：write方法</span><br><span class="line">101              */</span><br><span class="line">102             output = new NewOutputCollector(taskContext, job, umbilical, reporter);</span><br><span class="line">103         &#125;</span><br><span class="line">104         </span><br><span class="line">105         </span><br><span class="line">106         </span><br><span class="line">107         </span><br><span class="line">108 </span><br><span class="line">109         /**</span><br><span class="line">110          *  mapContext对象中一定包含三个方法</span><br><span class="line">111          *  </span><br><span class="line">112          *  找到了之前第一查看源码实现的方法的问题的答案：</span><br><span class="line">113          *  </span><br><span class="line">114          *      问题：找到谁调用MapContextImpl这个类的构造方法</span><br><span class="line">115          *  </span><br><span class="line">116          *      mapContext就是MapContextImpl的实例对象</span><br><span class="line">117          *      </span><br><span class="line">118          *      MapContextImpl类中一定有三个方法：</span><br><span class="line">119          *      </span><br><span class="line">120          *      input  ===  NewTrackingRecordReader</span><br><span class="line">121          *      </span><br><span class="line">122          *      </span><br><span class="line">123          *      </span><br><span class="line">124          *      确定的知识：</span><br><span class="line">125          *      </span><br><span class="line">126          *      1、mapContext对象中，一定有write方法</span><br><span class="line">127          *      </span><br><span class="line">128          *      2、通过观看MapContextImpl的组成，发现其实没有write方法</span><br><span class="line">129          *      </span><br><span class="line">130          *      解决：</span><br><span class="line">131          *      </span><br><span class="line">132          *      其实mapContext.write方法的调用是来自于MapContextImpl这个类的父类</span><br><span class="line">133          *      </span><br><span class="line">134          *      </span><br><span class="line">135          *      </span><br><span class="line">136          *      最底层的write方法：  output.write();</span><br><span class="line">137          */</span><br><span class="line">138         org.apache.hadoop.mapreduce.MapContext&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt; mapContext = </span><br><span class="line">139                 new MapContextImpl&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;(</span><br><span class="line">140                 job, getTaskID(), input, output, committer, reporter, split);</span><br><span class="line">141 </span><br><span class="line">142         /**</span><br><span class="line">143          * mapperContext的内部一定包含是三个犯法：</span><br><span class="line">144          * </span><br><span class="line">145          * nextKeyValue</span><br><span class="line">146          * getCurrentKey</span><br><span class="line">147          * getCurrentValue</span><br><span class="line">148          * </span><br><span class="line">149          * mapperContext的具体实现是依赖于new Context(context);</span><br><span class="line">150          * context = mapContext</span><br><span class="line">151          * </span><br><span class="line">152          * 结论：</span><br><span class="line">153          * </span><br><span class="line">154          * mapContext对象的内部一定包含以下三个方法：</span><br><span class="line">155          * </span><br><span class="line">156          * nextKeyValue</span><br><span class="line">157          * getCurrentKey</span><br><span class="line">158          * getCurrentValue</span><br><span class="line">159          * </span><br><span class="line">160          * </span><br><span class="line">161          * mapContext 中 也有一个方法叫做：write(key,value)</span><br><span class="line">162          */</span><br><span class="line">163         org.apache.hadoop.mapreduce.Mapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;.Context mapperContext = </span><br><span class="line">164                 new WrappedMapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;()</span><br><span class="line">165                 .getMapContext(mapContext);</span><br><span class="line">166 </span><br><span class="line">167         try &#123;</span><br><span class="line">168             </span><br><span class="line">169             </span><br><span class="line">170             </span><br><span class="line">171             </span><br><span class="line">172             input.initialize(split, mapperContext);</span><br><span class="line">173             </span><br><span class="line">174             </span><br><span class="line">175             </span><br><span class="line">176             /**</span><br><span class="line">177              * 复杂调用整个mapTask执行的入口</span><br><span class="line">178              * </span><br><span class="line">179              * 方法的逻辑构成：</span><br><span class="line">180              * </span><br><span class="line">181              *     1、重点方法在最后，或者在try中</span><br><span class="line">182              *  2、其他的代码，几乎只有两个任务：一个用来记录记日志或者完善流程。。 一个准备核心方法的参数</span><br><span class="line">183              */</span><br><span class="line">184             mapper.run(mapperContext);</span><br><span class="line">185             </span><br><span class="line">186             </span><br><span class="line">187             </span><br><span class="line">188             mapPhase.complete();</span><br><span class="line">189             setPhase(TaskStatus.Phase.SORT);</span><br><span class="line">190             statusUpdate(umbilical);</span><br><span class="line">191             input.close();</span><br><span class="line">192             input = null;</span><br><span class="line">193             output.close(mapperContext);</span><br><span class="line">194             output = null;</span><br><span class="line">195             </span><br><span class="line">196             </span><br><span class="line">197             </span><br><span class="line">198         &#125; finally &#123;</span><br><span class="line">199             closeQuietly(input);</span><br><span class="line">200             closeQuietly(output, mapperContext);</span><br><span class="line">201         &#125;</span><br><span class="line">202     &#125;</span><br></pre></td></tr></table></figure><p>能确定的是：mapperContext一定有上面说的那四个重要的方法，往上继续查找mapperContext</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"> /**</span><br><span class="line">143          * mapperContext的内部一定包含是三个犯法：</span><br><span class="line">144          * </span><br><span class="line">145          * nextKeyValue</span><br><span class="line">146          * getCurrentKey</span><br><span class="line">147          * getCurrentValue</span><br><span class="line">148          * </span><br><span class="line">149          * mapperContext的具体实现是依赖于new Context(context);</span><br><span class="line">150          * context = mapContext</span><br><span class="line">151          * </span><br><span class="line">152          * 结论：</span><br><span class="line">153          * </span><br><span class="line">154          * mapContext对象的内部一定包含以下三个方法：</span><br><span class="line">155          * </span><br><span class="line">156          * nextKeyValue</span><br><span class="line">157          * getCurrentKey</span><br><span class="line">158          * getCurrentValue</span><br><span class="line">159          * </span><br><span class="line">160          * </span><br><span class="line">161          * mapContext 中 也有一个方法叫做：write(key,value)</span><br><span class="line">162          */</span><br><span class="line">163         org.apache.hadoop.mapreduce.Mapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;.Context mapperContext = </span><br><span class="line">164                 new WrappedMapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;()</span><br><span class="line">165                 .getMapContext(mapContext);</span><br></pre></td></tr></table></figure><p>查看<strong>WrappedMapper.java</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br></pre></td><td class="code"><pre><span class="line">1 /**</span><br><span class="line">  2  * Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line">  3  * or more contributor license agreements.  See the NOTICE file</span><br><span class="line">  4  * distributed with this work for additional information</span><br><span class="line">  5  * regarding copyright ownership.  The ASF licenses this file</span><br><span class="line">  6  * to you under the Apache License, Version 2.0 (the</span><br><span class="line">  7  * &quot;License&quot;); you may not use this file except in compliance</span><br><span class="line">  8  * with the License.  You may obtain a copy of the License at</span><br><span class="line">  9  *</span><br><span class="line"> 10  *     http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"> 11  *</span><br><span class="line"> 12  * Unless required by applicable law or agreed to in writing, software</span><br><span class="line"> 13  * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"> 14  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"> 15  * See the License for the specific language governing permissions and</span><br><span class="line"> 16  * limitations under the License.</span><br><span class="line"> 17  */</span><br><span class="line"> 18 </span><br><span class="line"> 19 package org.apache.hadoop.mapreduce.lib.map;</span><br><span class="line"> 20 </span><br><span class="line"> 21 import java.io.IOException;</span><br><span class="line"> 22 import java.net.URI;</span><br><span class="line"> 23 </span><br><span class="line"> 24 import org.apache.hadoop.classification.InterfaceAudience;</span><br><span class="line"> 25 import org.apache.hadoop.classification.InterfaceStability;</span><br><span class="line"> 26 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 27 import org.apache.hadoop.conf.Configuration.IntegerRanges;</span><br><span class="line"> 28 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 29 import org.apache.hadoop.io.RawComparator;</span><br><span class="line"> 30 import org.apache.hadoop.mapreduce.Counter;</span><br><span class="line"> 31 import org.apache.hadoop.mapreduce.InputFormat;</span><br><span class="line"> 32 import org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"> 33 import org.apache.hadoop.mapreduce.JobID;</span><br><span class="line"> 34 import org.apache.hadoop.mapreduce.MapContext;</span><br><span class="line"> 35 import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"> 36 import org.apache.hadoop.mapreduce.OutputCommitter;</span><br><span class="line"> 37 import org.apache.hadoop.mapreduce.OutputFormat;</span><br><span class="line"> 38 import org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"> 39 import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"> 40 import org.apache.hadoop.mapreduce.TaskAttemptID;</span><br><span class="line"> 41 import org.apache.hadoop.security.Credentials;</span><br><span class="line"> 42 </span><br><span class="line"> 43 /**</span><br><span class="line"> 44  * A &#123;@link Mapper&#125; which wraps a given one to allow custom</span><br><span class="line"> 45  * &#123;@link Mapper.Context&#125; implementations.</span><br><span class="line"> 46  */</span><br><span class="line"> 47 @InterfaceAudience.Public</span><br><span class="line"> 48 @InterfaceStability.Evolving</span><br><span class="line"> 49 public class WrappedMapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; extends Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; &#123;</span><br><span class="line"> 50 </span><br><span class="line"> 51     /**</span><br><span class="line"> 52      * Get a wrapped &#123;@link Mapper.Context&#125; for custom implementations.</span><br><span class="line"> 53      * </span><br><span class="line"> 54      * @param mapContext</span><br><span class="line"> 55      *            &lt;code&gt;MapContext&lt;/code&gt; to be wrapped</span><br><span class="line"> 56      * @return a wrapped &lt;code&gt;Mapper.Context&lt;/code&gt; for custom implementations</span><br><span class="line"> 57      */</span><br><span class="line"> 58     public Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt;.Context getMapContext(</span><br><span class="line"> 59             MapContext&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; mapContext) &#123;</span><br><span class="line"> 60         return new Context(mapContext);</span><br><span class="line"> 61     &#125;</span><br><span class="line"> 62 </span><br><span class="line"> 63     @InterfaceStability.Evolving</span><br><span class="line"> 64     public class Context extends Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt;.Context &#123;</span><br><span class="line"> 65 </span><br><span class="line"> 66         protected MapContext&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; mapContext;</span><br><span class="line"> 67 </span><br><span class="line"> 68         public Context(MapContext&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; mapContext) &#123;</span><br><span class="line"> 69             this.mapContext = mapContext;</span><br><span class="line"> 70         &#125;</span><br><span class="line"> 71 </span><br><span class="line"> 72         /**</span><br><span class="line"> 73          * Get the input split for this map.</span><br><span class="line"> 74          */</span><br><span class="line"> 75         public InputSplit getInputSplit() &#123;</span><br><span class="line"> 76             return mapContext.getInputSplit();</span><br><span class="line"> 77         &#125;</span><br><span class="line"> 78 </span><br><span class="line"> 79         @Override</span><br><span class="line"> 80         public KEYIN getCurrentKey() throws IOException, InterruptedException &#123;</span><br><span class="line"> 81             return mapContext.getCurrentKey();</span><br><span class="line"> 82         &#125;</span><br><span class="line"> 83 </span><br><span class="line"> 84         @Override</span><br><span class="line"> 85         public VALUEIN getCurrentValue() throws IOException, InterruptedException &#123;</span><br><span class="line"> 86             return mapContext.getCurrentValue();</span><br><span class="line"> 87         &#125;</span><br><span class="line"> 88 </span><br><span class="line"> 89         @Override</span><br><span class="line"> 90         public boolean nextKeyValue() throws IOException, InterruptedException &#123;</span><br><span class="line"> 91             return mapContext.nextKeyValue();</span><br><span class="line"> 92         &#125;</span><br><span class="line"> 93 </span><br><span class="line"> 94         @Override</span><br><span class="line"> 95         public Counter getCounter(Enum&lt;?&gt; counterName) &#123;</span><br><span class="line"> 96             return mapContext.getCounter(counterName);</span><br><span class="line"> 97         &#125;</span><br><span class="line"> 98 </span><br><span class="line"> 99         @Override</span><br><span class="line">100         public Counter getCounter(String groupName, String counterName) &#123;</span><br><span class="line">101             return mapContext.getCounter(groupName, counterName);</span><br><span class="line">102         &#125;</span><br><span class="line">103 </span><br><span class="line">104         @Override</span><br><span class="line">105         public OutputCommitter getOutputCommitter() &#123;</span><br><span class="line">106             return mapContext.getOutputCommitter();</span><br><span class="line">107         &#125;</span><br><span class="line">108 </span><br><span class="line">109         @Override</span><br><span class="line">110         public void write(KEYOUT key, VALUEOUT value) throws IOException, InterruptedException &#123;</span><br><span class="line">111             mapContext.write(key, value);</span><br><span class="line">112         &#125;</span><br><span class="line">113 </span><br><span class="line">114         @Override</span><br><span class="line">115         public String getStatus() &#123;</span><br><span class="line">116             return mapContext.getStatus();</span><br><span class="line">117         &#125;</span><br><span class="line">118 </span><br><span class="line">119         @Override</span><br><span class="line">120         public TaskAttemptID getTaskAttemptID() &#123;</span><br><span class="line">121             return mapContext.getTaskAttemptID();</span><br><span class="line">122         &#125;</span><br><span class="line">123 </span><br><span class="line">124         @Override</span><br><span class="line">125         public void setStatus(String msg) &#123;</span><br><span class="line">126             mapContext.setStatus(msg);</span><br><span class="line">127         &#125;</span><br><span class="line">128 </span><br><span class="line">129         @Override</span><br><span class="line">130         public Path[] getArchiveClassPaths() &#123;</span><br><span class="line">131             return mapContext.getArchiveClassPaths();</span><br><span class="line">132         &#125;</span><br><span class="line">133 </span><br><span class="line">134         @Override</span><br><span class="line">135         public String[] getArchiveTimestamps() &#123;</span><br><span class="line">136             return mapContext.getArchiveTimestamps();</span><br><span class="line">137         &#125;</span><br><span class="line">138 </span><br><span class="line">139         @Override</span><br><span class="line">140         public URI[] getCacheArchives() throws IOException &#123;</span><br><span class="line">141             return mapContext.getCacheArchives();</span><br><span class="line">142         &#125;</span><br><span class="line">143 </span><br><span class="line">144         @Override</span><br><span class="line">145         public URI[] getCacheFiles() throws IOException &#123;</span><br><span class="line">146             return mapContext.getCacheFiles();</span><br><span class="line">147         &#125;</span><br><span class="line">148 </span><br><span class="line">149         @Override</span><br><span class="line">150         public Class&lt;? extends Reducer&lt;?, ?, ?, ?&gt;&gt; getCombinerClass() throws ClassNotFoundException &#123;</span><br><span class="line">151             return mapContext.getCombinerClass();</span><br><span class="line">152         &#125;</span><br><span class="line">153 </span><br><span class="line">154         @Override</span><br><span class="line">155         public Configuration getConfiguration() &#123;</span><br><span class="line">156             return mapContext.getConfiguration();</span><br><span class="line">157         &#125;</span><br><span class="line">158 </span><br><span class="line">159         @Override</span><br><span class="line">160         public Path[] getFileClassPaths() &#123;</span><br><span class="line">161             return mapContext.getFileClassPaths();</span><br><span class="line">162         &#125;</span><br><span class="line">163 </span><br><span class="line">164         @Override</span><br><span class="line">165         public String[] getFileTimestamps() &#123;</span><br><span class="line">166             return mapContext.getFileTimestamps();</span><br><span class="line">167         &#125;</span><br><span class="line">168 </span><br><span class="line">169         @Override</span><br><span class="line">170         public RawComparator&lt;?&gt; getCombinerKeyGroupingComparator() &#123;</span><br><span class="line">171             return mapContext.getCombinerKeyGroupingComparator();</span><br><span class="line">172         &#125;</span><br><span class="line">173 </span><br><span class="line">174         @Override</span><br><span class="line">175         public RawComparator&lt;?&gt; getGroupingComparator() &#123;</span><br><span class="line">176             return mapContext.getGroupingComparator();</span><br><span class="line">177         &#125;</span><br><span class="line">178 </span><br><span class="line">179         @Override</span><br><span class="line">180         public Class&lt;? extends InputFormat&lt;?, ?&gt;&gt; getInputFormatClass() throws ClassNotFoundException &#123;</span><br><span class="line">181             return mapContext.getInputFormatClass();</span><br><span class="line">182         &#125;</span><br><span class="line">183 </span><br><span class="line">184         @Override</span><br><span class="line">185         public String getJar() &#123;</span><br><span class="line">186             return mapContext.getJar();</span><br><span class="line">187         &#125;</span><br><span class="line">188 </span><br><span class="line">189         @Override</span><br><span class="line">190         public JobID getJobID() &#123;</span><br><span class="line">191             return mapContext.getJobID();</span><br><span class="line">192         &#125;</span><br><span class="line">193 </span><br><span class="line">194         @Override</span><br><span class="line">195         public String getJobName() &#123;</span><br><span class="line">196             return mapContext.getJobName();</span><br><span class="line">197         &#125;</span><br><span class="line">198 </span><br><span class="line">199         @Override</span><br><span class="line">200         public boolean getJobSetupCleanupNeeded() &#123;</span><br><span class="line">201             return mapContext.getJobSetupCleanupNeeded();</span><br><span class="line">202         &#125;</span><br><span class="line">203 </span><br><span class="line">204         @Override</span><br><span class="line">205         public boolean getTaskCleanupNeeded() &#123;</span><br><span class="line">206             return mapContext.getTaskCleanupNeeded();</span><br><span class="line">207         &#125;</span><br><span class="line">208 </span><br><span class="line">209         @Override</span><br><span class="line">210         public Path[] getLocalCacheArchives() throws IOException &#123;</span><br><span class="line">211             return mapContext.getLocalCacheArchives();</span><br><span class="line">212         &#125;</span><br><span class="line">213 </span><br><span class="line">214         @Override</span><br><span class="line">215         public Path[] getLocalCacheFiles() throws IOException &#123;</span><br><span class="line">216             return mapContext.getLocalCacheFiles();</span><br><span class="line">217         &#125;</span><br><span class="line">218 </span><br><span class="line">219         @Override</span><br><span class="line">220         public Class&lt;?&gt; getMapOutputKeyClass() &#123;</span><br><span class="line">221             return mapContext.getMapOutputKeyClass();</span><br><span class="line">222         &#125;</span><br><span class="line">223 </span><br><span class="line">224         @Override</span><br><span class="line">225         public Class&lt;?&gt; getMapOutputValueClass() &#123;</span><br><span class="line">226             return mapContext.getMapOutputValueClass();</span><br><span class="line">227         &#125;</span><br><span class="line">228 </span><br><span class="line">229         @Override</span><br><span class="line">230         public Class&lt;? extends Mapper&lt;?, ?, ?, ?&gt;&gt; getMapperClass() throws ClassNotFoundException &#123;</span><br><span class="line">231             return mapContext.getMapperClass();</span><br><span class="line">232         &#125;</span><br><span class="line">233 </span><br><span class="line">234         @Override</span><br><span class="line">235         public int getMaxMapAttempts() &#123;</span><br><span class="line">236             return mapContext.getMaxMapAttempts();</span><br><span class="line">237         &#125;</span><br><span class="line">238 </span><br><span class="line">239         @Override</span><br><span class="line">240         public int getMaxReduceAttempts() &#123;</span><br><span class="line">241             return mapContext.getMaxReduceAttempts();</span><br><span class="line">242         &#125;</span><br><span class="line">243 </span><br><span class="line">244         @Override</span><br><span class="line">245         public int getNumReduceTasks() &#123;</span><br><span class="line">246             return mapContext.getNumReduceTasks();</span><br><span class="line">247         &#125;</span><br><span class="line">248 </span><br><span class="line">249         @Override</span><br><span class="line">250         public Class&lt;? extends OutputFormat&lt;?, ?&gt;&gt; getOutputFormatClass() throws ClassNotFoundException &#123;</span><br><span class="line">251             return mapContext.getOutputFormatClass();</span><br><span class="line">252         &#125;</span><br><span class="line">253 </span><br><span class="line">254         @Override</span><br><span class="line">255         public Class&lt;?&gt; getOutputKeyClass() &#123;</span><br><span class="line">256             return mapContext.getOutputKeyClass();</span><br><span class="line">257         &#125;</span><br><span class="line">258 </span><br><span class="line">259         @Override</span><br><span class="line">260         public Class&lt;?&gt; getOutputValueClass() &#123;</span><br><span class="line">261             return mapContext.getOutputValueClass();</span><br><span class="line">262         &#125;</span><br><span class="line">263 </span><br><span class="line">264         @Override</span><br><span class="line">265         public Class&lt;? extends Partitioner&lt;?, ?&gt;&gt; getPartitionerClass() throws ClassNotFoundException &#123;</span><br><span class="line">266             return mapContext.getPartitionerClass();</span><br><span class="line">267         &#125;</span><br><span class="line">268 </span><br><span class="line">269         @Override</span><br><span class="line">270         public Class&lt;? extends Reducer&lt;?, ?, ?, ?&gt;&gt; getReducerClass() throws ClassNotFoundException &#123;</span><br><span class="line">271             return mapContext.getReducerClass();</span><br><span class="line">272         &#125;</span><br><span class="line">273 </span><br><span class="line">274         @Override</span><br><span class="line">275         public RawComparator&lt;?&gt; getSortComparator() &#123;</span><br><span class="line">276             return mapContext.getSortComparator();</span><br><span class="line">277         &#125;</span><br><span class="line">278 </span><br><span class="line">279         @Override</span><br><span class="line">280         public boolean getSymlink() &#123;</span><br><span class="line">281             return mapContext.getSymlink();</span><br><span class="line">282         &#125;</span><br><span class="line">283 </span><br><span class="line">284         @Override</span><br><span class="line">285         public Path getWorkingDirectory() throws IOException &#123;</span><br><span class="line">286             return mapContext.getWorkingDirectory();</span><br><span class="line">287         &#125;</span><br><span class="line">288 </span><br><span class="line">289         @Override</span><br><span class="line">290         public void progress() &#123;</span><br><span class="line">291             mapContext.progress();</span><br><span class="line">292         &#125;</span><br><span class="line">293 </span><br><span class="line">294         @Override</span><br><span class="line">295         public boolean getProfileEnabled() &#123;</span><br><span class="line">296             return mapContext.getProfileEnabled();</span><br><span class="line">297         &#125;</span><br><span class="line">298 </span><br><span class="line">299         @Override</span><br><span class="line">300         public String getProfileParams() &#123;</span><br><span class="line">301             return mapContext.getProfileParams();</span><br><span class="line">302         &#125;</span><br><span class="line">303 </span><br><span class="line">304         @Override</span><br><span class="line">305         public IntegerRanges getProfileTaskRange(boolean isMap) &#123;</span><br><span class="line">306             return mapContext.getProfileTaskRange(isMap);</span><br><span class="line">307         &#125;</span><br><span class="line">308 </span><br><span class="line">309         @Override</span><br><span class="line">310         public String getUser() &#123;</span><br><span class="line">311             return mapContext.getUser();</span><br><span class="line">312         &#125;</span><br><span class="line">313 </span><br><span class="line">314         @Override</span><br><span class="line">315         public Credentials getCredentials() &#123;</span><br><span class="line">316             return mapContext.getCredentials();</span><br><span class="line">317         &#125;</span><br><span class="line">318 </span><br><span class="line">319         @Override</span><br><span class="line">320         public float getProgress() &#123;</span><br><span class="line">321             return mapContext.getProgress();</span><br><span class="line">322         &#125;</span><br><span class="line">323     &#125;</span><br><span class="line">324 &#125;</span><br></pre></td></tr></table></figure><p>此类里面一定有那4个重要的方法，发现调用了mapContext，继续往上找</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">110          *  mapContext对象中一定包含三个方法</span><br><span class="line">111          *  </span><br><span class="line">112          *  找到了之前第一查看源码实现的方法的问题的答案：</span><br><span class="line">113          *  </span><br><span class="line">114          *      问题：找到谁调用MapContextImpl这个类的构造方法</span><br><span class="line">115          *  </span><br><span class="line">116          *      mapContext就是MapContextImpl的实例对象</span><br><span class="line">117          *      </span><br><span class="line">118          *      MapContextImpl类中一定有三个方法：</span><br><span class="line">119          *      </span><br><span class="line">120          *      input  ===  NewTrackingRecordReader</span><br><span class="line">121          *      </span><br><span class="line">122          *      </span><br><span class="line">123          *      </span><br><span class="line">124          *      确定的知识：</span><br><span class="line">125          *      </span><br><span class="line">126          *      1、mapContext对象中，一定有write方法</span><br><span class="line">127          *      </span><br><span class="line">128          *      2、通过观看MapContextImpl的组成，发现其实没有write方法</span><br><span class="line">129          *      </span><br><span class="line">130          *      解决：</span><br><span class="line">131          *      </span><br><span class="line">132          *      其实mapContext.write方法的调用是来自于MapContextImpl这个类的父类</span><br><span class="line">133          *      </span><br><span class="line">134          *      </span><br><span class="line">135          *      </span><br><span class="line">136          *      最底层的write方法：  output.write();</span><br><span class="line">137          */</span><br><span class="line">138         org.apache.hadoop.mapreduce.MapContext&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt; mapContext = </span><br><span class="line">139                 new MapContextImpl&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;(</span><br><span class="line">140                 job, getTaskID(), input, output, committer, reporter, split);</span><br></pre></td></tr></table></figure><p>mapConext就是这个类MapContextImpl的实例对象</p><p>继续确定:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mapConext = new MapContextImpl(input)</span><br><span class="line">mapConext.nextKeyVlaue()&#123;</span><br><span class="line"></span><br><span class="line">LineRecordReader real = input.createRecordReader();</span><br><span class="line"></span><br><span class="line">real.nextKeyValue();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看MapContextImpl.java源码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"> 1 public class MapContextImpl&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt;</span><br><span class="line"> 2         extends TaskInputOutputContextImpl&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt;</span><br><span class="line"> 3         implements MapContext&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; &#123;</span><br><span class="line"> 4     </span><br><span class="line"> 5     </span><br><span class="line"> 6     private RecordReader&lt;KEYIN, VALUEIN&gt; reader;</span><br><span class="line"> 7     private InputSplit split;</span><br><span class="line"> 8 </span><br><span class="line"> 9     public MapContextImpl(Configuration conf, </span><br><span class="line">10             TaskAttemptID taskid, </span><br><span class="line">11             RecordReader&lt;KEYIN, VALUEIN&gt; reader,</span><br><span class="line">12             RecordWriter&lt;KEYOUT, VALUEOUT&gt; writer, </span><br><span class="line">13             OutputCommitter committer, </span><br><span class="line">14             StatusReporter reporter,</span><br><span class="line">15             InputSplit split) &#123;</span><br><span class="line">16         </span><br><span class="line">17         </span><br><span class="line">18         </span><br><span class="line">19         // 通过super调用父类的构造方法</span><br><span class="line">20         super(conf, taskid, writer, committer, reporter);</span><br><span class="line">21         </span><br><span class="line">22         </span><br><span class="line">23         </span><br><span class="line">24         this.reader = reader;</span><br><span class="line">25         this.split = split;</span><br><span class="line">26     &#125;</span><br><span class="line">27 </span><br><span class="line">28     /**</span><br><span class="line">29      * Get the input split for this map.</span><br><span class="line">30      */</span><br><span class="line">31     public InputSplit getInputSplit() &#123;</span><br><span class="line">32         return split;</span><br><span class="line">33     &#125;</span><br><span class="line"></span><br><span class="line">40     @Override</span><br><span class="line">41     public KEYIN getCurrentKey() throws IOException, InterruptedException &#123;</span><br><span class="line">42         return reader.getCurrentKey();</span><br><span class="line">43     &#125;</span><br><span class="line">44 </span><br><span class="line">45     @Override</span><br><span class="line">46     public VALUEIN getCurrentValue() throws IOException, InterruptedException &#123;</span><br><span class="line">47         return reader.getCurrentValue();</span><br><span class="line">48     &#125;</span><br><span class="line">49 </span><br><span class="line">50     @Override</span><br><span class="line">51     public boolean nextKeyValue() throws IOException, InterruptedException &#123;</span><br><span class="line">52         return reader.nextKeyValue();</span><br><span class="line">53     &#125;</span><br><span class="line">54     </span><br><span class="line">55     </span><br><span class="line">56     </span><br><span class="line">57 </span><br><span class="line">58 &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（二十一）MapReduce实现Reduce Join（多个文件联合查询）</title>
      <link href="/2018-04-21-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%80%EF%BC%89MapReduce%E5%AE%9E%E7%8E%B0Reduce%20Join%EF%BC%88%E5%A4%9A%E4%B8%AA%E6%96%87%E4%BB%B6%E8%81%94%E5%90%88%E6%9F%A5%E8%AF%A2%EF%BC%89.html"/>
      <url>/2018-04-21-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%80%EF%BC%89MapReduce%E5%AE%9E%E7%8E%B0Reduce%20Join%EF%BC%88%E5%A4%9A%E4%B8%AA%E6%96%87%E4%BB%B6%E8%81%94%E5%90%88%E6%9F%A5%E8%AF%A2%EF%BC%89.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（二十一）MapReduce实现Reduce Join（多个文件联合查询）：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（二十一）MapReduce实现Reduce Join（多个文件联合查询）</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h1 id="MapReduce-Join"><a href="#MapReduce-Join" class="headerlink" title="MapReduce Join"></a>MapReduce Join</h1><p>对两份数据data1和data2进行关键词连接是一个很通用的问题，如果数据量比较小，可以在内存中完成连接。</p><p>如果数据量比较大，在内存进行连接操会发生OOM。mapreduce join可以用来解决大数据的连接。</p><h2 id="1-思路"><a href="#1-思路" class="headerlink" title="1 思路"></a>1 思路</h2><h3 id="1-1-reduce-join"><a href="#1-1-reduce-join" class="headerlink" title="1.1 reduce join"></a>1.1 reduce join</h3><p>在map阶段, 把关键字作为key输出，并在value中标记出数据是来自data1还是data2。因为在shuffle阶段已经自然按key分组，reduce阶段，判断每一个value是来自data1还是data2,在内部分成2组，做集合的乘积。</p><p>这种方法有2个问题：</p><blockquote><p>1, map阶段没有对数据瘦身，shuffle的网络传输和排序性能很低。</p><p>2, reduce端对2个集合做乘积计算，很耗内存，容易导致OOM。</p></blockquote><h3 id="1-2-map-join"><a href="#1-2-map-join" class="headerlink" title="1.2 map join"></a>1.2 map join</h3><p>两份数据中，如果有一份数据比较小，小数据全部加载到内存，按关键字建立索引。大数据文件作为map的输入文件，对map()函数每一对输入，都能够方便地和已加载到内存的小数据进行连接。把连接结果按key输出，经过shuffle阶段，reduce端得到的就是已经按key分组的，并且连接好了的数据。</p><p>这种方法，要使用hadoop中的DistributedCache把小数据分布到各个计算节点，每个map节点都要把小数据库加载到内存，按关键字建立索引。</p><blockquote><p>这种方法有明显的局限性：有一份数据比较小，在map端，能够把它加载到内存，并进行join操作。</p></blockquote><h3 id="1-3-使用内存服务器，扩大节点的内存空间"><a href="#1-3-使用内存服务器，扩大节点的内存空间" class="headerlink" title="1.3 使用内存服务器，扩大节点的内存空间"></a>1.3 使用内存服务器，扩大节点的内存空间</h3><p>针对map join，可以把一份数据存放到专门的内存服务器，在map()方法中，对每一个&lt;key,value&gt;的输入对，根据key到内存服务器中取出数据，进行连接</p><h3 id="1-4-使用BloomFilter过滤空连接的数据"><a href="#1-4-使用BloomFilter过滤空连接的数据" class="headerlink" title="1.4 使用BloomFilter过滤空连接的数据"></a>1.4 使用BloomFilter过滤空连接的数据</h3><p>对其中一份数据在内存中建立BloomFilter，另外一份数据在连接之前，用BloomFilter判断它的key是否存在，如果不存在，那这个记录是空连接，可以忽略。</p><h3 id="1-5-使用mapreduce专为join设计的包"><a href="#1-5-使用mapreduce专为join设计的包" class="headerlink" title="1.5 使用mapreduce专为join设计的包"></a>1.5 使用mapreduce专为join设计的包</h3><p>在mapreduce包里看到有专门为join设计的包，对这些包还没有学习，不知道怎么使用，只是在这里记录下来，作个提醒。</p><blockquote><p>jar： mapreduce-client-core.jar</p><p>package： org.apache.hadoop.mapreduce.lib.join</p></blockquote><h2 id="2-实现reduce-join"><a href="#2-实现reduce-join" class="headerlink" title="2 实现reduce join"></a>2 实现reduce join</h2><p>两个文件，此处只写出部分数据，测试数据movies.dat数据量为3883条，ratings.dat数据量为1000210条数据</p><p>movies.dat 数据格式为：1　　::　　Toy Story (1995)　　::　　Animation|Children’s|Comedy</p><p>对应字段中文解释：　　电影ID 　　电影名字　　　　　　　　电影类型</p><p>ratings.dat 数据格式为：1　　::　　1193　　::　　5　　::　　978300760</p><p>对应字段中文解释：　　用户ID　　电影ID　　　评分　　　　评分时间戳</p><p>2个文件进行关联实现代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line">import java.io.IOException;</span><br><span class="line">  2 import java.net.URI;</span><br><span class="line">  3 import java.util.ArrayList;</span><br><span class="line">  4 import java.util.List;</span><br><span class="line">  5 </span><br><span class="line">  6 import org.apache.hadoop.conf.Configuration;</span><br><span class="line">  7 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">  8 import org.apache.hadoop.fs.Path;</span><br><span class="line">  9 import org.apache.hadoop.io.IntWritable;</span><br><span class="line"> 10 import org.apache.hadoop.io.LongWritable;</span><br><span class="line"> 11 import org.apache.hadoop.io.Text;</span><br><span class="line"> 12 import org.apache.hadoop.mapreduce.Job;</span><br><span class="line"> 13 import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"> 14 import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"> 15 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"> 16 import org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"> 17 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"> 18 </span><br><span class="line"> 19 public class MovieMR1 &#123;</span><br><span class="line"> 20 </span><br><span class="line"> 21     public static void main(String[] args) throws Exception &#123;</span><br><span class="line"> 22         </span><br><span class="line"> 23         Configuration conf1 = new Configuration();</span><br><span class="line"> 24         /*conf1.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000/&quot;);</span><br><span class="line"> 25         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);*/</span><br><span class="line"> 26         FileSystem fs1 = FileSystem.get(conf1);</span><br><span class="line"> 27         </span><br><span class="line"> 28         </span><br><span class="line"> 29         Job job = Job.getInstance(conf1);</span><br><span class="line"> 30         </span><br><span class="line"> 31         job.setJarByClass(MovieMR1.class);</span><br><span class="line"> 32         </span><br><span class="line"> 33         job.setMapperClass(MoviesMapper.class);</span><br><span class="line"> 34         job.setReducerClass(MoviesReduceJoinReducer.class);</span><br><span class="line"> 35         </span><br><span class="line"> 36         job.setMapOutputKeyClass(Text.class);</span><br><span class="line"> 37         job.setMapOutputValueClass(Text.class);</span><br><span class="line"> 38         </span><br><span class="line"> 39         job.setOutputKeyClass(Text.class);</span><br><span class="line"> 40         job.setOutputValueClass(Text.class);</span><br><span class="line"> 41         </span><br><span class="line"> 42         Path inputPath1 = new Path(&quot;D:\\MR\\hw\\movie\\input\\movies&quot;);</span><br><span class="line"> 43         Path inputPath2 = new Path(&quot;D:\\MR\\hw\\movie\\input\\ratings&quot;);</span><br><span class="line"> 44         Path outputPath1 = new Path(&quot;D:\\MR\\hw\\movie\\output&quot;);</span><br><span class="line"> 45         if(fs1.exists(outputPath1)) &#123;</span><br><span class="line"> 46             fs1.delete(outputPath1, true);</span><br><span class="line"> 47         &#125;</span><br><span class="line"> 48         FileInputFormat.addInputPath(job, inputPath1);</span><br><span class="line"> 49         FileInputFormat.addInputPath(job, inputPath2);</span><br><span class="line"> 50         FileOutputFormat.setOutputPath(job, outputPath1);</span><br><span class="line"> 51         </span><br><span class="line"> 52         boolean isDone = job.waitForCompletion(true);</span><br><span class="line"> 53         System.exit(isDone ? 0 : 1);</span><br><span class="line"> 54     &#125;</span><br><span class="line"> 55 </span><br><span class="line"> 56     </span><br><span class="line"> 57     public static class MoviesMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123;</span><br><span class="line"> 58         </span><br><span class="line"> 59         Text outKey = new Text();</span><br><span class="line"> 60         Text outValue = new Text();</span><br><span class="line"> 61         StringBuilder sb = new StringBuilder();</span><br><span class="line"> 62         </span><br><span class="line"> 63         protected void map(LongWritable key, Text value,Context context) throws java.io.IOException ,InterruptedException &#123;</span><br><span class="line"> 64             </span><br><span class="line"> 65             FileSplit inputSplit = (FileSplit)context.getInputSplit();</span><br><span class="line"> 66             String name = inputSplit.getPath().getName();</span><br><span class="line"> 67             String[] split = value.toString().split(&quot;::&quot;);</span><br><span class="line"> 68             sb.setLength(0);</span><br><span class="line"> 69             </span><br><span class="line"> 70             if(name.equals(&quot;movies.dat&quot;)) &#123;</span><br><span class="line"> 71                 //                    1　　::　　Toy Story (1995)　　::　　Animation|Children&apos;s|Comedy</span><br><span class="line"> 72                 //对应字段中文解释：　　电影ID 　　   电影名字　　　　　　　　                 电影类型</span><br><span class="line"> 73                 outKey.set(split[0]);</span><br><span class="line"> 74                 StringBuilder append = sb.append(split[1]).append(&quot;\t&quot;).append(split[2]);</span><br><span class="line"> 75                 String str = &quot;movies#&quot;+append.toString();</span><br><span class="line"> 76                 outValue.set(str);</span><br><span class="line"> 77                 //System.out.println(outKey+&quot;---&quot;+outValue);</span><br><span class="line"> 78                 context.write(outKey, outValue);</span><br><span class="line"> 79             &#125;else&#123;</span><br><span class="line"> 80                 //                    1　　::　　1193　　::　　5　　::　　978300760</span><br><span class="line"> 81                 //对应字段中文解释：　　用户ID　　           电影ID　　　      评分　　　　   评分时间戳</span><br><span class="line"> 82                 outKey.set(split[1]);</span><br><span class="line"> 83                 StringBuilder append = sb.append(split[0]).append(&quot;\t&quot;).append(split[2]).append(&quot;\t&quot;).append(split[3]);</span><br><span class="line"> 84                 String str = &quot;ratings#&quot; + append.toString();</span><br><span class="line"> 85                 outValue.set(str);</span><br><span class="line"> 86                 //System.out.println(outKey+&quot;---&quot;+outValue);</span><br><span class="line"> 87                 context.write(outKey, outValue);</span><br><span class="line"> 88             &#125;</span><br><span class="line"> 89         </span><br><span class="line"> 90         &#125;;</span><br><span class="line"> 91         </span><br><span class="line"> 92     &#125;</span><br><span class="line"> 93     </span><br><span class="line"> 94     </span><br><span class="line"> 95     public static class MoviesReduceJoinReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123;</span><br><span class="line"> 96         //用来存放    电影ID    电影名称    电影类型    </span><br><span class="line"> 97         List&lt;String&gt; moviesList = new ArrayList&lt;&gt;();</span><br><span class="line"> 98         //用来存放    电影ID    用户ID 用户评分    时间戳</span><br><span class="line"> 99         List&lt;String&gt; ratingsList = new ArrayList&lt;&gt;();</span><br><span class="line">100         Text outValue = new Text();</span><br><span class="line">101         </span><br><span class="line">102         @Override</span><br><span class="line">103         protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context)</span><br><span class="line">104                 throws IOException, InterruptedException &#123;</span><br><span class="line">105             </span><br><span class="line">106             int count = 0;</span><br><span class="line">107             </span><br><span class="line">108             //迭代集合</span><br><span class="line">109             for(Text text : values) &#123;</span><br><span class="line">110                 </span><br><span class="line">111                 //将集合中的元素添加到对应的list中</span><br><span class="line">112                 if(text.toString().startsWith(&quot;movies#&quot;)) &#123;</span><br><span class="line">113                     String string = text.toString().split(&quot;#&quot;)[1];</span><br><span class="line">114                     </span><br><span class="line">115                     moviesList.add(string);</span><br><span class="line">116                 &#125;else if(text.toString().startsWith(&quot;ratings#&quot;))&#123;</span><br><span class="line">117                     String string = text.toString().split(&quot;#&quot;)[1];</span><br><span class="line">118                     ratingsList.add(string);</span><br><span class="line">119                 &#125;</span><br><span class="line">120             &#125;</span><br><span class="line">121             </span><br><span class="line">122             //获取2个集合的长度</span><br><span class="line">123             long moviesSize = moviesList.size();</span><br><span class="line">124             long ratingsSize = ratingsList.size();</span><br><span class="line">125             </span><br><span class="line">126             for(int i=0;i&lt;moviesSize;i++) &#123;</span><br><span class="line">127                 for(int j=0;j&lt;ratingsSize;j++) &#123;</span><br><span class="line">128                     outValue.set(moviesList.get(i)+&quot;\t&quot;+ratingsList.get(j));</span><br><span class="line">129                     //最后的输出是    电影ID    电影名称    电影类型    用户ID 用户评分    时间戳</span><br><span class="line">130                     context.write(key, outValue);</span><br><span class="line">131                 &#125;</span><br><span class="line">132             &#125;</span><br><span class="line">133             </span><br><span class="line">134             moviesList.clear();</span><br><span class="line">135             ratingsList.clear();</span><br><span class="line">136             </span><br><span class="line">137         &#125;</span><br><span class="line">138         </span><br><span class="line">139     &#125;</span><br><span class="line">140     </span><br><span class="line">141 &#125;</span><br></pre></td></tr></table></figure><p>最后的合并结果：　　电影ID　　电影名称　　电影类型　　用户ID　　用户评论　　时间戳</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180319220957397-215356698.png" alt="img"></p><h2 id="3-实现map-join"><a href="#3-实现map-join" class="headerlink" title="3 实现map join"></a>3 实现map join</h2><p>两个文件，此处只写出部分数据，测试数据movies.dat数据量为3883条，ratings.dat数据量为1000210条数据</p><p>movies.dat 数据格式为：1　　::　　Toy Story (1995)　　::　　Animation|Children’s|Comedy</p><p>对应字段中文解释：　　电影ID 　　电影名字　　　　　　　　电影类型</p><p>ratings.dat 数据格式为：1　　::　　1193　　::　　5　　::　　978300760</p><p>对应字段中文解释：　　用户ID　　电影ID　　　评分　　　　评分时间戳</p><p>需求：求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）</p><p>实现代码</p><p>MovieMR1_1.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"> 1 import java.io.DataInput;</span><br><span class="line"> 2 import java.io.DataOutput;</span><br><span class="line"> 3 import java.io.IOException;</span><br><span class="line"> 4 </span><br><span class="line"> 5 import org.apache.hadoop.io.WritableComparable;</span><br><span class="line"> 6 </span><br><span class="line"> 7 public class MovieRating implements WritableComparable&lt;MovieRating&gt;&#123;</span><br><span class="line"> 8     private String movieName;</span><br><span class="line"> 9     private int count;</span><br><span class="line">10     </span><br><span class="line">11     public String getMovieName() &#123;</span><br><span class="line">12         return movieName;</span><br><span class="line">13     &#125;</span><br><span class="line">14     public void setMovieName(String movieName) &#123;</span><br><span class="line">15         this.movieName = movieName;</span><br><span class="line">16     &#125;</span><br><span class="line">17     public int getCount() &#123;</span><br><span class="line">18         return count;</span><br><span class="line">19     &#125;</span><br><span class="line">20     public void setCount(int count) &#123;</span><br><span class="line">21         this.count = count;</span><br><span class="line">22     &#125;</span><br><span class="line">23     </span><br><span class="line">24     public MovieRating() &#123;&#125;</span><br><span class="line">25     </span><br><span class="line">26     public MovieRating(String movieName, int count) &#123;</span><br><span class="line">27         super();</span><br><span class="line">28         this.movieName = movieName;</span><br><span class="line">29         this.count = count;</span><br><span class="line">30     &#125;</span><br><span class="line">31     </span><br><span class="line">32     </span><br><span class="line">33     @Override</span><br><span class="line">34     public String toString() &#123;</span><br><span class="line">35         return  movieName + &quot;\t&quot; + count;</span><br><span class="line">36     &#125;</span><br><span class="line">37     @Override</span><br><span class="line">38     public void readFields(DataInput in) throws IOException &#123;</span><br><span class="line">39         movieName = in.readUTF();</span><br><span class="line">40         count = in.readInt();</span><br><span class="line">41     &#125;</span><br><span class="line">42     @Override</span><br><span class="line">43     public void write(DataOutput out) throws IOException &#123;</span><br><span class="line">44         out.writeUTF(movieName);</span><br><span class="line">45         out.writeInt(count);</span><br><span class="line">46     &#125;</span><br><span class="line">47     @Override</span><br><span class="line">48     public int compareTo(MovieRating o) &#123;</span><br><span class="line">49         return o.count - this.count ;</span><br><span class="line">50     &#125;</span><br><span class="line">51     </span><br><span class="line">52 &#125;</span><br></pre></td></tr></table></figure><p>MovieMR1_2.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">  1 import java.io.IOException;</span><br><span class="line">  2 </span><br><span class="line">  3 import org.apache.hadoop.conf.Configuration;</span><br><span class="line">  4 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">  5 import org.apache.hadoop.fs.Path;</span><br><span class="line">  6 import org.apache.hadoop.io.LongWritable;</span><br><span class="line">  7 import org.apache.hadoop.io.NullWritable;</span><br><span class="line">  8 import org.apache.hadoop.io.Text;</span><br><span class="line">  9 import org.apache.hadoop.mapreduce.Job;</span><br><span class="line"> 10 import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"> 11 import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"> 12 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"> 13 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"> 14 </span><br><span class="line"> 15 public class MovieMR1_2 &#123;</span><br><span class="line"> 16 </span><br><span class="line"> 17     public static void main(String[] args) throws Exception &#123;</span><br><span class="line"> 18         if(args.length &lt; 2) &#123;</span><br><span class="line"> 19             args = new String[2];</span><br><span class="line"> 20             args[0] = &quot;/movie/output/&quot;;</span><br><span class="line"> 21             args[1] = &quot;/movie/output_last/&quot;;</span><br><span class="line"> 22         &#125;</span><br><span class="line"> 23         </span><br><span class="line"> 24         </span><br><span class="line"> 25         Configuration conf1 = new Configuration();</span><br><span class="line"> 26         conf1.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000/&quot;);</span><br><span class="line"> 27         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line"> 28         FileSystem fs1 = FileSystem.get(conf1);</span><br><span class="line"> 29         </span><br><span class="line"> 30         </span><br><span class="line"> 31         Job job = Job.getInstance(conf1);</span><br><span class="line"> 32         </span><br><span class="line"> 33         job.setJarByClass(MovieMR1_2.class);</span><br><span class="line"> 34         </span><br><span class="line"> 35         job.setMapperClass(MoviesMapJoinRatingsMapper2.class);</span><br><span class="line"> 36         job.setReducerClass(MovieMR1Reducer2.class);</span><br><span class="line"> 37 </span><br><span class="line"> 38         </span><br><span class="line"> 39         job.setMapOutputKeyClass(MovieRating.class);</span><br><span class="line"> 40         job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"> 41         </span><br><span class="line"> 42         job.setOutputKeyClass(MovieRating.class);</span><br><span class="line"> 43         job.setOutputValueClass(NullWritable.class);</span><br><span class="line"> 44         </span><br><span class="line"> 45         </span><br><span class="line"> 46         Path inputPath1 = new Path(args[0]);</span><br><span class="line"> 47         Path outputPath1 = new Path(args[1]);</span><br><span class="line"> 48         if(fs1.exists(outputPath1)) &#123;</span><br><span class="line"> 49             fs1.delete(outputPath1, true);</span><br><span class="line"> 50         &#125;</span><br><span class="line"> 51         //对第一步的输出结果进行降序排序</span><br><span class="line"> 52         FileInputFormat.setInputPaths(job, inputPath1);</span><br><span class="line"> 53         FileOutputFormat.setOutputPath(job, outputPath1);</span><br><span class="line"> 54         </span><br><span class="line"> 55         boolean isDone = job.waitForCompletion(true);</span><br><span class="line"> 56         System.exit(isDone ? 0 : 1);</span><br><span class="line"> 57         </span><br><span class="line"> 58 </span><br><span class="line"> 59     &#125;</span><br><span class="line"> 60     </span><br><span class="line"> 61     //注意输出类型为自定义对象MovieRating，MovieRating按照降序排序</span><br><span class="line"> 62     public static class MoviesMapJoinRatingsMapper2 extends Mapper&lt;LongWritable, Text, MovieRating, NullWritable&gt;&#123;</span><br><span class="line"> 63         </span><br><span class="line"> 64         MovieRating outKey = new MovieRating();</span><br><span class="line"> 65         </span><br><span class="line"> 66         @Override</span><br><span class="line"> 67         protected void map(LongWritable key, Text value, Context context)</span><br><span class="line"> 68                 throws IOException, InterruptedException &#123;</span><br><span class="line"> 69             //&apos;Night Mother (1986)         70</span><br><span class="line"> 70             String[] split = value.toString().split(&quot;\t&quot;);</span><br><span class="line"> 71             </span><br><span class="line"> 72             outKey.setCount(Integer.parseInt(split[1]));;</span><br><span class="line"> 73             outKey.setMovieName(split[0]);</span><br><span class="line"> 74             </span><br><span class="line"> 75             context.write(outKey, NullWritable.get());</span><br><span class="line"> 76                         </span><br><span class="line"> 77         &#125;</span><br><span class="line"> 78                 </span><br><span class="line"> 79     &#125;</span><br><span class="line"> 80     </span><br><span class="line"> 81     //排序之后自然输出，只取前10部电影</span><br><span class="line"> 82     public static class MovieMR1Reducer2 extends Reducer&lt;MovieRating, NullWritable, MovieRating, NullWritable&gt;&#123;</span><br><span class="line"> 83         </span><br><span class="line"> 84         Text outKey = new Text();</span><br><span class="line"> 85         int count = 0;</span><br><span class="line"> 86         </span><br><span class="line"> 87         @Override</span><br><span class="line"> 88         protected void reduce(MovieRating key, Iterable&lt;NullWritable&gt; values,Context context) throws IOException, InterruptedException &#123;</span><br><span class="line"> 89 </span><br><span class="line"> 90             for(NullWritable value : values) &#123;</span><br><span class="line"> 91                 count++;</span><br><span class="line"> 92                 if(count &gt; 10) &#123;</span><br><span class="line"> 93                     return;</span><br><span class="line"> 94                 &#125;</span><br><span class="line"> 95                 context.write(key, value);</span><br><span class="line"> 96                 </span><br><span class="line"> 97             &#125;</span><br><span class="line"> 98         </span><br><span class="line"> 99         &#125;</span><br><span class="line">100         </span><br><span class="line">101     &#125;</span><br><span class="line">102 &#125;</span><br></pre></td></tr></table></figure><p>MovieRating.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line">  1 import java.io.BufferedReader;</span><br><span class="line">  2 import java.io.FileReader;</span><br><span class="line">  3 import java.io.IOException;</span><br><span class="line">  4 import java.net.URI;</span><br><span class="line">  5 import java.util.HashMap;</span><br><span class="line">  6 import java.util.Map;</span><br><span class="line">  7 </span><br><span class="line">  8 import org.apache.hadoop.conf.Configuration;</span><br><span class="line">  9 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 10 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 11 import org.apache.hadoop.io.IntWritable;</span><br><span class="line"> 12 import org.apache.hadoop.io.LongWritable;</span><br><span class="line"> 13 import org.apache.hadoop.io.Text;</span><br><span class="line"> 14 import org.apache.hadoop.mapreduce.Job;</span><br><span class="line"> 15 import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"> 16 import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"> 17 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"> 18 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"> 19 </span><br><span class="line"> 20 </span><br><span class="line"> 21 public class MovieMR1_1 &#123;</span><br><span class="line"> 22 </span><br><span class="line"> 23     public static void main(String[] args) throws Exception &#123;</span><br><span class="line"> 24         </span><br><span class="line"> 25         if(args.length &lt; 4) &#123;</span><br><span class="line"> 26             args = new String[4];</span><br><span class="line"> 27             args[0] = &quot;/movie/input/&quot;;</span><br><span class="line"> 28             args[1] = &quot;/movie/output/&quot;;</span><br><span class="line"> 29             args[2] = &quot;/movie/cache/movies.dat&quot;;</span><br><span class="line"> 30             args[3] = &quot;/movie/output_last/&quot;;</span><br><span class="line"> 31         &#125;</span><br><span class="line"> 32         </span><br><span class="line"> 33         </span><br><span class="line"> 34         Configuration conf1 = new Configuration();</span><br><span class="line"> 35         conf1.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000/&quot;);</span><br><span class="line"> 36         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line"> 37         FileSystem fs1 = FileSystem.get(conf1);</span><br><span class="line"> 38         </span><br><span class="line"> 39         </span><br><span class="line"> 40         Job job1 = Job.getInstance(conf1);</span><br><span class="line"> 41         </span><br><span class="line"> 42         job1.setJarByClass(MovieMR1_1.class);</span><br><span class="line"> 43         </span><br><span class="line"> 44         job1.setMapperClass(MoviesMapJoinRatingsMapper1.class);</span><br><span class="line"> 45         job1.setReducerClass(MovieMR1Reducer1.class);</span><br><span class="line"> 46         </span><br><span class="line"> 47         job1.setMapOutputKeyClass(Text.class);</span><br><span class="line"> 48         job1.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"> 49         </span><br><span class="line"> 50         job1.setOutputKeyClass(Text.class);</span><br><span class="line"> 51         job1.setOutputValueClass(IntWritable.class);</span><br><span class="line"> 52         </span><br><span class="line"> 53         </span><br><span class="line"> 54         </span><br><span class="line"> 55         //缓存普通文件到task运行节点的工作目录</span><br><span class="line"> 56         URI uri = new URI(&quot;hdfs://hadoop1:9000&quot;+args[2]);</span><br><span class="line"> 57         System.out.println(uri);</span><br><span class="line"> 58         job1.addCacheFile(uri);</span><br><span class="line"> 59         </span><br><span class="line"> 60         Path inputPath1 = new Path(args[0]);</span><br><span class="line"> 61         Path outputPath1 = new Path(args[1]);</span><br><span class="line"> 62         if(fs1.exists(outputPath1)) &#123;</span><br><span class="line"> 63             fs1.delete(outputPath1, true);</span><br><span class="line"> 64         &#125;</span><br><span class="line"> 65         FileInputFormat.setInputPaths(job1, inputPath1);</span><br><span class="line"> 66         FileOutputFormat.setOutputPath(job1, outputPath1);</span><br><span class="line"> 67         </span><br><span class="line"> 68         boolean isDone = job1.waitForCompletion(true);</span><br><span class="line"> 69         System.exit(isDone ? 0 : 1);</span><br><span class="line"> 70        </span><br><span class="line"> 71     &#125;</span><br><span class="line"> 72     </span><br><span class="line"> 73     public static class MoviesMapJoinRatingsMapper1 extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123;</span><br><span class="line"> 74         </span><br><span class="line"> 75         //用了存放加载到内存中的movies.dat数据</span><br><span class="line"> 76         private static Map&lt;String,String&gt; movieMap =  new HashMap&lt;&gt;();</span><br><span class="line"> 77         //key：电影ID</span><br><span class="line"> 78         Text outKey = new Text();</span><br><span class="line"> 79         //value：电影名+电影类型</span><br><span class="line"> 80         IntWritable outValue = new IntWritable();</span><br><span class="line"> 81         </span><br><span class="line"> 82         </span><br><span class="line"> 83         /**</span><br><span class="line"> 84          * movies.dat:    1::Toy Story (1995)::Animation|Children&apos;s|Comedy</span><br><span class="line"> 85          * </span><br><span class="line"> 86          * </span><br><span class="line"> 87          * 将小表(movies.dat)中的数据预先加载到内存中去</span><br><span class="line"> 88          * */</span><br><span class="line"> 89         @Override</span><br><span class="line"> 90         protected void setup(Context context) throws IOException, InterruptedException &#123;</span><br><span class="line"> 91             </span><br><span class="line"> 92             Path[] localCacheFiles = context.getLocalCacheFiles();</span><br><span class="line"> 93             </span><br><span class="line"> 94             String strPath = localCacheFiles[0].toUri().toString();</span><br><span class="line"> 95             </span><br><span class="line"> 96             BufferedReader br = new BufferedReader(new FileReader(strPath));</span><br><span class="line"> 97             String readLine;</span><br><span class="line"> 98             while((readLine = br.readLine()) != null) &#123;</span><br><span class="line"> 99                 </span><br><span class="line">100                 String[] split = readLine.split(&quot;::&quot;);</span><br><span class="line">101                 String movieId = split[0];</span><br><span class="line">102                 String movieName = split[1];</span><br><span class="line">103                 String movieType = split[2];</span><br><span class="line">104                 </span><br><span class="line">105                 movieMap.put(movieId, movieName+&quot;\t&quot;+movieType);</span><br><span class="line">106             &#125;</span><br><span class="line">107             </span><br><span class="line">108             br.close();</span><br><span class="line">109         &#125;</span><br><span class="line">110         </span><br><span class="line">111         </span><br><span class="line">112         /**</span><br><span class="line">113          * movies.dat:    1    ::    Toy Story (1995)    ::    Animation|Children&apos;s|Comedy    </span><br><span class="line">114          *                 电影ID    电影名字                    电影类型</span><br><span class="line">115          * </span><br><span class="line">116          * ratings.dat:    1    ::    1193    ::    5    ::    978300760</span><br><span class="line">117          *                 用户ID    电影ID        评分        评分时间戳</span><br><span class="line">118          * </span><br><span class="line">119          * value:    ratings.dat读取的数据</span><br><span class="line">120          * */</span><br><span class="line">121         @Override</span><br><span class="line">122         protected void map(LongWritable key, Text value, Context context)</span><br><span class="line">123                 throws IOException, InterruptedException &#123;</span><br><span class="line">124             </span><br><span class="line">125             String[] split = value.toString().split(&quot;::&quot;);</span><br><span class="line">126             </span><br><span class="line">127             String userId = split[0];</span><br><span class="line">128             String movieId = split[1];</span><br><span class="line">129             String movieRate = split[2];</span><br><span class="line">130             </span><br><span class="line">131             //根据movieId从内存中获取电影名和类型</span><br><span class="line">132             String movieNameAndType = movieMap.get(movieId);</span><br><span class="line">133             String movieName = movieNameAndType.split(&quot;\t&quot;)[0];</span><br><span class="line">134             String movieType = movieNameAndType.split(&quot;\t&quot;)[1];</span><br><span class="line">135             </span><br><span class="line">136             outKey.set(movieName);</span><br><span class="line">137             outValue.set(Integer.parseInt(movieRate));</span><br><span class="line">138             </span><br><span class="line">139             context.write(outKey, outValue);</span><br><span class="line">140             </span><br><span class="line">141         &#125;</span><br><span class="line">142             </span><br><span class="line">143     &#125;</span><br><span class="line">144 </span><br><span class="line">145     </span><br><span class="line">146     public static class MovieMR1Reducer1 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;</span><br><span class="line">147         //每部电影评论的次数</span><br><span class="line">148         int count;</span><br><span class="line">149         //评分次数</span><br><span class="line">150         IntWritable outValue = new IntWritable();</span><br><span class="line">151         </span><br><span class="line">152         @Override</span><br><span class="line">153         protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">154             </span><br><span class="line">155             count = 0;</span><br><span class="line">156             </span><br><span class="line">157             for(IntWritable value : values) &#123;</span><br><span class="line">158                 count++;</span><br><span class="line">159             &#125;</span><br><span class="line">160             </span><br><span class="line">161             outValue.set(count);</span><br><span class="line">162             </span><br><span class="line">163             context.write(key, outValue);</span><br><span class="line">164         &#125;</span><br><span class="line">165         </span><br><span class="line">166     &#125;</span><br><span class="line">167     </span><br><span class="line">168     </span><br><span class="line">169 &#125;</span><br></pre></td></tr></table></figure><p>最后的结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180319221116843-1408077683.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（二十）MapReduce求TopN</title>
      <link href="/2018-04-20-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%EF%BC%89MapReduce%E6%B1%82TopN.html"/>
      <url>/2018-04-20-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%E5%8D%81%EF%BC%89MapReduce%E6%B1%82TopN.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（二十）MapReduce求TopN：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        在Hadoop中，排序是MapReduce的灵魂，MapTask和ReduceTask均会对数据按Key排序，这个操作是MR框架的默认行为，不管你的业务逻辑上是否需要这一操作。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="技术点"><a href="#技术点" class="headerlink" title="技术点"></a>技术点</h2><p>MapReduce框架中，用到的排序主要有两种：快速排序和基于堆实现的优先级队列（PriorityQueue）。</p><h3 id="Mapper阶段"><a href="#Mapper阶段" class="headerlink" title="Mapper阶段"></a>Mapper阶段</h3><p>从map输出到环形缓冲区的数据会被排序（这是MR框架中改良的快速排序），这个排序涉及partition和key，当缓冲区容量占用80%，会spill数据到磁盘，生成IFile文件，Map结束后，会将IFile文件排序合并成一个大文件（基于堆实现的优先级队列），以供不同的reduce来拉取相应的数据。</p><h3 id="Reducer阶段"><a href="#Reducer阶段" class="headerlink" title="Reducer阶段"></a>Reducer阶段</h3><p>从Mapper端取回的数据已是部分有序，Reduce Task只需进行一次归并排序即可保证数据整体有序。为了提高效率，Hadoop将sort阶段和reduce阶段并行化，在sort阶段，Reduce Task为内存和磁盘中的文件建立了小顶堆，保存了指向该小顶堆根节点的迭代器，并不断的移动迭代器，以将key相同的数据顺次交给reduce()函数处理，期间移动迭代器的过程实际上就是不断调整小顶堆的过程（建堆→取堆顶元素→重新建堆→取堆顶元素…），这样，sort和reduce可以并行进行。</p><h2 id="分组Top-N分析"><a href="#分组Top-N分析" class="headerlink" title="分组Top N分析"></a>分组Top N分析</h2><p>在数据处理中，经常会碰到这样一个场景，对表数据按照某一字段分组，然后找出各自组内最大的几条记录情形。针对这种分组Top N问题，我们利用Hive、MapReduce等多种工具实现一下。</p><h3 id="场景模拟"><a href="#场景模拟" class="headerlink" title="场景模拟"></a>场景模拟</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">computer,huangxiaoming,85,86,41,75,93,42,85</span><br><span class="line">computer,xuzheng,54,52,86,91,42</span><br><span class="line">computer,huangbo,85,42,96,38</span><br><span class="line">english,zhaobenshan,54,52,86,91,42,85,75</span><br><span class="line">english,liuyifei,85,41,75,21,85,96,14</span><br><span class="line">algorithm,liuyifei,75,85,62,48,54,96,15</span><br><span class="line">computer,huangjiaju,85,75,86,85,85</span><br><span class="line">english,liuyifei,76,95,86,74,68,74,48</span><br><span class="line">english,huangdatou,48,58,67,86,15,33,85</span><br><span class="line">algorithm,huanglei,76,95,86,74,68,74,48</span><br><span class="line">algorithm,huangjiaju,85,75,86,85,85,74,86</span><br><span class="line">computer,huangdatou,48,58,67,86,15,33,85</span><br><span class="line">english,zhouqi,85,86,41,75,93,42,85,75,55,47,22</span><br><span class="line">english,huangbo,85,42,96,38,55,47,22</span><br><span class="line">algorithm,liutao,85,75,85,99,66</span><br><span class="line">computer,huangzitao,85,86,41,75,93,42,85</span><br><span class="line">math,wangbaoqiang,85,86,41,75,93,42,85</span><br><span class="line">computer,liujialing,85,41,75,21,85,96,14,74,86</span><br><span class="line">computer,liuyifei,75,85,62,48,54,96,15</span><br><span class="line">computer,liutao,85,75,85,99,66,88,75,91</span><br><span class="line">computer,huanglei,76,95,86,74,68,74,48</span><br><span class="line">english,liujialing,75,85,62,48,54,96,15</span><br><span class="line">math,huanglei,76,95,86,74,68,74,48</span><br><span class="line">math,huangjiaju,85,75,86,85,85,74,86</span><br><span class="line">math,liutao,48,58,67,86,15,33,85</span><br><span class="line">english,huanglei,85,75,85,99,66,88,75,91</span><br><span class="line">math,xuzheng,54,52,86,91,42,85,75</span><br><span class="line">math,huangxiaoming,85,75,85,99,66,88,75,91</span><br><span class="line">math,liujialing,85,86,41,75,93,42,85,75</span><br><span class="line">english,huangxiaoming,85,86,41,75,93,42,85</span><br><span class="line">algorithm,huangdatou,48,58,67,86,15,33,85</span><br><span class="line">algorithm,huangzitao,85,86,41,75,93,42,85,75</span><br></pre></td></tr></table></figure><p> 一、数据解释</p><p>数据字段个数不固定：<br>第一个是课程名称，总共四个课程，computer，math，english，algorithm，<br>第二个是学生姓名，后面是每次考试的分数</p><p>二、统计需求：<br>1、统计每门课程的参考人数和课程平均分</p><p>2、统计每门课程参考学生的平均分，并且按课程存入不同的结果文件，要求一门课程一个结果文件，并且按平均分从高到低排序，分数保留一位小数</p><p>3、求出每门课程参考学生成绩最高的学生的信息：课程，姓名和平均分</p><p>第一题</p><p>CourseScoreMR1.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line">  1 import java.io.IOException;</span><br><span class="line">  2 </span><br><span class="line">  3 import org.apache.hadoop.conf.Configuration;</span><br><span class="line">  4 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">  5 import org.apache.hadoop.fs.Path;</span><br><span class="line">  6 import org.apache.hadoop.io.DoubleWritable;</span><br><span class="line">  7 import org.apache.hadoop.io.LongWritable;</span><br><span class="line">  8 import org.apache.hadoop.io.Text;</span><br><span class="line">  9 import org.apache.hadoop.mapreduce.Job;</span><br><span class="line"> 10 import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"> 11 import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"> 12 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"> 13 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"> 14 </span><br><span class="line"> 15 public class CourseScoreMR1 &#123;</span><br><span class="line"> 16 </span><br><span class="line"> 17     public static void main(String[] args) throws Exception &#123;</span><br><span class="line"> 18         </span><br><span class="line"> 19         Configuration conf = new Configuration();</span><br><span class="line"> 20         FileSystem fs = FileSystem.get(conf);</span><br><span class="line"> 21         Job job = Job.getInstance(conf);</span><br><span class="line"> 22         </span><br><span class="line"> 23         </span><br><span class="line"> 24         job.setJarByClass(CourseScoreMR1.class);</span><br><span class="line"> 25         job.setMapperClass(CourseScoreMR1Mapper.class);</span><br><span class="line"> 26         job.setReducerClass(CourseScoreMR1Reducer.class);</span><br><span class="line"> 27         </span><br><span class="line"> 28         job.setMapOutputKeyClass(Text.class);</span><br><span class="line"> 29         job.setMapOutputValueClass(DoubleWritable.class);</span><br><span class="line"> 30         job.setOutputKeyClass(Text.class);</span><br><span class="line"> 31         job.setOutputValueClass(Text.class);</span><br><span class="line"> 32         </span><br><span class="line"> 33         </span><br><span class="line"> 34         Path inputPath = new Path(&quot;E:\\bigdata\\cs\\input&quot;);</span><br><span class="line"> 35         Path outputPath = new Path(&quot;E:\\bigdata\\cs\\output_1&quot;);</span><br><span class="line"> 36         FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line"> 37         if(fs.exists(outputPath))&#123;</span><br><span class="line"> 38             fs.delete(outputPath, true);</span><br><span class="line"> 39         &#125;</span><br><span class="line"> 40         FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line"> 41         </span><br><span class="line"> 42         </span><br><span class="line"> 43         boolean isDone = job.waitForCompletion(true);</span><br><span class="line"> 44         System.exit(isDone ? 0 : 1);</span><br><span class="line"> 45     &#125;</span><br><span class="line"> 46     </span><br><span class="line"> 47     public static class CourseScoreMR1Mapper extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt;&#123;</span><br><span class="line"> 48         </span><br><span class="line"> 49         /**</span><br><span class="line"> 50          *  数据的三个字段： course , name, score</span><br><span class="line"> 51          * </span><br><span class="line"> 52          *  value == algorithm,huangzitao,85,86,41,75,93,42,85,75</span><br><span class="line"> 53          *  </span><br><span class="line"> 54          *  输出的key和value：</span><br><span class="line"> 55          *  </span><br><span class="line"> 56          *  key ： course</span><br><span class="line"> 57          *  </span><br><span class="line"> 58          *  value : avgScore</span><br><span class="line"> 59          *  </span><br><span class="line"> 60          *  格式化数值相关的操作的API ： NumberFormat</span><br><span class="line"> 61          *                      SimpleDateFormat</span><br><span class="line"> 62          */</span><br><span class="line"> 63         </span><br><span class="line"> 64         Text outKey = new Text();</span><br><span class="line"> 65         DoubleWritable outValue = new DoubleWritable();</span><br><span class="line"> 66         </span><br><span class="line"> 67         @Override</span><br><span class="line"> 68         protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line"> 69             </span><br><span class="line"> 70             String[] split = value.toString().split(&quot;,&quot;);</span><br><span class="line"> 71             </span><br><span class="line"> 72             String course = split[0];</span><br><span class="line"> 73             </span><br><span class="line"> 74             int sum = 0;</span><br><span class="line"> 75             int count = 0;</span><br><span class="line"> 76             </span><br><span class="line"> 77             for(int i = 2; i&lt;split.length; i++)&#123;</span><br><span class="line"> 78                 int tempScore = Integer.parseInt(split[i]);</span><br><span class="line"> 79                 sum += tempScore;</span><br><span class="line"> 80                 </span><br><span class="line"> 81                 count++;</span><br><span class="line"> 82             &#125;</span><br><span class="line"> 83             </span><br><span class="line"> 84             double avgScore = 1D * sum / count;</span><br><span class="line"> 85             </span><br><span class="line"> 86             </span><br><span class="line"> 87             outKey.set(course);</span><br><span class="line"> 88             outValue.set(avgScore);</span><br><span class="line"> 89             </span><br><span class="line"> 90             context.write(outKey, outValue);</span><br><span class="line"> 91         &#125;</span><br><span class="line"> 92         </span><br><span class="line"> 93     &#125;</span><br><span class="line"> 94     </span><br><span class="line"> 95     public static class CourseScoreMR1Reducer extends Reducer&lt;Text, DoubleWritable, Text, Text&gt;&#123;</span><br><span class="line"> 96         </span><br><span class="line"> 97         </span><br><span class="line"> 98         Text outValue = new Text();</span><br><span class="line"> 99         /**</span><br><span class="line">100          * key :  course</span><br><span class="line">101          * </span><br><span class="line">102          * values : 98.7   87.6</span><br><span class="line">103          */</span><br><span class="line">104         @Override</span><br><span class="line">105         protected void reduce(Text key, Iterable&lt;DoubleWritable&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">106             </span><br><span class="line">107             double sum = 0;</span><br><span class="line">108             int count = 0;</span><br><span class="line">109             </span><br><span class="line">110             for(DoubleWritable dw : values)&#123;</span><br><span class="line">111                 sum += dw.get();</span><br><span class="line">112                 count ++;</span><br><span class="line">113             &#125;</span><br><span class="line">114             </span><br><span class="line">115             double lastAvgScore = sum / count;</span><br><span class="line">116             </span><br><span class="line">117             outValue.set(count+&quot;\t&quot; + lastAvgScore);</span><br><span class="line">118             </span><br><span class="line">119             context.write(key, outValue);</span><br><span class="line">120         &#125;</span><br><span class="line">121     &#125;</span><br><span class="line">122 &#125;</span><br></pre></td></tr></table></figure><p>第二题</p><p>CourseScoreMR2.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"> 1 import java.io.IOException;</span><br><span class="line"> 2 </span><br><span class="line"> 3 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 4 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 5 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 6 import org.apache.hadoop.io.LongWritable;</span><br><span class="line"> 7 import org.apache.hadoop.io.NullWritable;</span><br><span class="line"> 8 import org.apache.hadoop.io.Text;</span><br><span class="line"> 9 import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">10 import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">11 import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">12 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">13 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line">14 </span><br><span class="line">15 import com.ghgj.mr.exercise.pojo.CourseScore;</span><br><span class="line">16 import com.ghgj.mr.exercise.ptn.CSPartitioner;</span><br><span class="line">17 </span><br><span class="line">18 public class CourseScoreMR2&#123;</span><br><span class="line">19 </span><br><span class="line">20     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">21         </span><br><span class="line">22         Configuration conf = new Configuration();</span><br><span class="line">23 </span><br><span class="line">24         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">25         Job job = Job.getInstance(conf);</span><br><span class="line">26         </span><br><span class="line">27         </span><br><span class="line">28         job.setJarByClass(CourseScoreMR2.class);</span><br><span class="line">29         job.setMapperClass(CourseScoreMR2Mapper.class);</span><br><span class="line">30 //        job.setReducerClass(CourseScoreMR2Reducer.class);</span><br><span class="line">31         </span><br><span class="line">32         job.setMapOutputKeyClass(CourseScore.class);</span><br><span class="line">33         job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">34 //        job.setOutputKeyClass(CourseScore.class);</span><br><span class="line">35 //        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">36         </span><br><span class="line">37         </span><br><span class="line">38         job.setPartitionerClass(CSPartitioner.class);</span><br><span class="line">39         job.setNumReduceTasks(4);</span><br><span class="line">40         </span><br><span class="line">41         </span><br><span class="line">42         Path inputPath = new Path(&quot;E:\\bigdata\\cs\\input&quot;);</span><br><span class="line">43         Path outputPath = new Path(&quot;E:\\bigdata\\cs\\output_2&quot;);</span><br><span class="line">44         FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">45         if(fs.exists(outputPath))&#123;</span><br><span class="line">46             fs.delete(outputPath, true);</span><br><span class="line">47         &#125;</span><br><span class="line">48         FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line">49         </span><br><span class="line">50         </span><br><span class="line">51         boolean isDone = job.waitForCompletion(true);</span><br><span class="line">52         System.exit(isDone ? 0 : 1);</span><br><span class="line">53     &#125;</span><br><span class="line">54     </span><br><span class="line">55     public static class CourseScoreMR2Mapper extends Mapper&lt;LongWritable, Text, CourseScore, NullWritable&gt;&#123;</span><br><span class="line">56         </span><br><span class="line">57         CourseScore cs = new CourseScore();</span><br><span class="line">58         </span><br><span class="line">59         /**</span><br><span class="line">60          * value =  math,huangxiaoming,85,75,85,99,66,88,75,91</span><br><span class="line">61          */</span><br><span class="line">62         @Override</span><br><span class="line">63         protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">64             </span><br><span class="line">65             String[] split = value.toString().split(&quot;,&quot;);</span><br><span class="line">66             </span><br><span class="line">67             String course = split[0];</span><br><span class="line">68             String name = split[1];</span><br><span class="line">69             </span><br><span class="line">70             int sum = 0;</span><br><span class="line">71             int count = 0;</span><br><span class="line">72             </span><br><span class="line">73             for(int i = 2; i&lt;split.length; i++)&#123;</span><br><span class="line">74                 int tempScore = Integer.parseInt(split[i]);</span><br><span class="line">75                 sum += tempScore;</span><br><span class="line">76                 </span><br><span class="line">77                 count++;</span><br><span class="line">78             &#125;</span><br><span class="line">79             </span><br><span class="line">80             double avgScore = 1D * sum / count;</span><br><span class="line">81             </span><br><span class="line">82             cs.setCourse(course);</span><br><span class="line">83             cs.setName(name);</span><br><span class="line">84             cs.setScore(avgScore);</span><br><span class="line">85             </span><br><span class="line">86             context.write(cs, NullWritable.get());</span><br><span class="line">87         &#125;</span><br><span class="line">88         </span><br><span class="line">89     &#125;</span><br><span class="line">90     </span><br><span class="line">91     public static class CourseScoreMR2Reducer extends Reducer&lt;CourseScore, NullWritable, CourseScore, NullWritable&gt;&#123;</span><br><span class="line">92         </span><br><span class="line">93         @Override</span><br><span class="line">94         protected void reduce(CourseScore key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">95             </span><br><span class="line">96             </span><br><span class="line">97         &#125;</span><br><span class="line">98     &#125;</span><br><span class="line">99 &#125;</span><br></pre></td></tr></table></figure><p>CSPartitioner.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"> 1 import org.apache.hadoop.io.NullWritable;</span><br><span class="line"> 2 import org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"> 3 </span><br><span class="line"> 4 import com.ghgj.mr.exercise.pojo.CourseScore;</span><br><span class="line"> 5 </span><br><span class="line"> 6 public class CSPartitioner extends Partitioner&lt;CourseScore,NullWritable&gt;&#123;</span><br><span class="line"> 7 </span><br><span class="line"> 8     /**</span><br><span class="line"> 9      * </span><br><span class="line">10      */</span><br><span class="line">11     @Override</span><br><span class="line">12     public int getPartition(CourseScore key, NullWritable value, int numPartitions) &#123;</span><br><span class="line">13 </span><br><span class="line">14         String course = key.getCourse();</span><br><span class="line">15         if(course.equals(&quot;math&quot;))&#123;</span><br><span class="line">16             return 0;</span><br><span class="line">17         &#125;else if(course.equals(&quot;english&quot;))&#123;</span><br><span class="line">18             return 1;</span><br><span class="line">19         &#125;else if(course.equals(&quot;computer&quot;))&#123;</span><br><span class="line">20             return 2;</span><br><span class="line">21         &#125;else&#123;</span><br><span class="line">22             return 3;</span><br><span class="line">23         &#125;</span><br><span class="line">24         </span><br><span class="line">25         </span><br><span class="line">26     &#125;</span><br><span class="line">27 </span><br><span class="line">28     </span><br><span class="line">29 &#125;</span><br></pre></td></tr></table></figure><p> 第三题</p><p>CourseScoreMR3.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line">  1 import java.io.IOException;</span><br><span class="line">  2 </span><br><span class="line">  3 import org.apache.hadoop.conf.Configuration;</span><br><span class="line">  4 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">  5 import org.apache.hadoop.fs.Path;</span><br><span class="line">  6 import org.apache.hadoop.io.LongWritable;</span><br><span class="line">  7 import org.apache.hadoop.io.NullWritable;</span><br><span class="line">  8 import org.apache.hadoop.io.Text;</span><br><span class="line">  9 import org.apache.hadoop.mapreduce.Job;</span><br><span class="line"> 10 import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"> 11 import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"> 12 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"> 13 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"> 14 </span><br><span class="line"> 15 import com.ghgj.mr.exercise.gc.CourseScoreGC;</span><br><span class="line"> 16 import com.ghgj.mr.exercise.pojo.CourseScore;</span><br><span class="line"> 17 </span><br><span class="line"> 18 public class CourseScoreMR3&#123; </span><br><span class="line"> 19     </span><br><span class="line"> 20     private static final int TOPN = 3;</span><br><span class="line"> 21 </span><br><span class="line"> 22     public static void main(String[] args) throws Exception &#123;</span><br><span class="line"> 23         </span><br><span class="line"> 24         Configuration conf = new Configuration();</span><br><span class="line"> 25         FileSystem fs = FileSystem.get(conf);</span><br><span class="line"> 26         Job job = Job.getInstance(conf);</span><br><span class="line"> 27         </span><br><span class="line"> 28         </span><br><span class="line"> 29         job.setJarByClass(CourseScoreMR3.class);</span><br><span class="line"> 30         job.setMapperClass(CourseScoreMR2Mapper.class);</span><br><span class="line"> 31         job.setReducerClass(CourseScoreMR2Reducer.class);</span><br><span class="line"> 32         </span><br><span class="line"> 33         job.setMapOutputKeyClass(CourseScore.class);</span><br><span class="line"> 34         job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"> 35         job.setOutputKeyClass(CourseScore.class);</span><br><span class="line"> 36         job.setOutputValueClass(NullWritable.class);</span><br><span class="line"> 37         </span><br><span class="line"> 38         </span><br><span class="line"> 39 //        job.setPartitionerClass(CSPartitioner.class);</span><br><span class="line"> 40 //        job.setNumReduceTasks(4);</span><br><span class="line"> 41         </span><br><span class="line"> 42         </span><br><span class="line"> 43         // 指定分组规则</span><br><span class="line"> 44         job.setGroupingComparatorClass(CourseScoreGC.class);</span><br><span class="line"> 45         </span><br><span class="line"> 46         </span><br><span class="line"> 47         Path inputPath = new Path(&quot;E:\\bigdata\\cs\\input&quot;);</span><br><span class="line"> 48         Path outputPath = new Path(&quot;E:\\bigdata\\cs\\output_3_last&quot;);</span><br><span class="line"> 49         FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line"> 50         if(fs.exists(outputPath))&#123;</span><br><span class="line"> 51             fs.delete(outputPath, true);</span><br><span class="line"> 52         &#125;</span><br><span class="line"> 53         FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line"> 54         </span><br><span class="line"> 55         </span><br><span class="line"> 56         boolean isDone = job.waitForCompletion(true);</span><br><span class="line"> 57         System.exit(isDone ? 0 : 1);</span><br><span class="line"> 58     &#125;</span><br><span class="line"> 59     </span><br><span class="line"> 60     public static class CourseScoreMR2Mapper extends Mapper&lt;LongWritable, Text, CourseScore, NullWritable&gt;&#123;</span><br><span class="line"> 61         </span><br><span class="line"> 62         CourseScore cs = new CourseScore();</span><br><span class="line"> 63         </span><br><span class="line"> 64         /**</span><br><span class="line"> 65          * value =  math,huangxiaoming,85,75,85,99,66,88,75,91</span><br><span class="line"> 66          */</span><br><span class="line"> 67         @Override</span><br><span class="line"> 68         protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line"> 69             </span><br><span class="line"> 70             String[] split = value.toString().split(&quot;,&quot;);</span><br><span class="line"> 71             </span><br><span class="line"> 72             String course = split[0];</span><br><span class="line"> 73             String name = split[1];</span><br><span class="line"> 74             </span><br><span class="line"> 75             int sum = 0;</span><br><span class="line"> 76             int count = 0;</span><br><span class="line"> 77             </span><br><span class="line"> 78             for(int i = 2; i&lt;split.length; i++)&#123;</span><br><span class="line"> 79                 int tempScore = Integer.parseInt(split[i]);</span><br><span class="line"> 80                 sum += tempScore;</span><br><span class="line"> 81                 </span><br><span class="line"> 82                 count++;</span><br><span class="line"> 83             &#125;</span><br><span class="line"> 84             </span><br><span class="line"> 85             double avgScore = 1D * sum / count;</span><br><span class="line"> 86             </span><br><span class="line"> 87             cs.setCourse(course);</span><br><span class="line"> 88             cs.setName(name);</span><br><span class="line"> 89             cs.setScore(avgScore);</span><br><span class="line"> 90             </span><br><span class="line"> 91             context.write(cs, NullWritable.get());</span><br><span class="line"> 92         &#125;</span><br><span class="line"> 93         </span><br><span class="line"> 94     &#125;</span><br><span class="line"> 95     </span><br><span class="line"> 96     public static class CourseScoreMR2Reducer extends Reducer&lt;CourseScore, NullWritable, CourseScore, NullWritable&gt;&#123;</span><br><span class="line"> 97         </span><br><span class="line"> 98         int count = 0;</span><br><span class="line"> 99         </span><br><span class="line">100         /**</span><br><span class="line">101          * reducer阶段的reduce方法的调用参数：key相同的额一组key-vlaue</span><br><span class="line">102          * </span><br><span class="line">103          * redcuer阶段，每次遇到一个不同的key的key_value组， 那么reduce方法就会被调用一次。</span><br><span class="line">104          * </span><br><span class="line">105          * </span><br><span class="line">106          * values这个迭代器只能迭代一次。</span><br><span class="line">107          * values迭代器在迭代的过程中迭代出来的value会变，同时，这个value所对应的key也会跟着变         合理</span><br><span class="line">108          * </span><br><span class="line">109          */</span><br><span class="line">110         @Override</span><br><span class="line">111         protected void reduce(CourseScore key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">112             </span><br><span class="line">113             </span><br><span class="line">114             int count = 0;</span><br><span class="line">115             </span><br><span class="line">116             for(NullWritable nvl : values)&#123;</span><br><span class="line">117                 System.out.println(&quot;*********************************** &quot; + (++count) + &quot;      &quot; + key.toString());</span><br><span class="line">118                 </span><br><span class="line">119                 if(count == 3)&#123;</span><br><span class="line">120                     return;</span><br><span class="line">121                 &#125;</span><br><span class="line">122             &#125;</span><br><span class="line">123             </span><br><span class="line">124             </span><br><span class="line">125             // 原样输出</span><br><span class="line">126             /*for(NullWritable nvl :  values)&#123;</span><br><span class="line">127                 context.write(key, nvl);</span><br><span class="line">128             &#125;*/</span><br><span class="line">129             </span><br><span class="line">130             </span><br><span class="line">131             // 输出每门课程的最高分数   ,  预期结果中，key的显示都是一样的</span><br><span class="line">132 //            for(NullWritable nvl : values)&#123;</span><br><span class="line">133 //                System.out.println(key + &quot; - &quot; nvl);</span><br><span class="line">134 //                </span><br><span class="line">135 //                valueList.add(nvl);</span><br><span class="line">136 //            &#125;</span><br><span class="line">137             </span><br><span class="line">138 //            List&lt;Value&gt; valueList = null;</span><br><span class="line">139             // 预期结果中，key的显示都是一样的</span><br><span class="line">140             /*int count = 0;</span><br><span class="line">141             for(NullWritable nvl : values)&#123;</span><br><span class="line">142                 count++;</span><br><span class="line">143             &#125;</span><br><span class="line">144             for(int i = 0; i&lt;count; i++)&#123;</span><br><span class="line">145             valueList.get(i) = value</span><br><span class="line">146                 System.out.println(key + &quot; - &quot;+ value);</span><br><span class="line">147             &#125;*/</span><br><span class="line">148             </span><br><span class="line">149             </span><br><span class="line">150 //            math hello 1</span><br><span class="line">151 //            math hi  2</span><br><span class="line">152         &#125;</span><br><span class="line">153     &#125;</span><br><span class="line">154 &#125;</span><br></pre></td></tr></table></figure><p>CourseScoreGC.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"> 1 import org.apache.hadoop.io.WritableComparable;</span><br><span class="line"> 2 import org.apache.hadoop.io.WritableComparator;</span><br><span class="line"> 3 </span><br><span class="line"> 4 import com.ghgj.mr.exercise.pojo.CourseScore;</span><br><span class="line"> 5 </span><br><span class="line"> 6 /**</span><br><span class="line"> 7  *  分组规则的指定</span><br><span class="line"> 8  */</span><br><span class="line"> 9 public class CourseScoreGC extends WritableComparator&#123;</span><br><span class="line">10     </span><br><span class="line">11     public CourseScoreGC()&#123;</span><br><span class="line">12         super(CourseScore.class, true);</span><br><span class="line">13     &#125;</span><br><span class="line">14 </span><br><span class="line">15     /**</span><br><span class="line">16      *   </span><br><span class="line">17      *   方法的定义解释：</span><br><span class="line">18      *   </span><br><span class="line">19      *   方法的意义：一般来说，都可以从方法名找到一些提示</span><br><span class="line">20      *   方法的参数：将来你的MR程序中，要作为key的两个对象，是否是相同的对象</span><br><span class="line">21      *   方法的返回值： 返回值类型为int  当返回值为0的时候。证明， 两个参数对象，经过比较之后，是同一个对象</span><br><span class="line">22      *   </span><br><span class="line">23      *   在我们的需求中： 分组规则是  Course</span><br><span class="line">24      * </span><br><span class="line">25      */</span><br><span class="line">26     @Override</span><br><span class="line">27     public int compare(WritableComparable a, WritableComparable b) &#123;</span><br><span class="line">28 </span><br><span class="line">29         CourseScore cs1 = (CourseScore)a;</span><br><span class="line">30         CourseScore cs2 = (CourseScore)b;</span><br><span class="line">31         </span><br><span class="line">32         int compareTo = cs1.getCourse().compareTo(cs2.getCourse());</span><br><span class="line">33         </span><br><span class="line">34         return compareTo;</span><br><span class="line">35     &#125;</span><br><span class="line">36 &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（十九）MapReduce框架排序</title>
      <link href="/2018-04-19-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89MapReduce%E6%A1%86%E6%9E%B6%E6%8E%92%E5%BA%8F.html"/>
      <url>/2018-04-19-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89MapReduce%E6%A1%86%E6%9E%B6%E6%8E%92%E5%BA%8F.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（十九）MapReduce框架排序：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（十九）MapReduce框架排序</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="流量统计项目案例"><a href="#流量统计项目案例" class="headerlink" title="流量统计项目案例"></a>流量统计项目案例</h2><p>样本示例</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180316200328379-1669863657.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180316200409522-1315468418.png" alt="img"></p><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><p>1、 统计每一个用户（手机号）所耗费的总上行流量、总下行流量，总流量</p><p>2、 得出上题结果的基础之上再加一个需求：将统计结果按照总流量倒序排序</p><p>3、 将流量汇总统计结果按照手机归属地不同省份输出到不同文件中</p><p>第一题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> *    第一题：统计每一个用户（手机号）所耗费的总上行流量、总下行流量，总流量</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">public class FlowSumMR &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf, &quot;FlowSumMR&quot;);</span><br><span class="line">        job.setJarByClass(FlowSumMR.class);</span><br><span class="line">        </span><br><span class="line">        job.setMapperClass(FlowSumMRMapper.class);</span><br><span class="line">        job.setReducerClass(FlowSumMRReducer.class);</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(Text.class);</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        FileInputFormat.setInputPaths(job, new Path(&quot;E:/bigdata/flow/input/&quot;));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(&quot;E:/bigdata/flow/output_sum&quot;));</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        boolean isDone = job.waitForCompletion(true);</span><br><span class="line">        System.exit(isDone ? 0 : 1);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static class FlowSumMRMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123;</span><br><span class="line">        </span><br><span class="line">        /**</span><br><span class="line">         * value  =  1363157993044     18211575961    94-71-AC-CD-E6-18:CMCC-EASY    120.196.100.99    </span><br><span class="line">         * iface.qiyi.com    视频网站    15    12    1527    2106    200</span><br><span class="line">         */</span><br><span class="line">        @Override</span><br><span class="line">        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            String[] split = value.toString().split(&quot;\t&quot;);</span><br><span class="line">            </span><br><span class="line">            String outkey = split[1];</span><br><span class="line">            </span><br><span class="line">            String outValue = split[8] + &quot;\t&quot; + split[9];</span><br><span class="line">            </span><br><span class="line">            context.write(new Text(outkey), new Text(outValue));</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static class FlowSumMRReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123;</span><br><span class="line">        </span><br><span class="line">        @Override</span><br><span class="line">        protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            </span><br><span class="line">            int upFlow = 0;</span><br><span class="line">            int downFlow = 0;</span><br><span class="line">            int sumFlow = 0;</span><br><span class="line">            </span><br><span class="line">            for(Text t : values)&#123;</span><br><span class="line">                String[] split = t.toString().split(&quot;\t&quot;);</span><br><span class="line">                </span><br><span class="line">                int upTempFlow = Integer.parseInt(split[0]);</span><br><span class="line">                int downTempFlow = Integer.parseInt(split[1]);</span><br><span class="line">                </span><br><span class="line">                upFlow+=upTempFlow;</span><br><span class="line">                downFlow +=  downTempFlow;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            sumFlow = upFlow + downFlow;</span><br><span class="line">            </span><br><span class="line">            context.write(key, new Text(upFlow + &quot;\t&quot; + downFlow + &quot;\t&quot; + sumFlow));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第二题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">import comg.ghgj.mr.pojo.FlowBean;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 需求： 第二个题目，就是对第一个题目的结果数据，进行按照总流量倒叙排序</span><br><span class="line"> * </span><br><span class="line"> * </span><br><span class="line"> */</span><br><span class="line">public class FlowSortMR &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        </span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf, &quot;FlowSumMR&quot;);</span><br><span class="line">        job.setJarByClass(FlowSortMR.class);</span><br><span class="line">        </span><br><span class="line">        job.setMapperClass(FlowSortMRMapper.class);</span><br><span class="line">        job.setReducerClass(FlowSortMRReducer.class);</span><br><span class="line">        </span><br><span class="line">        job.setOutputKeyClass(FlowBean.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        FileInputFormat.setInputPaths(job, new Path(&quot;E:/bigdata/flow/output_sum&quot;));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(&quot;E:/bigdata/flow/output_sort_777&quot;));</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        boolean isDone = job.waitForCompletion(true);</span><br><span class="line">        System.exit(isDone ? 0 : 1);</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static class FlowSortMRMapper extends Mapper&lt;LongWritable, Text, FlowBean, NullWritable&gt;&#123;</span><br><span class="line">        </span><br><span class="line">        /**</span><br><span class="line">         * value  = 13602846565    26860680    40332600    67193280</span><br><span class="line">         */</span><br><span class="line">        @Override</span><br><span class="line">        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            </span><br><span class="line">            String[] split = value.toString().split(&quot;\t&quot;);</span><br><span class="line">            </span><br><span class="line">            FlowBean fb = new FlowBean(split[0], Long.parseLong(split[1]), Long.parseLong(split[2]));</span><br><span class="line">            </span><br><span class="line">            context.write(fb, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static class FlowSortMRReducer extends Reducer&lt;FlowBean, NullWritable, FlowBean, NullWritable&gt;&#123;</span><br><span class="line">        </span><br><span class="line">        @Override</span><br><span class="line">        protected void reduce(FlowBean key, Iterable&lt;NullWritable&gt; values, Context context)</span><br><span class="line">                throws IOException, InterruptedException &#123;</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            for(NullWritable nvl : values)&#123;</span><br><span class="line">                context.write(key, nvl);</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>FlowBean.java</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><span class="line">  1 import java.io.DataInput;</span><br><span class="line">  2 import java.io.DataOutput;</span><br><span class="line">  3 import java.io.IOException;</span><br><span class="line">  4 </span><br><span class="line">  5 import org.apache.hadoop.io.WritableComparable;</span><br><span class="line">  6 </span><br><span class="line">  7 /**</span><br><span class="line">  8  * 第一，定义好属性</span><br><span class="line">  9  * 第二，定义好属性的getter 和 setter方法</span><br><span class="line"> 10  * 第三，定义好构造方法（有参，无参）</span><br><span class="line"> 11  * 第四：定义好toString();</span><br><span class="line"> 12  * </span><br><span class="line"> 13  * </span><br><span class="line"> 14  * 详细解释：</span><br><span class="line"> 15  * </span><br><span class="line"> 16  * 如果一个自定义对象要作为key 必须要实现 WritableComparable 接口， 而不能实现 Writable, Comparable</span><br><span class="line"> 17  * </span><br><span class="line"> 18  * 如果一个自定义对象要作为value，那么只需要实现Writable接口即可</span><br><span class="line"> 19  */</span><br><span class="line"> 20 public class FlowBean implements WritableComparable&lt;FlowBean&gt;&#123;</span><br><span class="line"> 21 //public class FlowBean implements Comparable&lt;FlowBean&gt;&#123;</span><br><span class="line"> 22 </span><br><span class="line"> 23     private String phone;</span><br><span class="line"> 24     private long upFlow;</span><br><span class="line"> 25     private long downFlow;</span><br><span class="line"> 26     private long sumFlow;</span><br><span class="line"> 27     public String getPhone() &#123;</span><br><span class="line"> 28         return phone;</span><br><span class="line"> 29     &#125;</span><br><span class="line"> 30     public void setPhone(String phone) &#123;</span><br><span class="line"> 31         this.phone = phone;</span><br><span class="line"> 32     &#125;</span><br><span class="line"> 33     public long getUpFlow() &#123;</span><br><span class="line"> 34         return upFlow;</span><br><span class="line"> 35     &#125;</span><br><span class="line"> 36     public void setUpFlow(long upFlow) &#123;</span><br><span class="line"> 37         this.upFlow = upFlow;</span><br><span class="line"> 38     &#125;</span><br><span class="line"> 39     public long getDownFlow() &#123;</span><br><span class="line"> 40         return downFlow;</span><br><span class="line"> 41     &#125;</span><br><span class="line"> 42     public void setDownFlow(long downFlow) &#123;</span><br><span class="line"> 43         this.downFlow = downFlow;</span><br><span class="line"> 44     &#125;</span><br><span class="line"> 45     public long getSumFlow() &#123;</span><br><span class="line"> 46         return sumFlow;</span><br><span class="line"> 47     &#125;</span><br><span class="line"> 48     public void setSumFlow(long sumFlow) &#123;</span><br><span class="line"> 49         this.sumFlow = sumFlow;</span><br><span class="line"> 50     &#125;</span><br><span class="line"> 51     public FlowBean(String phone, long upFlow, long downFlow, long sumFlow) &#123;</span><br><span class="line"> 52         super();</span><br><span class="line"> 53         this.phone = phone;</span><br><span class="line"> 54         this.upFlow = upFlow;</span><br><span class="line"> 55         this.downFlow = downFlow;</span><br><span class="line"> 56         this.sumFlow = sumFlow;</span><br><span class="line"> 57     &#125;</span><br><span class="line"> 58     public FlowBean(String phone, long upFlow, long downFlow) &#123;</span><br><span class="line"> 59         super();</span><br><span class="line"> 60         this.phone = phone;</span><br><span class="line"> 61         this.upFlow = upFlow;</span><br><span class="line"> 62         this.downFlow = downFlow;</span><br><span class="line"> 63         this.sumFlow = upFlow + downFlow;</span><br><span class="line"> 64     &#125;</span><br><span class="line"> 65     public FlowBean() &#123;</span><br><span class="line"> 66         super();</span><br><span class="line"> 67         // TODO Auto-generated constructor stub</span><br><span class="line"> 68     &#125;</span><br><span class="line"> 69     @Override</span><br><span class="line"> 70     public String toString() &#123;</span><br><span class="line"> 71         return  phone + &quot;\t&quot; + upFlow + &quot;\t&quot; + downFlow + &quot;\t&quot; + sumFlow;</span><br><span class="line"> 72     &#125;</span><br><span class="line"> 73     </span><br><span class="line"> 74     </span><br><span class="line"> 75     </span><br><span class="line"> 76     </span><br><span class="line"> 77     /**</span><br><span class="line"> 78      * 把当前这个对象 --- 谁掉用这个write方法，谁就是当前对象</span><br><span class="line"> 79      * </span><br><span class="line"> 80      * FlowBean bean = new FlowBean();</span><br><span class="line"> 81      * </span><br><span class="line"> 82      * bean.write(out)    把bean这个对象的四个属性序列化出去</span><br><span class="line"> 83      * </span><br><span class="line"> 84      *  this = bean</span><br><span class="line"> 85      */</span><br><span class="line"> 86     @Override</span><br><span class="line"> 87     public void write(DataOutput out) throws IOException &#123;</span><br><span class="line"> 88         // TODO Auto-generated method stub</span><br><span class="line"> 89         </span><br><span class="line"> 90         out.writeUTF(phone);</span><br><span class="line"> 91         out.writeLong(upFlow);</span><br><span class="line"> 92         out.writeLong(downFlow);</span><br><span class="line"> 93         out.writeLong(sumFlow);</span><br><span class="line"> 94         </span><br><span class="line"> 95     &#125;</span><br><span class="line"> 96     </span><br><span class="line"> 97     </span><br><span class="line"> 98     //   序列化方法中的写出的字段顺序， 一定一定一定要和 反序列化中的 接收顺序一致。 类型也一定要一致</span><br><span class="line"> 99     </span><br><span class="line">100     </span><br><span class="line">101     /**</span><br><span class="line">102      * bean.readField();</span><br><span class="line">103      * </span><br><span class="line">104      *             upFlow = </span><br><span class="line">105      */</span><br><span class="line">106     @Override</span><br><span class="line">107     public void readFields(DataInput in) throws IOException &#123;</span><br><span class="line">108         // TODO Auto-generated method stub</span><br><span class="line">109         </span><br><span class="line">110         phone = in.readUTF();</span><br><span class="line">111         upFlow = in.readLong();</span><br><span class="line">112         downFlow = in.readLong();</span><br><span class="line">113         sumFlow = in.readLong();</span><br><span class="line">114         </span><br><span class="line">115     &#125;</span><br><span class="line">116     </span><br><span class="line">117     </span><br><span class="line">118     </span><br><span class="line">119     /**</span><br><span class="line">120      * Hadoop的序列化机制为什么不用   java自带的实现 Serializable这种方式？</span><br><span class="line">121      * </span><br><span class="line">122      * 本身Hadoop就是用来解决大数据问题的。</span><br><span class="line">123      * </span><br><span class="line">124      * 那么实现Serializable接口这种方式，在进行序列化的时候。除了会序列化属性值之外，还会携带很多跟当前这个对象的类相关的各种信息</span><br><span class="line">125      * </span><br><span class="line">126      * Hadoop采取了一种全新的序列化机制；只需要序列化 每个对象的属性值即可。</span><br><span class="line">127      */</span><br><span class="line">128     </span><br><span class="line">129     </span><br><span class="line">130     </span><br><span class="line">131     /*@Override</span><br><span class="line">132       public void readFields(DataInput in) throws IOException &#123;</span><br><span class="line">133         value = in.readLong();</span><br><span class="line">134       &#125;</span><br><span class="line">135 </span><br><span class="line">136       @Override</span><br><span class="line">137       public void write(DataOutput out) throws IOException &#123;</span><br><span class="line">138         out.writeLong(value);</span><br><span class="line">139       &#125;*/</span><br><span class="line">140     </span><br><span class="line">141     </span><br><span class="line">142     /**</span><br><span class="line">143      * 用来指定排序规则</span><br><span class="line">144      */</span><br><span class="line">145     @Override</span><br><span class="line">146     public int compareTo(FlowBean fb) &#123;</span><br><span class="line">147 </span><br><span class="line">148         long diff = this.getSumFlow() - fb.getSumFlow();</span><br><span class="line">149         </span><br><span class="line">150         if(diff == 0)&#123;</span><br><span class="line">151             return 0;</span><br><span class="line">152         &#125;else&#123;</span><br><span class="line">153             return diff &gt; 0 ? -1 : 1;</span><br><span class="line">154         &#125;</span><br><span class="line">155         </span><br><span class="line">156     &#125;</span><br><span class="line">157 &#125;</span><br></pre></td></tr></table></figure><p>第三题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">package comg.ghgj.mr.flow;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.partition.ProvincePartitioner;</span><br><span class="line"></span><br><span class="line">public class FlowPartitionerMR &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line">        FileSystem fs = FileSystem.get(conf);</span><br><span class="line">        Job job = Job.getInstance(conf, &quot;FlowSumMR&quot;);</span><br><span class="line">        job.setJarByClass(FlowPartitionerMR.class);</span><br><span class="line">        </span><br><span class="line">        job.setMapperClass(FlowPartitionerMRMapper.class);</span><br><span class="line">        job.setReducerClass(FlowPartitionerMRReducer.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(Text.class);</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        /**</span><br><span class="line">         * 非常重要的两句代码</span><br><span class="line">         */</span><br><span class="line">        job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line">        job.setNumReduceTasks(10);</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        FileInputFormat.setInputPaths(job, new Path(&quot;E:\\bigdata\\flow\\input&quot;));</span><br><span class="line">        Path outputPath = new Path(&quot;E:\\bigdata\\flow\\output_ptn2&quot;);</span><br><span class="line">        if(fs.exists(outputPath))&#123;</span><br><span class="line">            fs.delete(outputPath, true);</span><br><span class="line">        &#125;</span><br><span class="line">        FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        boolean isDone = job.waitForCompletion(true);</span><br><span class="line">        System.exit(isDone ? 0 : 1);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static class FlowPartitionerMRMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123;</span><br><span class="line">        </span><br><span class="line">        /**</span><br><span class="line">         * value  =  13502468823    101663100    1529437140    1631100240</span><br><span class="line">         */</span><br><span class="line">        @Override</span><br><span class="line">        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            String[] split = value.toString().split(&quot;\t&quot;);</span><br><span class="line">            </span><br><span class="line">            String outkey = split[1];</span><br><span class="line">            String outValue = split[8] + &quot;\t&quot; + split[9];</span><br><span class="line">            </span><br><span class="line">            context.write(new Text(outkey), new Text(outValue));</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static class FlowPartitionerMRReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123;</span><br><span class="line">        </span><br><span class="line">        @Override</span><br><span class="line">        protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            </span><br><span class="line">            int upFlow = 0;</span><br><span class="line">            int downFlow = 0;</span><br><span class="line">            int sumFlow = 0;</span><br><span class="line">            </span><br><span class="line">            for(Text t : values)&#123;</span><br><span class="line">                String[] split = t.toString().split(&quot;\t&quot;);</span><br><span class="line">                </span><br><span class="line">                int upTempFlow = Integer.parseInt(split[0]);</span><br><span class="line">                int downTempFlow = Integer.parseInt(split[1]);</span><br><span class="line">                </span><br><span class="line">                upFlow+=upTempFlow;</span><br><span class="line">                downFlow +=  downTempFlow;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            sumFlow = upFlow + downFlow;</span><br><span class="line">            </span><br><span class="line">            context.write(key, new Text(upFlow + &quot;\t&quot; + downFlow + &quot;\t&quot; + sumFlow));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（十八）MapReduce框架Combiner分区</title>
      <link href="/2018-04-18-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89MapReduce%E6%A1%86%E6%9E%B6Combiner%E5%88%86%E5%8C%BA.html"/>
      <url>/2018-04-18-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89MapReduce%E6%A1%86%E6%9E%B6Combiner%E5%88%86%E5%8C%BA.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（十八）MapReduce框架Combiner分区：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（十八）MapReduce框架Combiner分区</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="对combiner的理解"><a href="#对combiner的理解" class="headerlink" title="对combiner的理解"></a>对combiner的理解</h2><p>combiner其实属于优化方案，由于带宽限制，应该尽量map和reduce之间的数据传输数量。它在Map端把同一个key的键值对合并在一起并计算，计算规则与reduce一致，所以combiner也可以看作特殊的Reducer。</p><p>执行combiner操作要求开发者必须在程序中设置了combiner(程序中通过job.setCombinerClass(myCombine.class)自定义combiner操作)。</p><p>Combiner组件是用来做局部汇总的，就在mapTask中进行汇总；Reducer组件是用来做全局汇总的，最终的，最后一次汇总。</p><h2 id="哪里使用combiner"><a href="#哪里使用combiner" class="headerlink" title="哪里使用combiner?"></a>哪里使用combiner?</h2><p>1，map输出数据根据分区排序完成后，在写入文件之前会执行一次combine操作(前提是作业中设置了这个操作);</p><p>2，如果map输出比较大，溢出文件个数大于3(此值可以通过属性min.num.spills.for.combine配置)时，在merge的过程(多个spill文件合并为一个大文件)中前还会执行combiner操作;</p><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>不是每种作业都可以做combiner操作的，只有满足以下条件才可以：</p><p>1、Combiner 只能对 一个mapTask的中间结果进行汇总</p><p>2、如果想使用Reducer直接充当Combiner，那么必须满足： Reducer的输入和输出key-value类型是一致的。</p><blockquote><p>1）处于两个不同节点的mapTask的结果不能combiner到一起</p><p>2）处于同一个节点的两个MapTask的结果不能否combiner到一起</p><p>3）求最大值、求最小值、求和、去重时可直接使用Reducer充当Combiner，但是求平均值时不能直接使用Reducer充当Combiner。</p><p>　　原因：对2组值求平均值</p><p>　　2 3 4 5 6 == 20 / 5 == 4</p><p>　　4 5 6    == 15 / 3 == 5</p><p>　　<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>***</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></p><p>　　20+15 / 5+3 = 35 / 8</p><p>　　4.5</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（十七）MapReduce框架Partitoner分区</title>
      <link href="/2018-04-17-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89MapReduce%E6%A1%86%E6%9E%B6Partitoner%E5%88%86%E5%8C%BA.html"/>
      <url>/2018-04-17-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89MapReduce%E6%A1%86%E6%9E%B6Partitoner%E5%88%86%E5%8C%BA.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（十七）MapReduce框架Partitoner分区：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（十七）MapReduce框架Partitoner分区</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="Partitioner分区类的作用是什么？"><a href="#Partitioner分区类的作用是什么？" class="headerlink" title="Partitioner分区类的作用是什么？"></a>Partitioner分区类的作用是什么？</h2><p><strong>在进行MapReduce计算时，有时候需要把最终的输出数据分到不同的文件中，比如按照省份划分的话，需要把同一省份的数据放到一个文件中；按照性别划分的话，需要把同一性别的数据放到一个文件中。我们知道最终的输出数据是来自于Reducer任务。那么，如果要得到多个文件，意味着有同样数量的Reducer任务在运行。Reducer任务的数据来自于Mapper任务，也就说Mapper任务要划分数据，对于不同的数据分配给不同的Reducer任务运行。Mapper任务划分数据的过程就称作Partition。负责实现划分数据的类称作Partitioner。</strong></p><p>Partitoner类的源码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">package org.apache.hadoop.mapreduce.lib.partition;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line">/** Partition keys by their &#123;@link Object#hashCode()&#125;. */</span><br><span class="line">public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt; &#123;</span><br><span class="line"></span><br><span class="line">  /** Use &#123;@link Object#hashCode()&#125; to partition. */</span><br><span class="line">  public int getPartition(K key, V value,</span><br><span class="line">                          int numReduceTasks) &#123;</span><br><span class="line">    //默认使用key的hash值与上int的最大值，避免出现数据溢出 的情况</span><br><span class="line">    return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="getPartition-三个参数分别是什么？"><a href="#getPartition-三个参数分别是什么？" class="headerlink" title="getPartition()三个参数分别是什么？"></a>getPartition()三个参数分别是什么？</h2><p>　　HashPartitioner是处理Mapper任务输出的，getPartition()方法有三个形参，源码中<strong>key、value分别指的是Mapper任务的输出，numReduceTasks指的是设置的Reducer任务数量，默认值是1</strong>。那么任何整数与1相除的余数肯定是0。也就是说getPartition(…)方法的返回值总是0。也就是Mapper任务的输出总是送给一个Reducer任务，最终只能输出到一个文件中。</p><p>　　据此分析，如果想要最终输出到多个文件中，在Mapper任务中对数据应该划分到多个区中。那么，我们只需要按照一定的规则让getPartition(…)方法的返回值是0,1,2,3…即可。</p><p>　　大部分情况下，我们都会使用默认的分区函数，但有时我们又有一些，特殊的需求，而需要定制Partition来完成我们的业务，</p><p>案例如下：按照能否被5除尽去分区</p><p>1、如果除以5的余数是0， 放在0号分区<br>2、如果除以5的余数部是0， 放在1分区</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line">public class FivePartitioner extends Partitioner&lt;IntWritable, IntWritable&gt;&#123;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 我们的需求：按照能否被5除尽去分区</span><br><span class="line">     * </span><br><span class="line">     * 1、如果除以5的余数是0，  放在0号分区</span><br><span class="line">     * 2、如果除以5的余数部是0，  放在1分区</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    public int getPartition(IntWritable key, IntWritable value, int numPartitions) &#123;</span><br><span class="line">        </span><br><span class="line">        int intValue = key.get();</span><br><span class="line">        </span><br><span class="line">        if(intValue % 5 == 0)&#123;</span><br><span class="line">            return 0;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            if(intValue % 2 == 0)&#123;</span><br><span class="line">                return 1;</span><br><span class="line">            &#125;else&#123;</span><br><span class="line">                return 2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在运行Mapreduce程序时，只需在主函数里加入如下两行代码即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(FivePartitioner.class);</span><br><span class="line">job.setNumReduceTasks(3);//设置为3</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（十五）MapReduce的多Job串联和全局计数器</title>
      <link href="/2018-04-15-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89MapReduce%E7%9A%84%E5%A4%9AJob%E4%B8%B2%E8%81%94%E5%92%8C%E5%85%A8%E5%B1%80%E8%AE%A1%E6%95%B0%E5%99%A8.html"/>
      <url>/2018-04-15-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89MapReduce%E7%9A%84%E5%A4%9AJob%E4%B8%B2%E8%81%94%E5%92%8C%E5%85%A8%E5%B1%80%E8%AE%A1%E6%95%B0%E5%99%A8.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（十五）MapReduce的多Job串联和全局计数器：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（十五）MapReduce的多Job串联和全局计数器</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>MapReduce 多 Job 串联</p><p>需求</p><p>一个稍复杂点的处理逻辑往往需要多个 MapReduce 程序串联处理，多 job 的串联可以借助 MapReduce 框架的 JobControl 实现</p><p>实例</p><p>以下有两个 MapReduce 任务，分别是 Flow 的 SumMR 和 SortMR，其中有依赖关系：SumMR 的输出是 SortMR 的输入，所以 SortMR 的启动得在 SumMR 完成之后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf1 = new Configuration();</span><br><span class="line">        Configuration conf2 = new Configuration();</span><br><span class="line">        </span><br><span class="line">        Job job1 = Job.getInstance(conf1);</span><br><span class="line">        Job job2 = Job.getInstance(conf2);</span><br><span class="line">            </span><br><span class="line">        job1.setJarByClass(MRScore3.class);</span><br><span class="line">        job1.setMapperClass(MRMapper3_1.class);</span><br><span class="line">        //job.setReducerClass(ScoreReducer3.class);</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        job1.setMapOutputKeyClass(IntWritable.class);</span><br><span class="line">        job1.setMapOutputValueClass(StudentBean.class);</span><br><span class="line">        job1.setOutputKeyClass(IntWritable.class);</span><br><span class="line">        job1.setOutputValueClass(StudentBean.class);</span><br><span class="line">        </span><br><span class="line">        job1.setPartitionerClass(CoursePartitioner2.class);</span><br><span class="line">        </span><br><span class="line">        job1.setNumReduceTasks(4);</span><br><span class="line">        </span><br><span class="line">        Path inputPath = new Path(&quot;D:\\MR\\hw\\work3\\input&quot;);</span><br><span class="line">        Path outputPath = new Path(&quot;D:\\MR\\hw\\work3\\output_hw3_1&quot;);</span><br><span class="line">            </span><br><span class="line">        FileInputFormat.setInputPaths(job1, inputPath);</span><br><span class="line">        FileOutputFormat.setOutputPath(job1, outputPath);</span><br><span class="line">        </span><br><span class="line">        job2.setMapperClass(MRMapper3_2.class);</span><br><span class="line">        job2.setReducerClass(MRReducer3_2.class);</span><br><span class="line">        </span><br><span class="line">        job2.setMapOutputKeyClass(IntWritable.class);</span><br><span class="line">        job2.setMapOutputValueClass(StudentBean.class);</span><br><span class="line">        job2.setOutputKeyClass(StudentBean.class);</span><br><span class="line">        job2.setOutputValueClass(NullWritable.class);</span><br><span class="line">        </span><br><span class="line">        Path inputPath2 = new Path(&quot;D:\\MR\\hw\\work3\\output_hw3_1&quot;);</span><br><span class="line">        Path outputPath2 = new Path(&quot;D:\\MR\\hw\\work3\\output_hw3_end&quot;);</span><br><span class="line">            </span><br><span class="line">        FileInputFormat.setInputPaths(job2, inputPath2);</span><br><span class="line">        FileOutputFormat.setOutputPath(job2, outputPath2);</span><br><span class="line">        </span><br><span class="line">        JobControl control = new JobControl(&quot;Score3&quot;);</span><br><span class="line">        </span><br><span class="line">        ControlledJob aJob = new ControlledJob(job1.getConfiguration());</span><br><span class="line">        ControlledJob bJob = new ControlledJob(job2.getConfiguration());</span><br><span class="line">        // 设置作业依赖关系</span><br><span class="line">        bJob.addDependingJob(aJob);</span><br><span class="line">        </span><br><span class="line">        control.addJob(aJob);</span><br><span class="line">        control.addJob(bJob);</span><br><span class="line">        </span><br><span class="line">        Thread thread = new Thread(control);</span><br><span class="line">        thread.start();</span><br><span class="line">        </span><br><span class="line">        while(!control.allFinished()) &#123;</span><br><span class="line">            thread.sleep(1000);</span><br><span class="line">        &#125;</span><br><span class="line">        System.exit(0);</span><br></pre></td></tr></table></figure><h2 id="MapReduce-全局计数器"><a href="#MapReduce-全局计数器" class="headerlink" title="MapReduce 全局计数器"></a>MapReduce 全局计数器</h2><h3 id="MapReduce计数器是什么？"><a href="#MapReduce计数器是什么？" class="headerlink" title="MapReduce计数器是什么？"></a>MapReduce计数器是什么？</h3><p>  计数器是用来记录job的执行进度和状态的。它的作用可以理解为日志。我们可以在程序的某个位置插入计数器，记录数据或者进度的变化情况。</p><h3 id="MapReduce计数器能做什么？"><a href="#MapReduce计数器能做什么？" class="headerlink" title="MapReduce计数器能做什么？"></a>MapReduce计数器能做什么？</h3><p>  MapReduce 计数器（Counter）为我们提供一个窗口，用于观察 MapReduce Job 运行期的各种细节数据。对MapReduce性能调优很有帮助，MapReduce性能优化的评估大部分都是基于这些 Counter 的数值表现出来的。</p><h3 id="MapReduce-都有哪些内置计数器？"><a href="#MapReduce-都有哪些内置计数器？" class="headerlink" title="MapReduce 都有哪些内置计数器？"></a>MapReduce 都有哪些内置计数器？</h3><p>  MapReduce 自带了许多默认Counter，现在我们来分析这些默认 Counter 的含义，方便大家观察 Job 结果，如输入的字节数、输出的字节数、Map端输入/输出的字节数和条数、Reduce端的输入/输出的字节数和条数等。下面我们只需了解这些内置计数器，知道计数器组名称（groupName）和计数器名称（counterName），以后使用计数器会查找groupName和counterName即可。</p><h4 id="1、任务计数器"><a href="#1、任务计数器" class="headerlink" title="1、任务计数器"></a>1、任务计数器</h4><p>​    在任务执行过程中，任务计数器采集任务的相关信息，每个作业的所有任务的结果会被聚集起来。例如，MAP_INPUT_RECORDS 计数器统计每个map任务输入记录的总数，并在一个作业的所有map任务上进行聚集，使得最终数字是整个作业的所有输入记录的总数。任务计数器由其关联任务维护，并定期发送给TaskTracker，再由TaskTracker发送给 JobTracker。因此，计数器能够被全局地聚集。下面我们分别了解各种任务计数器。</p><p>　　<strong>1）MapReduce 任务计数器</strong></p><p>​      MapReduce 任务计数器的 groupName为org.apache.hadoop.mapreduce.TaskCounter，它包含的计数器如下表所示</p><table><thead><tr><th><strong>计数器名称</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>map 输入的记录数（MAP_INPUT_RECORDS）</td><td>作业中所有 map 已处理的输入记录数。每次 RecorderReader 读到一条记录并将其传给 map 的 map() 函数时，该计数器的值增加。</td></tr><tr><td>map 跳过的记录数（MAP_SKIPPED_RECORDS）</td><td>作业中所有 map 跳过的输入记录数。</td></tr><tr><td>map 输入的字节数（MAP_INPUT_BYTES）</td><td>作业中所有 map 已处理的未经压缩的输入数据的字节数。每次 RecorderReader 读到一条记录并 将其传给 map 的 map() 函数时，该计数器的值增加</td></tr><tr><td>分片split的原始字节数（SPLIT_RAW_BYTES）</td><td>由 map 读取的输入-分片对象的字节数。这些对象描述分片元数据（文件的位移和长度），而不是分片的数据自身，因此总规模是小的</td></tr><tr><td>map 输出的记录数（MAP_OUTPUT_RECORDS）</td><td>作业中所有 map 产生的 map 输出记录数。每次某一个 map 的Context 调用 write() 方法时，该计数器的值增加</td></tr><tr><td>map 输出的字节数（MAP_OUTPUT_BYTES）</td><td>作业中所有 map 产生的 未经压缩的输出数据的字节数。每次某一个 map 的 Context 调用 write() 方法时，该计数器的值增加。</td></tr><tr><td>map 输出的物化字节数（MAP_OUTPUT_MATERIALIZED_BYTES）</td><td>map 输出后确实写到磁盘上的字节数；若 map 输出压缩功能被启用，则会在计数器值上反映出来</td></tr><tr><td>combine 输入的记录数（COMBINE_INPUT_RECORDS）</td><td>作业中所有 Combiner（如果有）已处理的输入记录数。Combiner 的迭代器每次读一个值，该计数器的值增加。</td></tr><tr><td>combine 输出的记录数（COMBINE_OUTPUT_RECORDS）</td><td>作业中所有 Combiner（如果有）已产生的输出记录数。每当一个 Combiner 的 Context 调用 write() 方法时，该计数器的值增加。</td></tr><tr><td>reduce 输入的组（REDUCE_INPUT_GROUPS）</td><td>作业中所有 reducer 已经处理的不同的码分组的个数。每当某一个 reducer 的 reduce() 被调用时，该计数器的值增加。</td></tr><tr><td>reduce 输入的记录数（REDUCE_INPUT_RECORDS）</td><td>作业中所有 reducer 已经处理的输入记录的个数。每当某个 reducer 的迭代器读一个值时，该计数器的值增加。如果所有 reducer 已经处理完所有输入， 则该计数器的值与计数器 “map 输出的记录” 的值相同</td></tr><tr><td>reduce 输出的记录数（REDUCE_OUTPUT_RECORDS）</td><td>作业中所有 map 已经产生的 reduce 输出记录数。每当某一个 reducer 的 Context 调用 write() 方法时，该计数器的值增加。</td></tr><tr><td>reduce 跳过的组数（REDUCE_SKIPPED_GROUPS）</td><td>作业中所有 reducer 已经跳过的不同的码分组的个数。</td></tr><tr><td>reduce 跳过的记录数（REDUCE_SKIPPED_RECORDS）</td><td>作业中所有 reducer 已经跳过输入记录数。</td></tr><tr><td>reduce 经过 shuffle 的字节数（REDUCE_SHUFFLE_BYTES）</td><td>shuffle 将 map 的输出数据复制到 reducer 中的字节数。</td></tr><tr><td>溢出的记录数（SPILLED_RECORDS）</td><td>作业中所有 map和reduce 任务溢出到磁盘的记录数</td></tr><tr><td>CPU 毫秒（CPU_MILLISECONDS）</td><td>总计的 CPU 时间，以毫秒为单位，由/proc/cpuinfo获取</td></tr><tr><td>物理内存字节数（PHYSICAL_MEMORY_BYTES）</td><td>一个任务所用物理内存的字节数，由/proc/cpuinfo获取</td></tr><tr><td>虚拟内存字节数（VIRTUAL_MEMORY_BYTES）</td><td>一个任务所用虚拟内存的字节数，由/proc/cpuinfo获取</td></tr><tr><td>有效的堆字节数（COMMITTED_HEAP_BYTES）</td><td>在 JVM 中的总有效内存量（以字节为单位），可由Runtime().getRuntime().totaoMemory()获取。</td></tr><tr><td>GC 运行时间毫秒数（GC_TIME_MILLIS）</td><td>在任务执行过程中，垃圾收集器（garbage collection）花费的时间（以毫秒为单位）， 可由 GarbageCollector MXBean.getCollectionTime()获取；该计数器并未出现在1.x版本中。</td></tr><tr><td>由 shuffle 传输的 map 输出数（SHUFFLED_MAPS）</td><td>有 shuffle 传输到 reducer 的 map 输出文件数。</td></tr><tr><td>失败的 shuffle 数（SHUFFLE_MAPS）</td><td>在 shuffle 过程中，发生拷贝错误的 map 输出文件数，该计数器并没有包含在 1.x 版本中。</td></tr><tr><td>被合并的 map 输出数</td><td>在 shuffle 过程中，在 reduce 端被合并的 map 输出文件数，该计数器没有包含在 1.x 版本中。</td></tr></tbody></table><p>　　<strong>2）文件系统计数器</strong></p><p>​      文件系统计数器的 groupName为org.apache.hadoop.mapreduce.FileSystemCounter，它包含的计数器如下表所示</p><p>　　</p><table><thead><tr><th><strong>计数器名称</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>文件系统的读字节数（BYTES_READ）</td><td>由 map 和 reduce 等任务在各个文件系统中读取的字节数，各个文件系统分别对应一个计数器，可以是 Local、HDFS、S3和KFS等。</td></tr><tr><td>文件系统的写字节数（BYTES_WRITTEN）</td><td>由 map 和 reduce 等任务在各个文件系统中写的字节数。</td></tr></tbody></table><p> 　　<strong>3）FileInputFormat 计数器</strong></p><p>​      FileInputFormat 计数器的 groupName为org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter，它包含的计数器如下表所示，计数器名称列的括号（）内容即为counterName</p><table><thead><tr><th><strong>计数器名称</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>读取的字节数（BYTES_READ）</td><td>由 map 任务通过 FileInputFormat 读取的字节数。</td></tr></tbody></table><p>　　<strong>4）FileOutputFormat 计数器</strong></p><p>​      FileOutputFormat 计数器的 groupName为org.apache.hadoop.mapreduce.lib.input.FileOutputFormatCounter，它包含的计数器如下表所示</p><table><thead><tr><th><strong>计数器名称</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>写的字节数（BYTES_WRITTEN）</td><td>由 map 任务（针对仅含 map 的作业）或者 reduce 任务通过 FileOutputFormat 写的字节数。</td></tr></tbody></table><h4 id="2、作业计数器"><a href="#2、作业计数器" class="headerlink" title="2、作业计数器"></a>2、作业计数器</h4><p>​    作业计数器由 JobTracker（或者 YARN）维护，因此无需在网络间传输数据，这一点与包括 “用户定义的计数器” 在内的其它计数器不同。这些计数器都是作业级别的统计量，其值不会随着任务运行而改变。 作业计数器计数器的 groupName为org.apache.hadoop.mapreduce.JobCounter，它包含的计数器如下表所示</p><table><thead><tr><th><strong>计数器名称</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>启用的map任务数（TOTAL_LAUNCHED_MAPS）</td><td>启动的map任务数，包括以“推测执行” 方式启动的任务。</td></tr><tr><td>启用的 reduce 任务数（TOTAL_LAUNCHED_REDUCES）</td><td>启动的reduce任务数，包括以“推测执行” 方式启动的任务。</td></tr><tr><td>失败的map任务数（NUM_FAILED_MAPS）</td><td>失败的map任务数。</td></tr><tr><td>失败的 reduce 任务数（NUM_FAILED_REDUCES）</td><td>失败的reduce任务数。</td></tr><tr><td>数据本地化的 map 任务数（DATA_LOCAL_MAPS）</td><td>与输入数据在同一节点的 map 任务数。</td></tr><tr><td>机架本地化的 map 任务数（RACK_LOCAL_MAPS）</td><td>与输入数据在同一机架范围内、但不在同一节点上的 map 任务数。</td></tr><tr><td>其它本地化的 map 任务数（OTHER_LOCAL_MAPS）</td><td>与输入数据不在同一机架范围内的 map 任务数。由于机架之间的宽带资源相对较少，Hadoop 会尽量让 map 任务靠近输入数据执行，因此该计数器值一般比较小。</td></tr><tr><td>map 任务的总运行时间（SLOTS_MILLIS_MAPS）</td><td>map 任务的总运行时间，单位毫秒。该计数器包括以推测执行方式启动的任务。</td></tr><tr><td>reduce 任务的总运行时间（SLOTS_MILLIS_REDUCES）</td><td>reduce任务的总运行时间，单位毫秒。该值包括以推测执行方式启动的任务。</td></tr><tr><td>在保留槽之后，map任务等待的总时间（FALLOW_SLOTS_MILLIS_MAPS）</td><td>在为 map 任务保留槽之后所花费的总等待时间，单位是毫秒。</td></tr><tr><td>在保留槽之后，reduce 任务等待的总时间（FALLOW_SLOTS_MILLIS_REDUCES）</td><td>在为 reduce 任务保留槽之后，花在等待上的总时间，单位为毫秒。</td></tr></tbody></table><h2 id="计数器的该如何使用？"><a href="#计数器的该如何使用？" class="headerlink" title="计数器的该如何使用？"></a>计数器的该如何使用？</h2><p>  下面我们来介绍如何使用计数器。</p><h4 id="1、定义计数器"><a href="#1、定义计数器" class="headerlink" title="1、定义计数器"></a>1、定义计数器</h4><p>​    1)枚举声明计数器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 自定义枚举变量Enum </span><br><span class="line">Counter counter = context.getCounter(Enum enum)</span><br></pre></td></tr></table></figure><p> 2)自定义计数器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/ 自己命名groupName和counterName </span><br><span class="line">Counter counter = context.getCounter(String groupName,String counterName)</span><br></pre></td></tr></table></figure><h4 id="2、为计数器赋值"><a href="#2、为计数器赋值" class="headerlink" title="2、为计数器赋值"></a>2、为计数器赋值</h4><p>​    1)初始化计数器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">counter.setValue(long value);// 设置初始值</span><br></pre></td></tr></table></figure><p>　2)计数器自增</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">counter.increment(long incr);// 增加计数</span><br></pre></td></tr></table></figure><h4 id="3、获取计数器的值"><a href="#3、获取计数器的值" class="headerlink" title="3、获取计数器的值"></a>3、获取计数器的值</h4><p>　　1) 获取枚举计数器的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf = new Configuration(); </span><br><span class="line">Job job = new Job(conf, &quot;MyCounter&quot;); </span><br><span class="line">job.waitForCompletion(true); </span><br><span class="line">Counters counters=job.getCounters(); </span><br><span class="line">Counter counter=counters.findCounter(LOG_PROCESSOR_COUNTER.BAD_RECORDS_LONG);// 查找枚举计数器，假如Enum的变量为BAD_RECORDS_LONG </span><br><span class="line">long value=counter.getValue();//获取计数值</span><br></pre></td></tr></table></figure><p>　　2) 获取自定义计数器的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf = new Configuration(); </span><br><span class="line">Job job = new Job(conf, &quot;MyCounter&quot;); </span><br><span class="line">job.waitForCompletion(true); </span><br><span class="line">Counters counters = job.getCounters(); </span><br><span class="line">Counter counter=counters.findCounter(&quot;ErrorCounter&quot;,&quot;toolong&quot;);// 假如groupName为ErrorCounter，counterName为toolong </span><br><span class="line">long value = counter.getValue();// 获取计数值</span><br></pre></td></tr></table></figure><p>　　3) 获取内置计数器的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf = new Configuration(); </span><br><span class="line">Job job = new Job(conf, &quot;MyCounter&quot;); </span><br><span class="line">job.waitForCompletion(true); </span><br><span class="line">Counters counters=job.getCounters(); </span><br><span class="line">// 查找作业运行启动的reduce个数的计数器，groupName和counterName可以从内置计数器表格查询（前面已经列举有） </span><br><span class="line">Counter counter=counters.findCounter(&quot;org.apache.hadoop.mapreduce.JobCounter&quot;,&quot;TOTAL_LAUNCHED_REDUCES&quot;);// 假如groupName为org.apache.hadoop.mapreduce.JobCounter，counterName为TOTAL_LAUNCHED_REDUCES </span><br><span class="line">long value=counter.getValue();// 获取计数值</span><br></pre></td></tr></table></figure><p>　　4) 获取所有计数器的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf = new Configuration(); </span><br><span class="line">Job job = new Job(conf, &quot;MyCounter&quot;); </span><br><span class="line">Counters counters = job.getCounters(); </span><br><span class="line">for (CounterGroup group : counters) &#123; </span><br><span class="line">  for (Counter counter : group) &#123; </span><br><span class="line">    System.out.println(counter.getDisplayName() + &quot;: &quot; + counter.getName() + &quot;: &quot;+ counter.getValue()); </span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（十四）MapReduce的核心运行机制</title>
      <link href="/2018-04-14-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89MapReduce%E7%9A%84%E6%A0%B8%E5%BF%83%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6.html"/>
      <url>/2018-04-14-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89MapReduce%E7%9A%84%E6%A0%B8%E5%BF%83%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（十四）MapReduce的核心运行机制：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（十四）MapReduce的核心运行机制</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>一个完整的 MapReduce 程序在分布式运行时有两类实例进程：</p><p>1、MRAppMaster：负责整个程序的过程调度及状态协调</p><p>2、Yarnchild：负责 map 阶段的整个数据处理流程</p><p>3、Yarnchild：负责 reduce 阶段的整个数据处理流程 以上两个阶段 MapTask 和 ReduceTask 的进程都是 YarnChild，并不是说这 MapTask 和 ReduceTask 就跑在同一个 YarnChild 进行里</p><h2 id="MapReduce-套路图"><a href="#MapReduce-套路图" class="headerlink" title="MapReduce 套路图"></a>MapReduce 套路图</h2><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180319083752041-1813135943.png" alt="img"></p><h2 id="MapReduce-程序的运行"><a href="#MapReduce-程序的运行" class="headerlink" title="MapReduce 程序的运行"></a>MapReduce 程序的运行</h2><p>1、一个 mr 程序启动的时候，最先启动的是 MRAppMaster，MRAppMaster 启动后根据本次 job 的描述信息，计算出需要的 maptask 实例数量，然后向集群申请机器启动相应数量的 maptask 进程</p><p>2、 maptask 进程启动之后，根据给定的数据切片(哪个文件的哪个偏移量范围)范围进行数 据处理，主体流程为：</p><p>　　 A、利用客户指定的 InputFormat 来获取 RecordReader 读取数据，形成输入 KV 对</p><p>　　B、将输入 KV 对传递给客户定义的 map()方法，做逻辑运算，并将 map()方法输出的 KV 对收 集到缓存</p><p>　　C、将缓存中的 KV 对按照 K 分区排序后不断溢写到磁盘文件</p><p>3、 MRAppMaster 监控到所有 maptask 进程任务完成之后（真实情况是，某些 maptask 进 程处理完成后，就会开始启动 reducetask 去已完成的 maptask 处 fetch 数据），会根据客户指 定的参数启动相应数量的 reducetask 进程，并告知 reducetask 进程要处理的数据范围（数据 分区）</p><p>4、Reducetask 进程启动之后，根据 MRAppMaster 告知的待处理数据所在位置，从若干台 maptask 运行所在机器上获取到若干个 maptask 输出结果文件，并在本地进行重新归并排序， 然后按照相同 key 的 KV 为一个组，调用客户定义的 reduce()方法进行逻辑运算，并收集运 算输出的结果 KV，然后调用客户指定的 OutputFormat 将结果数据输出到外部存储</p><h2 id="mapTask的并行度"><a href="#mapTask的并行度" class="headerlink" title="mapTask的并行度"></a>mapTask的并行度</h2><p>Hadoop中MapTask的并行度的决定机制。在MapReduce程序的运行中，并不是MapTask越多就越好。需要考虑数据量的多少及机器的配置。如果数据量很少，可能任务启动的时间都远远超过数据的处理时间。同样可不是越少越好。</p><p>那么应该如何切分呢？</p><p>假如我们有一个300M的文件，它会在HDFS中被切成3块。0-128M,128-256M,256-300M。并被放置到不同的节点上去了。在MapReduce任务中，这3个Block会被分给3个MapTask。</p><p>MapTask在任务切片时实际上也是分配一个范围，只是这个范围是逻辑上的概念，与block的物理划分没有什么关系。但在实践过程中如果MapTask读取的数据不在运行的本机，则必须通过网络进行数据传输，对性能的影响非常大。所以常常采取的策略是就按照块的存储切分MapTask，使得每个MapTask尽可能读取本机的数据。</p><p>如果一个Block非常小，也可以把多个小Block交给一个MapTask。</p><p>所以MapTask的切分要看情况处理。默认的实现是按照Block大小进行切分。MapTask的切分工作由客户端（我们写的main方法）负责。一个切片就对应一个MapTask实例。</p><h3 id="MapTask并行度的决定机制"><a href="#MapTask并行度的决定机制" class="headerlink" title="MapTask并行度的决定机制"></a>MapTask并行度的决定机制</h3><p>1个job的map阶段并行度由客户端在提交job时决定。</p><p>而客户端对map阶段并行度的规划的基本逻辑为：</p><p>将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多个split），然后每一个split分配一个mapTask并行实例处理</p><p>这段逻辑及形成的切片规划描述文件，由FileInputFormat实现类的getSplits()方法完成，其过程如下图：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180315134603028-1642420953.png" alt="img"></p><h3 id="切片机制"><a href="#切片机制" class="headerlink" title="切片机制"></a>切片机制</h3><p>FileInputFormat 中默认的切片机制</p><blockquote><p>1、简单地按照文件的内容长度进行切片</p><p>2、切片大小，默认等于 block 大小</p><p>3、切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 比如待处理数据有两个文件：</p><p>　　File1.txt 200M</p><p>　　File2.txt 100M</p><p>经过 getSplits()方法处理之后，形成的切片信息是：</p><p>File1.txt-split1 0-128M</p><p>File1.txt-split2 129M-200M</p><p>File2.txt-split1 0-100M</p></blockquote><p><strong>FileInputFormat 中切片的大小的参数配置</strong></p><blockquote><p> 通过分析源码，在 FileInputFormat 中，计算切片大小的逻辑： long splitSize = computeSplitSize(blockSize, minSize, maxSize)，翻译一下就是求这三个值的中 间值</p><p>切片主要由这几个值来运算决定：</p><p><strong>blocksize：默认是 128M，可通过 dfs.blocksize 修改</strong></p><p><strong>minSize：默认是 1，可通过 mapreduce.input.fileinputformat.split.minsize 修改</strong></p><p><strong>maxsize：默认是 Long.MaxValue，可通过 mapreduce.input.fileinputformat.split.maxsize 修改</strong></p><p>因此，如果 maxsize 调的比 blocksize 小，则切片会小于 blocksize 如果 minsize 调的比 blocksize 大，则切片会大于 blocksize 但是，<strong>不论怎么调参数，都不能让多个小文件“划入”一个 split</strong></p></blockquote><h3 id="MapTask-并行度经验之谈"><a href="#MapTask-并行度经验之谈" class="headerlink" title="MapTask 并行度经验之谈"></a>MapTask 并行度经验之谈</h3><p>如果硬件配置为 2*12core + 64G，恰当的 map 并行度是大约每个节点 20-100 个 map，最好 每个 map 的执行时间至少一分钟。</p><p>1、如果 job 的每个 map 或者 reduce task 的运行时间都只有 30-40 秒钟，那么就减少该 job 的 map 或者 reduce 数，每一个 task(map|reduce)的 setup 和加入到调度器中进行调度，这个 中间的过程可能都要花费几秒钟，所以如果每个 task 都非常快就跑完了，就会在 task 的开 始和结束的时候浪费太多的时间。</p><p>配置 task 的 JVM 重用可以改善该问题：</p><p>mapred.job.reuse.jvm.num.tasks，默认是 1，表示一个 JVM 上最多可以顺序执行的 task 数目（属于同一个 Job）是 1。也就是说一个 task 启一个 JVM。这个值可以在 mapred-site.xml 中进行更改，当设置成多个，就意味着这多个 task 运行在同一个 JVM 上，但不是同时执行， 是排队顺序执行</p><p>2、如果 input 的文件非常的大，比如 1TB，可以考虑将 hdfs 上的每个 blocksize 设大，比如 设成 256MB 或者 512MB</p><h2 id="ReduceTask-并行度"><a href="#ReduceTask-并行度" class="headerlink" title="ReduceTask 并行度"></a>ReduceTask 并行度</h2><p>reducetask 的并行度同样影响整个 job 的执行并发度和执行效率，但与 maptask 的并发数由 切片数决定不同，Reducetask 数量的决定是可以直接手动设置： job.setNumReduceTasks(4);</p><p>默认值是 1，</p><p>手动设置为 4，表示运行 4 个 reduceTask，</p><p>设置为 0，表示不运行 reduceTask 任务，也就是没有 reducer 阶段，只有 mapper 阶段</p><p>如果数据分布不均匀，就有可能在 reduce 阶段产生数据倾斜</p><p>注意：reducetask 数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全 局汇总结果，就只能有 1 个 reducetask</p><p>尽量不要运行太多的 reducetask。对大多数 job 来说，最好 rduce 的个数最多和集群中的 reduce 持平，或者比集群的 reduce slots 小。这个对于小集群而言，尤其重要。</p><h3 id="ReduceTask-并行度决定机制"><a href="#ReduceTask-并行度决定机制" class="headerlink" title="ReduceTask 并行度决定机制"></a>ReduceTask 并行度决定机制</h3><p>1、job.setNumReduceTasks(number);<br>2、job.setReducerClass(MyReducer.class);<br>3、job.setPartitioonerClass(MyPTN.class);</p><p>分以下几种情况讨论：</p><p>1、如果number为1，并且2已经设置为自定义Reducer, reduceTask的个数就是1<br>不管用户编写的MR程序有没有设置Partitioner，那么该分区组件都不会起作用</p><p>2、如果number没有设置，并且2已经设置为自定义Reducer, reduceTask的个数就是1<br>在默认的分区组件的影响下，不管用户设置的number，不管是几，只要大于1，都是可以正常执行的。<br>如果在设置自定义的分区组件时，那么就需要注意：<br>你设置的reduceTasks的个数，必须要 ==== 分区编号中的最大值 + 1<br>最好的情况下：分区编号都是连续的。<br>那么reduceTasks = 分区编号的总个数 = 分区编号中的最大值 + 1</p><p>3、如果number为 &gt;= 2 并且2已经设置为自定义Reducer reduceTask的个数就是number<br>底层会有默认的数据分区组件在起作用</p><p>4、如果你设置了number的个数，但是没有设置自定义的reducer，那么该mapreduce程序不代表没有reducer阶段<br>真正的reducer中的逻辑，就是调用父类Reducer中的默认实现逻辑:原样输出<br>reduceTask的个数 就是 number</p><p>5、如果一个MR程序中，不想有reducer阶段。那么只需要做一下操作即可:<br>job.setNumberReudceTasks(0);<br>整个MR程序只有mapper阶段。没有reducer阶段。<br>那么就没有shuffle阶段</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（十三）MapReduce的初识</title>
      <link href="/2018-04-13-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89MapReduce%E7%9A%84%E5%88%9D%E8%AF%86.html"/>
      <url>/2018-04-13-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89MapReduce%E7%9A%84%E5%88%9D%E8%AF%86.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（十三）MapReduce的初识：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（十三）MapReduce的初识</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="MapReduce是什么"><a href="#MapReduce是什么" class="headerlink" title="MapReduce是什么"></a>MapReduce是什么</h2><p>首先让我们来重温一下 hadoop 的四大组件：</p><p>HDFS：分布式存储系统</p><p>MapReduce：分布式计算系统</p><p>YARN：hadoop 的资源调度系统</p><p>Common：以上三大组件的底层支撑组件，主要提供基础工具包和 RPC 框架等</p><p>MapReduce 是一个分布式运算程序的编程框架，是用户开发“基于 Hadoop 的数据分析应用” 的核心框架</p><p>MapReduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布 式运算程序，并发运行在一个 Hadoop 集群上</p><h2 id="为什么需要-MapReduce"><a href="#为什么需要-MapReduce" class="headerlink" title="为什么需要 MapReduce"></a>为什么需要 MapReduce</h2><p>1、海量数据在单机上处理因为硬件资源限制，无法胜任</p><p>2、而一旦将单机版程序扩展到集群来分布式运行，将极大增加程序的复杂度和开发难度</p><p>3、引入 MapReduce 框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将 分布式计算中的复杂性交由框架来处理</p><p>设想一个海量数据场景下的数据计算需求：</p><table><thead><tr><th>单机版：磁盘受限，内存受限，计算能力受限</th></tr></thead><tbody><tr><td>分布式版：1、 数据存储的问题，hadoop 提供了 hdfs 解决了数据存储这个问题2、 运算逻辑至少要分为两个阶段，先并发计算（map），然后汇总（reduce）结果3、 这两个阶段的计算如何启动？如何协调？4、 运算程序到底怎么执行？数据找程序还是程序找数据？5、 如何分配两个阶段的多个运算任务？6、 如何管理任务的执行过程中间状态，如何容错？7、 如何监控？8、 出错如何处理？抛异常？重试？</td></tr></tbody></table><p>　　可见在程序由单机版扩成分布式版时，会引入大量的复杂工作。为了提高开发效率，可以将 分布式程序中的公共功能封装成框架，让开发人员可以将精力集中于业务逻辑。</p><p>　　Hadoop 当中的 MapReduce 就是这样的一个分布式程序运算框架，它把大量分布式程序都会 涉及的到的内容都封装进了，让用户只用专注自己的业务逻辑代码的开发。它对应以上问题 的整体结构如下：</p><blockquote><p>MRAppMaster：MapReduce Application Master，分配任务，协调任务的运行</p><p>MapTask：阶段并发任，负责 mapper 阶段的任务处理 YARNChild</p><p>ReduceTask：阶段汇总任务，负责 reducer 阶段的任务处理 YARNChild</p></blockquote><h2 id="MapReduce做什么"><a href="#MapReduce做什么" class="headerlink" title="MapReduce做什么"></a>MapReduce做什么</h2><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180315125705908-67959108.png" alt="img"></p><p>简单地讲，MapReduce可以做<strong>大数据处理</strong>。所谓大数据处理，即以价值为导向，对大数据加工、挖掘和优化等各种处理。</p><p>　　MapReduce擅长处理大数据，它为什么具有这种能力呢？这可由MapReduce的设计思想发觉。MapReduce的思想就是“<strong>分而治之</strong>”。</p><p>　　（1）Mapper负责“分”，即把复杂的任务分解为若干个“简单的任务”来处理。“简单的任务”包含三层含义：一是数据或计算的规模相对原任务要大大缩小；二是就近计算原则，即任务会分配到存放着所需数据的节点上进行计算；三是这些小任务可以并行计算，彼此间几乎没有依赖关系。</p><p>　　（2）Reducer负责对map阶段的结果进行汇总。至于需要多少个Reducer，用户可以根据具体问题，通过在mapred-site.xml配置文件里设置参数mapred.reduce.tasks的值，缺省值为1。</p><h2 id="MapReduce-程序运行演示"><a href="#MapReduce-程序运行演示" class="headerlink" title="MapReduce 程序运行演示"></a>MapReduce 程序运行演示</h2><p>　　在 MapReduce 组件里，官方给我们提供了一些样例程序，其中非常有名的就是 wordcount 和 pi 程序。这些 MapReduce 程序的代码都在 hadoop-mapreduce-examples-2.7.5.jar 包里，这 个 jar 包在 hadoop 安装目录下的/share/hadoop/mapreduce/目录里 下面我们使用 hadoop 命令来试跑例子程序，看看运行效果</p><h3 id="MapReduce-示例-pi-的程序"><a href="#MapReduce-示例-pi-的程序" class="headerlink" title="MapReduce 示例 pi 的程序"></a>MapReduce 示例 pi 的程序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/hadoop-2.7.5/share/hadoop/mapreduce/</span><br><span class="line">[hadoop@hadoop1 mapreduce]$ pwd</span><br><span class="line">/home/hadoop/apps/hadoop-2.7.5/share/hadoop/mapreduce</span><br><span class="line">[hadoop@hadoop1 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.5.jar pi 5 5</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180315130928333-983517942.png" alt="img"></p><h3 id="MapReduce-示例-wordcount-的程序"><a href="#MapReduce-示例-wordcount-的程序" class="headerlink" title="MapReduce 示例 wordcount 的程序"></a>MapReduce 示例 wordcount 的程序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 mapreduce]$ hadoop jar hadoop-mapreduce-examples-2.7.5.jar wordcount /wc/input1/ /wc/output1/</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180315131431517-1980546434.png" alt="img"></p><p>查看结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 mapreduce]$ hadoop fs -cat /wc/output1/part-r-00000</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180315131549019-1405984561.png" alt="img"></p><h3 id="其他程序"><a href="#其他程序" class="headerlink" title="其他程序"></a>其他程序</h3><p>那除了这两个程序以外，还有没有官方提供的其他程序呢，还有就是它们的源码在哪里呢？</p><p>我们打开 mapreduce 的源码工程，里面有一个 hadoop-mapreduce-project 项目：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180315131829785-945137720.png" alt="img"></p><p>里面有一个例子程序的子项目：hadoop-mapreduce-examples</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180315131715303-2103985696.png" alt="img"></p><p>其中 src 是例子程序源码目录，pom.xml 是该项目的 maven 管理配置文件，我们打开该文件， 找到第 127 行，它告诉了我们例子程序的主程序入口：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180315131920701-1056138712.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180315132010964-1802898037.png" alt="img"></p><p>找到src\main\java\org\apache\hadoop\examples目录</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180315132128490-2142579900.png" alt="img"></p><p>打开主入口程序，看源代码：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180315132241257-1833275242.png" alt="img"></p><p>找到这一步，我们就能知道其实 wordcount 程序的实际程序就是 WordCount.class，这就是我 们想要找的例子程序的源码。</p><h3 id="WordCount-java源码"><a href="#WordCount-java源码" class="headerlink" title="WordCount.java源码"></a>WordCount.java源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">1 /**</span><br><span class="line"> 2  * Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line"> 3  * or more contributor license agreements.  See the NOTICE file</span><br><span class="line"> 4  * distributed with this work for additional information</span><br><span class="line"> 5  * regarding copyright ownership.  The ASF licenses this file</span><br><span class="line"> 6  * to you under the Apache License, Version 2.0 (the</span><br><span class="line"> 7  * &quot;License&quot;); you may not use this file except in compliance</span><br><span class="line"> 8  * with the License.  You may obtain a copy of the License at</span><br><span class="line"> 9  *</span><br><span class="line">10  *     http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">11  *</span><br><span class="line">12  * Unless required by applicable law or agreed to in writing, software</span><br><span class="line">13  * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">14  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">15  * See the License for the specific language governing permissions and</span><br><span class="line">16  * limitations under the License.</span><br><span class="line">17  */</span><br><span class="line">18 package org.apache.hadoop.examples;</span><br><span class="line">19 </span><br><span class="line">20 import java.io.IOException;</span><br><span class="line">21 import java.util.StringTokenizer;</span><br><span class="line">22 </span><br><span class="line">23 import org.apache.hadoop.conf.Configuration;</span><br><span class="line">24 import org.apache.hadoop.fs.Path;</span><br><span class="line">25 import org.apache.hadoop.io.IntWritable;</span><br><span class="line">26 import org.apache.hadoop.io.Text;</span><br><span class="line">27 import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">28 import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">29 import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">30 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">31 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line">32 import org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line">33 </span><br><span class="line">34 public class WordCount &#123;</span><br><span class="line">35 </span><br><span class="line">36   public static class TokenizerMapper </span><br><span class="line">37        extends Mapper&lt;Object, Text, Text, IntWritable&gt;&#123;</span><br><span class="line">38     </span><br><span class="line">39     private final static IntWritable one = new IntWritable(1);</span><br><span class="line">40     private Text word = new Text();</span><br><span class="line">41       </span><br><span class="line">42     public void map(Object key, Text value, Context context</span><br><span class="line">43                     ) throws IOException, InterruptedException &#123;</span><br><span class="line">44       StringTokenizer itr = new StringTokenizer(value.toString());</span><br><span class="line">45       while (itr.hasMoreTokens()) &#123;</span><br><span class="line">46         word.set(itr.nextToken());</span><br><span class="line">47         context.write(word, one);</span><br><span class="line">48       &#125;</span><br><span class="line">49     &#125;</span><br><span class="line">50   &#125;</span><br><span class="line">51   </span><br><span class="line">52   public static class IntSumReducer </span><br><span class="line">53        extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123;</span><br><span class="line">54     private IntWritable result = new IntWritable();</span><br><span class="line">55 </span><br><span class="line">56     public void reduce(Text key, Iterable&lt;IntWritable&gt; values, </span><br><span class="line">57                        Context context</span><br><span class="line">58                        ) throws IOException, InterruptedException &#123;</span><br><span class="line">59       int sum = 0;</span><br><span class="line">60       for (IntWritable val : values) &#123;</span><br><span class="line">61         sum += val.get();</span><br><span class="line">62       &#125;</span><br><span class="line">63       result.set(sum);</span><br><span class="line">64       context.write(key, result);</span><br><span class="line">65     &#125;</span><br><span class="line">66   &#125;</span><br><span class="line">67 </span><br><span class="line">68   public static void main(String[] args) throws Exception &#123;</span><br><span class="line">69     Configuration conf = new Configuration();</span><br><span class="line">70     String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();</span><br><span class="line">71     if (otherArgs.length &lt; 2) &#123;</span><br><span class="line">72       System.err.println(&quot;Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;&quot;);</span><br><span class="line">73       System.exit(2);</span><br><span class="line">74     &#125;</span><br><span class="line">75     Job job = Job.getInstance(conf, &quot;word count&quot;);</span><br><span class="line">76     job.setJarByClass(WordCount.class);</span><br><span class="line">77     job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">78     job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">79     job.setReducerClass(IntSumReducer.class);</span><br><span class="line">80     job.setOutputKeyClass(Text.class);</span><br><span class="line">81     job.setOutputValueClass(IntWritable.class);</span><br><span class="line">82     for (int i = 0; i &lt; otherArgs.length - 1; ++i) &#123;</span><br><span class="line">83       FileInputFormat.addInputPath(job, new Path(otherArgs[i]));</span><br><span class="line">84     &#125;</span><br><span class="line">85     FileOutputFormat.setOutputPath(job,</span><br><span class="line">86       new Path(otherArgs[otherArgs.length - 1]));</span><br><span class="line">87     System.exit(job.waitForCompletion(true) ? 0 : 1);</span><br><span class="line">88   &#125;</span><br><span class="line">89 &#125;</span><br></pre></td></tr></table></figure><h2 id="MapReduce-示例程序编写及编码规范"><a href="#MapReduce-示例程序编写及编码规范" class="headerlink" title="MapReduce 示例程序编写及编码规范"></a>MapReduce 示例程序编写及编码规范</h2><p>上一步，我们查看了 WordCount 这个 MapReduce 程序的源码编写，可以得出几点结论：</p><p>1、 该程序有一个 main 方法，来启动任务的运行，其中 job 对象就存储了该程序运行的必要 信息，比如指定 Mapper 类和 Reducer 类 job.setMapperClass(TokenizerMapper.class); job.setReducerClass(IntSumReducer.class);</p><p>2、 该程序中的 TokenizerMapper 类继承了 Mapper 类</p><p>3、 该程序中的 IntSumReducer 类继承了 Reducer 类</p><p><strong>总结：MapReduce 程序的业务编码分为两个大部分，一部分配置程序的运行信息，一部分 编写该 MapReduce 程序的业务逻辑，并且业务逻辑的 map 阶段和 reduce 阶段的代码分别继 承 Mapper 类和 Reducer 类</strong></p><h3 id="编写自己的-Wordcount-程序"><a href="#编写自己的-Wordcount-程序" class="headerlink" title="编写自己的 Wordcount 程序"></a>编写自己的 Wordcount 程序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br></pre></td><td class="code"><pre><span class="line">  1 package com.ghgj.mapreduce.wc.demo;</span><br><span class="line">  2 </span><br><span class="line">  3 import java.io.IOException;</span><br><span class="line">  4 </span><br><span class="line">  5 import org.apache.hadoop.conf.Configuration;</span><br><span class="line">  6 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">  7 import org.apache.hadoop.fs.Path;</span><br><span class="line">  8 import org.apache.hadoop.io.IntWritable;</span><br><span class="line">  9 import org.apache.hadoop.io.LongWritable;</span><br><span class="line"> 10 import org.apache.hadoop.io.Text;</span><br><span class="line"> 11 import org.apache.hadoop.mapreduce.Job;</span><br><span class="line"> 12 import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"> 13 import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"> 14 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"> 15 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"> 16 </span><br><span class="line"> 17 /**</span><br><span class="line"> 18  * </span><br><span class="line"> 19  * 描述: MapReduce出入门：WordCount例子程序 </span><br><span class="line"> 20  */</span><br><span class="line"> 21 public class WordCountMR &#123;</span><br><span class="line"> 22 </span><br><span class="line"> 23     /**</span><br><span class="line"> 24      * 该main方法是该mapreduce程序运行的入口，其中用一个Job类对象来管理程序运行时所需要的很多参数：</span><br><span class="line"> 25      * 比如，指定用哪个组件作为数据读取器、数据结果输出器 指定用哪个类作为map阶段的业务逻辑类，哪个类作为reduce阶段的业务逻辑类</span><br><span class="line"> 26      * 指定wordcount job程序的jar包所在路径 .... 以及其他各种需要的参数</span><br><span class="line"> 27      */</span><br><span class="line"> 28     public static void main(String[] args) throws Exception &#123;</span><br><span class="line"> 29         // 指定hdfs相关的参数</span><br><span class="line"> 30         Configuration conf = new Configuration();</span><br><span class="line"> 31 //         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop02:9000&quot;);</span><br><span class="line"> 32         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line"> 33 </span><br><span class="line"> 34         // 这是高可用的集群的配置文件。如果不是高可用集群，请自行替换配置文件</span><br><span class="line"> 35 //        conf.addResource(&quot;hdfs_config/core-site.xml&quot;);</span><br><span class="line"> 36 //        conf.addResource(&quot;hdfs_config/hdfs-site.xml&quot;);</span><br><span class="line"> 37 </span><br><span class="line"> 38         // conf.set(&quot;mapreduce.framework.name&quot;, &quot;yarn&quot;);</span><br><span class="line"> 39         // conf.set(&quot;yarn.resourcemanager.hostname&quot;, &quot;hadoop04&quot;);</span><br><span class="line"> 40 </span><br><span class="line"> 41         // 通过Configuration对象获取Job对象，该job对象会组织所有的该MapReduce程序所有的各种组件</span><br><span class="line"> 42         Job job = Job.getInstance(conf);</span><br><span class="line"> 43 </span><br><span class="line"> 44         // 设置jar包所在路径</span><br><span class="line"> 45         job.setJarByClass(WordCountMR.class);</span><br><span class="line"> 46 </span><br><span class="line"> 47         // 指定mapper类和reducer类</span><br><span class="line"> 48         job.setMapperClass(WordCountMapper.class);</span><br><span class="line"> 49         job.setReducerClass(WordCountReducer.class);</span><br><span class="line"> 50 </span><br><span class="line"> 51         // Mapper的输入key-value类型，由MapReduce框架决定</span><br><span class="line"> 52         // 指定maptask的输出类型</span><br><span class="line"> 53         job.setMapOutputKeyClass(Text.class);</span><br><span class="line"> 54         job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"> 55         // 假如 mapTask的输出key-value类型，跟reduceTask的输出key-value类型一致，那么，以上两句代码可以不用设置</span><br><span class="line"> 56 </span><br><span class="line"> 57         // reduceTask的输入key-value类型 就是 mapTask的输出key-value类型。所以不需要指定</span><br><span class="line"> 58         // 指定reducetask的输出类型</span><br><span class="line"> 59         job.setOutputKeyClass(Text.class);</span><br><span class="line"> 60         job.setOutputValueClass(IntWritable.class);</span><br><span class="line"> 61 </span><br><span class="line"> 62         // 为job指定输入数据的组件和输出数据的组件，以下两个参数是默认的，所以不指定也是OK的</span><br><span class="line"> 63         // job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line"> 64         // job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line"> 65 </span><br><span class="line"> 66         // 为该mapreduce程序制定默认的数据分区组件。默认是 HashPartitioner.class</span><br><span class="line"> 67         // job.setPartitionerClass(HashPartitioner.class);</span><br><span class="line"> 68 </span><br><span class="line"> 69         // 如果MapReduce程序在Eclipse中，运行，也可以读取Windows系统本地的文件系统中的数据</span><br><span class="line"> 70          Path inputPath = new Path(&quot;D:\\bigdata\\wordcount\\input&quot;);</span><br><span class="line"> 71          Path outputPath = new Path(&quot;D:\\bigdata\\wordcount\\output33&quot;);</span><br><span class="line"> 72 </span><br><span class="line"> 73         // 设置该MapReduce程序的ReduceTask的个数</span><br><span class="line"> 74         // job.setNumReduceTasks(3);</span><br><span class="line"> 75 </span><br><span class="line"> 76         // 指定该mapreduce程序数据的输入和输出路径</span><br><span class="line"> 77 //        Path inputPath = new Path(&quot;/wordcount/input&quot;);</span><br><span class="line"> 78 //        Path outputPath = new Path(&quot;/wordcount/output&quot;);</span><br><span class="line"> 79         // 该段代码是用来判断输出路径存在不存在，存在就删除，虽然方便操作，但请谨慎</span><br><span class="line"> 80         FileSystem fs = FileSystem.get(conf);</span><br><span class="line"> 81         if (fs.exists(outputPath)) &#123;</span><br><span class="line"> 82             fs.delete(outputPath, true);</span><br><span class="line"> 83         &#125;</span><br><span class="line"> 84 </span><br><span class="line"> 85         // 设置wordcount程序的输入路径</span><br><span class="line"> 86         FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line"> 87         // 设置wordcount程序的输出路径</span><br><span class="line"> 88         FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line"> 89 </span><br><span class="line"> 90         // job.submit();</span><br><span class="line"> 91         // 最后提交任务(verbose布尔值 决定要不要将运行进度信息输出给用户)</span><br><span class="line"> 92         boolean waitForCompletion = job.waitForCompletion(true);</span><br><span class="line"> 93         System.exit(waitForCompletion ? 0 : 1);</span><br><span class="line"> 94     &#125;</span><br><span class="line"> 95 </span><br><span class="line"> 96     /**</span><br><span class="line"> 97      * Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt;</span><br><span class="line"> 98      * </span><br><span class="line"> 99      * KEYIN 是指框架读取到的数据的key的类型，在默认的InputFormat下，读到的key是一行文本的起始偏移量，所以key的类型是Long</span><br><span class="line">100      * VALUEIN 是指框架读取到的数据的value的类型,在默认的InputFormat下，读到的value是一行文本的内容，所以value的类型是String</span><br><span class="line">101      * KEYOUT 是指用户自定义逻辑方法返回的数据中key的类型，由用户业务逻辑决定，在此wordcount程序中，我们输出的key是单词，所以是String</span><br><span class="line">102      * VALUEOUT 是指用户自定义逻辑方法返回的数据中value的类型，由用户业务逻辑决定,在此wordcount程序中，我们输出的value是单词的数量，所以是Integer</span><br><span class="line">103      * </span><br><span class="line">104      * 但是，String ，Long等jdk中自带的数据类型，在序列化时，效率比较低，hadoop为了提高序列化效率，自定义了一套序列化框架</span><br><span class="line">105      * 所以，在hadoop的程序中，如果该数据需要进行序列化（写磁盘，或者网络传输），就一定要用实现了hadoop序列化框架的数据类型</span><br><span class="line">106      * </span><br><span class="line">107      * Long ----&gt; LongWritable </span><br><span class="line">108      * String ----&gt; Text </span><br><span class="line">109      * Integer ----&gt; IntWritable </span><br><span class="line">110      * Null ----&gt; NullWritable</span><br><span class="line">111      */</span><br><span class="line">112     static class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">113 </span><br><span class="line">114         /**</span><br><span class="line">115          * LongWritable key : 该key就是value该行文本的在文件当中的起始偏移量</span><br><span class="line">116          * Text value ： 就是MapReduce框架默认的数据读取组件TextInputFormat读取文件当中的一行文本</span><br><span class="line">117          */</span><br><span class="line">118         @Override</span><br><span class="line">119         protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">120 </span><br><span class="line">121             // 切分单词</span><br><span class="line">122             String[] words = value.toString().split(&quot; &quot;);</span><br><span class="line">123             for (String word : words) &#123;</span><br><span class="line">124                 // 每个单词计数一次，也就是把单词组织成&lt;hello,1&gt;这样的key-value对往外写出</span><br><span class="line">125                 context.write(new Text(word), new IntWritable(1));</span><br><span class="line">126             &#125;</span><br><span class="line">127         &#125;</span><br><span class="line">128     &#125;</span><br><span class="line">129 </span><br><span class="line">130     /**</span><br><span class="line">131      * 首先，和前面一样，Reducer类也有输入和输出，输入就是Map阶段的处理结果，输出就是Reduce最后的输出</span><br><span class="line">132      * reducetask在调我们写的reduce方法,reducetask应该收到了前一阶段（map阶段）中所有maptask输出的数据中的一部分</span><br><span class="line">133      * （数据的key.hashcode%reducetask数==本reductask号），所以reducetaks的输入类型必须和maptask的输出类型一样</span><br><span class="line">134      * </span><br><span class="line">135      * reducetask将这些收到kv数据拿来处理时，是这样调用我们的reduce方法的： 先将自己收到的所有的kv对按照k分组（根据k是否相同）</span><br><span class="line">136      * 将某一组kv中的第一个kv中的k传给reduce方法的key变量，把这一组kv中所有的v用一个迭代器传给reduce方法的变量values</span><br><span class="line">137      */</span><br><span class="line">138     static class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">139 </span><br><span class="line">140         /**</span><br><span class="line">141          * Text key : mapTask输出的key值</span><br><span class="line">142          * Iterable&lt;IntWritable&gt; values ： key对应的value的集合（该key只是相同的一个key）</span><br><span class="line">143          * </span><br><span class="line">144          * reduce方法接收key值相同的一组key-value进行汇总计算</span><br><span class="line">145          */</span><br><span class="line">146         @Override</span><br><span class="line">147         protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">148 </span><br><span class="line">149             // 结果汇总</span><br><span class="line">150             int sum = 0;</span><br><span class="line">151             for (IntWritable v : values) &#123;</span><br><span class="line">152                 sum += v.get();</span><br><span class="line">153             &#125;</span><br><span class="line">154             // 汇总的结果往外输出</span><br><span class="line">155             context.write(key, new IntWritable(sum));</span><br><span class="line">156         &#125;</span><br><span class="line">157     &#125;</span><br><span class="line">158 &#125;</span><br></pre></td></tr></table></figure><h2 id="MapReduce-程序编写规范"><a href="#MapReduce-程序编写规范" class="headerlink" title="MapReduce 程序编写规范"></a>MapReduce 程序编写规范</h2><p>1、用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行 MR 程序的客户端)</p><p>2、Mapper 的输入数据是 KV 对的形式（KV 的类型可自定义）</p><p>3、Mapper 的输出数据是 KV 对的形式（KV 的类型可自定义）</p><p>4、Mapper 中的业务逻辑写在 map()方法中</p><p>5、map()方法（maptask 进程）对每一个&lt;k,v&gt;调用一次</p><p>6、Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV 对的形式</p><p>7、Reducer 的业务逻辑写在 reduce()方法中</p><p>8、Reducetask 进程对每一组相同 k 的&lt;k,v&gt;组调用一次 reduce()方法</p><p>9、用户自定义的 Mapper 和 Reducer 都要继承各自的父类</p><p>10、整个程序需要一个 Drvier 来进行提交，提交的是一个描述了各种必要信息的 job 对象</p><h3 id="WordCount-的业务逻辑"><a href="#WordCount-的业务逻辑" class="headerlink" title="WordCount 的业务逻辑"></a>WordCount 的业务逻辑</h3><p>1、 maptask 阶段处理每个数据分块的单词统计分析，思路是每遇到一个单词则把其转换成 一个 key-value 对，比如单词 hello，就转换成&lt;’hello’,1&gt;发送给 reducetask 去汇总</p><p>2、 reducetask 阶段将接受 maptask 的结果，来做汇总计数</p><h2 id="MapReduce-运行方式及-Debug"><a href="#MapReduce-运行方式及-Debug" class="headerlink" title="MapReduce 运行方式及 Debug"></a>MapReduce 运行方式及 Debug</h2><h3 id="集群运行模式"><a href="#集群运行模式" class="headerlink" title="集群运行模式"></a>集群运行模式</h3><p>打 jar 包，提交任务到集群运行，适用：生产环境，不适用：测试，调试，开发</p><blockquote><p>要点一：首先要把代码打成 jar 上传到 linux 服务器</p><p>要点二：用 hadoop jar 的命令去提交代码到 yarn 集群运行</p><p>要点三：处理的数据和输出结果应该位于 hdfs 文件系统</p><p>要点四：如果需要在 windows 中的 eclipse 当中直接提交 job 到集群，则需要修改 YarnRunner 类，这个比较复杂，不建议使用</p></blockquote><h3 id="本地运行模式"><a href="#本地运行模式" class="headerlink" title="本地运行模式"></a>本地运行模式</h3><p>Eclipse 开发环境下本地运行，好处是方便调试和测试</p><p>直接在IDE环境中进行环境 ： eclipse</p><blockquote><p>1、直接运行在本地，读取本地数据</p><p>2、直接运行在本地，读取远程的文件系统的数据</p><p>3、直接在IDE中提交任务给YARN集群运行</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（十二）分布式集群中HDFS系统的各种角色</title>
      <link href="/2018-04-12-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E4%B8%ADHDFS%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%90%84%E7%A7%8D%E8%A7%92%E8%89%B2.html"/>
      <url>/2018-04-12-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E4%B8%ADHDFS%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%90%84%E7%A7%8D%E8%A7%92%E8%89%B2.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（十二）分布式集群中HDFS系统的各种角色：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（十二）分布式集群中HDFS系统的各种角色</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h2><h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><p>理解 namenode 的工作机制尤其是元数据管理机制，以增强对 HDFS 工作原理的 理解，及培养 hadoop 集群运营中“性能调优”、“namenode”故障问题的分析解决能力</p><h3 id="问题场景"><a href="#问题场景" class="headerlink" title="问题场景"></a>问题场景</h3><p>1、Namenode 服务器的磁盘故障导致 namenode 宕机，如何挽救集群及数据？</p><p>2、Namenode 是否可以有多个？namenode 内存要配置多大？namenode 跟集群数据存储能 力有关系吗？</p><p>3、文件的 blocksize 究竟调大好还是调小好？结合 mapreduce</p><h3 id="NameNode的职责"><a href="#NameNode的职责" class="headerlink" title="NameNode的职责"></a>NameNode的职责</h3><p>1、负责客户端请求（读写数据  请求 ）的响应<br>2、维护目录树结构（ 元数据的管理： 查询，修改 ）<br>3、配置和应用副本存放策略<br>4、管理集群数据块负载均衡问题</p><h3 id="NameNode元数据的管理"><a href="#NameNode元数据的管理" class="headerlink" title="NameNode元数据的管理"></a>NameNode元数据的管理</h3><h4 id="WAL（Write-ahead-Log）-预写日志系统"><a href="#WAL（Write-ahead-Log）-预写日志系统" class="headerlink" title="WAL（Write ahead Log）: 预写日志系统"></a>WAL（Write ahead Log）: 预写日志系统</h4><p>　　在计算机科学中，预写式日志（Write-ahead logging，缩写 WAL）是关系数据库系统中 用于提供原子性和持久性（ACID 属性中的两个）的一系列技术。在使用 WAL 的系统中，所 有的修改在提交之前都要先写入 log 文件中。</p><p>　　Log 文件中通常包括 redo 和 undo 信息。这样做的目的可以通过一个例子来说明。假设 一个程序在执行某些操作的过程中机器掉电了。在重新启动时，程序可能需要知道当时执行 的操作是成功了还是部分成功或者是失败了。如果使用了 WAL，程序就可以检查 log 文件， 并对突然掉电时计划执行的操作内容跟实际上执行的操作内容进行比较。在这个比较的基础 上，程序就可以决定是撤销已做的操作还是继续完成已做的操作，或者是保持原样。</p><p>　　WAL 允许用 in-place 方式更新数据库。另一种用来实现原子更新的方法是 shadow paging， 它并不是 in-place 方式。用 in-place 方式做更新的主要优点是减少索引和块列表的修改。ARIES 是 WAL 系列技术常用的算法。在文件系统中，WAL 通常称为 journaling。PostgreSQL 也是用 WAL 来提供 point-in-time 恢复和数据库复制特性。</p><p>　　NameNode 对数据的管理采用了两种存储形式：<strong>内存和磁盘</strong></p><p>　　首先是<strong>内存</strong>中存储了一份完整的元数据，包括目录树结构，以及文件和数据块和副本存储地 的映射关系；</p><p>1、内存元数据 metadata（全部存在内存中），其次是在磁盘中也存储了一份完整的元数据。</p><p>2、磁盘元数据镜像文件 <strong>fsimage_0000000000000000555</strong></p><p>fsimage_0000000000000000555 等价于</p><p>edits_0000000000000000001-0000000000000000018</p><p>……</p><p>edits_0000000000000000444-0000000000000000555</p><p>合并之和</p><p>3、数据历史操作日志文件 edits：edits_0000000000000000001-0000000000000000018 （可通过日志运算出元数据，全部存在磁盘中）</p><p>4、数据预写操作日志文件 edits_inprogress_0000000000000000556 （存储在磁盘中）</p><p>metadata = 最新 fsimage_0000000000000000555 + edits_inprogress_0000000000000000556</p><p>metadata = 所有的 edits 之和（edits_001_002 + …… + edits_444_555 + edits_inprogress_556）</p><p>VERSION（存放 hdfs 集群的版本信息）文件解析：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#Sun Jan 06 20:12:30 CST 2017 ## 集群启动时间</span><br><span class="line">namespaceID=844434736 ## 文件系统唯一标识符</span><br><span class="line">clusterID=CID-5b7b7321-e43f-456e-bf41-18e77c5e5a40 ## 集群唯一标识符</span><br><span class="line">cTime=0 ## fsimage 创建的时间，初始为 0，随 layoutVersion 更新</span><br><span class="line">storageType=NAME_NODE ##节点类型</span><br><span class="line">blockpoolID=BP-265332847-192.168.123.202-1483581570658 ## 数据块池 ID，可以有多个</span><br><span class="line">layoutVersion=-60 ## hdfs 持久化数据结构的版本号</span><br></pre></td></tr></table></figure><p>查看 edits 文件信息： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs oev -i edits_0000000000000000482-0000000000000000483 -o edits.xml </span><br><span class="line">cat edits.xml</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180313131552008-238547508.png" alt="img"></p><p>查看 fsimage 镜像文件信息： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs oiv -i fsimage_0000000000000000348 -p XML -o fsimage.xml </span><br><span class="line">cat fsimage.xml</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180313131644568-552650219.png" alt="img"></p><h3 id="NameNode-元数据存储机制"><a href="#NameNode-元数据存储机制" class="headerlink" title="NameNode 元数据存储机制"></a>NameNode 元数据存储机制</h3><p>A、内存中有一份完整的元数据(内存 metadata)</p><p>B、磁盘有一个“准完整”的元数据镜像（fsimage）文件(在 namenode 的工作目录中)</p><p>C、用于衔接内存 metadata 和持久化元数据镜像 fsimage 之间的操作日志（edits 文件）</p><p>（PS：当客户端对 hdfs 中的文件进行新增或者修改操作，操作记录首先被记入 edits 日志 文件中，当客户端操作成功后，相应的元数据会更新到内存 metadata 中）</p><h2 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h2><h3 id="问题场景-1"><a href="#问题场景-1" class="headerlink" title="问题场景"></a>问题场景</h3><p>1、集群容量不够，怎么扩容？</p><p>2、如果有一些 datanode 宕机，该怎么办？</p><p>3、datanode 明明已启动，但是集群中的可用 datanode 列表中就是没有，怎么办？</p><p>Datanode 工作职责</p><p>1、存储管理用户的文件块数据</p><p>2、定期向 namenode 汇报自身所持有的 block 信息（通过心跳信息上报）</p><p>（PS：这点很重要，因为，当集群中发生某些 block 副本失效时，集群如何恢复 block 初始 副本数量的问题）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;!—HDFS 集群数据冗余块的自动删除时长，单位 ms，默认一个小时 --&gt;</span><br><span class="line">&lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt;</span><br><span class="line">&lt;value&gt;3600000&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Determines block reporting interval in milliseconds.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h3 id="Datanode-掉线判断时限参数"><a href="#Datanode-掉线判断时限参数" class="headerlink" title="Datanode 掉线判断时限参数"></a>Datanode 掉线判断时限参数</h3><p>datanode 进程死亡或者网络故障造成 datanode 无法与 namenode 通信，namenode 不会立即 把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS 默认的超时时长 为 10 分钟+30 秒。如果定义超时时间为 timeout，则超时时长的计算公式为： t</p><p>imeout = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval</p><p>而默认的 <strong>heartbeat.recheck.interval</strong> 大小为 5 分钟，<strong>dfs.heartbeat.interval</strong> 默认为 3 秒。 需要注意的是 hdfs-site.xml 配置文件中的 heartbeat.recheck.interval 的单位为毫秒， dfs.heartbeat.interval 的单位为秒。 所以，举个例子，如果 heartbeat.recheck.interval 设置为 5000（毫秒），dfs.heartbeat.interval 设置为 3（秒，默认），则总的超时时间为 40 秒。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;heartbeat.recheck.interval&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;5000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h2 id="SecondaryNameNode"><a href="#SecondaryNameNode" class="headerlink" title="SecondaryNameNode"></a>SecondaryNameNode</h2><h3 id="SecondaryNamenode-工作机制"><a href="#SecondaryNamenode-工作机制" class="headerlink" title="SecondaryNamenode 工作机制"></a>SecondaryNamenode 工作机制</h3><p>SecondaryNamenode 的作用就是分担 namenode 的合并元数据的压力。所以在配置 SecondaryNamenode 的工作节点时，一定切记，不要和 namenode 处于同一节点。但事实上， 只有在普通的伪分布式集群和分布式集群中才有会 SecondaryNamenode 这个角色，在 HA 或 者联邦集群中都不再出现该角色。在 HA 和联邦集群中，都是有 standby namenode 承担。</p><h3 id="元数据的-CheckPoint"><a href="#元数据的-CheckPoint" class="headerlink" title="元数据的 CheckPoint"></a>元数据的 CheckPoint</h3><p>每隔一段时间，会由 secondary namenode 将 namenode 上积累的所有 edits 和一个最新的 fsimage 下载到本地，并加载到内存进行 merge（这个过程称为 checkpoint） CheckPoint 详细过程图解：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180313132250508-1007395809.png" alt="img"></p><h4 id="CheckPoint-触发配置"><a href="#CheckPoint-触发配置" class="headerlink" title="CheckPoint 触发配置"></a>CheckPoint 触发配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dfs.namenode.checkpoint.check.period=60 ##检查触发条件是否满足的频率，60 秒</span><br><span class="line">dfs.namenode.checkpoint.dir=file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary</span><br><span class="line">##以上两个参数做 checkpoint 操作时，secondary namenode 的本地工作目录</span><br><span class="line">dfs.namenode.checkpoint.edits.dir=$&#123;dfs.namenode.checkpoint.dir&#125;</span><br><span class="line">dfs.namenode.checkpoint.max-retries=3 ##最大重试次数</span><br><span class="line">dfs.namenode.checkpoint.period=3600 ##两次 checkpoint 之间的时间间隔 3600 秒</span><br><span class="line">dfs.namenode.checkpoint.txns=1000000 ##两次 checkpoint 之间最大的操作记录</span><br></pre></td></tr></table></figure><h4 id="CheckPoint-附带作用"><a href="#CheckPoint-附带作用" class="headerlink" title="CheckPoint 附带作用"></a>CheckPoint 附带作用</h4><p>Namenode 和 SecondaryNamenode 的工作目录存储结构完全相同，所以，当 Namenode 故障 退出需要重新恢复时，可以从SecondaryNamenode的工作目录中将fsimage拷贝到Namenode 的工作目录，以恢复 namenode 的元数据</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（十一）HDFS的读写详解</title>
      <link href="/2018-04-11-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89HDFS%E7%9A%84%E8%AF%BB%E5%86%99%E8%AF%A6%E8%A7%A3.html"/>
      <url>/2018-04-11-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89HDFS%E7%9A%84%E8%AF%BB%E5%86%99%E8%AF%A6%E8%A7%A3.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（十一）HDFS的读写详解：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（十一）HDFS的读写详解</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="HDFS的写操作"><a href="#HDFS的写操作" class="headerlink" title="HDFS的写操作"></a>HDFS的写操作</h2><h3 id="《HDFS权威指南》图解HDFS写过程"><a href="#《HDFS权威指南》图解HDFS写过程" class="headerlink" title="《HDFS权威指南》图解HDFS写过程"></a>《HDFS权威指南》图解HDFS写过程</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180312131601322-859729566.png" alt="img"></p><p>​        <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180312133050238-544764862.png" alt="img"></p><h3 id="详细文字说明（术语）"><a href="#详细文字说明（术语）" class="headerlink" title="详细文字说明（术语）"></a>详细文字说明（术语）</h3><p>1、使用 HDFS 提供的客户端 Client，向远程的 namenode 发起 RPC 请求</p><p>2、namenode 会检查要创建的文件是否已经存在，创建者是否有权限进行操作，成功则会 为文件创建一个记录，否则会让客户端抛出异常；</p><p>3、当客户端开始写入文件的时候，客户端会将文件切分成多个 packets，并在内部以数据队列“data queue（数据队列）”的形式管理这些 packets，并向 namenode 申请 blocks，获 取用来存储 replicas 的合适的 datanode 列表，列表的大小根据 namenode 中 replication 的设定而定；</p><p>4、开始以 pipeline（管道）的形式将 packet 写入所有的 replicas 中。客户端把 packet 以流的 方式写入第一个 datanode，该 datanode 把该 packet 存储之后，再将其传递给在此 pipeline 中的下一个 datanode，直到最后一个 datanode，这种写数据的方式呈流水线的形式。</p><p>5、最后一个 datanode 成功存储之后会返回一个 ack packet（确认队列），在 pipeline 里传递 至客户端，在客户端的开发库内部维护着”ack queue”，成功收到 datanode 返回的 ack packet 后会从”data queue”移除相应的 packet。</p><p>6、如果传输过程中，有某个 datanode 出现了故障，那么当前的 pipeline 会被关闭，出现故 障的 datanode 会从当前的 pipeline 中移除，剩余的 block 会继续剩下的 datanode 中继续 以 pipeline 的形式传输，同时 namenode 会分配一个新的 datanode，保持 replicas 设定的 数量。</p><p>7、客户端完成数据的写入后，会对数据流调用 close()方法，关闭数据流；</p><p>8、只要写入了 dfs.replication.min（最小写入成功的副本数）的复本数（默认为 1），写操作 就会成功，并且这个块可以在集群中异步复制，直到达到其目标复本数（dfs.replication 的默认值为 3），因为 namenode 已经知道文件由哪些块组成，所以它在返回成功前只需 要等待数据块进行最小量的复制。</p><h3 id="详细文字说明（口语）"><a href="#详细文字说明（口语）" class="headerlink" title="详细文字说明（口语）"></a>详细文字说明（口语）</h3><p>1、客户端发起请求：hadoop fs -put hadoop.tar.gz /　</p><blockquote><p><strong>客户端怎么知道请求发给那个节点的哪个进程？</strong></p><p>因为客户端会提供一些工具来解析出来你所指定的HDFS集群的主节点是谁，以及端口号等信息，主要是通过URI来确定，</p><p>url：hdfs://hadoop1:9000</p><p>当前请求会包含一个非常重要的信息： 上传的数据的总大小</p></blockquote><p>2、namenode会响应客户端的这个请求</p><blockquote><p><strong>namenode的职责：</strong></p><p>1 管理元数据（抽象目录树结构）</p><p>用户上传的那个文件在对应的目录如果存在。那么HDFS集群应该作何处理，不会处理</p><p>用户上传的那个文件要存储的目录不存在的话，如果不存在不会创建</p><p>2、响应请求</p><p>真正的操作：做一系列的校验，</p><p>1、校验客户端的请求是否合理<br>2、校验客户端是否有权限进行上传</p></blockquote><p>3、如果namenode返回给客户端的结果是 通过， 那就是允许上传</p><blockquote><p>namenode会给客户端返回对应的所有的数据块的多个副本的存放节点列表，如：</p><p>file1_blk1 hadoop02，hadoop03，hadoop04<br>file1_blk2    hadoop03，hadoop04，hadoop05</p></blockquote><p>4、客户端在获取到了namenode返回回来的所有数据块的多个副本的存放地的数据之后，就可以按照顺序逐一进行数据块的上传操作</p><p>5、对要上传的数据块进行逻辑切片</p><blockquote><p>切片分成两个阶段:</p><p>1、规划怎么切<br>2、真正的切</p><p>物理切片： 1 和 2</p><p>逻辑切片： 1</p><p>file1_blk1 ： file1:0:128<br>file1_blk2 ： file1:128:256</p></blockquote><p>　　逻辑切片只是规划了怎么切</p><p>　　<img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180312133938697-2120240662.png" alt="img"></p><p>6、开始上传第一个数据块</p><p>7、客户端会做一系列准备操作</p><blockquote><p>1、依次发送请求去连接对应的datnaode</p><p>pipline : client - node1 - node2 - node3</p><p>按照一个个的数据包的形式进行发送的。</p><p>每次传输完一个数据包，每个副本节点都会进行校验，依次原路给客户端</p><p>2、在客户端会启动一个服务：</p><p>用户就是用来等到将来要在这个pipline数据管道上进行传输的数据包的校验信息</p><p>客户端就能知道当前从clinet到写node1,2,3三个节点上去的数据是否都写入正确和成功</p></blockquote><p>8、clinet会正式的把这个快中的所有packet都写入到对应的副本节点</p><blockquote><p>1、block是最大的一个单位，它是最终存储于DataNode上的数据粒度，由<strong>dfs.block.size</strong>参数决定，2.x版本默认是<strong>128M</strong>；注：这个参数由客户端配置决定；如：System.out.println(conf.get(“dfs.blocksize”));//结果是134217728</p><p>2、packet是中等的一个单位，它是数据由DFSClient流向DataNode的粒度，以<strong>dfs.write.packet.size</strong>参数为参考值，默认是<strong>64K</strong>；注：这个参数为参考值，是指真正在进行数据传输时，会以它为基准进行调整，调整的原因是一个packet有特定的结构，调整的目标是这个packet的大小刚好包含结构中的所有成员，同时也保证写到DataNode后当前block的大小不超过设定值；</p><p>如：System.out.println(conf.get(“dfs.write.packet.size”));//结果是65536</p><p>3、chunk是最小的一个单位，它是DFSClient到DataNode数据传输中进行数据校验的粒度，由io.bytes.per.checksum参数决定，默认是<strong>512B</strong>；注：事实上一个chunk还包含<strong>4B的校验值</strong>，因而chunk写入packet时是<strong>516B</strong>；数据与检验值的比值为128:1，所以对于一个128M的block会有一个1M的校验文件与之对应；</p><p>如：System.out.println(conf.get(“io.bytes.per.checksum”));//结果是512</p></blockquote><p>9、clinet进行校验，如果校验通过，表示该数据块写入成功</p><p>10、重复7 8 9 三个操作，来继续上传其他的数据块</p><p>11、客户端在意识到所有的数据块都写入成功之后，会给namenode发送一个反馈，就是告诉namenode当前客户端上传的数据已经成功。</p><h2 id="HDFS读操作"><a href="#HDFS读操作" class="headerlink" title="HDFS读操作"></a>HDFS读操作</h2><h3 id="《HDFS权威指南》图解HDFS读过程"><a href="#《HDFS权威指南》图解HDFS读过程" class="headerlink" title="《HDFS权威指南》图解HDFS读过程"></a>《HDFS权威指南》图解HDFS读过程</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180313104157141-1682640973.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180313104237694-682511475.png" alt="img"></p><h3 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h3><p>1、客户端调用FileSystem 实例的open 方法，获得这个文件对应的输入流InputStream。</p><p>2、通过RPC 远程调用NameNode ，获得NameNode 中此文件对应的数据块保存位置，包括这个文件的副本的保存位置( 主要是各DataNode的地址) 。</p><p>3、获得输入流之后，客户端调用read 方法读取数据。选择最近的DataNode 建立连接并读取数据。</p><p>4、如果客户端和其中一个DataNode 位于同一机器(比如MapReduce 过程中的mapper 和reducer)，那么就会直接从本地读取数据。</p><p>5、到达数据块末端，关闭与这个DataNode 的连接，然后重新查找下一个数据块。</p><p>6、不断执行第2 - 5 步直到数据全部读完。</p><p>7、客户端调用close ，关闭输入流DF S InputStream。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（十）HDFS API的使用</title>
      <link href="/2018-04-10-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%EF%BC%89HDFS%20API%E7%9A%84%E4%BD%BF%E7%94%A8.html"/>
      <url>/2018-04-10-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%EF%BC%89HDFS%20API%E7%9A%84%E4%BD%BF%E7%94%A8.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（十）HDFS API的使用：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（十）HDFS API的使用</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>HDFS API的高级编程</p><p>HDFS的API就两个：<strong>FileSystem 和Configuration</strong></p><h2 id="1、文件的上传和下载"><a href="#1、文件的上传和下载" class="headerlink" title="1、文件的上传和下载"></a>1、文件的上传和下载</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"> 1 package com.ghgj.hdfs.api;</span><br><span class="line"> 2 </span><br><span class="line"> 3 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 4 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 5 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 6 </span><br><span class="line"> 7 public class HDFS_GET_AND_PUT &#123;</span><br><span class="line"> 8 </span><br><span class="line"> 9     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">10         </span><br><span class="line">11         </span><br><span class="line">12         Configuration conf = new Configuration();</span><br><span class="line">13         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);</span><br><span class="line">14         conf.set(&quot;dfs.replication&quot;, &quot;2&quot;);</span><br><span class="line">15         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">16         </span><br><span class="line">17         </span><br><span class="line">18         /**</span><br><span class="line">19          * 更改操作用户有两种方式:</span><br><span class="line">20          * </span><br><span class="line">21          * 1、直接设置运行换种的用户名为hadoop</span><br><span class="line">22          * </span><br><span class="line">23          *     VM arguments ;   -DHADOOP_USER_NAME=hadoop</span><br><span class="line">24          * </span><br><span class="line">25          * 2、在代码中进行声明</span><br><span class="line">26          * </span><br><span class="line">27          *  System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">28          */</span><br><span class="line">29         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">30         </span><br><span class="line">31         // 上传</span><br><span class="line">32         fs.copyFromLocalFile(new Path(&quot;c:/sss.txt&quot;), new Path(&quot;/a/ggg.txt&quot;));</span><br><span class="line">33         </span><br><span class="line">34         </span><br><span class="line">35         </span><br><span class="line">36         /**</span><br><span class="line">37          * .crc  ： 校验文件</span><br><span class="line">38          * </span><br><span class="line">39          * 每个块的元数据信息都只会记录合法数据的起始偏移量：  qqq.txt  blk_41838 :  0 - 1100byte</span><br><span class="line">40          * </span><br><span class="line">41          * 如果进行非法的数据追加。最终是能够下载合法数据。</span><br><span class="line">42          * 由于你在数据的中间， 也就是说在 0 -1100 之间的范围进行了数据信息的更改。 造成了采用CRC算法计算出来校验值，和最初存入进HDFS的校验值</span><br><span class="line">43          * 不一致。HDFS就认为当前这个文件被损坏了。</span><br><span class="line">44          */</span><br><span class="line">45         </span><br><span class="line">46         </span><br><span class="line">47         // 下载 </span><br><span class="line">48         fs.copyToLocalFile(new Path(&quot;/a/qqq.txt&quot;), new Path(&quot;c:/qqq3.txt&quot;));</span><br><span class="line">49         </span><br><span class="line">50         </span><br><span class="line">51         /**</span><br><span class="line">52          * 上传和下载的API的底层封装其实就是 ： FileUtil.copy(....)</span><br><span class="line">53          */</span><br><span class="line">54         </span><br><span class="line">55         fs.close();</span><br><span class="line">56     &#125;</span><br><span class="line">57 &#125;</span><br></pre></td></tr></table></figure><h2 id="2、配置文件conf"><a href="#2、配置文件conf" class="headerlink" title="2、配置文件conf"></a>2、配置文件conf</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"> 1 package com.exam.hdfs;</span><br><span class="line"> 2 </span><br><span class="line"> 3 import java.io.IOException;</span><br><span class="line"> 4 import java.util.Iterator;</span><br><span class="line"> 5 import java.util.Map.Entry;</span><br><span class="line"> 6 </span><br><span class="line"> 7 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 8 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 9 </span><br><span class="line">10 public class TestConf1 &#123;</span><br><span class="line">11 </span><br><span class="line">12     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">13         </span><br><span class="line">14         </span><br><span class="line">15         /**</span><br><span class="line">16          * 底层会加载一堆的配置文件：</span><br><span class="line">17          * </span><br><span class="line">18          * core-default.xml</span><br><span class="line">19          * hdfs-default.xml</span><br><span class="line">20          * mapred-default.xml</span><br><span class="line">21          * yarn-default.xml</span><br><span class="line">22          */</span><br><span class="line">23         Configuration conf = new Configuration();</span><br><span class="line">24 //        conf.addResource(&quot;hdfs-default.xml&quot;);</span><br><span class="line">25         </span><br><span class="line">26         /**</span><br><span class="line">27          * 当前这个hdfs-site.xml文件就放置在这个项目中的src下。也就是classpath路径下。</span><br><span class="line">28          * 所以 FS在初始化的时候，会把hdfs-site.xml这个文件中的name-value对解析到conf中</span><br><span class="line">29          * </span><br><span class="line">30          * </span><br><span class="line">31          * 但是：</span><br><span class="line">32          * </span><br><span class="line">33          * 1、如果hdfs-site.xml 不在src下， 看是否能加载？？？  不能</span><br><span class="line">34          * </span><br><span class="line">35          * 2、如果文件名不叫做 hdfs-default.xml 或者 hdsf-site.xml  看是否能自动加载？？？  不能</span><br><span class="line">36          * </span><br><span class="line">37          * 得出的结论：</span><br><span class="line">38          * </span><br><span class="line">39          * 如果需要项目代码自动加载配置文件中的信息，那么就必须把配置文件改成-default.xml或者-site.xml的名称</span><br><span class="line">40          * 而且必须放置在src下</span><br><span class="line">41          * </span><br><span class="line">42          * 那如果不叫这个名，或者不在src下，也需要加载这些配置文件中的参数：</span><br><span class="line">43          * </span><br><span class="line">44          * 必须使用conf对象提供的一些方法去手动加载</span><br><span class="line">45          */</span><br><span class="line">46 //        conf.addResource(&quot;hdfs-site.xml&quot;);</span><br><span class="line">47         conf.set(&quot;dfs.replication&quot;, &quot;1&quot;);</span><br><span class="line">48         conf.addResource(&quot;myconfig/hdfs-site.xml&quot;);</span><br><span class="line">49         </span><br><span class="line">50         </span><br><span class="line">51         /**</span><br><span class="line">52          * 依次加载的参数信息的顺序是：</span><br><span class="line">53          * </span><br><span class="line">54          * 1、加载 core/hdfs/mapred/yarn-default.xml</span><br><span class="line">55          * </span><br><span class="line">56          * 2、加载通过conf.addResources()加载的配置文件</span><br><span class="line">57          * </span><br><span class="line">58          * 3、加载conf.set(name, value)</span><br><span class="line">59          */</span><br><span class="line">60         </span><br><span class="line">61         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">62         </span><br><span class="line">63         System.out.println(conf.get(&quot;dfs.replication&quot;));</span><br><span class="line">64 </span><br><span class="line">65         </span><br><span class="line">66         Iterator&lt;Entry&lt;String, String&gt;&gt; iterator = conf.iterator();</span><br><span class="line">67         while(iterator.hasNext())&#123;</span><br><span class="line">68             Entry&lt;String, String&gt; e = iterator.next();</span><br><span class="line">69             System.out.println(e.getKey() + &quot;\t&quot; + e.getValue());</span><br><span class="line">70         &#125;</span><br><span class="line">71     &#125;</span><br><span class="line">72 &#125;</span><br></pre></td></tr></table></figure><h2 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br></pre></td><td class="code"><pre><span class="line">  1 log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).</span><br><span class="line">  2 log4j:WARN Please initialize the log4j system properly.</span><br><span class="line">  3 log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span><br><span class="line">  4 1</span><br><span class="line">  5 hadoop.security.groups.cache.secs    300</span><br><span class="line">  6 dfs.datanode.cache.revocation.timeout.ms    900000</span><br><span class="line">  7 dfs.namenode.resource.check.interval    5000</span><br><span class="line">  8 s3.client-write-packet-size    65536</span><br><span class="line">  9 dfs.client.https.need-auth    false</span><br><span class="line"> 10 dfs.replication    1</span><br><span class="line"> 11 hadoop.security.group.mapping.ldap.directory.search.timeout    10000</span><br><span class="line"> 12 dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold    10737418240</span><br><span class="line"> 13 hadoop.work.around.non.threadsafe.getpwuid    false</span><br><span class="line"> 14 dfs.namenode.write-lock-reporting-threshold-ms    5000</span><br><span class="line"> 15 fs.ftp.host.port    21</span><br><span class="line"> 16 dfs.namenode.avoid.read.stale.datanode    false</span><br><span class="line"> 17 dfs.journalnode.rpc-address    0.0.0.0:8485</span><br><span class="line"> 18 hadoop.security.kms.client.encrypted.key.cache.expiry    43200000</span><br><span class="line"> 19 ipc.client.connection.maxidletime    10000</span><br><span class="line"> 20 hadoop.registry.zk.session.timeout.ms    60000</span><br><span class="line"> 21 tfile.io.chunk.size    1048576</span><br><span class="line"> 22 fs.automatic.close    true</span><br><span class="line"> 23 ha.health-monitor.sleep-after-disconnect.ms    1000</span><br><span class="line"> 24 io.map.index.interval    128</span><br><span class="line"> 25 dfs.namenode.https-address    0.0.0.0:50470</span><br><span class="line"> 26 dfs.mover.max-no-move-interval    60000</span><br><span class="line"> 27 io.seqfile.sorter.recordlimit    1000000</span><br><span class="line"> 28 fs.s3n.multipart.uploads.enabled    false</span><br><span class="line"> 29 hadoop.util.hash.type    murmur</span><br><span class="line"> 30 dfs.namenode.replication.min    1</span><br><span class="line"> 31 dfs.datanode.directoryscan.threads    1</span><br><span class="line"> 32 dfs.namenode.fs-limits.min-block-size    1048576</span><br><span class="line"> 33 dfs.datanode.directoryscan.interval    21600</span><br><span class="line"> 34 fs.AbstractFileSystem.file.impl    org.apache.hadoop.fs.local.LocalFs</span><br><span class="line"> 35 dfs.namenode.acls.enabled    false</span><br><span class="line"> 36 dfs.client.short.circuit.replica.stale.threshold.ms    1800000</span><br><span class="line"> 37 net.topology.script.number.args    100</span><br><span class="line"> 38 hadoop.http.authentication.token.validity    36000</span><br><span class="line"> 39 fs.s3.block.size    67108864</span><br><span class="line"> 40 dfs.namenode.resource.du.reserved    104857600</span><br><span class="line"> 41 ha.failover-controller.graceful-fence.rpc-timeout.ms    5000</span><br><span class="line"> 42 s3native.bytes-per-checksum    512</span><br><span class="line"> 43 dfs.namenode.datanode.registration.ip-hostname-check    true</span><br><span class="line"> 44 dfs.namenode.path.based.cache.block.map.allocation.percent    0.25</span><br><span class="line"> 45 dfs.namenode.backup.http-address    0.0.0.0:50105</span><br><span class="line"> 46 hadoop.security.group.mapping    org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback</span><br><span class="line"> 47 dfs.namenode.edits.noeditlogchannelflush    false</span><br><span class="line"> 48 dfs.datanode.cache.revocation.polling.ms    500</span><br><span class="line"> 49 dfs.namenode.audit.loggers    default</span><br><span class="line"> 50 hadoop.security.groups.cache.warn.after.ms    5000</span><br><span class="line"> 51 io.serializations    org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization</span><br><span class="line"> 52 dfs.namenode.lazypersist.file.scrub.interval.sec    300</span><br><span class="line"> 53 fs.s3a.threads.core    15</span><br><span class="line"> 54 hadoop.security.crypto.buffer.size    8192</span><br><span class="line"> 55 hadoop.http.cross-origin.allowed-methods    GET,POST,HEAD</span><br><span class="line"> 56 hadoop.registry.zk.retry.interval.ms    1000</span><br><span class="line"> 57 dfs.http.policy    HTTP_ONLY</span><br><span class="line"> 58 hadoop.registry.secure    false</span><br><span class="line"> 59 dfs.namenode.replication.interval    3</span><br><span class="line"> 60 dfs.namenode.safemode.min.datanodes    0</span><br><span class="line"> 61 dfs.client.file-block-storage-locations.num-threads    10</span><br><span class="line"> 62 nfs.dump.dir    /tmp/.hdfs-nfs</span><br><span class="line"> 63 dfs.namenode.secondary.https-address    0.0.0.0:50091</span><br><span class="line"> 64 hadoop.kerberos.kinit.command    kinit</span><br><span class="line"> 65 dfs.block.access.token.lifetime    600</span><br><span class="line"> 66 dfs.webhdfs.enabled    true</span><br><span class="line"> 67 dfs.client.use.datanode.hostname    false</span><br><span class="line"> 68 dfs.namenode.delegation.token.max-lifetime    604800000</span><br><span class="line"> 69 fs.trash.interval    0</span><br><span class="line"> 70 dfs.datanode.drop.cache.behind.writes    false</span><br><span class="line"> 71 dfs.namenode.avoid.write.stale.datanode    false</span><br><span class="line"> 72 dfs.namenode.num.extra.edits.retained    1000000</span><br><span class="line"> 73 s3.blocksize    67108864</span><br><span class="line"> 74 ipc.client.connect.max.retries.on.timeouts    45</span><br><span class="line"> 75 dfs.datanode.data.dir    /home/hadoop/data/hadoopdata/data</span><br><span class="line"> 76 fs.s3.buffer.dir    $&#123;hadoop.tmp.dir&#125;/s3</span><br><span class="line"> 77 fs.s3n.block.size    67108864</span><br><span class="line"> 78 nfs.exports.allowed.hosts    * rw</span><br><span class="line"> 79 ha.health-monitor.connect-retry-interval.ms    1000</span><br><span class="line"> 80 hadoop.security.instrumentation.requires.admin    false</span><br><span class="line"> 81 hadoop.registry.zk.retry.ceiling.ms    60000</span><br><span class="line"> 82 nfs.rtmax    1048576</span><br><span class="line"> 83 dfs.client.mmap.cache.size    256</span><br><span class="line"> 84 dfs.datanode.data.dir.perm    700</span><br><span class="line"> 85 io.file.buffer.size    4096</span><br><span class="line"> 86 dfs.namenode.backup.address    0.0.0.0:50100</span><br><span class="line"> 87 dfs.client.datanode-restart.timeout    30</span><br><span class="line"> 88 dfs.datanode.readahead.bytes    4194304</span><br><span class="line"> 89 dfs.namenode.xattrs.enabled    true</span><br><span class="line"> 90 io.mapfile.bloom.size    1048576</span><br><span class="line"> 91 ipc.client.connect.retry.interval    1000</span><br><span class="line"> 92 dfs.client-write-packet-size    65536</span><br><span class="line"> 93 dfs.namenode.checkpoint.txns    1000000</span><br><span class="line"> 94 dfs.datanode.bp-ready.timeout    20</span><br><span class="line"> 95 dfs.datanode.transfer.socket.send.buffer.size    131072</span><br><span class="line"> 96 hadoop.security.kms.client.authentication.retry-count    1</span><br><span class="line"> 97 dfs.client.block.write.retries    3</span><br><span class="line"> 98 fs.swift.impl    org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem</span><br><span class="line"> 99 ha.failover-controller.graceful-fence.connection.retries    1</span><br><span class="line">100 hadoop.registry.zk.connection.timeout.ms    15000</span><br><span class="line">101 dfs.namenode.safemode.threshold-pct    0.999f</span><br><span class="line">102 dfs.cachereport.intervalMsec    10000</span><br><span class="line">103 hadoop.security.java.secure.random.algorithm    SHA1PRNG</span><br><span class="line">104 ftp.blocksize    67108864</span><br><span class="line">105 dfs.namenode.list.cache.directives.num.responses    100</span><br><span class="line">106 dfs.namenode.kerberos.principal.pattern    *</span><br><span class="line">107 file.stream-buffer-size    4096</span><br><span class="line">108 dfs.datanode.dns.nameserver    default</span><br><span class="line">109 fs.s3a.max.total.tasks    1000</span><br><span class="line">110 dfs.namenode.replication.considerLoad    true</span><br><span class="line">111 nfs.allow.insecure.ports    true</span><br><span class="line">112 dfs.namenode.edits.journal-plugin.qjournal    org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager</span><br><span class="line">113 dfs.client.write.exclude.nodes.cache.expiry.interval.millis    600000</span><br><span class="line">114 dfs.client.mmap.cache.timeout.ms    3600000</span><br><span class="line">115 ipc.client.idlethreshold    4000</span><br><span class="line">116 io.skip.checksum.errors    false</span><br><span class="line">117 ftp.stream-buffer-size    4096</span><br><span class="line">118 fs.s3a.fast.upload    false</span><br><span class="line">119 dfs.client.failover.connection.retries.on.timeouts    0</span><br><span class="line">120 file.blocksize    67108864</span><br><span class="line">121 ftp.replication    3</span><br><span class="line">122 dfs.namenode.replication.work.multiplier.per.iteration    2</span><br><span class="line">123 hadoop.security.authorization    false</span><br><span class="line">124 hadoop.http.authentication.simple.anonymous.allowed    true</span><br><span class="line">125 s3native.client-write-packet-size    65536</span><br><span class="line">126 hadoop.rpc.socket.factory.class.default    org.apache.hadoop.net.StandardSocketFactory</span><br><span class="line">127 file.bytes-per-checksum    512</span><br><span class="line">128 dfs.datanode.slow.io.warning.threshold.ms    300</span><br><span class="line">129 fs.har.impl.disable.cache    true</span><br><span class="line">130 rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB    org.apache.hadoop.ipc.ProtobufRpcEngine</span><br><span class="line">131 io.seqfile.lazydecompress    true</span><br><span class="line">132 dfs.namenode.reject-unresolved-dn-topology-mapping    false</span><br><span class="line">133 hadoop.common.configuration.version    0.23.0</span><br><span class="line">134 hadoop.security.authentication    simple</span><br><span class="line">135 dfs.datanode.drop.cache.behind.reads    false</span><br><span class="line">136 dfs.image.compression.codec    org.apache.hadoop.io.compress.DefaultCodec</span><br><span class="line">137 dfs.client.read.shortcircuit.streams.cache.size    256</span><br><span class="line">138 file.replication    1</span><br><span class="line">139 dfs.namenode.top.num.users    10</span><br><span class="line">140 dfs.namenode.accesstime.precision    3600000</span><br><span class="line">141 dfs.namenode.fs-limits.max-xattrs-per-inode    32</span><br><span class="line">142 dfs.image.transfer.timeout    60000</span><br><span class="line">143 io.mapfile.bloom.error.rate    0.005</span><br><span class="line">144 nfs.wtmax    1048576</span><br><span class="line">145 hadoop.security.kms.client.encrypted.key.cache.size    500</span><br><span class="line">146 dfs.namenode.edit.log.autoroll.check.interval.ms    300000</span><br><span class="line">147 fs.s3a.multipart.purge    false</span><br><span class="line">148 dfs.namenode.support.allow.format    true</span><br><span class="line">149 hadoop.hdfs.configuration.version    1</span><br><span class="line">150 fs.s3a.connection.establish.timeout    5000</span><br><span class="line">151 hadoop.security.group.mapping.ldap.search.attr.member    member</span><br><span class="line">152 dfs.secondary.namenode.kerberos.internal.spnego.principal    $&#123;dfs.web.authentication.kerberos.principal&#125;</span><br><span class="line">153 dfs.stream-buffer-size    4096</span><br><span class="line">154 hadoop.ssl.client.conf    ssl-client.xml</span><br><span class="line">155 dfs.namenode.invalidate.work.pct.per.iteration    0.32f</span><br><span class="line">156 fs.s3a.multipart.purge.age    86400</span><br><span class="line">157 dfs.journalnode.https-address    0.0.0.0:8481</span><br><span class="line">158 dfs.namenode.top.enabled    true</span><br><span class="line">159 hadoop.security.kms.client.encrypted.key.cache.low-watermark    0.3f</span><br><span class="line">160 dfs.namenode.max.objects    0</span><br><span class="line">161 hadoop.user.group.static.mapping.overrides    dr.who=;</span><br><span class="line">162 fs.s3a.fast.buffer.size    1048576</span><br><span class="line">163 dfs.bytes-per-checksum    512</span><br><span class="line">164 dfs.datanode.max.transfer.threads    4096</span><br><span class="line">165 dfs.block.access.key.update.interval    600</span><br><span class="line">166 ipc.maximum.data.length    67108864</span><br><span class="line">167 tfile.fs.input.buffer.size    262144</span><br><span class="line">168 ha.failover-controller.new-active.rpc-timeout.ms    60000</span><br><span class="line">169 dfs.client.cached.conn.retry    3</span><br><span class="line">170 dfs.client.read.shortcircuit    false</span><br><span class="line">171 hadoop.ssl.hostname.verifier    DEFAULT</span><br><span class="line">172 dfs.datanode.hdfs-blocks-metadata.enabled    false</span><br><span class="line">173 dfs.datanode.directoryscan.throttle.limit.ms.per.sec    0</span><br><span class="line">174 dfs.image.transfer.chunksize    65536</span><br><span class="line">175 hadoop.http.authentication.type    simple</span><br><span class="line">176 dfs.namenode.list.encryption.zones.num.responses    100</span><br><span class="line">177 dfs.client.https.keystore.resource    ssl-client.xml</span><br><span class="line">178 s3native.blocksize    67108864</span><br><span class="line">179 net.topology.impl    org.apache.hadoop.net.NetworkTopology</span><br><span class="line">180 dfs.client.failover.sleep.base.millis    500</span><br><span class="line">181 io.seqfile.compress.blocksize    1000000</span><br><span class="line">182 dfs.namenode.path.based.cache.refresh.interval.ms    30000</span><br><span class="line">183 dfs.namenode.decommission.interval    30</span><br><span class="line">184 dfs.permissions.superusergroup    supergroup</span><br><span class="line">185 dfs.namenode.fs-limits.max-directory-items    1048576</span><br><span class="line">186 hadoop.registry.zk.retry.times    5</span><br><span class="line">187 dfs.ha.log-roll.period    120</span><br><span class="line">188 fs.AbstractFileSystem.ftp.impl    org.apache.hadoop.fs.ftp.FtpFs</span><br><span class="line">189 ftp.bytes-per-checksum    512</span><br><span class="line">190 dfs.user.home.dir.prefix    /user</span><br><span class="line">191 dfs.namenode.checkpoint.edits.dir    $&#123;dfs.namenode.checkpoint.dir&#125;</span><br><span class="line">192 dfs.client.socket.send.buffer.size    131072</span><br><span class="line">193 ipc.client.fallback-to-simple-auth-allowed    false</span><br><span class="line">194 dfs.blockreport.initialDelay    0</span><br><span class="line">195 dfs.namenode.inotify.max.events.per.rpc    1000</span><br><span class="line">196 dfs.namenode.heartbeat.recheck-interval    300000</span><br><span class="line">197 dfs.namenode.safemode.extension    30000</span><br><span class="line">198 dfs.client.failover.sleep.max.millis    15000</span><br><span class="line">199 dfs.namenode.delegation.key.update-interval    86400000</span><br><span class="line">200 dfs.datanode.transfer.socket.recv.buffer.size    131072</span><br><span class="line">201 hadoop.rpc.protection    authentication</span><br><span class="line">202 fs.permissions.umask-mode    022</span><br><span class="line">203 fs.s3.sleepTimeSeconds    10</span><br><span class="line">204 dfs.namenode.fs-limits.max-xattr-size    16384</span><br><span class="line">205 ha.health-monitor.rpc-timeout.ms    45000</span><br><span class="line">206 hadoop.http.staticuser.user    dr.who</span><br><span class="line">207 dfs.datanode.http.address    0.0.0.0:50075</span><br><span class="line">208 fs.s3a.connection.maximum    15</span><br><span class="line">209 fs.s3a.paging.maximum    5000</span><br><span class="line">210 fs.AbstractFileSystem.viewfs.impl    org.apache.hadoop.fs.viewfs.ViewFs</span><br><span class="line">211 dfs.namenode.blocks.per.postponedblocks.rescan    10000</span><br><span class="line">212 fs.ftp.host    0.0.0.0</span><br><span class="line">213 dfs.lock.suppress.warning.interval    10s</span><br><span class="line">214 hadoop.http.authentication.kerberos.keytab    $&#123;user.home&#125;/hadoop.keytab</span><br><span class="line">215 fs.s3a.impl    org.apache.hadoop.fs.s3a.S3AFileSystem</span><br><span class="line">216 hadoop.registry.zk.root    /registry</span><br><span class="line">217 hadoop.jetty.logs.serve.aliases    true</span><br><span class="line">218 dfs.namenode.fs-limits.max-blocks-per-file    1048576</span><br><span class="line">219 dfs.balancer.keytab.enabled    false</span><br><span class="line">220 dfs.client.block.write.replace-datanode-on-failure.enable    true</span><br><span class="line">221 hadoop.http.cross-origin.max-age    1800</span><br><span class="line">222 io.compression.codec.bzip2.library    system-native</span><br><span class="line">223 dfs.namenode.checkpoint.dir    file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary</span><br><span class="line">224 dfs.client.use.legacy.blockreader.local    false</span><br><span class="line">225 dfs.namenode.top.windows.minutes    1,5,25</span><br><span class="line">226 ipc.ping.interval    60000</span><br><span class="line">227 net.topology.node.switch.mapping.impl    org.apache.hadoop.net.ScriptBasedMapping</span><br><span class="line">228 nfs.mountd.port    4242</span><br><span class="line">229 dfs.storage.policy.enabled    true</span><br><span class="line">230 dfs.namenode.list.cache.pools.num.responses    100</span><br><span class="line">231 fs.df.interval    60000</span><br><span class="line">232 nfs.server.port    2049</span><br><span class="line">233 ha.zookeeper.parent-znode    /hadoop-ha</span><br><span class="line">234 hadoop.http.cross-origin.allowed-headers    X-Requested-With,Content-Type,Accept,Origin</span><br><span class="line">235 dfs.datanode.block-pinning.enabled    false</span><br><span class="line">236 dfs.namenode.num.checkpoints.retained    2</span><br><span class="line">237 fs.s3a.attempts.maximum    10</span><br><span class="line">238 s3native.stream-buffer-size    4096</span><br><span class="line">239 io.seqfile.local.dir    $&#123;hadoop.tmp.dir&#125;/io/local</span><br><span class="line">240 fs.s3n.multipart.copy.block.size    5368709120</span><br><span class="line">241 dfs.encrypt.data.transfer.cipher.key.bitlength    128</span><br><span class="line">242 dfs.client.mmap.retry.timeout.ms    300000</span><br><span class="line">243 dfs.datanode.sync.behind.writes    false</span><br><span class="line">244 dfs.namenode.fslock.fair    true</span><br><span class="line">245 hadoop.ssl.keystores.factory.class    org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</span><br><span class="line">246 dfs.permissions.enabled    true</span><br><span class="line">247 fs.AbstractFileSystem.hdfs.impl    org.apache.hadoop.fs.Hdfs</span><br><span class="line">248 dfs.blockreport.split.threshold    1000000</span><br><span class="line">249 dfs.datanode.balance.bandwidthPerSec    1048576</span><br><span class="line">250 dfs.block.scanner.volume.bytes.per.second    1048576</span><br><span class="line">251 hadoop.security.random.device.file.path    /dev/urandom</span><br><span class="line">252 fs.s3.maxRetries    4</span><br><span class="line">253 hadoop.http.filter.initializers    org.apache.hadoop.http.lib.StaticUserWebFilter</span><br><span class="line">254 dfs.namenode.stale.datanode.interval    30000</span><br><span class="line">255 ipc.client.rpc-timeout.ms    0</span><br><span class="line">256 fs.client.resolve.remote.symlinks    true</span><br><span class="line">257 dfs.default.chunk.view.size    32768</span><br><span class="line">258 hadoop.ssl.enabled.protocols    TLSv1</span><br><span class="line">259 dfs.namenode.decommission.blocks.per.interval    500000</span><br><span class="line">260 dfs.namenode.handler.count    10</span><br><span class="line">261 dfs.image.transfer.bandwidthPerSec    0</span><br><span class="line">262 rpc.metrics.quantile.enable    false</span><br><span class="line">263 hadoop.ssl.enabled    false</span><br><span class="line">264 dfs.replication.max    512</span><br><span class="line">265 dfs.namenode.name.dir    /home/hadoop/data/hadoopdata/name</span><br><span class="line">266 dfs.namenode.read-lock-reporting-threshold-ms    5000</span><br><span class="line">267 dfs.datanode.https.address    0.0.0.0:50475</span><br><span class="line">268 dfs.datanode.failed.volumes.tolerated    0</span><br><span class="line">269 ipc.client.kill.max    10</span><br><span class="line">270 fs.s3a.threads.max    256</span><br><span class="line">271 ipc.server.listen.queue.size    128</span><br><span class="line">272 dfs.client.domain.socket.data.traffic    false</span><br><span class="line">273 dfs.block.access.token.enable    false</span><br><span class="line">274 dfs.blocksize    134217728</span><br><span class="line">275 fs.s3a.connection.timeout    50000</span><br><span class="line">276 fs.s3a.threads.keepalivetime    60</span><br><span class="line">277 file.client-write-packet-size    65536</span><br><span class="line">278 dfs.datanode.address    0.0.0.0:50010</span><br><span class="line">279 ha.failover-controller.cli-check.rpc-timeout.ms    20000</span><br><span class="line">280 ha.zookeeper.acl    world:anyone:rwcda</span><br><span class="line">281 ipc.client.connect.max.retries    10</span><br><span class="line">282 dfs.encrypt.data.transfer    false</span><br><span class="line">283 dfs.namenode.write.stale.datanode.ratio    0.5f</span><br><span class="line">284 ipc.client.ping    true</span><br><span class="line">285 dfs.datanode.shared.file.descriptor.paths    /dev/shm,/tmp</span><br><span class="line">286 dfs.short.circuit.shared.memory.watcher.interrupt.check.ms    60000</span><br><span class="line">287 hadoop.tmp.dir    /home/hadoop/data/hadoopdata</span><br><span class="line">288 dfs.datanode.handler.count    10</span><br><span class="line">289 dfs.client.failover.max.attempts    15</span><br><span class="line">290 dfs.balancer.max-no-move-interval    60000</span><br><span class="line">291 dfs.client.read.shortcircuit.streams.cache.expiry.ms    300000</span><br><span class="line">292 dfs.namenode.block-placement-policy.default.prefer-local-node    true</span><br><span class="line">293 hadoop.ssl.require.client.cert    false</span><br><span class="line">294 hadoop.security.uid.cache.secs    14400</span><br><span class="line">295 dfs.client.read.shortcircuit.skip.checksum    false</span><br><span class="line">296 dfs.namenode.resource.checked.volumes.minimum    1</span><br><span class="line">297 hadoop.registry.rm.enabled    false</span><br><span class="line">298 dfs.namenode.quota.init-threads    4</span><br><span class="line">299 dfs.namenode.max.extra.edits.segments.retained    10000</span><br><span class="line">300 dfs.webhdfs.user.provider.user.pattern    ^[A-Za-z_][A-Za-z0-9._-]*[$]?$</span><br><span class="line">301 dfs.client.mmap.enabled    true</span><br><span class="line">302 dfs.client.file-block-storage-locations.timeout.millis    1000</span><br><span class="line">303 dfs.datanode.block.id.layout.upgrade.threads    12</span><br><span class="line">304 dfs.datanode.use.datanode.hostname    false</span><br><span class="line">305 hadoop.fuse.timer.period    5</span><br><span class="line">306 dfs.client.context    default</span><br><span class="line">307 fs.trash.checkpoint.interval    0</span><br><span class="line">308 dfs.journalnode.http-address    0.0.0.0:8480</span><br><span class="line">309 dfs.balancer.address    0.0.0.0:0</span><br><span class="line">310 dfs.namenode.lock.detailed-metrics.enabled    false</span><br><span class="line">311 dfs.namenode.delegation.token.renew-interval    86400000</span><br><span class="line">312 ha.health-monitor.check-interval.ms    1000</span><br><span class="line">313 dfs.namenode.retrycache.heap.percent    0.03f</span><br><span class="line">314 ipc.client.connect.timeout    20000</span><br><span class="line">315 dfs.reformat.disabled    false</span><br><span class="line">316 dfs.blockreport.intervalMsec    21600000</span><br><span class="line">317 fs.s3a.multipart.threshold    2147483647</span><br><span class="line">318 dfs.https.server.keystore.resource    ssl-server.xml</span><br><span class="line">319 hadoop.http.cross-origin.enabled    false</span><br><span class="line">320 io.map.index.skip    0</span><br><span class="line">321 dfs.balancer.block-move.timeout    0</span><br><span class="line">322 io.native.lib.available    true</span><br><span class="line">323 s3.replication    3</span><br><span class="line">324 dfs.namenode.kerberos.internal.spnego.principal    $&#123;dfs.web.authentication.kerberos.principal&#125;</span><br><span class="line">325 fs.AbstractFileSystem.har.impl    org.apache.hadoop.fs.HarFs</span><br><span class="line">326 hadoop.security.kms.client.encrypted.key.cache.num.refill.threads    2</span><br><span class="line">327 fs.s3n.multipart.uploads.block.size    67108864</span><br><span class="line">328 dfs.image.compress    false</span><br><span class="line">329 dfs.datanode.dns.interface    default</span><br><span class="line">330 dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction    0.75f</span><br><span class="line">331 tfile.fs.output.buffer.size    262144</span><br><span class="line">332 fs.du.interval    600000</span><br><span class="line">333 dfs.client.failover.connection.retries    0</span><br><span class="line">334 dfs.namenode.edit.log.autoroll.multiplier.threshold    2.0</span><br><span class="line">335 hadoop.security.group.mapping.ldap.ssl    false</span><br><span class="line">336 dfs.namenode.top.window.num.buckets    10</span><br><span class="line">337 fs.s3a.buffer.dir    $&#123;hadoop.tmp.dir&#125;/s3a</span><br><span class="line">338 dfs.namenode.checkpoint.check.period    60</span><br><span class="line">339 fs.defaultFS    hdfs://hadoop1:9000</span><br><span class="line">340 fs.s3a.multipart.size    104857600</span><br><span class="line">341 dfs.client.slow.io.warning.threshold.ms    30000</span><br><span class="line">342 dfs.datanode.max.locked.memory    0</span><br><span class="line">343 dfs.namenode.retrycache.expirytime.millis    600000</span><br><span class="line">344 hadoop.security.group.mapping.ldap.search.attr.group.name    cn</span><br><span class="line">345 dfs.client.block.write.replace-datanode-on-failure.best-effort    false</span><br><span class="line">346 dfs.ha.fencing.ssh.connect-timeout    30000</span><br><span class="line">347 dfs.datanode.scan.period.hours    504</span><br><span class="line">348 hadoop.registry.zk.quorum    localhost:2181</span><br><span class="line">349 dfs.namenode.fs-limits.max-component-length    255</span><br><span class="line">350 hadoop.http.cross-origin.allowed-origins    *</span><br><span class="line">351 dfs.namenode.enable.retrycache    true</span><br><span class="line">352 dfs.datanode.du.reserved    0</span><br><span class="line">353 dfs.datanode.ipc.address    0.0.0.0:50020</span><br><span class="line">354 hadoop.registry.system.acls    sasl:yarn@, sasl:mapred@, sasl:hdfs@</span><br><span class="line">355 dfs.namenode.path.based.cache.retry.interval.ms    30000</span><br><span class="line">356 hadoop.security.crypto.cipher.suite    AES/CTR/NoPadding</span><br><span class="line">357 dfs.client.block.write.replace-datanode-on-failure.policy    DEFAULT</span><br><span class="line">358 dfs.namenode.http-address    0.0.0.0:50070</span><br><span class="line">359 hadoop.security.crypto.codec.classes.aes.ctr.nopadding    org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec</span><br><span class="line">360 dfs.ha.tail-edits.period    60</span><br><span class="line">361 hadoop.security.groups.negative-cache.secs    30</span><br><span class="line">362 hadoop.ssl.server.conf    ssl-server.xml</span><br><span class="line">363 hadoop.registry.jaas.context    Client</span><br><span class="line">364 s3native.replication    3</span><br><span class="line">365 hadoop.security.group.mapping.ldap.search.filter.group    (objectClass=group)</span><br><span class="line">366 hadoop.http.authentication.kerberos.principal    HTTP/_HOST@LOCALHOST</span><br><span class="line">367 dfs.namenode.startup.delay.block.deletion.sec    0</span><br><span class="line">368 hadoop.security.group.mapping.ldap.search.filter.user    (&amp;(objectClass=user)(sAMAccountName=&#123;0&#125;))</span><br><span class="line">369 dfs.namenode.edits.dir    $&#123;dfs.namenode.name.dir&#125;</span><br><span class="line">370 dfs.namenode.checkpoint.max-retries    3</span><br><span class="line">371 s3.stream-buffer-size    4096</span><br><span class="line">372 ftp.client-write-packet-size    65536</span><br><span class="line">373 dfs.datanode.fsdatasetcache.max.threads.per.volume    4</span><br><span class="line">374 hadoop.security.sensitive-config-keys    password$,fs.s3.*[Ss]ecret.?[Kk]ey,fs.azure.account.key.*,dfs.webhdfs.oauth2.[a-z]+.token,hadoop.security.sensitive-config-keys</span><br><span class="line">375 dfs.namenode.decommission.max.concurrent.tracked.nodes    100</span><br><span class="line">376 dfs.namenode.name.dir.restore    false</span><br><span class="line">377 ipc.server.log.slow.rpc    false</span><br><span class="line">378 dfs.heartbeat.interval    3</span><br><span class="line">379 dfs.namenode.secondary.http-address    hadoop3:50090</span><br><span class="line">380 ha.zookeeper.session-timeout.ms    5000</span><br><span class="line">381 s3.bytes-per-checksum    512</span><br><span class="line">382 fs.s3a.connection.ssl.enabled    true</span><br><span class="line">383 hadoop.http.authentication.signature.secret.file    $&#123;user.home&#125;/hadoop-http-auth-signature-secret</span><br><span class="line">384 hadoop.fuse.connection.timeout    300</span><br><span class="line">385 dfs.namenode.checkpoint.period    3600</span><br><span class="line">386 ipc.server.max.connections    0</span><br><span class="line">387 dfs.ha.automatic-failover.enabled    false</span><br></pre></td></tr></table></figure><h2 id="3、列出指定目录下的文件以及块的信息"><a href="#3、列出指定目录下的文件以及块的信息" class="headerlink" title="3、列出指定目录下的文件以及块的信息"></a>3、列出指定目录下的文件以及块的信息</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"> 1 package com.exam.hdfs;</span><br><span class="line"> 2 </span><br><span class="line"> 3 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 4 import org.apache.hadoop.fs.BlockLocation;</span><br><span class="line"> 5 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 6 import org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"> 7 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 8 import org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"> 9 </span><br><span class="line">10 public class TestHDFS1 &#123;</span><br><span class="line">11 </span><br><span class="line">12     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">13 </span><br><span class="line">14         Configuration conf = new Configuration();</span><br><span class="line">15         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">16         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);</span><br><span class="line">17         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">18 </span><br><span class="line">19         /**</span><br><span class="line">20          * 列出指定的目录下的所有文件</span><br><span class="line">21          */</span><br><span class="line">22         RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true);</span><br><span class="line">23         while(listFiles.hasNext())&#123;</span><br><span class="line">24             LocatedFileStatus file = listFiles.next();</span><br><span class="line">25             </span><br><span class="line">26             </span><br><span class="line">27             System.out.println(file.getPath()+&quot;\t&quot;);</span><br><span class="line">28             System.out.println(file.getPath().getName()+&quot;\t&quot;);</span><br><span class="line">29             System.out.println(file.getLen()+&quot;\t&quot;);</span><br><span class="line">30             System.out.println(file.getReplication()+&quot;\t&quot;);</span><br><span class="line">31             </span><br><span class="line">32             /**</span><br><span class="line">33              * blockLocations的长度是几？  是什么意义？</span><br><span class="line">34              * </span><br><span class="line">35              * 块的数量</span><br><span class="line">36              */</span><br><span class="line">37             BlockLocation[] blockLocations = file.getBlockLocations();</span><br><span class="line">38             System.out.println(blockLocations.length+&quot;\t&quot;);</span><br><span class="line">39             </span><br><span class="line">40             for(BlockLocation bl : blockLocations)&#123;</span><br><span class="line">41                 String[] hosts = bl.getHosts();</span><br><span class="line">42                 </span><br><span class="line">43                 System.out.print(hosts[0] + &quot;-&quot; + hosts[1]+&quot;\t&quot;);</span><br><span class="line">44             &#125;</span><br><span class="line">45             System.out.println();</span><br><span class="line">46             </span><br><span class="line">47         &#125;</span><br><span class="line">48         </span><br><span class="line">49         </span><br><span class="line">50     &#125;</span><br><span class="line">51 &#125;</span><br></pre></td></tr></table></figure><p>输出结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1 hdfs://hadoop1:9000/aa/bb/cc/hadoop.tar.gz    </span><br><span class="line">2 hadoop.tar.gz    </span><br><span class="line">3 199007110    </span><br><span class="line">4 2    </span><br><span class="line">5 3    </span><br><span class="line">6 hadoop3-hadoop1    hadoop1-hadoop2    hadoop1-hadoop4</span><br></pre></td></tr></table></figure><h2 id="4、上传文件"><a href="#4、上传文件" class="headerlink" title="4、上传文件"></a>4、上传文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"> 1 package com.exam.hdfs;</span><br><span class="line"> 2 </span><br><span class="line"> 3 import java.io.File;</span><br><span class="line"> 4 import java.io.FileInputStream;</span><br><span class="line"> 5 import java.io.InputStream;</span><br><span class="line"> 6 </span><br><span class="line"> 7 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 8 import org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"> 9 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">10 import org.apache.hadoop.fs.Path;</span><br><span class="line">11 import org.apache.hadoop.io.IOUtils;</span><br><span class="line">12 </span><br><span class="line">13 public class UploadDataByStream &#123;</span><br><span class="line">14 </span><br><span class="line">15     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">16         </span><br><span class="line">17         </span><br><span class="line">18         Configuration conf = new Configuration();</span><br><span class="line">19         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">20         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);</span><br><span class="line">21         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">22         </span><br><span class="line">23         </span><br><span class="line">24         InputStream in = new FileInputStream(new File(&quot;d:/abc.tar.gz&quot;));</span><br><span class="line">25         FSDataOutputStream out = fs.create(new Path(&quot;/aa/abc.tar.gz&quot;));</span><br><span class="line">26         </span><br><span class="line">27         </span><br><span class="line">28         IOUtils.copyBytes(in, out, 4096, true);</span><br><span class="line">29         </span><br><span class="line">30         fs.close();</span><br><span class="line">31         </span><br><span class="line">32     &#125;</span><br><span class="line">33 &#125;</span><br></pre></td></tr></table></figure><h2 id="5、下载文件"><a href="#5、下载文件" class="headerlink" title="5、下载文件"></a>5、下载文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"> 1 package com.exam.hdfs;</span><br><span class="line"> 2 </span><br><span class="line"> 3 import java.io.File;</span><br><span class="line"> 4 import java.io.FileOutputStream;</span><br><span class="line"> 5 import java.io.OutputStream;</span><br><span class="line"> 6 </span><br><span class="line"> 7 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 8 import org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"> 9 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">10 import org.apache.hadoop.fs.Path;</span><br><span class="line">11 import org.apache.hadoop.io.IOUtils;</span><br><span class="line">12 </span><br><span class="line">13 public class DownloadDataByStream &#123;</span><br><span class="line">14 </span><br><span class="line">15     </span><br><span class="line">16     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">17         </span><br><span class="line">18         Configuration conf = new Configuration();</span><br><span class="line">19         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">20         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);</span><br><span class="line">21         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">22         </span><br><span class="line">23         </span><br><span class="line">24         FSDataInputStream in = fs.open(new Path(&quot;/aa/abc.tar.gz&quot;));</span><br><span class="line">25         OutputStream out = new FileOutputStream(new File(&quot;D:/abc.sh&quot;));</span><br><span class="line">26         </span><br><span class="line">27         </span><br><span class="line">28         IOUtils.copyBytes(in, out, 4096, true);</span><br><span class="line">29         </span><br><span class="line">30         fs.close();</span><br><span class="line">31         </span><br><span class="line">32     &#125;</span><br><span class="line">33 &#125;</span><br></pre></td></tr></table></figure><h2 id="6、删除某个路径下特定类型的文件，比如class类型文件，比如txt类型文件"><a href="#6、删除某个路径下特定类型的文件，比如class类型文件，比如txt类型文件" class="headerlink" title="6、删除某个路径下特定类型的文件，比如class类型文件，比如txt类型文件"></a>6、删除某个路径下特定类型的文件，比如class类型文件，比如txt类型文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"> 1 package com.exam.hdfs;</span><br><span class="line"> 2 </span><br><span class="line"> 3 import java.net.URI;</span><br><span class="line"> 4 </span><br><span class="line"> 5 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 6 import org.apache.hadoop.fs.FileStatus;</span><br><span class="line"> 7 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 8 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 9 </span><br><span class="line">10 public class HDFS_DELETE_CLASS &#123;</span><br><span class="line">11     </span><br><span class="line">12     public static final String FILETYPE = &quot;tar.gz&quot;;</span><br><span class="line">13     public static final String DELETE_PATH = &quot;/aa&quot;;</span><br><span class="line">14     </span><br><span class="line">15     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">16         </span><br><span class="line">17         new HDFS_DELETE_CLASS().rmrClassFile(new Path(DELETE_PATH));</span><br><span class="line">18     &#125;</span><br><span class="line">19     </span><br><span class="line">20     public void rmrClassFile(Path path) throws Exception&#123;</span><br><span class="line">21         </span><br><span class="line">22         // 首先获取集群必要的信息，以得到FileSystem的示例对象fs</span><br><span class="line">23         Configuration conf = new Configuration();</span><br><span class="line">24         FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop1:9000&quot;), conf, &quot;hadoop&quot;);</span><br><span class="line">25         </span><br><span class="line">26         // 首先检查path本身是文件夹还是目录</span><br><span class="line">27         FileStatus fileStatus = fs.getFileStatus(path);</span><br><span class="line">28         boolean directory = fileStatus.isDirectory();</span><br><span class="line">29         </span><br><span class="line">30         // 根据该目录是否是文件或者文件夹进行相应的操作</span><br><span class="line">31         if(directory)&#123;</span><br><span class="line">32             // 如果是目录</span><br><span class="line">33             checkAndDeleteDirectory(path, fs);</span><br><span class="line">34         &#125;else&#123;</span><br><span class="line">35             // 如果是文件，检查该文件名是不是FILETYPE类型的文件</span><br><span class="line">36             checkAndDeleteFile(path, fs);</span><br><span class="line">37         &#125;</span><br><span class="line">38     &#125;</span><br><span class="line">39     </span><br><span class="line">40     // 处理目录</span><br><span class="line">41     public static void checkAndDeleteDirectory(Path path, FileSystem fs) throws Exception&#123;</span><br><span class="line">42         // 查看该path目录下一级子目录和子文件的状态</span><br><span class="line">43         FileStatus[] listStatus = fs.listStatus(path);</span><br><span class="line">44         for(FileStatus fStatus: listStatus)&#123;</span><br><span class="line">45             Path p = fStatus.getPath();</span><br><span class="line">46             // 如果是文件，并且是以FILETYPE结尾，则删掉，否则继续遍历下一级目录</span><br><span class="line">47             if(fStatus.isFile())&#123;</span><br><span class="line">48                 checkAndDeleteFile(p, fs);</span><br><span class="line">49             &#125;else&#123;</span><br><span class="line">50                 checkAndDeleteDirectory(p, fs);</span><br><span class="line">51             &#125;</span><br><span class="line">52         &#125;</span><br><span class="line">53     &#125;</span><br><span class="line">54     </span><br><span class="line">55     // 檢查文件是否符合刪除要求，如果符合要求則刪除，不符合要求则不做处理</span><br><span class="line">56     public static void checkAndDeleteFile(Path path, FileSystem fs) throws Exception&#123;</span><br><span class="line">57         String name = path.getName();</span><br><span class="line">58         System.out.println(name);</span><br><span class="line">59         /*// 直接判断有没有FILETYPE这个字符串,不是特别稳妥，并且会有误操作，所以得判断是不是以FILETYPE结尾</span><br><span class="line">60         if(name.indexOf(FILETYPE) != -1)&#123;</span><br><span class="line">61             fs.delete(path, true);</span><br><span class="line">62         &#125;*/</span><br><span class="line">63         // 判断是不是以FILETYPE结尾</span><br><span class="line">64         int startIndex = name.length() - FILETYPE.length();</span><br><span class="line">65         int endIndex = name.length();</span><br><span class="line">66         // 求得文件后缀名</span><br><span class="line">67         String fileSuffix = name.substring(startIndex, endIndex);</span><br><span class="line">68         if(fileSuffix.equals(FILETYPE))&#123;</span><br><span class="line">69             fs.delete(path, true);</span><br><span class="line">70         &#125;</span><br><span class="line">71     &#125;</span><br><span class="line">72 &#125;</span><br></pre></td></tr></table></figure><h2 id="7、删除HDFS集群中的所有空文件和空目录"><a href="#7、删除HDFS集群中的所有空文件和空目录" class="headerlink" title="7、删除HDFS集群中的所有空文件和空目录"></a>7、删除HDFS集群中的所有空文件和空目录</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line">  1 public class DeleteEmptyDirAndFile &#123;</span><br><span class="line">  2     </span><br><span class="line">  3     static FileSystem fs = null;</span><br><span class="line">  4 </span><br><span class="line">  5     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">  6         </span><br><span class="line">  7         initFileSystem();</span><br><span class="line">  8 </span><br><span class="line">  9 //         创建测试数据</span><br><span class="line"> 10 //        makeTestData();</span><br><span class="line"> 11 </span><br><span class="line"> 12         // 删除测试数据</span><br><span class="line"> 13 //        deleteTestData();</span><br><span class="line"> 14 </span><br><span class="line"> 15         // 删除指定文件夹下的空文件和空文件夹</span><br><span class="line"> 16         deleteEmptyDirAndFile(new Path(&quot;/aa&quot;));</span><br><span class="line"> 17     &#125;</span><br><span class="line"> 18     </span><br><span class="line"> 19     /**</span><br><span class="line"> 20      * 删除指定文件夹下的 空文件 和 空文件夹</span><br><span class="line"> 21      * @throws Exception </span><br><span class="line"> 22      */</span><br><span class="line"> 23     public static void deleteEmptyDirAndFile(Path path) throws Exception &#123;</span><br><span class="line"> 24         </span><br><span class="line"> 25         //当是空文件夹时</span><br><span class="line"> 26         FileStatus[] listStatus = fs.listStatus(path);</span><br><span class="line"> 27         if(listStatus.length == 0)&#123;</span><br><span class="line"> 28             fs.delete(path, true);</span><br><span class="line"> 29             return;</span><br><span class="line"> 30         &#125;</span><br><span class="line"> 31         </span><br><span class="line"> 32         // 该方法的结果：包括指定目录的  文件 和 文件夹</span><br><span class="line"> 33         RemoteIterator&lt;LocatedFileStatus&gt; listLocatedStatus = fs.listLocatedStatus(path);</span><br><span class="line"> 34         </span><br><span class="line"> 35         while (listLocatedStatus.hasNext()) &#123;</span><br><span class="line"> 36             LocatedFileStatus next = listLocatedStatus.next();</span><br><span class="line"> 37 </span><br><span class="line"> 38             Path currentPath = next.getPath();</span><br><span class="line"> 39             // 获取父目录</span><br><span class="line"> 40             Path parent = next.getPath().getParent();</span><br><span class="line"> 41             </span><br><span class="line"> 42             // 如果是文件夹，继续往下遍历，删除符合条件的文件（空文件夹）</span><br><span class="line"> 43             if (next.isDirectory()) &#123;</span><br><span class="line"> 44                 </span><br><span class="line"> 45                 // 如果是空文件夹</span><br><span class="line"> 46                 if(fs.listStatus(currentPath).length == 0)&#123;</span><br><span class="line"> 47                     // 删除掉</span><br><span class="line"> 48                     fs.delete(currentPath, true);</span><br><span class="line"> 49                 &#125;else&#123;</span><br><span class="line"> 50                     // 不是空文件夹，那么则继续遍历</span><br><span class="line"> 51                     if(fs.exists(currentPath))&#123;</span><br><span class="line"> 52                         deleteEmptyDirAndFile(currentPath);</span><br><span class="line"> 53                     &#125;</span><br><span class="line"> 54                 &#125;</span><br><span class="line"> 55                 </span><br><span class="line"> 56             // 如果是文件</span><br><span class="line"> 57             &#125; else &#123;</span><br><span class="line"> 58                 // 获取文件的长度</span><br><span class="line"> 59                 long fileLength = next.getLen();</span><br><span class="line"> 60                 // 当文件是空文件时， 删除</span><br><span class="line"> 61                 if(fileLength == 0)&#123;</span><br><span class="line"> 62                     fs.delete(currentPath, true);</span><br><span class="line"> 63                 &#125;</span><br><span class="line"> 64             &#125;</span><br><span class="line"> 65             </span><br><span class="line"> 66             // 当空文件夹或者空文件删除时，有可能导致父文件夹为空文件夹，</span><br><span class="line"> 67             // 所以每次删除一个空文件或者空文件的时候都需要判断一下，如果真是如此，那么就需要把该文件夹也删除掉</span><br><span class="line"> 68             int length = fs.listStatus(parent).length;</span><br><span class="line"> 69             if(length == 0)&#123;</span><br><span class="line"> 70                 fs.delete(parent, true);</span><br><span class="line"> 71             &#125;</span><br><span class="line"> 72         &#125;</span><br><span class="line"> 73     &#125;</span><br><span class="line"> 74     </span><br><span class="line"> 75     /**</span><br><span class="line"> 76      * 初始化FileSystem对象之用</span><br><span class="line"> 77      */</span><br><span class="line"> 78     public static void initFileSystem() throws Exception&#123;</span><br><span class="line"> 79         Configuration conf = new Configuration();</span><br><span class="line"> 80         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line"> 81         conf.addResource(&quot;config/core-site.xml&quot;);</span><br><span class="line"> 82         conf.addResource(&quot;config/hdfs-site.xml&quot;);</span><br><span class="line"> 83         fs = FileSystem.get(conf);</span><br><span class="line"> 84     &#125;</span><br><span class="line"> 85 </span><br><span class="line"> 86     /**</span><br><span class="line"> 87      * 创建 测试 数据之用</span><br><span class="line"> 88      */</span><br><span class="line"> 89     public static void makeTestData() throws Exception &#123;</span><br><span class="line"> 90         </span><br><span class="line"> 91         String emptyFilePath = &quot;D:\\bigdata\\1704mr_test\\empty.txt&quot;;</span><br><span class="line"> 92         String notEmptyFilePath = &quot;D:\\bigdata\\1704mr_test\\notEmpty.txt&quot;;</span><br><span class="line"> 93 </span><br><span class="line"> 94         // 空文件夹 和 空文件 的目录</span><br><span class="line"> 95         String path1 = &quot;/aa/bb1/cc1/dd1/&quot;;</span><br><span class="line"> 96         fs.mkdirs(new Path(path1));</span><br><span class="line"> 97         fs.mkdirs(new Path(&quot;/aa/bb1/cc1/dd2/&quot;));</span><br><span class="line"> 98         fs.copyFromLocalFile(new Path(emptyFilePath), new Path(path1));</span><br><span class="line"> 99         fs.copyFromLocalFile(new Path(notEmptyFilePath), new Path(path1));</span><br><span class="line">100 </span><br><span class="line">101         // 空文件 的目录</span><br><span class="line">102         String path2 = &quot;/aa/bb1/cc2/dd2/&quot;;</span><br><span class="line">103         fs.mkdirs(new Path(path2));</span><br><span class="line">104         fs.copyFromLocalFile(new Path(emptyFilePath), new Path(path2));</span><br><span class="line">105 </span><br><span class="line">106         // 非空文件 的目录</span><br><span class="line">107         String path3 = &quot;/aa/bb2/cc3/dd3&quot;;</span><br><span class="line">108         fs.mkdirs(new Path(path3));</span><br><span class="line">109         fs.copyFromLocalFile(new Path(notEmptyFilePath), new Path(path3));</span><br><span class="line">110 </span><br><span class="line">111         // 空 文件夹</span><br><span class="line">112         String path4 = &quot;/aa/bb2/cc4/dd4&quot;;</span><br><span class="line">113         fs.mkdirs(new Path(path4));</span><br><span class="line">114 </span><br><span class="line">115         System.out.println(&quot;测试数据创建成功&quot;);</span><br><span class="line">116     &#125;</span><br><span class="line">117 </span><br><span class="line">118     /**</span><br><span class="line">119      * 删除 指定文件夹</span><br><span class="line">120      * @throws Exception </span><br><span class="line">121      */</span><br><span class="line">122     public static void deleteTestData() throws Exception &#123;</span><br><span class="line">123         boolean delete = fs.delete(new Path(&quot;/aa&quot;), true);</span><br><span class="line">124         System.out.println(delete ? &quot;删除数据成功&quot; : &quot;删除数据失败&quot;);</span><br><span class="line">125     &#125;</span><br><span class="line">126 </span><br><span class="line">127 &#125;</span><br></pre></td></tr></table></figure><h2 id="8、手动拷贝某个特定的数据块（比如某个文件的第二个数据块）"><a href="#8、手动拷贝某个特定的数据块（比如某个文件的第二个数据块）" class="headerlink" title="8、手动拷贝某个特定的数据块（比如某个文件的第二个数据块）"></a>8、手动拷贝某个特定的数据块（比如某个文件的第二个数据块）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"> 1 /**</span><br><span class="line"> 2      * 手动拷贝某个特定的数据块（比如某个文件的第二个数据块）</span><br><span class="line"> 3      * */</span><br><span class="line"> 4     public static void copyBlock(String str,int num) &#123;</span><br><span class="line"> 5         </span><br><span class="line"> 6         Path path = new Path(str);</span><br><span class="line"> 7         </span><br><span class="line"> 8         BlockLocation[] localtions = new BlockLocation[0] ;</span><br><span class="line"> 9         </span><br><span class="line">10         try &#123;</span><br><span class="line">11             FileStatus fileStatus = fs.getFileStatus(path);</span><br><span class="line">12             </span><br><span class="line">13             localtions = fs.getFileBlockLocations(fileStatus, 0, fileStatus.getLen());</span><br><span class="line">14             </span><br><span class="line">15             /*for(int i=0;i&lt;localtions.length;i++) &#123;</span><br><span class="line">16                 //0,134217728,hadoop1,hadoop3</span><br><span class="line">17                 //134217728,64789382,hadoop3,hadoop1</span><br><span class="line">18                 System.out.println(localtions[i]);</span><br><span class="line">19             &#125;*/</span><br><span class="line">20             </span><br><span class="line">21             /*System.out.println(localtions[num-1].getOffset());</span><br><span class="line">22             System.out.println(localtions[num-1].getLength());</span><br><span class="line">23             String[] hosts = localtions[num-1].getHosts();*/</span><br><span class="line">24             </span><br><span class="line">25             FSDataInputStream open = fs.open(path);</span><br><span class="line">26             open.seek(localtions[num-1].getOffset());</span><br><span class="line">27             OutputStream out = new FileOutputStream(new File(&quot;D:/abc.tar.gz&quot;));</span><br><span class="line">28             IOUtils.copyBytes(open, out,4096,true);</span><br><span class="line">29             </span><br><span class="line">30             </span><br><span class="line">31             </span><br><span class="line">32         &#125; catch (IOException e) &#123;</span><br><span class="line">33             e.printStackTrace();</span><br><span class="line">34         &#125;</span><br><span class="line">35         </span><br><span class="line">36     &#125;</span><br></pre></td></tr></table></figure><h2 id="9、编写程序统计出HDFS文件系统中文件大小小于HDFS集群中的默认块大小的文件占比"><a href="#9、编写程序统计出HDFS文件系统中文件大小小于HDFS集群中的默认块大小的文件占比" class="headerlink" title="9、编写程序统计出HDFS文件系统中文件大小小于HDFS集群中的默认块大小的文件占比"></a>9、编写程序统计出HDFS文件系统中文件大小小于HDFS集群中的默认块大小的文件占比</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"> 1 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 2 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 3 import org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"> 4 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 5 import org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"> 6 </span><br><span class="line"> 7 /**</span><br><span class="line"> 8  * </span><br><span class="line"> 9  * 编写程序统计出HDFS文件系统中文件大小小于HDFS集群中的默认块大小的文件占比</span><br><span class="line">10  * 比如：大于等于128M的文件个数为98，小于128M的文件总数为2，所以答案是2%</span><br><span class="line">11  */</span><br><span class="line">12 public class Exam1_SmallFilePercent &#123;</span><br><span class="line">13     </span><br><span class="line">14     private static int DEFAULT_BLOCKSIZE = 128 * 1024 * 1024;</span><br><span class="line">15 </span><br><span class="line">16     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">17         </span><br><span class="line">18         </span><br><span class="line">19         Configuration conf = new Configuration();</span><br><span class="line">20         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);</span><br><span class="line">21         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">22         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">23         </span><br><span class="line">24         </span><br><span class="line">25         Path path = new Path(&quot;/&quot;);</span><br><span class="line">26         float smallFilePercent = getSmallFilePercent(fs, path);</span><br><span class="line">27         System.out.println(smallFilePercent);</span><br><span class="line">28         </span><br><span class="line">29         </span><br><span class="line">30         fs.close();</span><br><span class="line">31     &#125;</span><br><span class="line">32 </span><br><span class="line">33     /**</span><br><span class="line">34      * 该方法求出指定目录下的小文件和总文件数的对比</span><br><span class="line">35      * @throws Exception </span><br><span class="line">36      */</span><br><span class="line">37     private static float getSmallFilePercent(FileSystem fs, Path path) throws Exception &#123;</span><br><span class="line">38         // TODO Auto-generated method stub</span><br><span class="line">39         </span><br><span class="line">40         int smallFile = 0;</span><br><span class="line">41         int totalFile = 0;</span><br><span class="line">42         </span><br><span class="line">43         RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(path, false);</span><br><span class="line">44         while(listFiles.hasNext())&#123;</span><br><span class="line">45             totalFile++;</span><br><span class="line">46             LocatedFileStatus next = listFiles.next();</span><br><span class="line">47             long len = next.getLen();</span><br><span class="line">48             if(len &lt; DEFAULT_BLOCKSIZE)&#123;</span><br><span class="line">49                 smallFile++;</span><br><span class="line">50             &#125;</span><br><span class="line">51         &#125;</span><br><span class="line">52         System.out.println(smallFile+&quot; : &quot;+totalFile);</span><br><span class="line">53         </span><br><span class="line">54         return smallFile * 1f /totalFile;</span><br><span class="line">55     &#125;</span><br><span class="line">56     </span><br><span class="line">57 &#125;</span><br></pre></td></tr></table></figure><h2 id="10、编写程序统计出HDFS文件系统中的平均数据块数（数据块总数-文件总数）"><a href="#10、编写程序统计出HDFS文件系统中的平均数据块数（数据块总数-文件总数）" class="headerlink" title="10、编写程序统计出HDFS文件系统中的平均数据块数（数据块总数/文件总数）"></a>10、编写程序统计出HDFS文件系统中的平均数据块数（数据块总数/文件总数）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"> 1 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 2 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 3 import org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"> 4 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 5 import org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"> 6 </span><br><span class="line"> 7 /**</span><br><span class="line"> 8  * </span><br><span class="line"> 9  * 编写程序统计出HDFS文件系统中的平均数据块数（数据块总数/文件总数）</span><br><span class="line">10  * 比如：一个文件有5个块，一个文件有3个块，那么平均数据块数为4</span><br><span class="line">11  * 如果还有一个文件，并且数据块就1个，那么整个HDFS的平均数据块数就是3</span><br><span class="line">12  */</span><br><span class="line">13 public class Exam2_HDSFAvgBlocks &#123;</span><br><span class="line">14     </span><br><span class="line">15     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">16         </span><br><span class="line">17         </span><br><span class="line">18         Configuration conf = new Configuration();</span><br><span class="line">19         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000&quot;);</span><br><span class="line">20         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">21         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">22         </span><br><span class="line">23         </span><br><span class="line">24         Path path = new Path(&quot;/&quot;);</span><br><span class="line">25         float avgHDFSBlocks = getHDFSAvgBlocks(fs, path);</span><br><span class="line">26         System.out.println(&quot;HDFS的平均数据块个数为：&quot; + avgHDFSBlocks);</span><br><span class="line">27         </span><br><span class="line">28         </span><br><span class="line">29         fs.close();</span><br><span class="line">30     &#125;</span><br><span class="line">31 </span><br><span class="line">32     /**</span><br><span class="line">33      * 求出指定目录下的所有文件的平均数据块个数</span><br><span class="line">34      */</span><br><span class="line">35     private static float getHDFSAvgBlocks(FileSystem fs, Path path) throws Exception &#123;</span><br><span class="line">36         // TODO Auto-generated method stub</span><br><span class="line">37         </span><br><span class="line">38         int totalFiles = 0;        // 总文件数</span><br><span class="line">39         int totalBlocks = 0;    // 总数据块数</span><br><span class="line">40         </span><br><span class="line">41         RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(path, false);</span><br><span class="line">42         </span><br><span class="line">43         while(listFiles.hasNext())&#123;</span><br><span class="line">44             LocatedFileStatus next = listFiles.next();</span><br><span class="line">45             int length = next.getBlockLocations().length;</span><br><span class="line">46             totalBlocks += length;</span><br><span class="line">47             if(next.getLen() != 0)&#123;</span><br><span class="line">48                 totalFiles++;</span><br><span class="line">49             &#125;</span><br><span class="line">50         &#125;</span><br><span class="line">51         System.out.println(totalBlocks+&quot; : &quot;+totalFiles);</span><br><span class="line">52         </span><br><span class="line">53         return totalBlocks * 1f / totalFiles;</span><br><span class="line">54     &#125;</span><br><span class="line">55     </span><br><span class="line">56 &#125;</span><br></pre></td></tr></table></figure><h2 id="11、编写程序统计出HDFS文件系统中的平均副本数（副本总数-总数据块数）"><a href="#11、编写程序统计出HDFS文件系统中的平均副本数（副本总数-总数据块数）" class="headerlink" title="11、编写程序统计出HDFS文件系统中的平均副本数（副本总数/总数据块数）"></a>11、编写程序统计出HDFS文件系统中的平均副本数（副本总数/总数据块数）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"> 1 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 2 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 3 import org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"> 4 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 5 import org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"> 6 </span><br><span class="line"> 7 /**</span><br><span class="line"> 8  * 编写程序统计出HDFS文件系统中的平均副本数（副本总数/总数据块数）</span><br><span class="line"> 9  * 比如：总共两个文件，一个文件5个数据块，每个数据块3个副本，第二个文件2个数据块，每个文件2个副本，最终的平均副本数 = （3*3 + 2*2）/（3+2）= 2.8</span><br><span class="line">10  */</span><br><span class="line">11 public class Exam3_HDSFAvgBlockCopys &#123;</span><br><span class="line">12     </span><br><span class="line">13     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">14         </span><br><span class="line">15         </span><br><span class="line">16         Configuration conf = new Configuration();</span><br><span class="line">17         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop02:9000&quot;);</span><br><span class="line">18         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">19         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">20         </span><br><span class="line">21         </span><br><span class="line">22         Path path = new Path(&quot;/&quot;);</span><br><span class="line">23         float avgHDFSBlockCopys = getHDFSAvgBlockCopys(fs, path);</span><br><span class="line">24         System.out.println(&quot;HDFS的平均数据块个数为：&quot; + avgHDFSBlockCopys);</span><br><span class="line">25         </span><br><span class="line">26         </span><br><span class="line">27         fs.close();</span><br><span class="line">28     &#125;</span><br><span class="line">29 </span><br><span class="line">30     /**</span><br><span class="line">31      * 求出指定目录下的所有文件的平均数据块个数</span><br><span class="line">32      */</span><br><span class="line">33     private static float getHDFSAvgBlockCopys(FileSystem fs, Path path) throws Exception &#123;</span><br><span class="line">34         // TODO Auto-generated method stub</span><br><span class="line">35         </span><br><span class="line">36         int totalCopy = 0;        // 总副本数</span><br><span class="line">37         int totalBlocks = 0;    // 总数据块数</span><br><span class="line">38         </span><br><span class="line">39         RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(path, false);</span><br><span class="line">40         </span><br><span class="line">41         while(listFiles.hasNext())&#123;</span><br><span class="line">42             LocatedFileStatus next = listFiles.next();</span><br><span class="line">43 </span><br><span class="line">44             int length = next.getBlockLocations().length;</span><br><span class="line">45             short replication = next.getReplication();</span><br><span class="line">46             </span><br><span class="line">47             totalBlocks += length;</span><br><span class="line">48             totalCopy += length * replication;</span><br><span class="line">49         &#125;</span><br><span class="line">50         System.out.println(totalCopy+&quot; : &quot;+totalBlocks);</span><br><span class="line">51         </span><br><span class="line">52         return totalCopy * 1f / totalBlocks;</span><br><span class="line">53     &#125;</span><br><span class="line">54     </span><br><span class="line">55 &#125;</span><br></pre></td></tr></table></figure><h2 id="12、统计HDFS整个文件系统中的不足指定数据块大小的数据块的比例"><a href="#12、统计HDFS整个文件系统中的不足指定数据块大小的数据块的比例" class="headerlink" title="12、统计HDFS整个文件系统中的不足指定数据块大小的数据块的比例"></a>12、统计HDFS整个文件系统中的不足指定数据块大小的数据块的比例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"> 1 import java.io.IOException;</span><br><span class="line"> 2 </span><br><span class="line"> 3 import org.apache.hadoop.conf.Configuration;</span><br><span class="line"> 4 import org.apache.hadoop.fs.BlockLocation;</span><br><span class="line"> 5 import org.apache.hadoop.fs.FileSystem;</span><br><span class="line"> 6 import org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"> 7 import org.apache.hadoop.fs.Path;</span><br><span class="line"> 8 import org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"> 9 </span><br><span class="line">10 /**</span><br><span class="line">11  * 统计HDFS整个文件系统中的不足指定数据块大小的数据块的比例</span><br><span class="line">12  * 比如指定的数据块大小是128M，总数据块有100个，不是大小为完整的128M的数据块有5个，那么不足指定数据块大小的数据块的比例就为5%</span><br><span class="line">13  * 注意：千万注意考虑不同文件的指定数据块大小可能不一致。所以千万不能用默认的128M一概而论</span><br><span class="line">14  */</span><br><span class="line">15 public class Exam4_LTBlockSize &#123;</span><br><span class="line">16 </span><br><span class="line">17     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">18         </span><br><span class="line">19         Configuration conf = new Configuration();</span><br><span class="line">20         conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop02:9000&quot;);</span><br><span class="line">21         System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);</span><br><span class="line">22         FileSystem fs = FileSystem.get(conf);</span><br><span class="line">23         </span><br><span class="line">24         Path path = new Path(&quot;/&quot;);</span><br><span class="line">25         float avgHDFSBlockCopys = getLessThanBlocksizeBlocks(fs, path);</span><br><span class="line">26         System.out.println(&quot;HDFS的不足指定数据块大小的数据块数目为：&quot; + avgHDFSBlockCopys);</span><br><span class="line">27         </span><br><span class="line">28         fs.close();</span><br><span class="line">29     &#125;</span><br><span class="line">30 </span><br><span class="line">31     private static float getLessThanBlocksizeBlocks(FileSystem fs, Path path) throws Exception &#123;</span><br><span class="line">32         // TODO Auto-generated method stub</span><br><span class="line">33         </span><br><span class="line">34         int totalBlocks = 0;                // 总副本数</span><br><span class="line">35         int lessThenBlocksizeBlocks = 0;    // 总数据块数</span><br><span class="line">36         </span><br><span class="line">37         RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(path, false);</span><br><span class="line">38         </span><br><span class="line">39         while(listFiles.hasNext())&#123;</span><br><span class="line">40             LocatedFileStatus next = listFiles.next();</span><br><span class="line">41 </span><br><span class="line">42             BlockLocation[] blockLocations = next.getBlockLocations();</span><br><span class="line">43             int length = blockLocations.length;</span><br><span class="line">44             </span><br><span class="line">45             if(length != 0)&#123;</span><br><span class="line">46                 totalBlocks += length;</span><br><span class="line">47                 long lastBlockSize = blockLocations[length - 1].getLength();</span><br><span class="line">48                 long blockSize = next.getBlockSize();</span><br><span class="line">49                 if(lastBlockSize &lt; blockSize)&#123;</span><br><span class="line">50                     lessThenBlocksizeBlocks++;</span><br><span class="line">51                 &#125;</span><br><span class="line">52             &#125;</span><br><span class="line">53         &#125;</span><br><span class="line">54         System.out.println(lessThenBlocksizeBlocks+&quot; : &quot;+totalBlocks);</span><br><span class="line">55         </span><br><span class="line">56         return lessThenBlocksizeBlocks * 1f / totalBlocks;</span><br><span class="line">57     &#125;</span><br><span class="line">58 &#125;</span><br></pre></td></tr></table></figure><h2 id="13、统计出一个给定数组的蓄水总量（把数组的每个位置的数看是做地势高低）"><a href="#13、统计出一个给定数组的蓄水总量（把数组的每个位置的数看是做地势高低）" class="headerlink" title="13、统计出一个给定数组的蓄水总量（把数组的每个位置的数看是做地势高低）"></a>13、统计出一个给定数组的蓄水总量（把数组的每个位置的数看是做地势高低）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">  1 /**</span><br><span class="line">  2         统计出一个给定数组的蓄水总量（把数组的每个位置的数看是做地势高低）</span><br><span class="line">  3         比如：int[] intArray = new int[]&#123;4,3,2,5,6,4,4,7&#125;</span><br><span class="line">  4         能蓄水：[0,1,2,0,0,2,2,0] 所以总量是：7</span><br><span class="line">  5         </span><br><span class="line">  6     核心思路：把数组切成很多个 01数组，每一层一个01数组，统计每个01数组中的合法0的总个数（数组的左边第一个1的中间区间中的0的个数）即可</span><br><span class="line">  7  */</span><br><span class="line">  8 public class Exam5_WaterStoreOfArray &#123;</span><br><span class="line">  9 </span><br><span class="line"> 10     public static void main(String[] args) &#123;</span><br><span class="line"> 11         </span><br><span class="line"> 12 //        int[] intArray = new int[]&#123;4,3,2,5,6,4,4,7&#125;;</span><br><span class="line"> 13 //        int[] intArray = new int[]&#123;1,2,3,4,5,6&#125;;</span><br><span class="line"> 14         int[] intArray = new int[]&#123;3,1,2,7,3,8,4,9,5,6&#125;;</span><br><span class="line"> 15         </span><br><span class="line"> 16         int totalWater = getArrayWater(intArray);</span><br><span class="line"> 17         System.out.println(totalWater);</span><br><span class="line"> 18     &#125;</span><br><span class="line"> 19     </span><br><span class="line"> 20     /**</span><br><span class="line"> 21      * 求出数组中的水数</span><br><span class="line"> 22      */</span><br><span class="line"> 23     private static int getArrayWater(int[] intArray) &#123;</span><br><span class="line"> 24         </span><br><span class="line"> 25         int findMaxValueOfArray = findMaxValueOfArray(intArray);</span><br><span class="line"> 26         int findMinValueOfArray = findMinValueOfArray(intArray);</span><br><span class="line"> 27         int length = intArray.length;</span><br><span class="line"> 28         </span><br><span class="line"> 29         int totalWater = 0;</span><br><span class="line"> 30         </span><br><span class="line"> 31         // 循环次数就是最大值和最小值的差</span><br><span class="line"> 32         for(int i=findMinValueOfArray; i&lt;findMaxValueOfArray; i++)&#123;</span><br><span class="line"> 33             // 循环构造每一层的01数组</span><br><span class="line"> 34             int[] tempArray = new int[length];</span><br><span class="line"> 35             for(int j=0; j&lt;length; j++)&#123;</span><br><span class="line"> 36                 if(intArray[j] &gt; i)&#123;</span><br><span class="line"> 37                     tempArray[j] = 1;</span><br><span class="line"> 38                 &#125;else&#123;</span><br><span class="line"> 39                     tempArray[j] = 0;</span><br><span class="line"> 40                 &#125;</span><br><span class="line"> 41             &#125;</span><br><span class="line"> 42             // 获取每一个01数组的合法0个数</span><br><span class="line"> 43             int waterOfOneZeroArray = getWaterOfOneZeroArray(tempArray);</span><br><span class="line"> 44             totalWater += waterOfOneZeroArray;</span><br><span class="line"> 45         &#125;</span><br><span class="line"> 46         return totalWater;</span><br><span class="line"> 47     &#125;</span><br><span class="line"> 48     </span><br><span class="line"> 49 </span><br><span class="line"> 50     /**</span><br><span class="line"> 51      * 寻找逻辑是：从左右开始各找一个1，然后这两个1之间的所有0的个数，就是水数</span><br><span class="line"> 52      */</span><br><span class="line"> 53     private static int getWaterOfOneZeroArray(int[] tempArray) &#123;</span><br><span class="line"> 54         </span><br><span class="line"> 55         int length = tempArray.length;</span><br><span class="line"> 56         int toatalWater = 0;</span><br><span class="line"> 57         </span><br><span class="line"> 58         // 找左边的1</span><br><span class="line"> 59         int i = 0;</span><br><span class="line"> 60         while(i &lt; length)&#123;</span><br><span class="line"> 61             if(tempArray[i] == 1)&#123;</span><br><span class="line"> 62                 break;</span><br><span class="line"> 63             &#125;</span><br><span class="line"> 64             i++;</span><br><span class="line"> 65         &#125;</span><br><span class="line"> 66         </span><br><span class="line"> 67         // 从右边开始找1</span><br><span class="line"> 68         int j=length-1;</span><br><span class="line"> 69         while(j &gt;= i)&#123;</span><br><span class="line"> 70             if(tempArray[j] == 1)&#123;</span><br><span class="line"> 71                 break;</span><br><span class="line"> 72             &#125;</span><br><span class="line"> 73             j--;</span><br><span class="line"> 74         &#125;</span><br><span class="line"> 75         </span><br><span class="line"> 76         // 找以上两个1之间的0的个数。</span><br><span class="line"> 77         if(i == j || i + 1 == j)&#123;</span><br><span class="line"> 78             return 0;</span><br><span class="line"> 79         &#125;else&#123;</span><br><span class="line"> 80             for(int k=i+1; k&lt;j; k++)&#123;</span><br><span class="line"> 81                 if(tempArray[k] == 0)&#123;</span><br><span class="line"> 82                     toatalWater++;</span><br><span class="line"> 83                 &#125;</span><br><span class="line"> 84             &#125;</span><br><span class="line"> 85             return toatalWater;</span><br><span class="line"> 86         &#125;</span><br><span class="line"> 87     &#125;</span><br><span class="line"> 88 </span><br><span class="line"> 89     /**</span><br><span class="line"> 90      * </span><br><span class="line"> 91      * 描述：找出一个数组中的最大值</span><br><span class="line"> 92      */</span><br><span class="line"> 93     public static int findMaxValueOfArray(int[] intArray)&#123;</span><br><span class="line"> 94         int length = intArray.length;</span><br><span class="line"> 95         if(length == 0)&#123;</span><br><span class="line"> 96             return 0;</span><br><span class="line"> 97         &#125;else if(length == 1)&#123;</span><br><span class="line"> 98             return intArray[0];</span><br><span class="line"> 99         &#125;else&#123;</span><br><span class="line">100             int max = intArray[0];</span><br><span class="line">101             for(int i=1; i&lt;length; i++)&#123;</span><br><span class="line">102                 if(intArray[i] &gt; max)&#123;</span><br><span class="line">103                     max = intArray[i];</span><br><span class="line">104                 &#125;</span><br><span class="line">105             &#125;</span><br><span class="line">106             return max;</span><br><span class="line">107         &#125;</span><br><span class="line">108     &#125;</span><br><span class="line">109     </span><br><span class="line">110     /**</span><br><span class="line">111      * 找出一个数组中的最小值</span><br><span class="line">112      */</span><br><span class="line">113     public static int findMinValueOfArray(int[] intArray)&#123;</span><br><span class="line">114         int length = intArray.length;</span><br><span class="line">115         if(length == 0)&#123;</span><br><span class="line">116             return 0;</span><br><span class="line">117         &#125;else if(length == 1)&#123;</span><br><span class="line">118             return intArray[0];</span><br><span class="line">119         &#125;else&#123;</span><br><span class="line">120             int min = intArray[0];</span><br><span class="line">121             for(int i=1; i&lt;length; i++)&#123;</span><br><span class="line">122                 if(intArray[i] &lt; min)&#123;</span><br><span class="line">123                     min = intArray[i];</span><br><span class="line">124                 &#125;</span><br><span class="line">125             &#125;</span><br><span class="line">126             return min;</span><br><span class="line">127         &#125;</span><br><span class="line">128     &#125;</span><br><span class="line">129 &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（九）HDFS深入理解</title>
      <link href="/2018-04-09-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%B9%9D%EF%BC%89HDFS%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3.html"/>
      <url>/2018-04-09-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%B9%9D%EF%BC%89HDFS%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（九）HDFS深入理解：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（九）HDFS深入理解</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="HDFS的优点和缺点"><a href="#HDFS的优点和缺点" class="headerlink" title="HDFS的优点和缺点"></a>HDFS的优点和缺点</h2><h3 id="HDFS的优点"><a href="#HDFS的优点" class="headerlink" title="HDFS的优点"></a>HDFS的优点</h3><p>1、可构建在廉价机器上</p><p>　　　　通过多副本提高可靠性，提供了容错和恢复机制</p><p>　　　　服务器节点的宕机是常态   必须理性对象 </p><p>2、高容错性</p><p>　　　　数据自动保存多个副本，副本丢失后，自动恢复</p><p>　　　　<strong>HDFS的核心设计思想：  分散均匀存储 + 备份冗余存储</strong></p><p>3、适合批处理</p><p>　　　　移动计算而非数据，数据位置暴露给计算框架</p><p>　　　　海量数据的计算 任务 最终是一定要被切分成很多的小任务进行</p><p>4、适合大数据处理</p><p>　　　　GB、TB、甚至 PB 级数据，百万规模以上的文件数量，10K+节点规模</p><p>5、流式文件访问</p><p>　　　　 一次性写入，多次读取，保证数据一致性</p><h3 id="HDFS的缺点"><a href="#HDFS的缺点" class="headerlink" title="HDFS的缺点"></a>HDFS的缺点</h3><h4 id="不适合以下操作"><a href="#不适合以下操作" class="headerlink" title="不适合以下操作"></a>不适合以下操作</h4><p>1、低延迟数据访问</p><p>　　　　比如毫秒级 低延迟与高吞吐率</p><p>2、小文件存取</p><p>　　　　占用 NameNode 大量内存 150b* 1000W = 15E,1.5G 寻道时间超过读取时间</p><p>3、并发写入、文件随机修改</p><p>　　　　一个文件只能有一个写者 仅支持 append</p><h4 id="抛出问题：HDFS文件系统为什么不适用于存储小文件？"><a href="#抛出问题：HDFS文件系统为什么不适用于存储小文件？" class="headerlink" title="抛出问题：HDFS文件系统为什么不适用于存储小文件？"></a>抛出问题：HDFS文件系统为什么不适用于存储小文件？</h4><p>这是和HDFS系统底层设计实现有关系的，HDFS本身的设计就是用来解决海量大文件数据的存储.，他天生喜欢大数据的处理，大文件存储在HDFS中，会被切分成很多的小数据块，任何一个文件不管有多小，都是一个独立的数据块，而这些数据块的信息则是保存在元数据中的，在之前的博客HDFS基础里面介绍过在HDFS集群的namenode中会存储元数据的信息，这里再说一下，元数据的信息主要包括以下3部分：</p><p>　　1）抽象目录树</p><p>　　2）文件和数据块的映射关系，一个数据块的元数据大小大约是150byte</p><p>　　3）数据块的多个副本存储地</p><p>而元数据的存储在磁盘（1和2）和内存中（1、2和3），而服务器中的内存是有上限的，举个例子：</p><p>有100个1M的文件存储进入HDFS系统，那么数据块的个数就是100个，元数据的大小就是100*150byte，消耗了15000byte的内存，但是只存储了100M的数据。</p><p>有1个100M的文件存储进入HDFS系统，那么数据块的个数就是1个，元数据的大小就是150byte，消耗量150byte的内存，存储量100M的数据。</p><p>所以说HDFS文件系统不适用于存储小文件。</p><h2 id="HDFS的辅助功能"><a href="#HDFS的辅助功能" class="headerlink" title="HDFS的辅助功能"></a>HDFS的辅助功能</h2><p>HDFS作为一个文件系统。有两个最主要的功能：<strong>上传和下载</strong>。而为了保障这两个功能的完美和高效实现，HDFS提供了很多的辅助功能</p><h3 id="1-心跳机制"><a href="#1-心跳机制" class="headerlink" title="1.心跳机制"></a>1.心跳机制</h3><h4 id="普通话讲解"><a href="#普通话讲解" class="headerlink" title="普通话讲解"></a>普通话讲解</h4><p>1、 Hadoop 是 Master/Slave 结构，Master 中有 NameNode 和 ResourceManager，Slave 中有 Datanode 和 NodeManager </p><p>2、 Master 启动的时候会启动一个 IPC（Inter-Process Comunication，进程间通信）server 服 务，等待 slave 的链接</p><p>3、 Slave 启动时，会主动链接 master 的 ipc server 服务，并且每隔 3 秒链接一次 master，这 个间隔时间是可以调整的，参数为 dfs.heartbeat.interval，这个每隔一段时间去连接一次 的机制，我们形象的称为心跳。Slave 通过心跳汇报自己的信息给 master，master 也通 过心跳给 slave 下达命令，</p><p>4、 NameNode 通过心跳得知 Datanode 的状态 ，ResourceManager 通过心跳得知 NodeManager 的状态</p><p>5、 如果 master 长时间都没有收到 slave 的心跳，就认为该 slave 挂掉了。！！！！！</p><h4 id="大白话讲解"><a href="#大白话讲解" class="headerlink" title="大白话讲解"></a>大白话讲解</h4><p>1、DataNode启动的时候会向NameNode汇报信息，就像钉钉上班打卡一样，你打卡之后，你领导才知道你今天来上班了，同样的道理，DataNode也需要向NameNode进行汇报，只不过每次汇报的时间间隔有点短而已，默认是3秒中，<strong>DataNode向NameNode汇报的信息有2点，一个是自身DataNode的状态信息，另一个是自身DataNode所持有的所有的数据块的信息。</strong>而DataNode是不会知道他保存的所有的数据块副本到底是属于哪个文件，这些都是存储在NameNode的元数据中。</p><p>2、按照规定，每个DataNode都是需要向NameNode进行汇报。那么如果从某个时刻开始，某个DataNode再也不向NameNode进行汇报了。 有可能宕机了。因为只要通过网络传输数据，就一定存在一种可能： 丢失 或者 延迟。</p><p>3、HDFS的标准： NameNode如果连续10次没有收到DataNode的汇报。 那么NameNode就会认为该DataNode存在宕机的可能。</p><p>4、DataNode启动好了之后，会专门启动一个线程，去负责给NameNode发送心跳数据包，如果说整个DataNode没有任何问题，但是仅仅只是当前负责发送信条数据包的线程挂了。NameNode会发送命令向这个DataNode进行确认。查看这个发送心跳数据包的服务是否还能正常运行，而为了保险起见，NameNode会向DataNode确认2遍，每5分钟确认一次。如果2次都没有返回 结果，那么NameNode就会认为DataNode已经GameOver了！！！</p><p><strong>最终NameNode判断一个DataNode死亡的时间计算公式：</strong></p><p><strong>timeout = 10 * 心跳间隔时间  + 2 * 检查一次消耗的时间</strong></p><p> 心跳间隔时间：dfs.heartbeat.interval 心跳时间：3s<br>检查一次消耗的时间：heartbeat.recheck.interval checktime : 5min</p><p>最终结果默认是630s。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180309200331518-737230339.png" alt="img"></p><h3 id="2-安全模式"><a href="#2-安全模式" class="headerlink" title="2.安全模式"></a>2.安全模式</h3><p>1、HDFS的启动和关闭都是先启动NameNode，在启动DataNode，最后在启动secondarynamenode。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180309200650648-965578498.png" alt="img"></p><p>2、决定HDFS集群的启动时长会有两个因素：</p><p>　　1）磁盘元数据的大小</p><p>　　2）datanode的节点个数</p><p> 当元数据很大，或者 节点个数很多的时候，那么HDFS的启动，需要一段很长的时间，那么在还没有完全启动的时候HDFS能否对外提供服务？</p><p>在HDFS的启动命令start-dfs.sh执行的时候，HDFS会自动进入安全模式</p><p>为了确保用户的操作是可以高效的执行成功的，在HDFS发现自身不完整的时候，会进入安全模式。保护自己。</p><p> <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180309201610593-1303464002.png" alt="img"></p><p>在正常启动之后，如果HDFS发现所有的数据都是齐全的，那么HDFS会启动的退出安全模式</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180309201716199-1209493841.png" alt="img"></p><p>3、对安全模式进行测试</p><p>安全模式常用操作命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -safemode leave //强制 NameNode 退出安全模式</span><br><span class="line"></span><br><span class="line">hdfs dfsadmin -safemode enter //进入安全模式</span><br><span class="line"></span><br><span class="line">hdfs dfsadmin -safemode get //查看安全模式状态</span><br><span class="line"></span><br><span class="line">hdfs dfsadmin -safemode wait //等待，一直到安全模式结束</span><br></pre></td></tr></table></figure><p>手工进入安全模式进行测试</p><p>1、测试创建文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hdfs dfsadmin -safemode enter</span><br><span class="line">Safe mode is ON</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -mkdir -p /xx/yy/zz</span><br><span class="line">mkdir: Cannot create directory /xx/yy/zz. Name node is in safe mode.</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180309202244027-371942508.png" alt="img"></p><p>2、测试下载文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ ls</span><br><span class="line">apps  data</span><br><span class="line">[hadoop@hadoop1 ~]$ hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is ON</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -get /aa/1.txt ~/1.txt</span><br><span class="line">[hadoop@hadoop1 ~]$ ls</span><br><span class="line">1.txt  apps  data</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180309202543275-2048560291.png" alt="img"></p><p>3、测试上传</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -put 1.txt /a/xx.txt</span><br><span class="line">put: Cannot create file/a/xx.txt._COPYING_. Name node is in safe mode.</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180309202633279-1165284600.png" alt="img"></p><p>4、得出结论，在安全模式下：</p><p>如果一个操作涉及到元数据的修改的话。都不能进行操作</p><p>如果一个操作仅仅只是查询。那是被允许的。</p><p>所谓的安全模式，仅仅只是保护namenode，而不是保护datanode</p><h3 id="3-副本存放策略"><a href="#3-副本存放策略" class="headerlink" title="3.副本存放策略"></a>3.副本存放策略</h3><p>第一副本：放置在上传文件的DataNode上；如果是集群外提交，则随机挑选一台磁盘不太慢、CPU不太忙的节点上；<br>第二副本：放置在于第一个副本不同的机架的节点上；<br>第三副本：与第二个副本相同机架的不同节点上；<br>如果还有更多的副本：随机放在节点中；</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180309202954396-121440380.png" alt="img"></p><h3 id="4-负载均衡"><a href="#4-负载均衡" class="headerlink" title="4.负载均衡"></a>4.负载均衡</h3><p>负载均衡理想状态：节点均衡、机架均衡和磁盘均衡。</p><p>Hadoop的HDFS集群非常容易出现机器与机器之间磁盘利用率不平衡的情况，例如：当集群内新增、删除节点，或者某个节点机器内硬盘存储达到饱和值。当数据不平衡时，Map任务可能会分配到没有存储数据的机器，这将导致网络带宽的消耗，也无法很好的进行本地计算。<br>当HDFS负载不均衡时，需要对HDFS进行数据的负载均衡调整，即对各节点机器上数据的存储分布进行调整。从而，让数据均匀的分布在各个DataNode上，均衡IO性能，防止热点的发生。进行数据的负载均衡调整，必须要满足如下原则：</p><ul><li><ul><li>数据平衡不能导致数据块减少，数据块备份丢失</li><li>管理员可以中止数据平衡进程</li><li>每次移动的数据量以及占用的网络资源，必须是可控的</li><li>数据均衡过程，不能影响namenode的正常工作</li></ul></li></ul><h4 id="负载均衡的原理"><a href="#负载均衡的原理" class="headerlink" title="负载均衡的原理"></a>负载均衡的原理</h4><p>数据均衡过程的核心是一个数据均衡算法，该数据均衡算法将不断迭代数据均衡逻辑，直至集群内数据均衡为止。该数据均衡算法每次迭代的逻辑如下：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180309203437807-414611294.png" alt="img"></p><p>步骤分析如下：</p><ol><li>数据均衡服务（Rebalancing Server）首先要求 NameNode 生成 DataNode 数据分布分析报告,获取每个DataNode磁盘使用情况</li><li>Rebalancing Server汇总需要移动的数据分布情况，计算具体数据块迁移路线图。数据块迁移路线图，确保网络内最短路径</li><li>开始数据块迁移任务，Proxy Source Data Node复制一块需要移动数据块</li><li>将复制的数据块复制到目标DataNode上</li><li>删除原始数据块</li><li>目标DataNode向Proxy Source Data Node确认该数据块迁移完成</li><li>Proxy Source Data Node向Rebalancing Server确认本次数据块迁移完成。然后继续执行这个过程，直至集群达到数据均衡标准</li></ol><h4 id="DataNode分组"><a href="#DataNode分组" class="headerlink" title="DataNode分组"></a><strong>DataNode分组</strong></h4><p>在第2步中，HDFS会把当前的DataNode节点,根据阈值的设定情况划分到Over、Above、Below、Under四个组中。在移动数据块的时候，Over组、Above组中的块向Below组、Under组移动。四个组定义如下：</p><p> <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180309203541810-527363557.png" alt="img"></p><ul><li><strong>Over组</strong>：此组中的DataNode的均满足</li></ul><blockquote><p>DataNode_usedSpace_percent <strong>&gt;</strong> Cluster_usedSpace_percent + threshold</p></blockquote><ul><li><strong>Above组</strong>：此组中的DataNode的均满足</li></ul><blockquote><p>Cluster_usedSpace_percent + threshold <strong>&gt;</strong> DataNode_ usedSpace _percent <strong>&gt;</strong>Cluster_usedSpace_percent</p></blockquote><ul><li><strong>Below组</strong>：此组中的DataNode的均满足</li></ul><blockquote><p>Cluster_usedSpace_percent <strong>&gt;</strong> DataNode_ usedSpace_percent <strong>&gt;</strong> Cluster_ usedSpace_percent – threshold</p></blockquote><ul><li><strong>Under组</strong>：此组中的DataNode的均满足</li></ul><blockquote><p>Cluster_usedSpace_percent – threshold <strong>&gt;</strong> DataNode_usedSpace_percent</p></blockquote><h4 id="Hadoop-HDFS-数据自动平衡脚本使用方法"><a href="#Hadoop-HDFS-数据自动平衡脚本使用方法" class="headerlink" title="Hadoop HDFS 数据自动平衡脚本使用方法"></a>Hadoop HDFS 数据自动平衡脚本使用方法</h4><p>在Hadoop中，包含一个start-balancer.sh脚本，通过运行这个工具，启动HDFS数据均衡服务。该工具可以做到热插拔，即无须重启计算机和 Hadoop 服务。HadoopHome/bin目录下的start−balancer.sh脚本就是该任务的启动脚本。启动命令为：‘HadoopHome/bin目录下的start−balancer.sh脚本就是该任务的启动脚本。启动命令为：‘Hadoop_home/bin/start-balancer.sh –threshold`</p><p><strong>影响Balancer的几个参数：</strong></p><ul><li>-threshold<ul><li>默认设置：10，参数取值范围：0-100</li><li>参数含义：判断集群是否平衡的阈值。理论上，该参数设置的越小，整个集群就越平衡</li></ul></li><li>dfs.balance.bandwidthPerSec<ul><li>默认设置：1048576（1M/S）</li><li>参数含义：Balancer运行时允许占用的带宽</li></ul></li></ul><p>示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#启动数据均衡，默认阈值为 10%</span><br><span class="line">$Hadoop_home/bin/start-balancer.sh</span><br><span class="line"></span><br><span class="line">#启动数据均衡，阈值 5%</span><br><span class="line">bin/start-balancer.sh –threshold 5</span><br><span class="line"></span><br><span class="line">#停止数据均衡</span><br><span class="line">$Hadoop_home/bin/stop-balancer.sh</span><br></pre></td></tr></table></figure><p>在hdfs-site.xml文件中可以设置数据均衡占用的网络带宽限制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.balance.bandwidthPerSec&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1048576&lt;/value&gt;</span><br><span class="line">&lt;description&gt; Specifies the maximum bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second. &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（八）在eclispe上搭建Hadoop开发环境</title>
      <link href="/2018-04-08-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%85%AB%EF%BC%89%E5%9C%A8eclispe%E4%B8%8A%E6%90%AD%E5%BB%BAHadoop%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83.html"/>
      <url>/2018-04-08-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%85%AB%EF%BC%89%E5%9C%A8eclispe%E4%B8%8A%E6%90%AD%E5%BB%BAHadoop%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（八）在eclispe上搭建Hadoop开发环境：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（八）在eclispe上搭建Hadoop开发环境</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、添加插件"><a href="#一、添加插件" class="headerlink" title="一、添加插件"></a>一、添加插件</h2><p>将hadoop-eclipse-plugin-2.7.5.jar放入eclipse的plugins文件夹中</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308154051462-1401508460.png" alt="img"></p><h2 id="二、在Windows上安装Hadoop2-7-5"><a href="#二、在Windows上安装Hadoop2-7-5" class="headerlink" title="二、在Windows上安装Hadoop2.7.5"></a>二、在Windows上安装Hadoop2.7.5</h2><p>版本最好与Linux集群中的hadoop版本保持一致</p><h3 id="1、将hadoop-2-7-5-centos-6-7-tar-gz解压到Windows上的C盘software目录中"><a href="#1、将hadoop-2-7-5-centos-6-7-tar-gz解压到Windows上的C盘software目录中" class="headerlink" title="1、将hadoop-2.7.5-centos-6.7.tar.gz解压到Windows上的C盘software目录中"></a>1、将hadoop-2.7.5-centos-6.7.tar.gz解压到Windows上的C盘software目录中</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308154339115-1805232233.png" alt="img"></p><h3 id="2、配置hadoop的环境变量"><a href="#2、配置hadoop的环境变量" class="headerlink" title="2、配置hadoop的环境变量"></a>2、配置hadoop的环境变量</h3><p>HADOOP_HOME=C:\software\hadoop-2.7.5</p><p>Path=C:\software\hadoop-2.7.5\bin</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308154510997-103258140.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308154557609-1388278579.png" alt="img"></p><h3 id="3、修改Hadoop安装目录C-software-hadoop-2-7-5-etc-hadoop中hadoop-env-cmd"><a href="#3、修改Hadoop安装目录C-software-hadoop-2-7-5-etc-hadoop中hadoop-env-cmd" class="headerlink" title="3、修改Hadoop安装目录C:\software\hadoop-2.7.5\etc\hadoop中hadoop-env.cmd"></a>3、修改Hadoop安装目录C:\software\hadoop-2.7.5\etc\hadoop中hadoop-env.cmd</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308154859077-1127422408.png" alt="img"></p><h3 id="4、查看Hadoop版本"><a href="#4、查看Hadoop版本" class="headerlink" title="4、查看Hadoop版本"></a>4、查看Hadoop版本</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308155021341-738318916.png" alt="img"></p><h3 id="5、添加Windows支持文件"><a href="#5、添加Windows支持文件" class="headerlink" title="5、添加Windows支持文件"></a>5、添加Windows支持文件</h3><p>因为安装的Hadoop编译的版本是CentOS6.7的版本，在Windows上运行需要添加文件</p><p>1）winutils.exe 放在windows平台中你安装的hadoop的bin目录下</p><p>2)  hadoop.dll 放在windows操作系统的 c:/windows/system32目录下</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308155406046-1355832744.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308155443757-1039726356.png" alt="img"></p><h3 id="6、重新启动eclipse"><a href="#6、重新启动eclipse" class="headerlink" title="6、重新启动eclipse"></a>6、重新启动eclipse</h3><h2 id="三、eclipse中的配置"><a href="#三、eclipse中的配置" class="headerlink" title="三、eclipse中的配置"></a>三、eclipse中的配置</h2><h3 id="1、重新启动eclipse-打开windows-gt-Preferences的Hadoop-Map-Reduce中设置安装目录"><a href="#1、重新启动eclipse-打开windows-gt-Preferences的Hadoop-Map-Reduce中设置安装目录" class="headerlink" title="1、重新启动eclipse,打开windows-&gt;Preferences的Hadoop Map/Reduce中设置安装目录"></a>1、重新启动eclipse,打开windows-&gt;Preferences的Hadoop Map/Reduce中设置安装目录</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308155752096-511832683.png" alt="img"></p><h3 id="2、打开Windows-gt-Open-Perspective中的Map-Reduce，在此perspective下进行hadoop程序开发"><a href="#2、打开Windows-gt-Open-Perspective中的Map-Reduce，在此perspective下进行hadoop程序开发" class="headerlink" title="2、打开Windows-&gt;Open Perspective中的Map/Reduce，在此perspective下进行hadoop程序开发"></a>2、打开Windows-&gt;Open Perspective中的Map/Reduce，在此perspective下进行hadoop程序开发</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308155934297-243834670.png" alt="img"></p><h3 id="3、打开Windows-gt-Show-View中的Map-Reduce-Locations，如下图右键选择New-Hadoop-location…新建hadoop连接。"><a href="#3、打开Windows-gt-Show-View中的Map-Reduce-Locations，如下图右键选择New-Hadoop-location…新建hadoop连接。" class="headerlink" title="3、打开Windows-&gt;Show View中的Map/Reduce Locations，如下图右键选择New Hadoop location…新建hadoop连接。"></a>3、打开Windows-&gt;Show View中的Map/Reduce Locations，如下图右键选择New Hadoop location…新建hadoop连接。</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308160103914-1316418968.png" alt="img"></p><h3 id="4、配置相关信息"><a href="#4、配置相关信息" class="headerlink" title="4、配置相关信息"></a>4、配置相关信息</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308160325842-467885961.png" alt="img"></p><h3 id="5、配置成功之后再右侧显示如下"><a href="#5、配置成功之后再右侧显示如下" class="headerlink" title="5、配置成功之后再右侧显示如下"></a>5、配置成功之后再右侧显示如下</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308180013559-1845551841.png" alt="img"></p><h2 id="四、创建HDFS项目"><a href="#四、创建HDFS项目" class="headerlink" title="四、创建HDFS项目"></a>四、创建HDFS项目</h2><h3 id="1、创建一个java-project"><a href="#1、创建一个java-project" class="headerlink" title="1、创建一个java project"></a>1、创建一个java project</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308180220231-1398031805.png" alt="img"></p><h3 id="2、添加jar包这里使用第二种"><a href="#2、添加jar包这里使用第二种" class="headerlink" title="2、添加jar包这里使用第二种"></a>2、添加jar包这里使用第二种</h3><p>有三种方式可以往项目中添加jar依赖:</p><p>　　1）直接创建一个lib文件夹，然后放入对应的依赖包，最后add build path</p><p>　　　　优点：移植方便<br>　　　　缺点：项目臃肿</p><p>　　2）在eclipse中创建user libarary, 然后引入</p><p>　　　　优点：解决了不同项目中的相同jar的重复依赖问题， 不是直接放入，是引入的方式<br>　　　　缺点：移植不方便</p><p>　　3）最后直接使用maven管理jar依赖</p><p>　　　　完美解决方案：使用maven 我们在项目中只需要编写好：pom.xml文件即可</p><p>目前只是操作HDFS，所以只需要引入common和HDFS相关的jar包即可。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308180447392-1207856159.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308180829174-1230824972.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308180901189-1859717267.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308181018133-1939062048.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308181056211-110785669.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308181137589-681113668.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308181211631-1848268606.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308182643690-1637352725.png" alt="img"></p><p>Hadoop的common、hdfs、MapReduce、yarn的相关jar包的位置在安装目录的C:\software\hadoop-2.7.5\share\hadoop文件夹中，各自文件夹下的jar包是核心jar包，lib下的jar包是核心jar包的依赖jar包，都需要引入</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308181726194-2039286732.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308182747306-479197237.png" alt="img"></p><p>hdfs的jar包用相同的方法引入</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308182858537-1478007176.png" alt="img"></p><p>这样项目就成功引入了common和hdfs相关的jar包</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308182950690-131271322.png" alt="img"></p><h3 id="3、创建测试类"><a href="#3、创建测试类" class="headerlink" title="3、创建测试类"></a>3、创建测试类</h3><p><img src="https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif" alt="img"> View Code</p><p>测试之前</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308184855904-14807455.png" alt="img"></p><p>测试之后</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308185056617-1042374587.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（七）Hadoop集群shell常用命令</title>
      <link href="/2018-04-07-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%B8%83%EF%BC%89Hadoop%E9%9B%86%E7%BE%A4shell%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html"/>
      <url>/2018-04-07-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%B8%83%EF%BC%89Hadoop%E9%9B%86%E7%BE%A4shell%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（七）Hadoop集群shell常用命令：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（七）Hadoop集群shell常用命令</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="Hadoop常用命令"><a href="#Hadoop常用命令" class="headerlink" title="Hadoop常用命令"></a>Hadoop常用命令</h2><h3 id="启动HDFS集群"><a href="#启动HDFS集群" class="headerlink" title="启动HDFS集群"></a>启动HDFS集群</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ start-dfs.sh</span><br><span class="line">Starting namenodes on [hadoop1]</span><br><span class="line">hadoop1: starting namenode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-namenode-hadoop1.out</span><br><span class="line">hadoop2: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop2.out</span><br><span class="line">hadoop3: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop3.out</span><br><span class="line">hadoop4: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop4.out</span><br><span class="line">hadoop1: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop1.out</span><br><span class="line">Starting secondary namenodes [hadoop3]</span><br><span class="line">hadoop3: starting secondarynamenode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-secondarynamenode-hadoop3.out</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><h3 id="启动YARN集群"><a href="#启动YARN集群" class="headerlink" title="启动YARN集群"></a>启动YARN集群</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop4 ~]$ start-yarn.sh</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-resourcemanager-hadoop4.out</span><br><span class="line">hadoop2: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop2.out</span><br><span class="line">hadoop3: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop3.out</span><br><span class="line">hadoop4: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop4.out</span><br><span class="line">hadoop1: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop1.out</span><br><span class="line">[hadoop@hadoop4 ~]$</span><br></pre></td></tr></table></figure><h3 id="查看HDFS系统根目录"><a href="#查看HDFS系统根目录" class="headerlink" title="查看HDFS系统根目录"></a>查看HDFS系统根目录</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -ls /</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-03-03 11:42 /test</span><br><span class="line">drwx------   - hadoop supergroup          0 2018-03-03 11:42 /tmp</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><h3 id="创建文件夹"><a href="#创建文件夹" class="headerlink" title="创建文件夹"></a>创建文件夹</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -mkdir /a</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -ls /</span><br><span class="line">Found 3 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-03-08 11:09 /a</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-03-03 11:42 /test</span><br><span class="line">drwx------   - hadoop supergroup          0 2018-03-03 11:42 /tmp</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><h3 id="级联创建文件夹"><a href="#级联创建文件夹" class="headerlink" title="级联创建文件夹"></a>级联创建文件夹</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -mkdir -p /aa/bb/cc</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><h3 id="查看hsdf系统根目录下的所有文件包括子文件夹里面的文件"><a href="#查看hsdf系统根目录下的所有文件包括子文件夹里面的文件" class="headerlink" title="查看hsdf系统根目录下的所有文件包括子文件夹里面的文件"></a>查看hsdf系统根目录下的所有文件包括子文件夹里面的文件</h3><p><strong>[hadoop@hadoop1 ~]$ hadoop fs -ls -R /aa</strong><br>drwxr-xr-x - hadoop supergroup 0 2018-03-08 11:12 /aa/bb<br>drwxr-xr-x - hadoop supergroup 0 2018-03-08 11:12 /aa/bb/cc<br>[hadoop@hadoop1 ~]$</p><h3 id="上传文件"><a href="#上传文件" class="headerlink" title="上传文件"></a>上传文件</h3><p>[hadoop@hadoop1 ~]$ ls<br>apps data words.txt<br><strong>[hadoop@hadoop1 ~]$ hadoop fs -put words.txt /aa</strong><br><strong>[hadoop@hadoop1 ~]$ hadoop fs -copyFromLocal words.txt /aa/bb</strong><br>[hadoop@hadoop1 ~]$</p><h3 id="下载文件"><a href="#下载文件" class="headerlink" title="下载文件"></a>下载文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -get /aa/words.txt ~/newwords.txt</span><br><span class="line">[hadoop@hadoop1 ~]$ ls</span><br><span class="line">apps  data  newwords.txt  words.txt</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -copyToLocal /aa/words.txt ~/newwords1.txt</span><br><span class="line">[hadoop@hadoop1 ~]$ ls</span><br><span class="line">apps  data  newwords1.txt  newwords.txt  words.txt</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><h3 id="合并下载"><a href="#合并下载" class="headerlink" title="合并下载"></a>合并下载</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -getmerge /aa/words.txt /aa/bb/words.txt ~/2words.txt</span><br><span class="line">[hadoop@hadoop1 ~]$ ll</span><br><span class="line">总用量 24</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop   78 3月   8 12:42 2words.txt</span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop 4096 3月   3 10:30 apps</span><br><span class="line">drwxrwxr-x. 3 hadoop hadoop 4096 3月   3 11:40 data</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop   39 3月   8 11:49 newwords1.txt</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop   39 3月   8 11:48 newwords.txt</span><br><span class="line">-rw-rw-r--. 1 hadoop hadoop   39 3月   3 11:31 words.txt</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><h3 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h3><p>从HDFS一个路径拷贝到HDFS另一个路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -ls /a</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -cp /aa/words.txt /a</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -ls /a</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   2 hadoop supergroup         39 2018-03-08 12:46 /a/words.txt</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><h3 id="移动"><a href="#移动" class="headerlink" title="移动"></a>移动</h3><p>在HDFS目录中移动文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -ls /aa/bb/cc</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -mv /a/words.txt /aa/bb/cc</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -ls /aa/bb/cc</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   2 hadoop supergroup         39 2018-03-08 12:46 /aa/bb/cc/words.txt</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><p>删除文件或文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -rm /aa/bb/cc/words.txt</span><br><span class="line">18/03/08 12:49:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.</span><br><span class="line">Deleted /aa/bb/cc/words.txt</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -ls /aa/bb/cc</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><p>删除空目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -rmdir /aa/bb/cc/</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -ls /aa/bb/</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   2 hadoop supergroup         39 2018-03-08 11:43 /aa/bb/words.txt</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><p>强制删除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -rm /aa/bb/</span><br><span class="line">rm: `/aa/bb&apos;: Is a directory</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -rm -r /aa/bb/</span><br><span class="line">18/03/08 12:51:31 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.</span><br><span class="line">Deleted /aa/bb</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -ls /aa</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   2 hadoop supergroup         39 2018-03-08 11:41 /aa/words.txt</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><h3 id="从本地剪切文件到HDFS上"><a href="#从本地剪切文件到HDFS上" class="headerlink" title="从本地剪切文件到HDFS上"></a>从本地剪切文件到HDFS上</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ ls</span><br><span class="line">apps  data  hello.txt</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -moveFromLocal ~/hello.txt /aa</span><br><span class="line">[hadoop@hadoop1 ~]$ ls</span><br><span class="line">apps  data</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><h3 id="追加文件"><a href="#追加文件" class="headerlink" title="追加文件"></a>追加文件</h3><p>追加之前hello.txt到words.txt之前</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308125830426-570415929.png" alt="img"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -appendToFile ~/hello.txt /aa/words.txt</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><p>追加之前hello.txt到words.txt之后</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308130118866-1783962726.png" alt="img"></p><h3 id="查看文件内容"><a href="#查看文件内容" class="headerlink" title="查看文件内容"></a>查看文件内容</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -cat /aa/hello.txt</span><br><span class="line">hello</span><br><span class="line">hello</span><br><span class="line">hello</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><h3 id="chgrp"><a href="#chgrp" class="headerlink" title="chgrp"></a>chgrp</h3><p>使用方法：hadoop fs -chgrp [-R] GROUP URI [URI …] Change group association of files. With -R, make the change recursively through the directory structure. The user must be the owner of files, or else a super-user. Additional information is in the <a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_permissions_guide.html" target="_blank" rel="noopener">Permissions User Guide</a>. –&gt;</p><p>改变文件所属的组。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见<a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_permissions_guide.html" target="_blank" rel="noopener">HDFS权限用户指南</a>。</p><h3 id="chmod"><a href="#chmod" class="headerlink" title="chmod"></a>chmod</h3><p>使用方法：hadoop fs -chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; URI [URI …]</p><p>改变文件的权限。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见<a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_permissions_guide.html" target="_blank" rel="noopener">HDFS权限用户指南</a>。</p><h3 id="chown"><a href="#chown" class="headerlink" title="chown"></a>chown</h3><p>使用方法：hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</p><p>改变文件的拥有者。使用-R将使改变在目录结构下递归进行。命令的使用者必须是超级用户。更多的信息请参见<a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_permissions_guide.html" target="_blank" rel="noopener">HDFS权限用户指南</a>。</p><h3 id="du"><a href="#du" class="headerlink" title="du"></a>du</h3><p>使用方法：hadoop fs -du URI [URI …]</p><p>显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。<br>示例：<br>hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://host:port/user/hadoop/dir1<br>返回值：<br>成功返回0，失败返回-1。 </p><h3 id="dus"><a href="#dus" class="headerlink" title="dus"></a>dus</h3><p>使用方法：hadoop fs -dus <args></args></p><p>显示文件的大小。</p><h3 id="expunge"><a href="#expunge" class="headerlink" title="expunge"></a>expunge</h3><p>使用方法：hadoop fs -expunge</p><p>清空回收站。请参考<a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_design.html" target="_blank" rel="noopener">HDFS设计</a>文档以获取更多关于回收站特性的信息。</p><h3 id="setrep"><a href="#setrep" class="headerlink" title="setrep"></a>setrep</h3><p>使用方法：hadoop fs -setrep [-R] <path></path></p><p>改变一个文件的副本系数。-R选项用于递归改变目录下所有文件的副本系数。</p><p>示例：</p><ul><li>hadoop fs -setrep -w 3 -R /user/hadoop/dir1</li></ul><p>返回值：</p><p>成功返回0，失败返回-1。</p><h3 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h3><p>使用方法：hadoop fs -tail [-f] URI</p><p>将文件尾部1K字节的内容输出到stdout。支持-f选项，行为和Unix中一致。</p><p>示例：</p><ul><li>hadoop fs -tail pathname</li></ul><p>返回值：<br>成功返回0，失败返回-1。</p><h3 id="test"><a href="#test" class="headerlink" title="test"></a>test</h3><p>使用方法：hadoop fs -test -[ezd] URI</p><p>选项：<br>-e 检查文件是否存在。如果存在则返回0。<br>-z 检查文件是否是0字节。如果是则返回0。<br>-d 如果路径是个目录，则返回1，否则返回0。</p><p>示例：</p><ul><li><ul><li>hadoop fs -test -e filename</li></ul></li></ul><h3 id="查看集群的工作状态"><a href="#查看集群的工作状态" class="headerlink" title="查看集群的工作状态"></a>查看集群的工作状态</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hdfs dfsadmin -report</span><br><span class="line">Configured Capacity: 73741402112 (68.68 GB)</span><br><span class="line">Present Capacity: 52781039616 (49.16 GB)</span><br><span class="line">DFS Remaining: 52780457984 (49.16 GB)</span><br><span class="line">DFS Used: 581632 (568 KB)</span><br><span class="line">DFS Used%: 0.00%</span><br><span class="line">Under replicated blocks: 0</span><br><span class="line">Blocks with corrupt replicas: 0</span><br><span class="line">Missing blocks: 0</span><br><span class="line">Missing blocks (with replication factor 1): 0</span><br><span class="line"></span><br><span class="line">-------------------------------------------------</span><br><span class="line">Live datanodes (4):</span><br><span class="line"></span><br><span class="line">Name: 192.168.123.102:50010 (hadoop1)</span><br><span class="line">Hostname: hadoop1</span><br><span class="line">Decommission Status : Normal</span><br><span class="line">Configured Capacity: 18435350528 (17.17 GB)</span><br><span class="line">DFS Used: 114688 (112 KB)</span><br><span class="line">Non DFS Used: 4298661888 (4.00 GB)</span><br><span class="line">DFS Remaining: 13193277440 (12.29 GB)</span><br><span class="line">DFS Used%: 0.00%</span><br><span class="line">DFS Remaining%: 71.57%</span><br><span class="line">Configured Cache Capacity: 0 (0 B)</span><br><span class="line">Cache Used: 0 (0 B)</span><br><span class="line">Cache Remaining: 0 (0 B)</span><br><span class="line">Cache Used%: 100.00%</span><br><span class="line">Cache Remaining%: 0.00%</span><br><span class="line">Xceivers: 1</span><br><span class="line">Last contact: Thu Mar 08 13:05:11 CST 2018</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Name: 192.168.123.105:50010 (hadoop4)</span><br><span class="line">Hostname: hadoop4</span><br><span class="line">Decommission Status : Normal</span><br><span class="line">Configured Capacity: 18435350528 (17.17 GB)</span><br><span class="line">DFS Used: 49152 (48 KB)</span><br><span class="line">Non DFS Used: 4295872512 (4.00 GB)</span><br><span class="line">DFS Remaining: 13196132352 (12.29 GB)</span><br><span class="line">DFS Used%: 0.00%</span><br><span class="line">DFS Remaining%: 71.58%</span><br><span class="line">Configured Cache Capacity: 0 (0 B)</span><br><span class="line">Cache Used: 0 (0 B)</span><br><span class="line">Cache Remaining: 0 (0 B)</span><br><span class="line">Cache Used%: 100.00%</span><br><span class="line">Cache Remaining%: 0.00%</span><br><span class="line">Xceivers: 1</span><br><span class="line">Last contact: Thu Mar 08 13:05:13 CST 2018</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Name: 192.168.123.103:50010 (hadoop2)</span><br><span class="line">Hostname: hadoop2</span><br><span class="line">Decommission Status : Normal</span><br><span class="line">Configured Capacity: 18435350528 (17.17 GB)</span><br><span class="line">DFS Used: 233472 (228 KB)</span><br><span class="line">Non DFS Used: 4295700480 (4.00 GB)</span><br><span class="line">DFS Remaining: 13196120064 (12.29 GB)</span><br><span class="line">DFS Used%: 0.00%</span><br><span class="line">DFS Remaining%: 71.58%</span><br><span class="line">Configured Cache Capacity: 0 (0 B)</span><br><span class="line">Cache Used: 0 (0 B)</span><br><span class="line">Cache Remaining: 0 (0 B)</span><br><span class="line">Cache Used%: 100.00%</span><br><span class="line">Cache Remaining%: 0.00%</span><br><span class="line">Xceivers: 1</span><br><span class="line">Last contact: Thu Mar 08 13:05:11 CST 2018</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Name: 192.168.123.104:50010 (hadoop3)</span><br><span class="line">Hostname: hadoop3</span><br><span class="line">Decommission Status : Normal</span><br><span class="line">Configured Capacity: 18435350528 (17.17 GB)</span><br><span class="line">DFS Used: 184320 (180 KB)</span><br><span class="line">Non DFS Used: 4296941568 (4.00 GB)</span><br><span class="line">DFS Remaining: 13194928128 (12.29 GB)</span><br><span class="line">DFS Used%: 0.00%</span><br><span class="line">DFS Remaining%: 71.57%</span><br><span class="line">Configured Cache Capacity: 0 (0 B)</span><br><span class="line">Cache Used: 0 (0 B)</span><br><span class="line">Cache Remaining: 0 (0 B)</span><br><span class="line">Cache Used%: 100.00%</span><br><span class="line">Cache Remaining%: 0.00%</span><br><span class="line">Xceivers: 1</span><br><span class="line">Last contact: Thu Mar 08 13:05:10 CST 2018</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（六）HDFS基础</title>
      <link href="/2018-04-06-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%85%AD%EF%BC%89HDFS%E5%9F%BA%E7%A1%80.html"/>
      <url>/2018-04-06-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%85%AD%EF%BC%89HDFS%E5%9F%BA%E7%A1%80.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（六）HDFS基础：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（六）HDFS基础</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="HDFS前言"><a href="#HDFS前言" class="headerlink" title="HDFS前言"></a>HDFS前言</h2><p>HDFS：Hadoop Distributed File System ，Hadoop分布式文件系统，主要用来解决海量数据的存储问题</p><h3 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h3><p>1、分散均匀存储 dfs.blocksize = 128M</p><p>2、备份冗余存储 dfs.replication = 3</p><h3 id="在大数据系统中作用"><a href="#在大数据系统中作用" class="headerlink" title="在大数据系统中作用"></a>在大数据系统中作用</h3><p>为各类分布式运算框架（如：mapreduce，spark，tez，……）提供数据存储服务。</p><h3 id="重点概念"><a href="#重点概念" class="headerlink" title="重点概念"></a>重点概念</h3><p>文件切块，副本存放，元数据</p><h2 id="HDFS的概念和特性"><a href="#HDFS的概念和特性" class="headerlink" title="HDFS的概念和特性"></a><strong>HDFS的概念和特性</strong></h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a><strong>概念</strong></h3><p><strong>首先，它是一个文件系统</strong>，用于存储文件，通过统一的命名空间——目录树来定位文件</p><p><strong>其次，它是分布式的</strong>，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色；</p><h3 id="重要特性"><a href="#重要特性" class="headerlink" title="重要特性"></a><strong>重要特性</strong></h3><p>（1）HDFS中的文件在物理上是<strong>分块存储（block）</strong>，块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M</p><p>（2）HDFS文件系统会给客户端提供一个<strong>统一的抽象目录树</strong>，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data</p><p>（3）<strong>目录结构及文件分块信息**</strong>(元数据)**的管理由namenode节点承担</p><p>——namenode是HDFS集群主节点，负责维护整个hdfs文件系统的目录树，以及每一个路径（文件）所对应的block块信息（block的id，及所在的datanode服务器）</p><p>（4）文件的各个block的存储管理由datanode节点承担</p><p>—- datanode是HDFS集群从节点，每一个block都可以在多个datanode上存储多个副本（副本数量也可以通过参数设置dfs.replication）</p><p>（5）HDFS是设计成适应一次写入，多次读出的场景，且不支持文件的修改</p><p><em>(注：适合用来做数据分析，并不适合用来做网盘应用，因为，不便修改，延迟大，网络开销大，成本太高)</em></p><h2 id="图解HDFS"><a href="#图解HDFS" class="headerlink" title="图解HDFS"></a>图解HDFS</h2><p>通过上面的描述我们知道，hdfs很多特点：</p><p>　　保存多个副本，且提供容错机制，副本丢失或宕机自动恢复（默认存3份）。</p><p>　　运行在廉价的机器上</p><p>　　适合大数据的处理。HDFS默认会将文件分割成block，，在hadoop2.x以上版本默认128M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180308190320147-1328604927.png" alt="img"></p><p>如上图所示，HDFS也是按照Master和Slave的结构。分NameNode、SecondaryNameNode、DataNode这几个角色。<br>　　　　NameNode：是Master节点，是大领导。管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间；<br>　　　　SecondaryNameNode：是一个小弟，分担大哥namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode。<br>　　　　DataNode：Slave节点，奴隶，干活的。负责存储client发来的数据块block；执行数据块的读写操作。<br>　　　　热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作。<br>　　　　冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失。<br>　　　　fsimage:元数据镜像文件（文件系统的目录树。）<br>　　　　edits：元数据的操作日志（针对文件系统做的修改操作记录）<br>　　　　namenode内存中存储的是=fsimage+edits。<br>　　　　SecondaryNameNode负责定时默认1小时，从namenode上，获取fsimage和edits来进行合并，然后再发送给namenode。减少namenode的工作量。</p><h2 id="HDFS的局限性"><a href="#HDFS的局限性" class="headerlink" title="HDFS的局限性"></a>HDFS的局限性</h2><p>　　1）低延时数据访问。在用户交互性的应用中，应用需要在ms或者几个s的时间内得到响应。由于HDFS为高吞吐率做了设计，也因此牺牲了快速响应。对于低延时的应用，可以考虑使用HBase或者Cassandra。<br>　　2）大量的小文件。标准的HDFS数据块的大小是64M，存储小文件并不会浪费实际的存储空间，但是无疑会增加了在NameNode上的元数据，大量的小文件会影响整个集群的性能。</p><p>　　　　前面我们知道，Btrfs为小文件做了优化-inline file，对于小文件有很好的空间优化和访问时间优化。<br>　　3）多用户写入，修改文件。HDFS的文件只能有一个写入者，而且写操作只能在文件结尾以追加的方式进行。它不支持多个写入者，也不支持在文件写入后，对文件的任意位置的修改。<br>　　　　但是在大数据领域，分析的是已经存在的数据，这些数据一旦产生就不会修改，因此，HDFS的这些特性和设计局限也就很容易理解了。HDFS为大数据领域的数据分析，提供了非常重要而且十分基础的文件存储功能。</p><h2 id="HDFS保证可靠性的措施"><a href="#HDFS保证可靠性的措施" class="headerlink" title="HDFS保证可靠性的措施"></a>HDFS保证可靠性的措施</h2><p>　　1）冗余备份</p><p>　　　　每个文件存储成一系列数据块（Block）。为了容错，文件的所有数据块都会有副本（副本数量即复制因子，课配置）（dfs.replication）</p><p>　　2）副本存放</p><p>　　　　采用机架感知（Rak-aware）的策略来改进数据的可靠性、高可用和网络带宽的利用率</p><p>　　3）心跳检测</p><p>　　　　NameNode周期性地从集群中的每一个DataNode接受心跳包和块报告，收到心跳包说明该DataNode工作正常</p><p>　　4）安全模式</p><p>　　　　系统启动时，NameNode会进入一个安全模式。此时不会出现数据块的写操作。</p><p>　　5）数据完整性检测</p><p>　　　　HDFS客户端软件实现了对HDFS文件内容的校验和（Checksum）检查（dfs.bytes-per-checksum）。　</p><h2 id="单点故障（单点失效）问题"><a href="#单点故障（单点失效）问题" class="headerlink" title="单点故障（单点失效）问题"></a>单点故障（单点失效）问题</h2><h3 id="单点故障问题"><a href="#单点故障问题" class="headerlink" title="单点故障问题"></a>单点故障问题</h3><p>　　如果NameNode失效，那么客户端或MapReduce作业均无法读写查看文件</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>　　1）启动一个拥有文件系统元数据的新NameNode（这个一般不采用，因为复制元数据非常耗时间）</p><p>　　2）配置一对活动-备用（Active-Sandby）NameNode，活动NameNode失效时，备用NameNode立即接管，用户不会有明显中断感觉。</p><p>　　　　共享编辑日志文件（借助NFS、zookeeper等）</p><p>　　　　DataNode同时向两个NameNode汇报数据块信息</p><p>　　　　客户端采用特定机制处理 NameNode失效问题，该机制对用户透明</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（五）Hadoop集群搭建模式和各模式问题</title>
      <link href="/2018-04-05-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%94%EF%BC%89Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E6%A8%A1%E5%BC%8F%E5%92%8C%E5%90%84%E6%A8%A1%E5%BC%8F%E9%97%AE%E9%A2%98.html"/>
      <url>/2018-04-05-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%94%EF%BC%89Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E6%A8%A1%E5%BC%8F%E5%92%8C%E5%90%84%E6%A8%A1%E5%BC%8F%E9%97%AE%E9%A2%98.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（五）Hadoop集群搭建模式和各模式问题：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（五）Hadoop集群搭建模式和各模式问题</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="分布式集群的通用问题"><a href="#分布式集群的通用问题" class="headerlink" title="分布式集群的通用问题"></a>分布式集群的通用问题</h2><p>当前的HDFS和YARN都是一主多从的分布式架构，主从节点—管理者和工作者</p><p>问题：如果主节点或是管理者宕机了。会出现什么问题？</p><p>群龙无首，整个集群不可用。所以在一主多从的架构中都会有一个通用的问题：</p><p>当集群中的主节点宕机之后，整个集群不可用。这个现象叫做：单点故障。SPOF</p><h4 id="单点故障讲述的概念有两点"><a href="#单点故障讲述的概念有两点" class="headerlink" title="单点故障讲述的概念有两点"></a>单点故障讲述的概念有两点</h4><p>1、如果说宕机的那个节点是从节点，那么整个集群能够继续运行，并且对外提供正常的服务。</p><p>2、如果说宕机的那个节点是主节点，那么整个集群就处于宕机状态。</p><p>通用的解决方案：高可用</p><p>概念：当正在对外提供服务器的主从节点宕机，那么备用的主节点立马上位对外提供服务。无缝的瞬时切换。</p><p>皇帝驾崩，太子继位。</p><h2 id="集群的搭建的集中通用模式"><a href="#集群的搭建的集中通用模式" class="headerlink" title="集群的搭建的集中通用模式"></a>集群的搭建的集中通用模式</h2><h3 id="1、单机模式"><a href="#1、单机模式" class="headerlink" title="1、单机模式"></a>1、单机模式</h3><p>　　表示所有的分布式系统都是单机的。</p><h3 id="2、伪分布式模式（搭建在了只有一个节点的集群中）"><a href="#2、伪分布式模式（搭建在了只有一个节点的集群中）" class="headerlink" title="2、伪分布式模式（搭建在了只有一个节点的集群中）"></a>2、伪分布式模式（搭建在了只有一个节点的集群中）</h3><p>　　表示集群中的所有角色都分配给了一个节点。</p><p>　　表示整个集群被安装在了只有一个节点的集群中的。</p><p>　　主要用于做快速使用，去模拟分布式的效果。</p><h3 id="3、分布式模式"><a href="#3、分布式模式" class="headerlink" title="3、分布式模式"></a>3、分布式模式</h3><p>　　表示集群中的节点会被分配成很多种角色，分散在整个集群中。</p><p>　　主要用于学习测试等等一些场景中。</p><h3 id="4、高可用模式"><a href="#4、高可用模式" class="headerlink" title="4、高可用模式"></a>4、高可用模式</h3><p>　　表示整个集群中的主节点会有多个</p><p>　　注意区分：能够对外提供服务的主节点还是只有一个。其他的主节点全部处于一个热备的状态。</p><p>　　正在对外提供服务的主节点：active　　有且仅有一个</p><p>　　热备的主节点：standby　　可以有多个</p><p>　　工作模式：1、在任意时刻，只有一个主节点是active的，active的主节点对外提供服务</p><p>　　　　　　　2、在任意时刻，都应至少有一个standby的主节点，等待active的宕机来进行接替</p><p>　　架构模式：就是为了解决分布式集群中的通用问题SPOF</p><p>　　不管是分布式架构还是高可用架构，都存在一个问题：主从结构—从节点数量太多了。最直观的的问题：造成主节点的工作压力过载，主节点会宕机，当前的这种现象是一种死循环</p><h3 id="5、联邦模式"><a href="#5、联邦模式" class="headerlink" title="5、联邦模式"></a>5、联邦模式</h3><p>　　表示当前集群中的主从节点都可以有很多个。</p><p>　　1）主节点：可以有很多个的意思是说：同时对外提供服务的主节点有很多个。</p><p>　　　　　　      重点：每一个主节点都是用来管理整个集群中的一部分</p><p>　　2）从节点：一定会有很多个。</p><p>　　在联邦模式下还是会有问题：</p><p>　　虽然这个集群中的一个主节点的压力被分摊到了多个主节点。但是这个多个主节点依然会有一个问题：SOFP</p><h2 id="安装Hadoop集群中的一些通用问题"><a href="#安装Hadoop集群中的一些通用问题" class="headerlink" title="安装Hadoop集群中的一些通用问题"></a>安装Hadoop集群中的一些通用问题</h2><p>1、假如安装不成功，并且不知道应该怎么去解决这个安装错误：重装</p><p>　  需要做的处理：处理安装步骤中不同的部分即可。第一次安装和重装时候的不同步骤：</p><p>　  1）到修改配置文件以前，全部都不用动</p><p>　  2）检查配置文件是否都正确</p><p>　　　　先检查一个节点上的配置文件是否都正确，如果都正确，重新分发一次即可</p><p>　  3）在安装分布式集群时，所有节点中的安装的安装目录和安装者，需要检查和确定</p><p>　  4）删掉数据目录</p><p>　　　　A.　删除主节点的工作目录：namenode的数据目录</p><p>　　　　　   删除即可，只需要在主节点删除即可</p><p>　　　　B.　删除从节点的工作目录：datanode的数据目录</p><p>　　　　　   删除即可，把每个从节点上的这个对应数据目录都删掉</p><p>　　　　　   如果以上两份数据都被删除了之后。整个集群当中就相当于没有存储任何的历史数据。所以就是一个全新的集群</p><p> 　5）在确保数据正常和安装包都正常之后，进行重新初始化</p><p>　　   重点强调： hadoop集群的初始化，其实就是初始化HDFS集群， 只能在主节点进行初始化</p><p>　　   如果你只需要搭建YARN集群，那么是可以不用做初始化的。</p><p>　 6）启动集群</p><p>　 7）验证集群是否成功</p><h2 id="Linux环境变量加载的顺序"><a href="#Linux环境变量加载的顺序" class="headerlink" title="Linux环境变量加载的顺序"></a>Linux环境变量加载的顺序</h2><p>用户环境变量 ：仅仅只是当前用户使用 ~/.bashrc　　 ~/.bash_profile<br>系统环境变量 ：给当前系统中的所有用户使用 /etc/profile</p><p>任何普通用户在进行登录的时候：会同时加载几个环境变量的配置文件：</p><p>按顺序：<br>1、/etc/profile<br>2、<del>/.bash_profile<br>3、</del>/.bashrc</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（四）Hadoop集群搭建和简单应用</title>
      <link href="/2018-04-04-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%9B%9B%EF%BC%89Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E5%92%8C%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8.html"/>
      <url>/2018-04-04-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%9B%9B%EF%BC%89Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E5%92%8C%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（四）Hadoop集群搭建和简单应用：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（四）Hadoop集群搭建和简单应用</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="概念了解"><a href="#概念了解" class="headerlink" title="概念了解"></a>概念了解</h2><p>主从结构：在一个集群中，会有部分节点充当主服务器的角色，其他服务器都是从服务器的角色，当前这种架构模式叫做主从结构。</p><p>主从结构分类：</p><p>1、一主多从</p><p>2、多主多从</p><p>Hadoop中的HDFS和YARN都是主从结构，主从结构中的主节点和从节点有多重概念方式：</p><p>1、主节点　　从节点</p><p>2、master　　slave</p><p>3、管理者　　工作者</p><p>4、leader　　follower</p><p>Hadoop集群中各个角色的名称：</p><table><thead><tr><th>服务</th><th>主节点</th><th>从节点</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode</td><td>DataNode</td></tr><tr><td>YARN</td><td>ResourceManager</td><td>NodeManager</td></tr></tbody></table><h2 id="集群服务器规划"><a href="#集群服务器规划" class="headerlink" title="集群服务器规划"></a>集群服务器规划</h2><p>使用4台CentOS-6.7虚拟机进行集群搭建</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303115310907-1555873629.png" alt="img"></p><h2 id="软件安装步骤概述"><a href="#软件安装步骤概述" class="headerlink" title="软件安装步骤概述"></a>软件安装步骤概述</h2><p>1、获取安装包</p><p>2、解压缩和安装</p><p>3、修改配置文件</p><p>4、初始化，配置环境变量，启动，验证</p><h2 id="Hadoop安装"><a href="#Hadoop安装" class="headerlink" title="Hadoop安装"></a>Hadoop安装</h2><h3 id="1、规划"><a href="#1、规划" class="headerlink" title="1、规划"></a>1、规划</h3><p>规划安装用户：hadoop</p><p>规划安装目录：/home/hadoop/apps</p><p>规划数据目录：/home/hadoop/data</p><p>注：apps和data文件夹需要自己单独创建</p><h3 id="2、上传解压缩"><a href="#2、上传解压缩" class="headerlink" title="2、上传解压缩"></a>2、上传解压缩</h3><p>注：使用hadoop用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 apps]$ ls</span><br><span class="line">hadoop-2.7.5-centos-6.7.tar.gz</span><br><span class="line">[hadoop@hadoop1 apps]$ tar -zxvf hadoop-2.7.5-centos-6.7.tar.gz</span><br></pre></td></tr></table></figure><h3 id="3、修改配置文件"><a href="#3、修改配置文件" class="headerlink" title="3、修改配置文件"></a>3、修改配置文件</h3><p>配置文件目录：/home/hadoop/apps/hadoop-2.7.5/etc/hadoop</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303103331419-2007341529.png" alt="img"></p><h4 id="A-hadoop-env-sh"><a href="#A-hadoop-env-sh" class="headerlink" title="A.　hadoop-env.sh"></a>A.　hadoop-env.sh</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ vi hadoop-env.sh</span><br></pre></td></tr></table></figure><p>修改JAVA_HOME</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_73</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303104035548-3546092.png" alt="img"></p><p>B.　core-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ vi core-site.xml</span><br></pre></td></tr></table></figure><p>fs.defaultFS ： 这个属性用来指定namenode的hdfs协议的文件系统通信地址，可以指定一个主机+端口，也可以指定为一个namenode服务（这个服务内部可以有多台namenode实现ha的namenode服务</p><p>hadoop.tmp.dir : hadoop集群在工作的时候存储的一些临时文件的目录</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/data/hadoopdata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303104511807-906110891.png" alt="img"></p><p>C.　hdfs-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ vi hdfs-site.xml</span><br></pre></td></tr></table></figure><p> dfs.namenode.name.dir：namenode数据的存放地点。也就是namenode元数据存放的地方，记录了hdfs系统中文件的元数据。</p><p> dfs.datanode.data.dir： datanode数据的存放地点。也就是block块存放的目录了。</p><p>dfs.replication：hdfs的副本数设置。也就是上传一个文件，其分割为block块后，每个block的冗余副本个数，默认配置是3。</p><p>dfs.secondary.http.address：secondarynamenode 运行节点的信息，和 namenode 不同节点</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/data/hadoopdata/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">description</span>&gt;</span>为了保证元数据的安全一般配置多个不同目录<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/data/hadoopdata/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">description</span>&gt;</span>datanode 的数据存储目录<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">description</span>&gt;</span>HDFS 的数据块的副本存储个数, 默认是3<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.secondary.http.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop3:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">description</span>&gt;</span>secondarynamenode 运行节点的信息，和 namenode 不同节点<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303104911508-1011649393.png" alt="img"></p><p>D.　mapred-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ cp mapred-site.xml.template mapred-site.xml</span><br><span class="line">[hadoop@hadoop1 hadoop]$ vi mapred-site.xml</span><br></pre></td></tr></table></figure><p> mapreduce.framework.name：指定mr框架为yarn方式,Hadoop二代MP也基于资源管理系统Yarn来运行 。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303105128551-1997748350.png" alt="img"></p><p>E.　yarn-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ vi yarn-site.xml</span><br></pre></td></tr></table></figure><p> yarn.resourcemanager.hostname：yarn总管理器的IPC通讯地址</p><p> yarn.nodemanager.aux-services：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line"></span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;hadoop4&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">                &lt;description&gt;YARN 集群为 MapReduce 程序提供的 shuffle 服务&lt;/description&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303105358230-1169766109.png" alt="img"></p><p>F.　slaves</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ vi slaves </span><br><span class="line">hadoop1</span><br><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br><span class="line">hadoop4</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303105518279-1427162583.png" alt="img"></p><h3 id="4、把安装包分别分发给其他的节点"><a href="#4、把安装包分别分发给其他的节点" class="headerlink" title="4、把安装包分别分发给其他的节点"></a>4、把安装包分别分发给其他的节点</h3><p>重点强调： 每台服务器中的hadoop安装包的目录必须一致， 安装包的配置信息还必须保持一致<br>重点强调： 每台服务器中的hadoop安装包的目录必须一致， 安装包的配置信息还必须保持一致<br>重点强调： 每台服务器中的hadoop安装包的目录必须一致， 安装包的配置信息还必须保持一致</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ scp -r ~/apps/hadoop-2.7.5/ hadoop2:~/apps/</span><br><span class="line">[hadoop@hadoop1 hadoop]$ scp -r ~/apps/hadoop-2.7.5/ hadoop3:~/apps/</span><br><span class="line">[hadoop@hadoop1 hadoop]$ scp -r ~/apps/hadoop-2.7.5/ hadoop4:~/apps/</span><br></pre></td></tr></table></figure><p>注意：上面的命令等同于下面的命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ scp -r ~/apps/hadoop-2.7.5/ hadoop@hadoop2:~/apps/</span><br></pre></td></tr></table></figure><h3 id="5、配置Hadoop环境变量"><a href="#5、配置Hadoop环境变量" class="headerlink" title="5、配置Hadoop环境变量"></a>5、配置Hadoop环境变量</h3><p>千万注意：</p><p>1、如果你使用root用户进行安装。 vi /etc/profile 即可 系统变量</p><p>2、如果你使用普通用户进行安装。 vi ~/.bashrc 用户变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ vi .bashrc</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303110711007-1612467070.png" alt="img"></p><p>使环境变量生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 bin]$ source ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="6、查看hadoop版本"><a href="#6、查看hadoop版本" class="headerlink" title="6、查看hadoop版本"></a>6、查看hadoop版本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 bin]$ hadoop version</span><br><span class="line">Hadoop 2.7.5</span><br><span class="line">Subversion Unknown -r Unknown</span><br><span class="line">Compiled by root on 2017-12-24T05:30Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum 9f118f95f47043332d51891e37f736e9</span><br><span class="line">This command was run using /home/hadoop/apps/hadoop-2.7.5/share/hadoop/common/hadoop-common-2.7.5.jar</span><br><span class="line">[hadoop@hadoop1 bin]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303111035963-1326015903.png" alt="img"></p><h3 id="7、Hadoop初始化"><a href="#7、Hadoop初始化" class="headerlink" title="7、Hadoop初始化"></a>7、Hadoop初始化</h3><p>注意：HDFS初始化只能在主节点上进行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop namenode -format</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303111431844-1654882274.png" alt="img"></p><h3 id="8、启动"><a href="#8、启动" class="headerlink" title="8、启动"></a>8、启动</h3><p>A.　启动HDFS</p><p>注意：不管在集群中的那个节点都可以</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ start-dfs.sh</span><br><span class="line">Starting namenodes on [hadoop1]</span><br><span class="line">hadoop1: starting namenode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-namenode-hadoop1.out</span><br><span class="line">hadoop3: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop3.out</span><br><span class="line">hadoop2: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop2.out</span><br><span class="line">hadoop4: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop4.out</span><br><span class="line">hadoop1: starting datanode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-datanode-hadoop1.out</span><br><span class="line">Starting secondary namenodes [hadoop3]</span><br><span class="line">hadoop3: starting secondarynamenode, logging to /home/hadoop/apps/hadoop-2.7.5/logs/hadoop-hadoop-secondarynamenode-hadoop3.out</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303111708314-1318291140.png" alt="img"></p><p>B.　启动YARN</p><p>注意：只能在主节点中进行启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop4 ~]$ start-yarn.sh</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-resourcemanager-hadoop4.out</span><br><span class="line">hadoop2: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop2.out</span><br><span class="line">hadoop3: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop3.out</span><br><span class="line">hadoop4: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop4.out</span><br><span class="line">hadoop1: starting nodemanager, logging to /home/hadoop/apps/hadoop-2.7.5/logs/yarn-hadoop-nodemanager-hadoop1.out</span><br><span class="line">[hadoop@hadoop4 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303112021638-408158798.png" alt="img"></p><h3 id="9、查看4台服务器的进程"><a href="#9、查看4台服务器的进程" class="headerlink" title="9、查看4台服务器的进程"></a>9、查看4台服务器的进程</h3><p>hadoop1</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303112120800-1423701937.png" alt="img"></p><p>hadoop2</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303112139568-661988009.png" alt="img"></p><p>hadoop3</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303112211083-74589397.png" alt="img"></p><p>hadoop4</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303112225280-1304512419.png" alt="img"></p><h3 id="10、启动HDFS和YARN的web管理界面"><a href="#10、启动HDFS和YARN的web管理界面" class="headerlink" title="10、启动HDFS和YARN的web管理界面"></a>10、启动HDFS和YARN的web管理界面</h3><p>HDFS : <a href="http://192.168.123.102:50070" target="_blank" rel="noopener">http://192.168.123.102:50070</a><br>YARN ： <a href="http://hadoop05:8088" target="_blank" rel="noopener">http://hadoop05:8088</a></p><p>疑惑： fs.defaultFS = hdfs://hadoop02:9000</p><p>解答：客户单访问HDFS集群所使用的URL地址</p><p>同时，HDFS提供了一个web管理界面 端口：50070</p><h4 id="HDFS界面"><a href="#HDFS界面" class="headerlink" title="HDFS界面"></a>HDFS界面</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303112510116-894901884.png" alt="img"></p><p>点击Datanodes可以查看四个节点</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303112628507-1186899766.png" alt="img"></p><h4 id="YARN界面"><a href="#YARN界面" class="headerlink" title="YARN界面"></a>YARN界面</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303112735895-1867837449.png" alt="img"></p><p>点击Nodes可以查看节点</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303112833214-1809517028.png" alt="img"></p><h2 id="Hadoop的简单使用"><a href="#Hadoop的简单使用" class="headerlink" title="Hadoop的简单使用"></a>Hadoop的简单使用</h2><h3 id="创建文件夹"><a href="#创建文件夹" class="headerlink" title="创建文件夹"></a>创建文件夹</h3><p>在HDFS上创建一个文件夹/test/input</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -mkdir -p /test/input</span><br></pre></td></tr></table></figure><h3 id="查看创建的文件夹"><a href="#查看创建的文件夹" class="headerlink" title="查看创建的文件夹"></a>查看创建的文件夹</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-03-03 11:33 /test</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -ls /test</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-03-03 11:33 /test/input</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303113610630-1170857589.png" alt="img"></p><h3 id="上传文件"><a href="#上传文件" class="headerlink" title="上传文件"></a>上传文件</h3><p>创建一个文件words.txt</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ vi words.txt</span><br><span class="line">hello zhangsan</span><br><span class="line">hello lisi</span><br><span class="line">hello wangwu</span><br></pre></td></tr></table></figure><p>上传到HDFS的/test/input文件夹中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -put ~/words.txt /test/input</span><br></pre></td></tr></table></figure><p> 查看是否上传成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -ls /test/input</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   2 hadoop supergroup         39 2018-03-03 11:37 /test/input/words.txt</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303113847538-1539539423.png" alt="img"></p><h3 id="下载文件"><a href="#下载文件" class="headerlink" title="下载文件"></a>下载文件</h3><p>将刚刚上传的文件下载到~/data文件夹中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -get /test/input/words.txt ~/data</span><br></pre></td></tr></table></figure><p>查看是否下载成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ ls data</span><br><span class="line">hadoopdata  words.txt</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303114047756-345390954.png" alt="img"></p><h3 id="运行一个mapreduce的例子程序：-wordcount"><a href="#运行一个mapreduce的例子程序：-wordcount" class="headerlink" title="运行一个mapreduce的例子程序： wordcount"></a>运行一个mapreduce的例子程序： wordcount</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop jar ~/apps/hadoop-2.7.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.5.jar wordcount /test/input /test/output</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303114406061-137199367.png" alt="img"></p><p>在YARN Web界面查看</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303114707863-2058235690.png" alt="img"></p><p>查看结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -ls /test/output</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   2 hadoop supergroup          0 2018-03-03 11:42 /test/output/_SUCCESS</span><br><span class="line">-rw-r--r--   2 hadoop supergroup         35 2018-03-03 11:42 /test/output/part-r-00000</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -cat /test/output/part-r-00000</span><br><span class="line">hello    3</span><br><span class="line">lisi    1</span><br><span class="line">wangwu    1</span><br><span class="line">zhangsan    1</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303114542861-1807717747.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（三）Hadoop-2.7.5在CentOS-6.7上的编译</title>
      <link href="/2018-04-02-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%B8%89%EF%BC%89Hadoop-2.7.5%E5%9C%A8CentOS-6.7%E4%B8%8A%E7%9A%84%E7%BC%96%E8%AF%91.html"/>
      <url>/2018-04-02-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%B8%89%EF%BC%89Hadoop-2.7.5%E5%9C%A8CentOS-6.7%E4%B8%8A%E7%9A%84%E7%BC%96%E8%AF%91.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（三）Hadoop-2.7.5在CentOS-6.7上的编译：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（三）Hadoop-2.7.5在CentOS-6.7上的编译</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="下载Hadoop源码"><a href="#下载Hadoop源码" class="headerlink" title="下载Hadoop源码"></a>下载Hadoop源码</h2><h3 id="1、登录官网"><a href="#1、登录官网" class="headerlink" title="1、登录官网"></a>1、登录官网</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180302192813605-2005710468.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180302193152655-1055111483.png" alt="img"></p><h3 id="2、确定你要安装的软件的版本"><a href="#2、确定你要安装的软件的版本" class="headerlink" title="2、确定你要安装的软件的版本"></a>2、确定你要安装的软件的版本</h3><p>一个选取原则： 不新不旧的稳定版本</p><p>几个标准：</p><p>　　1）一般来说，刚刚发布的大版本都是有很多问题</p><p>　　2）应该选择某个大版本中的最后一个小版本</p><h2 id="阅读编译文档"><a href="#阅读编译文档" class="headerlink" title="阅读编译文档"></a>阅读编译文档</h2><p>1、准备一个hadoop源码包，我选择的hadoop的版本是：hadoop-2.7.5-src.tar.gz，在hadoop-2.7.5-src.tar.gz的源码包根目录下有一个文档叫做BUINDING.txt，这其中说明了编译hadoop所需要的一些编译环境相关的东西。不同的hadoop版本的要求都不一样。对应的版本参照BUINDING.txt。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Requirements:</span><br><span class="line"></span><br><span class="line">* Unix System</span><br><span class="line">* JDK 1.7+</span><br><span class="line">* Maven 3.0 or later</span><br><span class="line">* Findbugs 1.3.9 (if running findbugs)</span><br><span class="line">* ProtocolBuffer 2.5.0</span><br><span class="line">* CMake 2.6 or newer (if compiling native code), must be 3.0 or newer on Mac</span><br><span class="line">* Zlib devel (if compiling native code)</span><br><span class="line">* openssl devel ( if compiling native hadoop-pipes and to get the best HDFS encryption performance )</span><br><span class="line">* Linux FUSE (Filesystem in Userspace) version 2.6 or above ( if compiling fuse_dfs )</span><br><span class="line">* Internet connection for first build (to fetch all Maven and Hadoop dependencies)</span><br></pre></td></tr></table></figure><h2 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h2><p>对应以上需求，我们准备好所要求版本的这些软件。</p><h3 id="JDK的安装"><a href="#JDK的安装" class="headerlink" title="JDK的安装"></a>JDK的安装</h3><p>选择版本：jdk1.8.0_73</p><h3 id="安装依赖包"><a href="#安装依赖包" class="headerlink" title="安装依赖包"></a>安装依赖包</h3><p>根据编译指导文件BUILDING.txt，安装相关依赖程序包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master soft]# yum -y install gcc-c++ build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-devua svn openssl-devel ncurses-devel</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303140816784-127230994.png" alt="img"></p><h3 id="安装Maven"><a href="#安装Maven" class="headerlink" title="安装Maven"></a>安装Maven</h3><p>编译要求：Maven 3.0 or later<br>安装软件：apache-maven-3.0.5-bin.tar.gz</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`[root@hadoop1 soft]# ls``apache-maven-3.3.9-bin.tar.gz``[root@hadoop1 soft]# chmod 755 apache-maven-3.3.9-bin.tar.gz``[root@hadoop1 soft]# tar -zxvf apache-maven-3.3.9-bin.tar.gz &lt;br&gt;。。。&lt;br&gt;[root@hadoop1 soft]# mv apache-maven-3.3.9 /opt/&lt;br&gt;[root@hadoop1 soft]# vi /etc/profile`</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180302195254383-1359802075.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180302195527196-640953091.png" alt="img"></p><p>配置mvn的环境变量</p><p>export M2_HOME=/opt/apache-maven-3.3.<br>export PATH=$PATH:$M2_HOME/bin</p><p>测试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 soft]# mvn -v</span><br><span class="line">Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)</span><br><span class="line">Maven home: /opt/apache-maven-3.3.9</span><br><span class="line">Java version: 1.8.0_73, vendor: Oracle Corporation</span><br><span class="line">Java home: /usr/local/jdk1.8.0_73/jre</span><br><span class="line">Default locale: zh_CN, platform encoding: UTF-8</span><br><span class="line">OS name: &quot;linux&quot;, version: &quot;2.6.32-573.el6.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot;</span><br><span class="line">[root@hadoop1 soft]#</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180302195804401-1802492587.png" alt="img"></p><h3 id="安装Findbugs"><a href="#安装Findbugs" class="headerlink" title="安装Findbugs"></a>安装Findbugs</h3><p>编译要求：Findbugs 1.3.9<br>安装软件：findbugs-3.0.1.tar.gz</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 soft]# ls</span><br><span class="line">findbugs-3.0.1.tar.gz</span><br><span class="line">[root@hadoop1 soft]# chmod 755 findbugs-3.0.1.tar.gz </span><br><span class="line">[root@hadoop1 soft]# tar -zxvf findbugs-3.0.1.tar.gz -C /opt</span><br></pre></td></tr></table></figure><p> 配置Findbugs环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 soft]# vi /etc/profile</span><br></pre></td></tr></table></figure><p>export FINDBUGS_HOME=/opt/findbugs-3.0.1<br>export PATH=$PATH:$FINDBUGS_HOME/bin</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180302200617709-2078177175.png" alt="img"></p><p>测试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 soft]# findbugs -version</span><br><span class="line">3.0.1</span><br><span class="line">[root@hadoop1 soft]#</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180302200720858-324428380.png" alt="img"></p><h3 id="安装ProtocolBuffer"><a href="#安装ProtocolBuffer" class="headerlink" title="安装ProtocolBuffer"></a>安装ProtocolBuffer</h3><p>编译要求：ProtocolBuffer 2.5.0<br>安装软件：protobuf-2.5.0.tar.gz，不建议用其它版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 soft]# ls</span><br><span class="line">protobuf-2.5.0.tar.gz</span><br><span class="line">[root@hadoop1 soft]# chmod 755 protobuf-2.5.0.tar.gz </span><br><span class="line">[root@hadoop1 soft]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt[root@hadoop1 soft]# cd /opt/protobuf-2.5.0/[root@hadoop1 protobuf-2.5.0]# ./configure [root@hadoop1 protobuf-2.5.0]# make[root@hadoop1 protobuf-2.5.0]# make install</span><br></pre></td></tr></table></figure><p>测试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 protobuf-2.5.0]# protoc --version</span><br><span class="line">libprotoc 2.5.0</span><br><span class="line">[root@hadoop1 protobuf-2.5.0]#</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180302202301280-1196746479.png" alt="img"></p><h3 id="修改maven的配置文件，添加maven的下载源"><a href="#修改maven的配置文件，添加maven的下载源" class="headerlink" title="修改maven的配置文件，添加maven的下载源"></a>修改maven的配置文件，添加maven的下载源</h3><p>[root@hadoop1 protobuf-2.5.0]# cd /opt/apache-maven-3.3.9/conf/<br>[root@hadoop1 conf]# vi settings.xml</p><p>在mirrors中添加alimaven的下载源</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mirrors</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- mirror</span></span><br><span class="line"><span class="comment">     | Specifies a repository mirror site to use instead of a given repository. The repository that</span></span><br><span class="line"><span class="comment">     | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used</span></span><br><span class="line"><span class="comment">     | for inheritance and direct lookup purposes, and must be unique across the set of mirrors.</span></span><br><span class="line"><span class="comment">     |</span></span><br><span class="line"><span class="comment">    </span></span><br><span class="line"><span class="comment">        &lt;mirror&gt;</span></span><br><span class="line"><span class="comment">        &lt;id&gt;alimaven&lt;/id&gt;</span></span><br><span class="line"><span class="comment">        &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;</span></span><br><span class="line"><span class="comment">         &lt;name&gt;aliyun maven&lt;/name&gt;</span></span><br><span class="line"><span class="comment">        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;</span></span><br><span class="line"><span class="comment">        &lt;/mirror&gt;</span></span><br><span class="line"><span class="comment">        &lt;mirror&gt;</span></span><br><span class="line"><span class="comment">        &lt;id&gt;mirrorId&lt;/id&gt;</span></span><br><span class="line"><span class="comment">        &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt;</span></span><br><span class="line"><span class="comment">        &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt;</span></span><br><span class="line"><span class="comment">        &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt;</span></span><br><span class="line"><span class="comment">        &lt;/mirror&gt;</span></span><br><span class="line"><span class="comment">     --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirrors</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="安装Ant"><a href="#安装Ant" class="headerlink" title="安装Ant"></a>安装Ant</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 soft]# ls</span><br><span class="line">apache-ant-1.9.4-bin.tar.gz  hadoop-2.7.5-src</span><br><span class="line">[root@hadoop1 soft]# tar -zxvf apache-ant-1.9.4-bin.tar.gz -C /opt/</span><br></pre></td></tr></table></figure><p>配置环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 apache-ant-1.9.4]# vi /etc/profile</span><br><span class="line">#Ant</span><br><span class="line">export ANT_HOME=/opt/apache-ant-1.9.4</span><br><span class="line">export PATH=$PATH:$ANT_HOME/bin</span><br></pre></td></tr></table></figure><p>检测</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 apache-ant-1.9.4]# ant -version</span><br><span class="line">Apache Ant(TM) version 1.9.4 compiled on April 29 2014</span><br><span class="line">[root@hadoop1 apache-ant-1.9.4]#</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303094806773-1281893074.png" alt="img"></p><h3 id="安装Snappy"><a href="#安装Snappy" class="headerlink" title="安装Snappy"></a>安装Snappy</h3><p>解压</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 soft]# ls</span><br><span class="line">hadoop-2.7.5-src  snappy-1.1.1.tar.gz</span><br><span class="line">[root@hadoop1 soft]# tar -zxvf snappy-1.1.1.tar.gz -C /opt/</span><br></pre></td></tr></table></figure><p>安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 soft]# cd /opt/snappy-1.1.1/</span><br><span class="line">[root@hadoop1 snappy-1.1.1]# ./configure </span><br><span class="line">[root@hadoop1 snappy-1.1.1]# make </span><br><span class="line">[root@hadoop1 snappy-1.1.1]# make install</span><br></pre></td></tr></table></figure><p>查看snappy文件库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 snappy-1.1.1]# ls -lh /usr/local/lib | grep snappy</span><br><span class="line">-rw-r--r--  1 root root 228K 3月   3 09:51 libsnappy.a</span><br><span class="line">-rwxr-xr-x  1 root root  953 3月   3 09:51 libsnappy.la</span><br><span class="line">lrwxrwxrwx  1 root root   18 3月   3 09:51 libsnappy.so -&gt; libsnappy.so.1.2.0</span><br><span class="line">lrwxrwxrwx  1 root root   18 3月   3 09:51 libsnappy.so.1 -&gt; libsnappy.so.1.2.0</span><br><span class="line">-rwxr-xr-x  1 root root 145K 3月   3 09:51 libsnappy.so.1.2.0</span><br><span class="line">[root@hadoop1 snappy-1.1.1]#</span><br></pre></td></tr></table></figure><h2 id="开始编译hadoop"><a href="#开始编译hadoop" class="headerlink" title="开始编译hadoop"></a>开始编译hadoop</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 soft]# ls</span><br><span class="line">[root@hadoop1 soft]# tar -zxvf hadoop-2.7.5-src.tar.gz</span><br></pre></td></tr></table></figure><p>在编译之前防止java.lang.OutOfMemoryError:Java heap space堆栈问题，在centos系统中执行命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 snappy-1.1.1]# export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot;</span><br></pre></td></tr></table></figure><p>进入源码包下，执行命令进行编译</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 snappy-1.1.1]# cd /soft/hadoop-2.7.5-src/</span><br><span class="line">[root@hadoop1 hadoop-2.7.5-src]# mvn package -Pdist,native,docs -DskipTests -Dtar</span><br></pre></td></tr></table></figure><p> 如果中途编译失败，并且不要文档的话，请使用这个命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# mvn clear package -Pdist,native -DskipTests -Dtar -Dsnappy.lib=/usr/local/lib -Dbundle.snappy -Drequire.openssl</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303191804648-376409087.png" alt="img"></p><p>编译成功之后，hadoop-2.7.5.tar.gz位于/soft/hadoop-2.7.5-src/hadoop-dist/target目录下，这是编译后文件夹的状态</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180303192135977-329404593.png" alt="img"></p><p>至此，大功告成！！！</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习之路（二）Hadoop发展背景</title>
      <link href="/2018-04-01-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%EF%BC%89Hadoop%E5%8F%91%E5%B1%95%E8%83%8C%E6%99%AF.html"/>
      <url>/2018-04-01-Hadoop%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%EF%BC%89Hadoop%E5%8F%91%E5%B1%95%E8%83%8C%E6%99%AF.html</url>
      
        <content type="html"><![CDATA[<p>** Hadoop学习之路（二）Hadoop发展背景：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop学习之路（二）Hadoop发展背景</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="Hadoop产生的背景"><a href="#Hadoop产生的背景" class="headerlink" title="Hadoop产生的背景"></a>Hadoop产生的背景</h2><ol><li><p>HADOOP<strong>最早起源于Nutch</strong>。Nutch的设计目标是构建一个大型的全网搜索引擎，包括网页抓取、索引、查询等功能，但随着抓取网页数量的增加，<strong>遇到了严重的可扩展性问题——</strong>如何解决数十亿网页的存储和索引问题。</p></li><li><p>2003年开始<strong>谷歌陆续发表的三篇论文为该问题提供了可行的解决方案</strong>。</p></li></ol><p>——分布式文件系统（GFS），可用于处理海量网页的<strong>存储</strong></p><p>——分布式计算框架MAPREDUCE，可用于处理海量网页的<strong>索引计算</strong>问题。</p><p>——BigTable 数据库：OLTP 联机事务处理 Online Transaction Processing 增删改<br>　　　　　　　　　　OLAP 联机分析处理 Online Analysis Processing 查询<br>　　　　　　　　　　真正的作用：提供了一种可以在超大数据集中进行实时CRUD操作的功能</p><p>3.Nutch的开发人员完成了相应的<strong>开源实现HDFS和MAPREDUCE</strong>，并从Nutch中剥离成为独立项目HADOOP，到2008年1月，HADOOP成为Apache顶级项目，迎来了它的快速发展期。</p><h2 id="Hadoop是啥"><a href="#Hadoop是啥" class="headerlink" title="Hadoop是啥"></a>Hadoop是啥</h2><p>Hadoop的官网：<a href="http://hadoop.apache.org/" target="_blank" rel="noopener">http://hadoop.apache.org/</a></p><p>1、Hadoop是Apache旗下的一套开源软件平台</p><p>2、Hadoop提供的功能：利用服务器集群，根据户自定义业逻辑对海量数进行分布式处理</p><p>3、Hadoop的核心组件：</p><p>　　1）<strong>Hadoop Common</strong>：支持其他Hadoop模块的常用工具。</p><p>　　2)  <strong>Hadoop分布式文件系统（HDFS™）</strong>：一种分布式文件系统，可提供对应用程序数据的高吞吐量访问。</p><p>　　3)  <strong>Hadoop YARN</strong>：作业调度和集群资源管理的框架。</p><p>　　<strong>4)  Hadoop MapReduce</strong>：一种用于并行处理大型数据集的基于YARN的系统。</p><p>　　大数据的处理主要就是<strong>存储</strong>和<strong>计算</strong>。</p><p>如果说安装hadoop集群，其实就是安装了两个东西： 一个操作系统YARN 和 一个文件系统HDFS。其实MapReduce就是运行在YARN之上的应用。</p><p>操作系统 　　文件系统 　　应用程序<br>win7    　　　　NTFS　　　  QQ，WeChat<br>YARN    　　　 HDFS    　　    MapReduce</p><p>4、hadoop的概念：</p><p>　　狭义上： 就是apache的一个顶级项目：apahce hadoop</p><p>　　广义上: 就是指以hadoop为核心的整个大数据处理体系</p><p>5、Apache的其他Hadoop相关项目包括：</p><ul><li><a href="http://incubator.apache.org/ambari/" target="_blank" rel="noopener"><strong>Ambari™</strong></a>：一种用于供应，管理和监控Apache Hadoop集群的基于Web的工具，其中包括对Hadoop HDFS，Hadoop MapReduce，Hive，HCatalog，HBase，ZooKeeper，Oozie，Pig和Sqoop的支持。Ambari还提供了一个用于查看群集运行状况的仪表板，例如热图和可以直观地查看MapReduce，Pig和Hive应用程序的功能，以及以用户友好的方式诊断其性能特征的功能。</li><li><a href="http://avro.apache.org/" target="_blank" rel="noopener"><strong>Avro™</strong></a>：数据序列化系统。</li><li><a href="http://cassandra.apache.org/" target="_blank" rel="noopener"><strong>Cassandra™</strong></a>：无单点故障的可扩展多主数据库。</li><li><a href="http://incubator.apache.org/chukwa/" target="_blank" rel="noopener"><strong>Chukwa™</strong></a>：管理大型分布式系统的数据收集系统。</li><li><a href="http://hbase.apache.org/" target="_blank" rel="noopener"><strong>HBase™</strong></a>：可扩展的分布式数据库，支持大型表格的结构化数据存储。</li><li><a href="http://hive.apache.org/" target="_blank" rel="noopener"><strong>Hive™</strong></a>：提供数据汇总和即席查询的数据仓库基础架构。</li><li><a href="http://mahout.apache.org/" target="_blank" rel="noopener"><strong>Mahout™</strong></a>：可扩展的机器学习和数据挖掘库。</li><li><a href="http://pig.apache.org/" target="_blank" rel="noopener"><strong>Pig™</strong></a>：用于并行计算的高级数据流语言和执行框架。</li><li><a href="http://spark.incubator.apache.org/" target="_blank" rel="noopener"><strong>Spark™</strong></a>：用于Hadoop数据的快速和通用计算引擎。Spark提供了一个简单而富有表现力的编程模型，它支持广泛的应用程序，包括ETL，机器学习，流处理和图计算。</li><li><a href="http://tez.incubator.apache.org/" target="_blank" rel="noopener"><strong>Tez™</strong></a>：一种基于Hadoop YARN的通用数据流编程框架，它提供了一个强大且灵活的引擎，可执行任意DAG任务来处理批处理和交互式用例的数据。Hado™，Pig™和Hadoop生态系统中的其他框架以及其他商业软件（例如ETL工具）正在采用Tez来替代Hadoop™MapReduce作为底层执行引擎。</li><li><a href="http://zookeeper.apache.org/" target="_blank" rel="noopener"><strong>ZooKeeper™</strong></a>：分布式应用程序的高性能协调服务。</li></ul><h2 id="HADOOP在大数据、云计算中的位置和关系"><a href="#HADOOP在大数据、云计算中的位置和关系" class="headerlink" title="HADOOP在大数据、云计算中的位置和关系"></a><strong>HADOOP在大数据、云计算中的位置和关系</strong></h2><p>1、云计算是分布式计算、并行计算、网格计算、多核计算、网络存储、虚拟化、负载均衡等传统计算机技术和互联网技术融合发展的产物。借助IaaS(基础设施即服务)、PaaS(平台即服务)、SaaS（软件即服务）等业务模式，把强大的计算能力提供给终端用户。1、</p><p>2、现阶段，云计算的<strong>两大底层支撑技术</strong>为“<strong>虚拟化</strong>”和“<strong>大数据技术</strong>”</p><p>3、 而HADOOP则是云计算的PaaS层的解决方案之一，并不等同于PaaS，更不等同于云计算本身。</p><h2 id="Hadoop的技术应用"><a href="#Hadoop的技术应用" class="headerlink" title="Hadoop的技术应用"></a>Hadoop的技术应用</h2><h4 id="HADOOP应用于数据服务基础平台建设"><a href="#HADOOP应用于数据服务基础平台建设" class="headerlink" title="HADOOP应用于数据服务基础平台建设"></a><strong>HADOOP应用于数据服务基础平台建设</strong></h4><p><strong><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180302190557551-1606375842.png" alt="img"></strong></p><h4 id="HADOOP用于用户画像"><a href="#HADOOP用于用户画像" class="headerlink" title="HADOOP用于用户画像"></a><strong>HADOOP用于用户画像</strong></h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180302190637930-1990174101.png" alt="img"></p><p> 该图是中国电信的用户画像标签体系。</p><h4 id="HADOOP用于网站点击流日志-数据-挖掘"><a href="#HADOOP用于网站点击流日志-数据-挖掘" class="headerlink" title="HADOOP用于网站点击流日志**数据**挖掘"></a><strong>HADOOP用于网站点击流日志**</strong>数据<strong>**挖掘</strong></h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180302190852771-1457100418.png" alt="img"></p><p><strong>总结：hadoop并不会跟某个具体的行业或者某个具体的业务挂钩，它只是一种用来做海量数据分析处理的工具。</strong></p><h2 id="HADOOP生态圈以及各组成部分的简介"><a href="#HADOOP生态圈以及各组成部分的简介" class="headerlink" title="HADOOP生态圈以及各组成部分的简介"></a><strong>HADOOP生态圈以及各组成部分的简介</strong></h2><p><strong><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180302191054664-57367875.png" alt="img"></strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180302191105833-1985207815.png" alt="img"></p><p>重点组件：</p><p>HDFS：Hadoop的分布式文件存储系统。</p><p>MapReduce：Hadoop的分布式程序运算框架，也可以叫做一种编程模型。</p><p>Hive：基于Hadoop的类SQL数据仓库工具</p><p>Hbase：基于Hadoop的列式分布式NoSQL数据库</p><p>ZooKeeper：分布式协调服务组件</p><p>Mahout：基于MapReduce/Flink/Spark等分布式运算框架的机器学习算法库</p><p>Oozie/Azkaban：工作流调度引擎</p><p>Sqoop：数据迁入迁出工具</p><p>Flume：日志采集工具</p><h2 id="获取数据的三种方式"><a href="#获取数据的三种方式" class="headerlink" title="获取数据的三种方式"></a>获取数据的三种方式</h2><p>1、自己公司收集的数据–日志     或者     数据库中的数据</p><p>2、有一些数据可以通过爬虫从网络中进行爬取</p><p>3、从第三方机构购买</p><h2 id="国内HADOOP的就业情况分析"><a href="#国内HADOOP的就业情况分析" class="headerlink" title="国内HADOOP的就业情况分析"></a><strong>国内HADOOP的就业情况分析</strong></h2><h3 id="1、HADOOP就业整体情况"><a href="#1、HADOOP就业整体情况" class="headerlink" title="1、HADOOP就业整体情况"></a>1、HADOOP就业整体情况</h3><p>A. 大数据产业已纳入<strong>国家十三五规划</strong></p><p>B. 各大城市都在进行<strong>智慧城市项目</strong>建设，而智慧城市的根基就是大数据综合平台</p><p>C. 互联网时代数据的种类，增长都呈现<strong>爆发式增长</strong>，各行业对数据的价值日益重视</p><p>D. 相对于传统JAVAEE技术领域来说，大数据领域的<strong>人才相对稀缺</strong></p><p>E. 随着现代社会的发展，数据处理和数据挖掘的重要性只会增不会减，因此，大数据技术是一个尚在蓬勃发展且具有<strong>长远前景的领域</strong></p><h3 id="2、-HADOOP就业职位要求"><a href="#2、-HADOOP就业职位要求" class="headerlink" title="2、 HADOOP就业职位要求"></a>2、 HADOOP就业职位要求</h3><p>大数据是个复合专业，包括应用开发、软件平台、算法、数据挖掘等，因此，<strong>大数据技术领域的就业选择是多样的</strong>，但就HADOOP而言，通常都需要具备以下技能或知识：</p><p>硬实力</p><p>A. HADOOP分布式集群的平台搭建</p><p>B. HADOOP分布式文件系统HDFS的原理理解及使用</p><p>C. HADOOP分布式运算框架MAPREDUCE的原理理解及编程</p><p>D. Hive数据仓库工具的熟练应用</p><p>E. Flume、sqoop、oozie等辅助工具的熟练使用</p><p>F. Shell/python等脚本语言的开发能力</p><p>软实力</p><p>A.  解决问题的能力（调试，阅读文档）</p><p>B.  沟通协调能力（寻求帮助）</p><p>C.  学习提升自己的能力（自我提高）</p><p>D.   组织管控能力（管理能力）</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式锁的几种实现方式</title>
      <link href="/dslock.html"/>
      <url>/dslock.html</url>
      
        <content type="html"><![CDATA[<p>** 分布式锁的几种实现方式：** &lt;Excerpt in index | 首页摘要&gt;<br>在分布式架构中，由于多线程和多台服务器，何难保证顺序性。如果需要对某一个资源进行限制，比如票务，比如请求幂等性控制等，这个时候分布式锁就排上用处。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="什么是分布式锁"><a href="#什么是分布式锁" class="headerlink" title="什么是分布式锁"></a>什么是分布式锁</h2><p>分布式锁是控制分布式系统或不同系统之间共同访问共享资源的一种锁实现，如果不同的系统或同一个系统的不同主机之间共享了某个资源时，往往需要互斥来防止彼此干扰来保证一致性。</p><h2 id="分布式锁需要解决的问题"><a href="#分布式锁需要解决的问题" class="headerlink" title="分布式锁需要解决的问题"></a>分布式锁需要解决的问题</h2><ol><li>互斥性：任意时刻，只能有一个客户端获取锁，不能同时有两个客户端获取到锁。</li><li>安全性：锁只能被持有该锁的客户端删除，不能由其它客户端删除。</li><li>死锁：获取锁的客户端因为某些原因（如down机等）而未能释放锁，其它客户端再也无法获取到该锁。</li><li>容错：当部分节点（redis节点等）down机时，客户端仍然能够获取锁和释放锁。</li></ol><h2 id="分布式锁的实现方式"><a href="#分布式锁的实现方式" class="headerlink" title="分布式锁的实现方式"></a>分布式锁的实现方式</h2><ol><li><p>数据库实现</p></li><li><p>缓存实现，比如redis</p></li><li><p>zookeeper实现</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 分布式架构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式系统理论基础</title>
      <link href="/dsbasic.html"/>
      <url>/dsbasic.html</url>
      
        <content type="html"><![CDATA[<p>** 分布式系统理论基础：** &lt;Excerpt in index | 首页摘要&gt;<br>分布式系统不是万能，不能解决所有痛点。在高可用，一致性，分区容错性必须有所权衡。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="CAP理论"><a href="#CAP理论" class="headerlink" title="CAP理论"></a>CAP理论</h2><p>定理：任何分布式架构都只能同时满足两点，无法三者兼顾。</p><ul><li>Consistency（一致性），数据一致更新，所有的数据变动都是同步的。</li><li>Availability（可用性），好的响应性能。</li><li>Partition tolerance（分区容忍性）可靠性，机器宕机是否影响使用。</li></ul><p>关系数据库的ACID模型拥有 高一致性 + 可用性 很难进行分区：</p><ol><li>Atomicity原子性：一个事务中所有操作都必须全部完成，要么全部不完成。</li><li>Consistency一致性. 在事务开始或结束时，数据库应该在一致状态。</li><li>Isolation隔离性. 事务将假定只有它自己在操作数据库，彼此不知晓。</li><li>Durability持久性 一旦事务完成，就不能返回。<br>跨数据库两段提交事务：2PC (two-phase commit)， 2PC is the anti-scalability pattern (Pat Helland)<br>是反可伸缩模式的，JavaEE中的JTA事务可以支持2PC。因为2PC是反模式，尽量不要使用2PC，使用BASE来回避。</li></ol><h2 id="BASE理论"><a href="#BASE理论" class="headerlink" title="BASE理论"></a>BASE理论</h2><ul><li>Basically Available 基本可用，支持分区失败</li><li>Soft state 软状态，允许状态某个时间短不同步，或者异步</li><li>Eventually consistent 最终一致性，要求数据最终结果一致，而不是时刻高度一致。</li></ul><h2 id="paxos协议"><a href="#paxos协议" class="headerlink" title="paxos协议"></a>paxos协议</h2><p>Paxos算法的目的是为了解决分布式环境下一致性的问题。多个节点并发操纵数据，如何保证在读写过程中数据的一致性，并且解决方案要能适应分布式环境下的不可靠性（系统如何就一个值达到统一）。</p><h3 id="Paxos的两个组件"><a href="#Paxos的两个组件" class="headerlink" title="Paxos的两个组件:"></a>Paxos的两个组件:</h3><ul><li>Proposer：提议发起者，处理客户端请求，将客户端的请求发送到集群中，以便决定这个值是否可以被批准。</li><li>Acceptor:提议批准者，负责处理接收到的提议，他们的回复就是一次投票。会存储一些状态来决定是否接收一个值</li></ul><h3 id="Paxos有两个原则"><a href="#Paxos有两个原则" class="headerlink" title="Paxos有两个原则"></a>Paxos有两个原则</h3><ol><li>安全原则—保证不能做错的事<ul><li>a） 针对某个实例的表决只能有一个值被批准，不能出现一个被批准的值被另一个值覆盖的情况；(假设有一个值被多数Acceptor批准了，那么这个值就只能被学习)</li><li>b） 每个节点只能学习到已经被批准的值，不能学习没有被批准的值。</li></ul></li><li>存活原则—只要有多数服务器存活并且彼此间可以通信，最终都要做到的下列事情：<ul><li>a）最终会批准某个被提议的值；</li><li>b）一个值被批准了，其他服务器最终会学习到这个值。</li></ul></li></ol><h2 id="zab协议-ZooKeeper-Atomic-broadcast-protocol"><a href="#zab协议-ZooKeeper-Atomic-broadcast-protocol" class="headerlink" title="zab协议(ZooKeeper Atomic broadcast protocol)"></a>zab协议(ZooKeeper Atomic broadcast protocol)</h2><p>ZAB协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。</p><h3 id="Phase-0-Leader-election（选举阶段）"><a href="#Phase-0-Leader-election（选举阶段）" class="headerlink" title="Phase 0: Leader election（选举阶段）"></a>Phase 0: Leader election（选举阶段）</h3><p>节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。只有到达 Phase 3 准 leader 才会成为真正的 leader。这一阶段的目的是就是为了选出一个准 leader，然后进入下一个阶段。</p><h3 id="Phase-1-Discovery（发现阶段）"><a href="#Phase-1-Discovery（发现阶段）" class="headerlink" title="Phase 1: Discovery（发现阶段）"></a>Phase 1: Discovery（发现阶段）</h3><p>在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。这个一阶段的主要目的是发现当前大多数节点接收的最新提议，并且准 leader 生成新的 epoch，让 followers 接受，更新它们的 acceptedEpoch。<br>一个 follower 只会连接一个 leader，如果有一个节点 f 认为另一个 follower p 是 leader，f 在尝试连接 p 时会被拒绝，f 被拒绝之后，就会进入 Phase 0。</p><h3 id="Phase-2-Synchronization（同步阶段）"><a href="#Phase-2-Synchronization（同步阶段）" class="headerlink" title="Phase 2: Synchronization（同步阶段）"></a>Phase 2: Synchronization（同步阶段）</h3><p>同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。只有当 quorum 都同步完成，准 leader 才会成为真正的 leader。follower 只会接收 zxid 比自己的 lastZxid 大的提议。</p><h3 id="Phase-3-Broadcast（广播阶段）"><a href="#Phase-3-Broadcast（广播阶段）" class="headerlink" title="Phase 3: Broadcast（广播阶段）"></a>Phase 3: Broadcast（广播阶段）</h3><p>到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。</p><h2 id="raft协议"><a href="#raft协议" class="headerlink" title="raft协议"></a>raft协议</h2><p>在Raft中，每个结点会处于下面三种状态中的一种：</p><h3 id="follower"><a href="#follower" class="headerlink" title="follower"></a>follower</h3><p>所有结点都以follower的状态开始。如果没收到leader消息则会变成candidate状态。</p><h3 id="candidate"><a href="#candidate" class="headerlink" title="candidate"></a>candidate</h3><p>会向其他结点“拉选票”，如果得到大部分的票则成为leader。这个过程就叫做Leader选举(Leader Election)</p><h3 id="leader"><a href="#leader" class="headerlink" title="leader"></a>leader</h3><p>所有对系统的修改都会先经过leader。每个修改都会写一条日志(log entry)。leader收到修改请求后的过程如下，这个过程叫做日志复制(Log Replication)：</p><pre><code>1. 复制日志到所有follower结点(replicate entry)2. 大部分结点响应时才提交日志3. 通知所有follower结点日志已提交4. 所有follower也提交日志5. 现在整个系统处于一致的状态</code></pre>]]></content>
      
      
      <categories>
          
          <category> 分布式架构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> protocol </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为什么使用zookeeper？</title>
      <link href="/zookeeper.html"/>
      <url>/zookeeper.html</url>
      
        <content type="html"><![CDATA[<p>** 为什么使用zookeeper？：** &lt;Excerpt in index | 首页摘要&gt;<br>随着大型互联网的发展，分布式系统应用越来越来越广泛，zookeeper成了分布式系统的标配。集群容错，动态负载均衡，动态扩容，异地多活等架构都依赖于zookeeper而搭建。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="zookeeper是什么？"><a href="#zookeeper是什么？" class="headerlink" title="zookeeper是什么？"></a>zookeeper是什么？</h2><p>zookeeper是由雅虎创建的，基于google chubby,一个开源的分布式协调服务，是分布式数据一致性的解决方案。</p><h2 id="zookeeper的特性"><a href="#zookeeper的特性" class="headerlink" title="zookeeper的特性"></a>zookeeper的特性</h2><ul><li>顺序一致性，从同一个客户端发起的事务请求，最终会严格按照顺序被应用到zookeeper中。</li><li>原子性，事务请求在所有集群是一致的，要么都成功，要么都失败。</li><li>可靠性，一旦服务器成功应用某个事务，那么所有集群中一定同步并保留。</li><li>实时性，一个事务被应用，客户端能立即从服务端读取到状态变化。</li></ul><h2 id="zookeeper的原理？"><a href="#zookeeper的原理？" class="headerlink" title="zookeeper的原理？"></a>zookeeper的原理？</h2><p>基于分布式协议pasxo，而实现了自己的zab协议。保证数据的一致性。</p><h2 id="zookeeper的数据模型"><a href="#zookeeper的数据模型" class="headerlink" title="zookeeper的数据模型"></a>zookeeper的数据模型</h2><ul><li>持久化节点，节点创建后一直存在，直到主动删除。</li><li>持久化有序节点，每个节点都会为它的一级子节点维护一个顺序。</li><li>临时节点，临时节点的生命周期和客户端会话保持一直。客户端会话失效，节点自动清理。</li><li>临时有序节点，临时节点基础上多一个顺序性特性。</li></ul><h2 id="zookeeper使用场景有哪些？"><a href="#zookeeper使用场景有哪些？" class="headerlink" title="zookeeper使用场景有哪些？"></a>zookeeper使用场景有哪些？</h2><ul><li>订阅发布<ul><li>watcher机制</li><li>统一配置管理(disconf)</li></ul></li><li>分布式锁（redis也可以）</li><li>分布式队列</li><li>负载均衡(dubbo)</li><li>ID生成器</li><li>master选举(kafka,hadoop,hbase)</li></ul><h2 id="集群角色有哪些？"><a href="#集群角色有哪些？" class="headerlink" title="集群角色有哪些？"></a>集群角色有哪些？</h2><h3 id="leader"><a href="#leader" class="headerlink" title="leader"></a>leader</h3><ol><li>事务请求的唯一调度者和处理者，保证集群事务的处理顺序</li><li>集群内部服务的调度者</li></ol><h3 id="follower"><a href="#follower" class="headerlink" title="follower"></a>follower</h3><ol><li>处理非事务请求，以及转发事务请求到leader</li><li>参与事务请求提议的投票</li><li>参与leader选举的投票</li></ol><h3 id="observer"><a href="#observer" class="headerlink" title="observer"></a>observer</h3><ol><li>观察集群中最新状态的变化并同步到observer服务器上</li><li>增加observer不影响集群事务处理能力，还能提升非事务请求的处理能力</li></ol><h2 id="zookeeper集群为什么是奇数？"><a href="#zookeeper集群为什么是奇数？" class="headerlink" title="zookeeper集群为什么是奇数？"></a>zookeeper集群为什么是奇数？</h2><p>zookeeper事务请求提议需要超过半数的机器，假如是2(n+1)台,需要n+2台机器同意，<br>由于在增删改操作中需要半数以上服务器通过，来分析以下情况。<br>2台服务器，至少2台正常运行才行（2的半数为1，半数以上最少为2），正常运行1台服务器都不允许挂掉<br>3台服务器，至少2台正常运行才行（3的半数为1.5，半数以上最少为2），正常运行可以允许1台服务器挂掉<br>4台服务器，至少3台正常运行才行（4的半数为2，半数以上最少为3），正常运行可以允许1台服务器挂掉<br>5台服务器，至少3台正常运行才行（5的半数为2.5，半数以上最少为3），正常运行可以允许2台服务器挂掉<br>6台服务器，至少3台正常运行才行（6的半数为3，半数以上最少为4），正常运行可以允许2台服务器挂掉</p><h2 id="zookeeper日志管理？"><a href="#zookeeper日志管理？" class="headerlink" title="zookeeper日志管理？"></a>zookeeper日志管理？</h2><h2 id="leader选举的原理"><a href="#leader选举的原理" class="headerlink" title="leader选举的原理"></a>leader选举的原理</h2><h2 id="未完待续"><a href="#未完待续" class="headerlink" title="未完待续"></a>未完待续</h2>]]></content>
      
      
      <categories>
          
          <category> 分布式架构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>查找链表倒数第n个元素</title>
      <link href="/descNode.html"/>
      <url>/descNode.html</url>
      
        <content type="html"><![CDATA[<p>** 查找链表倒数第n个元素：** &lt;Excerpt in index | 首页摘要&gt;<br>链表应用很广泛，有单向链表，双向链表。单向链表如何查找倒数第n个元素呢？本文以java代码实现链表反向查找。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="单向链表的定义"><a href="#单向链表的定义" class="headerlink" title="单向链表的定义"></a>单向链表的定义</h2><p>单向链表，主要有数据存储，下一个节点的引用这两个元素组成。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public class Node &#123;</span><br><span class="line">    int value;</span><br><span class="line">    Node next;</span><br><span class="line"></span><br><span class="line">    Node(int value) &#123;</span><br><span class="line">        this.value = value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="遍历倒数第n个元素"><a href="#遍历倒数第n个元素" class="headerlink" title="遍历倒数第n个元素"></a>遍历倒数第n个元素</h2><p>在查找过程中，设置两个指针，让其中一个指针比另一个指针先前移k-1步，<br>然后两个指针同时往前移动。循环直到先行的指针指为NULL时，另一个指针所指的位置就是所要找的位置<br>算法复杂度为o（n）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">public Node findDescEle(Node head, int k) &#123;</span><br><span class="line">    if (k &lt; 1 || head == null) &#123;</span><br><span class="line">        return null;</span><br><span class="line">    &#125;</span><br><span class="line">    Node p1 = head;</span><br><span class="line">    Node p2 = head;</span><br><span class="line">    //前移k-1步</span><br><span class="line">    int step = 0;</span><br><span class="line">    for (int i = 0; i &lt; k; i++) &#123;</span><br><span class="line">        step++;</span><br><span class="line">        if (p1.next != null) &#123;</span><br><span class="line">            p1 = p1.next;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    while (p1 != null) &#123;</span><br><span class="line">        step++;</span><br><span class="line">        p1 = p1.next;</span><br><span class="line">        p2 = p2.next;</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(&quot;o(n)==&quot; + step);</span><br><span class="line">    return p2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>查找链表倒数第n个元素，复杂度为o(n),使用两个指针即可简单实现。</p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sprigmvc项目转为springboot</title>
      <link href="/sprigmvc2boot.html"/>
      <url>/sprigmvc2boot.html</url>
      
        <content type="html"><![CDATA[<p>** sprigmvc项目转为springboot：** &lt;Excerpt in index | 首页摘要&gt;<br>是否有老掉牙的springmvc项目，想转成springboot项目，看这个文章就对了。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><ul><li>如果你的项目连maven项目都不是，请自行转为maven项目，在按照本教程进行。</li><li>本教程适用于spring+springmvc+mybatis+shiro的maven项目。</li></ul><h2 id="1-修改pom文件依赖"><a href="#1-修改pom文件依赖" class="headerlink" title="1.修改pom文件依赖"></a>1.修改pom文件依赖</h2><ol><li><p>删除之前的spring依赖，添加springboot依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-parent<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.5.9.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">&lt;!-- 这个是剔除掉自带的 tomcat部署的--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-tomcat<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- tomcat容器部署 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-tomcat<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;scope&gt;compile&lt;/scope&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis.spring.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis-spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-devtools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">optional</span>&gt;</span>true<span class="tag">&lt;/<span class="name">optional</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 支持 @ConfigurationProperties 注解 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-configuration-processor<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">optional</span>&gt;</span>true<span class="tag">&lt;/<span class="name">optional</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.tomcat.embed<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>tomcat-embed-jasper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>添加springboot构建插件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.7<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.7<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.5.9.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goal</span>&gt;</span>repackage<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="2-添加application启动文件"><a href="#2-添加application启动文件" class="headerlink" title="2.添加application启动文件"></a>2.添加application启动文件</h2><p>注意，如果Application在controller，service，dao的上一层包里，无需配置<code>@ComponentScan</code>,<br>否则，需要指明要扫描的包。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="comment">//@ComponentScan(&#123;"com.cms.controller","com.cms.service","com.cms.dao"&#125;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> <span class="keyword">extends</span> <span class="title">SpringBootServletInitializer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> SpringApplicationBuilder <span class="title">configure</span><span class="params">(SpringApplicationBuilder application)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> application.sources(Application.class);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        SpringApplication.run(Application.class, args);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-添加springboot配置文件"><a href="#3-添加springboot配置文件" class="headerlink" title="3.添加springboot配置文件"></a>3.添加springboot配置文件</h2><ol><li>在resources下面添加application.properties文件</li><li>添加基本配置<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#默认前缀</span><br><span class="line">server.contextPath=/</span><br><span class="line"># 指定环境</span><br><span class="line">spring.profiles.active=local</span><br><span class="line"># jsp配置</span><br><span class="line">spring.mvc.view.prefix=/WEB-INF/jsp/</span><br><span class="line">spring.mvc.view.suffix=.jsp</span><br><span class="line">#log配置文件</span><br><span class="line">logging.config=classpath:logback-cms.xml</span><br><span class="line">#log路径</span><br><span class="line">logging.path=/Users/mac/work-tommy/cms-springboot/logs/</span><br><span class="line">#数据源</span><br><span class="line">spring.datasource.name=adminDataSource</span><br><span class="line">spring.datasource.driverClassName = com.mysql.jdbc.Driver</span><br><span class="line">spring.datasource.url = jdbc:mysql://localhost:3306/mycms?useUnicode=true&amp;autoReconnect=true&amp;characterEncoding=utf-8</span><br><span class="line">spring.datasource.username = root</span><br><span class="line">spring.datasource.password = 123456</span><br></pre></td></tr></table></figure></li></ol><h2 id="4-使用-Configuration注入配置"><a href="#4-使用-Configuration注入配置" class="headerlink" title="4.使用@Configuration注入配置"></a>4.使用@Configuration注入配置</h2><ol><li><p>注入mybatis配置,分页插件请自主选择</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@MapperScan</span>(basePackages = <span class="string">"com.kuwo.dao"</span>,sqlSessionTemplateRef  = <span class="string">"adminSqlSessionTemplate"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AdminDataSourceConfig</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span>(name = <span class="string">"adminDataSource"</span>)</span><br><span class="line">    <span class="meta">@ConfigurationProperties</span>(prefix = <span class="string">"spring.datasource"</span>)</span><br><span class="line">    <span class="meta">@Primary</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataSource <span class="title">adminDataSource</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> DataSourceBuilder.create().build();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span>(name = <span class="string">"adminSqlSessionFactory"</span>)</span><br><span class="line">    <span class="meta">@Primary</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SqlSessionFactory <span class="title">adminSqlSessionFactory</span><span class="params">(@Qualifier(<span class="string">"adminDataSource"</span>)</span> DataSource dataSource) <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        SqlSessionFactoryBean bean = <span class="keyword">new</span> SqlSessionFactoryBean();</span><br><span class="line">        bean.setDataSource(dataSource);</span><br><span class="line">        <span class="comment">//分页插件</span></span><br><span class="line"><span class="comment">//        PageHelper pageHelper = new PageHelper();</span></span><br><span class="line">        PagePlugin pagePlugin = <span class="keyword">new</span> PagePlugin();</span><br><span class="line"><span class="comment">//        Properties props = new Properties();</span></span><br><span class="line"><span class="comment">//        props.setProperty("reasonable", "true");</span></span><br><span class="line"><span class="comment">//        props.setProperty("supportMethodsArguments", "true");</span></span><br><span class="line"><span class="comment">//        props.setProperty("returnPageInfo", "check");</span></span><br><span class="line"><span class="comment">//        props.setProperty("params", "count=countSql");</span></span><br><span class="line"><span class="comment">//        pageHelper.setProperties(props);</span></span><br><span class="line">        <span class="comment">//添加插件</span></span><br><span class="line">        bean.setPlugins(<span class="keyword">new</span> Interceptor[]&#123;pagePlugin&#125;);</span><br><span class="line">        <span class="comment">// 添加mybatis配置文件</span></span><br><span class="line">        bean.setConfigLocation(<span class="keyword">new</span> DefaultResourceLoader().getResource(<span class="string">"classpath:mybatis/mybatis-config.xml"</span>));</span><br><span class="line">        <span class="comment">// 添加mybatis映射文件</span></span><br><span class="line">        bean.setMapperLocations(<span class="keyword">new</span> PathMatchingResourcePatternResolver().getResources(<span class="string">"classpath:mybatis/system/*.xml"</span>));</span><br><span class="line">        <span class="keyword">return</span> bean.getObject();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span>(name = <span class="string">"adminTransactionManager"</span>)</span><br><span class="line">    <span class="meta">@Primary</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataSourceTransactionManager <span class="title">adminTransactionManager</span><span class="params">(@Qualifier(<span class="string">"adminDataSource"</span>)</span> DataSource dataSource) </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> DataSourceTransactionManager(dataSource);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span>(name = <span class="string">"adminSqlSessionTemplate"</span>)</span><br><span class="line">    <span class="meta">@Primary</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SqlSessionTemplate <span class="title">adminSqlSessionTemplate</span><span class="params">(@Qualifier(<span class="string">"adminSqlSessionFactory"</span>)</span> SqlSessionFactory sqlSessionFactory) <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> SqlSessionTemplate(sqlSessionFactory);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>添加Interceptor配置,注意addInterceptor的顺序，不要搞乱了</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InterceptorConfiguration</span> <span class="keyword">extends</span> <span class="title">WebMvcConfigurerAdapter</span></span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addInterceptors</span><span class="params">(InterceptorRegistry registry)</span> </span>&#123;</span><br><span class="line">        registry.addInterceptor(<span class="keyword">new</span> LoginHandlerInterceptor());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>添加shiro配置文件</p><ul><li>注意：本来使用redis做session缓存，但是和shiro集成发现一个问题，user对象存储以后，从shiro中获取后，无法进行类型转换，所以暂时放弃了redis做session缓存。</li></ul></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ShiroConfiguration</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;spring.redis.host&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> String host;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;spring.redis.port&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> port;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;spring.redis.timeout&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> timeout;</span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> LifecycleBeanPostProcessor <span class="title">getLifecycleBeanPostProcessor</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> LifecycleBeanPostProcessor();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * ShiroFilterFactoryBean 处理拦截资源文件问题。</span></span><br><span class="line"><span class="comment">     * 注意：单独一个ShiroFilterFactoryBean配置是或报错的，因为在</span></span><br><span class="line"><span class="comment">     * 初始化ShiroFilterFactoryBean的时候需要注入：SecurityManager</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     Filter Chain定义说明</span></span><br><span class="line"><span class="comment">     1、一个URL可以配置多个Filter，使用逗号分隔</span></span><br><span class="line"><span class="comment">     2、当设置多个过滤器时，全部验证通过，才视为通过</span></span><br><span class="line"><span class="comment">     3、部分过滤器可指定参数，如perms，roles</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ShiroFilterFactoryBean <span class="title">shiroFilter</span><span class="params">(SecurityManager securityManager)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"ShiroConfiguration.shirFilter()"</span>);</span><br><span class="line">        ShiroFilterFactoryBean shiroFilterFactoryBean  = <span class="keyword">new</span> ShiroFilterFactoryBean();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 必须设置 SecurityManager</span></span><br><span class="line">        shiroFilterFactoryBean.setSecurityManager(securityManager);</span><br><span class="line">        <span class="comment">// 如果不设置默认会自动寻找Web工程根目录下的"/login.jsp"页面</span></span><br><span class="line">        shiroFilterFactoryBean.setLoginUrl(<span class="string">"/login_toLogin"</span>);</span><br><span class="line">        <span class="comment">// 登录成功后要跳转的链接</span></span><br><span class="line">        shiroFilterFactoryBean.setSuccessUrl(<span class="string">"/usersPage"</span>);</span><br><span class="line">        <span class="comment">//未授权界面;</span></span><br><span class="line">        shiroFilterFactoryBean.setUnauthorizedUrl(<span class="string">"/403"</span>);</span><br><span class="line">        <span class="comment">//拦截器.</span></span><br><span class="line">        Map&lt;String,String&gt; filterChainDefinitionMap = <span class="keyword">new</span> LinkedHashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//配置退出 过滤器,其中的具体的退出代码Shiro已经替我们实现了</span></span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/logout"</span>, <span class="string">"logout"</span>);</span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/login_toLogin"</span>, <span class="string">"anon"</span>);</span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/login_login"</span>, <span class="string">"anon"</span>);</span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/static/login/**"</span>,<span class="string">"anon"</span>);</span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/static/js/**"</span>,<span class="string">"anon"</span>);</span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/uploadFiles/uploadImgs/**"</span>,<span class="string">"anon"</span>);</span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/code.do"</span>,<span class="string">"anon"</span>);</span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/font-awesome/**"</span>,<span class="string">"anon"</span>);</span><br><span class="line">        <span class="comment">//&lt;!-- 过滤链定义，从上向下顺序执行，一般将 /**放在最为下边 --&gt;:这是一个坑呢，一不小心代码就不好使了;</span></span><br><span class="line">        <span class="comment">//&lt;!-- authc:所有url都必须认证通过才可以访问; anon:所有url都都可以匿名访问--&gt;</span></span><br><span class="line"></span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/**"</span>, <span class="string">"authc"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        shiroFilterFactoryBean.setFilterChainDefinitionMap(filterChainDefinitionMap);</span><br><span class="line">        <span class="keyword">return</span> shiroFilterFactoryBean;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SecurityManager <span class="title">securityManager</span><span class="params">()</span></span>&#123;</span><br><span class="line">        DefaultWebSecurityManager securityManager =  <span class="keyword">new</span> DefaultWebSecurityManager();</span><br><span class="line">        <span class="comment">//设置realm.</span></span><br><span class="line">        securityManager.setRealm(myShiroRealm());</span><br><span class="line">        <span class="comment">// 自定义缓存实现 使用redis</span></span><br><span class="line">        <span class="comment">//securityManager.setCacheManager(cacheManager());</span></span><br><span class="line">        <span class="comment">// 自定义session管理 使用redis</span></span><br><span class="line">        securityManager.setSessionManager(sessionManager());</span><br><span class="line">        <span class="keyword">return</span> securityManager;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ShiroRealm <span class="title">myShiroRealm</span><span class="params">()</span></span>&#123;</span><br><span class="line">        ShiroRealm myShiroRealm = <span class="keyword">new</span> ShiroRealm();</span><br><span class="line"><span class="comment">//        myShiroRealm.setCredentialsMatcher(hashedCredentialsMatcher());</span></span><br><span class="line">        <span class="keyword">return</span> myShiroRealm;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *  开启shiro aop注解支持.</span></span><br><span class="line"><span class="comment">     *  使用代理方式;所以需要开启代码支持;</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> securityManager</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> AuthorizationAttributeSourceAdvisor <span class="title">authorizationAttributeSourceAdvisor</span><span class="params">(SecurityManager securityManager)</span></span>&#123;</span><br><span class="line">        AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor = <span class="keyword">new</span> AuthorizationAttributeSourceAdvisor();</span><br><span class="line">        authorizationAttributeSourceAdvisor.setSecurityManager(securityManager);</span><br><span class="line">        <span class="keyword">return</span> authorizationAttributeSourceAdvisor;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 配置shiro redisManager</span></span><br><span class="line"><span class="comment">     * 使用的是shiro-redis开源插件</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RedisManager <span class="title">redisManager</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        RedisManager redisManager = <span class="keyword">new</span> RedisManager();</span><br><span class="line">        redisManager.setHost(host);</span><br><span class="line">        redisManager.setPort(port);</span><br><span class="line">        redisManager.setExpire(<span class="number">1800</span>);</span><br><span class="line">        redisManager.setTimeout(timeout);</span><br><span class="line">        <span class="comment">// redisManager.setPassword(password);</span></span><br><span class="line">        <span class="keyword">return</span> redisManager;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * cacheManager 缓存 redis实现</span></span><br><span class="line"><span class="comment">     * 使用的是shiro-redis开源插件</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RedisCacheManager <span class="title">cacheManager</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        RedisCacheManager redisCacheManager = <span class="keyword">new</span> RedisCacheManager();</span><br><span class="line">        redisCacheManager.setRedisManager(redisManager());</span><br><span class="line">        <span class="keyword">return</span> redisCacheManager;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * RedisSessionDAO shiro sessionDao层的实现 通过redis</span></span><br><span class="line"><span class="comment">     * 使用的是shiro-redis开源插件</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RedisSessionDAO <span class="title">redisSessionDAO</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        RedisSessionDAO redisSessionDAO = <span class="keyword">new</span> RedisSessionDAO();</span><br><span class="line">        redisSessionDAO.setRedisManager(redisManager());</span><br><span class="line">        <span class="keyword">return</span> redisSessionDAO;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DefaultWebSessionManager <span class="title">sessionManager</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        DefaultWebSessionManager sessionManager = <span class="keyword">new</span> DefaultWebSessionManager();</span><br><span class="line"><span class="comment">//        sessionManager.setSessionDAO(redisSessionDAO());</span></span><br><span class="line">        <span class="keyword">return</span> sessionManager;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>搞了一天时间把项目转成springboot，查阅各种资料，希望这篇文章能够为你带来帮助。</p>]]></content>
      
      
      <categories>
          
          <category> 项目实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows连接Linux虚拟机里面的Docker容器</title>
      <link href="/2018-02-02-Windows%E8%BF%9E%E6%8E%A5Linux%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%87%8C%E9%9D%A2%E7%9A%84Docker%E5%AE%B9%E5%99%A8.html"/>
      <url>/2018-02-02-Windows%E8%BF%9E%E6%8E%A5Linux%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%87%8C%E9%9D%A2%E7%9A%84Docker%E5%AE%B9%E5%99%A8.html</url>
      
        <content type="html"><![CDATA[<p>** Windows连接Linux虚拟机里面的Docker容器：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Windows连接Linux虚拟机里面的Docker容器</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、Windows、Linux虚拟机、docker关系图"><a href="#一、Windows、Linux虚拟机、docker关系图" class="headerlink" title="一、Windows、Linux虚拟机、docker关系图"></a>一、Windows、Linux虚拟机、docker关系图</h2><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180510183714998-435698473.png" alt="img"></p><p>如果此时在Windows宿主机中pingDocker容器是ping不同的，因为在宿主机上没有通往172.17.0.0/24网络的路由，宿主机会将发往172.17.0.0/24网络的数据发往默认路由，这样就无法到达容器。</p><h2 id="二、操作"><a href="#二、操作" class="headerlink" title="二、操作"></a>二、操作</h2><h3 id="2-1-关闭Linux中的防火墙"><a href="#2-1-关闭Linux中的防火墙" class="headerlink" title="2.1　关闭Linux中的防火墙"></a>2.1　关闭Linux中的防火墙</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata ~]# systemctl stop firewalld.service</span><br></pre></td></tr></table></figure><h3 id="2-2-在docker容器中安装并启用ssh服务"><a href="#2-2-在docker容器中安装并启用ssh服务" class="headerlink" title="2.2　在docker容器中安装并启用ssh服务"></a>2.2　在docker容器中安装并启用ssh服务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@spark6 redis]# /usr/sbin/sshd -D &amp;</span><br></pre></td></tr></table></figure><h3 id="2-3-Windows宿主机与虚拟机CentOS网络互通"><a href="#2-3-Windows宿主机与虚拟机CentOS网络互通" class="headerlink" title="2.3　Windows宿主机与虚拟机CentOS网络互通"></a>2.3　Windows宿主机与虚拟机CentOS网络互通</h3><p>可通过Xshell连接</p><h3 id="2-4-虚拟机CentOS和Docker容器网络互通"><a href="#2-4-虚拟机CentOS和Docker容器网络互通" class="headerlink" title="2.4　虚拟机CentOS和Docker容器网络互通"></a>2.4　虚拟机CentOS和Docker容器网络互通</h3><p>在CentOS中可以通过docker exec -it <container_id> /bin/bash命令进入容器内部</container_id></p><h3 id="2-5-在Windows中添加到docker容器网段的路由"><a href="#2-5-在Windows中添加到docker容器网段的路由" class="headerlink" title="2.5　在Windows中添加到docker容器网段的路由"></a>2.5　在Windows中添加到docker容器网段的路由</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\WINDOWS\system32&gt;route add 172.17.0.0 mask 255.255.255.0 192.168.123.110</span><br></pre></td></tr></table></figure><p>该路由表示通往172.17.0.0/24网络的数据包通过192.168.123.110来转发。</p><h3 id="2-6-测试"><a href="#2-6-测试" class="headerlink" title="2.6　测试"></a>2.6　测试</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180510184332503-516086162.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CentOS7 安装Docker</title>
      <link href="/2018-02-01-CentOS7%20%E5%AE%89%E8%A3%85Docker.html"/>
      <url>/2018-02-01-CentOS7%20%E5%AE%89%E8%A3%85Docker.html</url>
      
        <content type="html"><![CDATA[<p>** CentOS7 安装Docker：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （一）Spark初识</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、检查系统内核"><a href="#一、检查系统内核" class="headerlink" title="一、检查系统内核"></a>一、检查系统内核</h2><p>Docker 要求 CentOS 系统的内核版本高于 3.10 ，查看本页面的前提条件来验证你的CentOS 版本是否支持 Docker 。</p><p>通过 <strong>uname -r</strong> 命令查看你当前的内核版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata ~]# uname -r</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508193509650-2014211576.png" alt="img"></p><h2 id="二、安装Docker"><a href="#二、安装Docker" class="headerlink" title="二、安装Docker"></a>二、安装Docker</h2><h3 id="2-1-安装"><a href="#2-1-安装" class="headerlink" title="2.1　安装"></a>2.1　安装</h3><p>Docker 软件包和依赖包已经包含在默认的 CentOS-Extras 软件源里，安装命令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata ~]# yum -y install docker</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508193906464-1646665952.png" alt="img"></p><h3 id="2-2-查看docker版本"><a href="#2-2-查看docker版本" class="headerlink" title="2.2　查看docker版本"></a>2.2　查看docker版本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata ~]# docker version</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508194053682-191708110.png" alt="img"></p><h3 id="2-3-启动docker"><a href="#2-3-启动docker" class="headerlink" title="2.3　启动docker"></a>2.3　启动docker</h3><p>方式一：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata ~]# service docker start</span><br><span class="line">Redirecting to /bin/systemctl start docker.service</span><br><span class="line">[root@bigdata ~]#</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508195058989-163681608.png" alt="img"></p><p>方式二：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata ~]# systemctl start docker.service</span><br><span class="line">[root@bigdata ~]# ps aux | grep docker</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509091656878-775534744.png" alt="img"></p><h2 id="三、建立docker用户和组"><a href="#三、建立docker用户和组" class="headerlink" title="三、建立docker用户和组"></a>三、建立docker用户和组</h2><h3 id="3-1-创建用户及组"><a href="#3-1-创建用户及组" class="headerlink" title="3.1　创建用户及组"></a>3.1　创建用户及组</h3><p>默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata ~]# groupadd docker</span><br><span class="line">[root@bigdata ~]# useradd -g docker docker</span><br></pre></td></tr></table></figure><h3 id="3-2-使用新创建的用户运行helloworld"><a href="#3-2-使用新创建的用户运行helloworld" class="headerlink" title="3.2　使用新创建的用户运行helloworld"></a>3.2　使用新创建的用户运行helloworld</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[docker@bigdata ~]$ docker run hello-world</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[docker@bigdata ~]$ docker run hello-world</span><br><span class="line">Unable to find image &apos;hello-world:latest&apos; locally</span><br><span class="line">Trying to pull repository docker.io/library/hello-world ... </span><br><span class="line">latest: Pulling from docker.io/library/hello-world</span><br><span class="line">9bb5a5d4561a: Pulling fs layer </span><br><span class="line">/usr/bin/docker-current: error pulling image configuration: Get https://dseasb33srnrn.cloudfront.net/registry-v2/docker/registry/v2/blobs/sha256/e3/e38bc07ac18ee64e6d59cf2eafcdddf9cec2364dfe129fe0af75f1b0194e0c96/data?Expires=1525823399&amp;Signature=SjqbSNVW5X~uDhy9jXvuLqv22jC3auyGRx4JCRE1ceXkdh0Qpsc21VmhIXwAO6XcxwyJ1gGNVQhnJWYozOWXjysL8taJFBCxKNqAD9Cy~TCt-iMi06z9dHX6-WxxIU3WJ4LbCT7RxsWIKArTVKmPvyQdD4Djkgr~rWzoL6eyTfg_&amp;Key-Pair-Id=APKAJECH5M7VWIS5YZ6Q: net/http: TLS handshake timeout.</span><br><span class="line">See &apos;/usr/bin/docker-current run --help&apos;.</span><br><span class="line">[docker@bigdata ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508195840234-1463116301.png" alt="img"></p><h3 id="3-3-解决报错"><a href="#3-3-解决报错" class="headerlink" title="3.3　解决报错"></a>3.3　解决报错</h3><p>如上图报错<strong>/usr/bin/docker-current: error pulling image configuration。。。</strong></p><p><strong>出现这个问题，一般的原因是无法连接到 docker hub通过（使用root用户执行以下命令）：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata ~]# cat /etc/sysconfig/docker</span><br></pre></td></tr></table></figure><p>在文件中添加以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--registry-mirror=http://f2d6cb40.m.daocloud.io</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508200747683-1514086239.png" alt="img"></p><p>重启docker</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata ~]# service docker restart</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508201036527-1369269603.png" alt="img"></p><p>再次运行helloworld（docker用户）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[docker@bigdata ~]$ docker run hello-world</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508201117738-2122905401.png" alt="img"></p><p>由于本地没有hello-world这个镜像，所以会下载一个hello-world的镜像，并在容器内运行。</p><h2 id="四、安装centos镜像"><a href="#四、安装centos镜像" class="headerlink" title="四、安装centos镜像"></a>四、安装centos镜像</h2><h3 id="4-1-下载镜像"><a href="#4-1-下载镜像" class="headerlink" title="4.1　下载镜像"></a>4.1　下载镜像</h3><p>从 Docker 镜像仓库获取镜像的命令是 docker pull。其命令格式为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签]</span><br></pre></td></tr></table></figure><p>可以直接使用docker pull centos:7命令安装镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[docker@bigdata ~]$ docker pull centos:7</span><br></pre></td></tr></table></figure><p> <img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508201539077-637092308.png" alt="img"></p><h3 id="4-2-查看拥有的镜像"><a href="#4-2-查看拥有的镜像" class="headerlink" title="4.2　查看拥有的镜像"></a>4.2　查看拥有的镜像</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[docker@bigdata ~]$ docker image ls</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508201643678-1847283881.png" alt="img"></p><p>一个是centos镜像，另一个是我们之前使用docker run hello-world命令下载的镜像。</p><p>镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。</p><h3 id="4-3-运行容器"><a href="#4-3-运行容器" class="headerlink" title="4.3　运行容器"></a>4.3　运行容器</h3><p>有了镜像后，我们就能够以这个镜像为基础启动并运行一个容器。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[docker@bigdata ~]$ docker run -it --rm centos bash</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508202330251-712010418.png" alt="img"></p><p>docker run 就是运行容器的命令，说明一下上面用到的参数。</p><blockquote><ul><li>-it：这是两个参数，一个是 -i：交互式操作，一个是 -t 终端。我们这里打算进入 bash 执行一些命令并查看返回结果，因 此我们需要交互式终端。</li><li>–rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 –rm 可以避免浪费空间。</li><li>centos ：这是指用centos  镜像为基础来启动容器。</li><li>bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 bash。</li></ul></blockquote><h3 id="4-4-在容器中运行命令"><a href="#4-4-在容器中运行命令" class="headerlink" title="4.4　在容器中运行命令"></a>4.4　在容器中运行命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@cb55b5f51685 /]# cat /etc/os-release</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508202542401-1937736501.png" alt="img"></p><p>进入容器后，我们可以在 Shell 下操作，执行任何所需的命令。这里，我们执行了 cat /etc/os-release，这是 Linux 常用的查看当前系统版本的命令，从返回的结果可以看到容器内是 CentOS Linux 系统。<br>最后我们可以通过 exit 退出了这个容器。</p><h3 id="4-5-查看镜像、容器、数据卷所占用的空间"><a href="#4-5-查看镜像、容器、数据卷所占用的空间" class="headerlink" title="4.5　查看镜像、容器、数据卷所占用的空间"></a>4.5　查看镜像、容器、数据卷所占用的空间</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[docker@bigdata ~]$ docker system df</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508202805476-1397312883.png" alt="img"></p><h3 id="4-6-容器退出再次进入报错"><a href="#4-6-容器退出再次进入报错" class="headerlink" title="4.6　容器退出再次进入报错"></a>4.6　容器退出再次进入报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[docker@bigdata ~]$ docker run -it -v /home/docker/build:/root/build --privileged -h hadoop1 --name hadoop1 centos /bin/bash</span><br><span class="line">/usr/bin/docker-current: Error response from daemon: Conflict. The container name &quot;/hadoop1&quot; is already in use by container a094bdef9e1cac62a17022e568fe9b1eb021e13adf8ed2624a71be5a2e42c618. You have to remove (or rename) that container to be able to reuse that name..</span><br><span class="line">See &apos;/usr/bin/docker-current run --help&apos;.</span><br><span class="line">[docker@bigdata ~]$</span><br></pre></td></tr></table></figure><blockquote><ul><li><code>docker ps</code>: 查看当前运行的容器</li><li><code>docker ps -a</code>:查看所有容器，包括停止的。</li></ul></blockquote><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508205404959-1539539627.png" alt="img"></p><p>标题含义：</p><blockquote><ul><li>CONTAINER ID:容器的唯一表示ID。</li><li>IMAGE:创建容器时使用的镜像。</li><li>COMMAND:容器最后运行的命令。</li><li>CREATED:创建容器的时间。</li><li>STATUS:容器状态。</li><li>PORTS:对外开放的端口。</li><li>NAMES:容器名。可以和容器ID一样唯一标识容器，同一台宿主机上不允许有同名容器存在，否则会冲突。</li></ul></blockquote><p>使用命令停止并删除这个容器就可以</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508205933028-1455812154.png" alt="img"></p><h2 id="五、运行容器"><a href="#五、运行容器" class="headerlink" title="五、运行容器"></a>五、运行容器</h2><h3 id="5-1-使用命令运行容器"><a href="#5-1-使用命令运行容器" class="headerlink" title="5.1　使用命令运行容器"></a>5.1　使用命令运行容器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[docker@bigdata ~]$ docker run -it -v /home/docker/build:/root/build --privileged -h hadoop1 --name hadoop1 centos /bin/bash</span><br></pre></td></tr></table></figure><p>以centos镜像启动一个容器，容器名是hadoop1，主机名是hadoop1，并且将基于容器的centos系统的/root/build目录与本地/home/docker/build共享。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508203250136-988615963.png" alt="img"></p><p>参数解释：</p><blockquote><ul><li>-v 表示基于容器的centos系统的/root/build目录与本地/home/hadoop/build共享；这可以很方便将本地文件上传到Docker内部的centos系统；</li><li>-h 指定主机名为hadoop1</li><li>–-name  指定容器名</li><li>/bin/bash  使用bash命令</li></ul></blockquote><h3 id="六、刚安装的系统非常纯净，需要安装必备的软件"><a href="#六、刚安装的系统非常纯净，需要安装必备的软件" class="headerlink" title="六、刚安装的系统非常纯净，需要安装必备的软件"></a>六、刚安装的系统非常纯净，需要安装必备的软件</h3><h3 id="6-1-安装vim"><a href="#6-1-安装vim" class="headerlink" title="6.1　安装vim"></a>6.1　安装vim</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 /]# yum install vim</span><br></pre></td></tr></table></figure><h3 id="6-2-升级及安装sshd"><a href="#6-2-升级及安装sshd" class="headerlink" title="6.2　升级及安装sshd"></a>6.2　升级及安装sshd</h3><h4 id="6-2-1-安装"><a href="#6-2-1-安装" class="headerlink" title="6.2.1　安装"></a>6.2.1　安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 /]# yum -y update</span><br><span class="line">[root@hadoop1 /]# yum -y install openssh-server</span><br><span class="line">[root@hadoop1 /]# yum -y install openssh-clients</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509092643850-1118717983.png" alt="img"></p><p>编辑sshd的配置文件/etc/ssh/sshd_config，将其中的UsePAM yes改为UsePAM no</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 /]# vi /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509093038727-176729187.png" alt="img"></p><h4 id="6-2-2-启动"><a href="#6-2-2-启动" class="headerlink" title="6.2.2　启动"></a>6.2.2　启动</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 /]# /usr/sbin/sshd -D</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509093402474-923839093.png" alt="img"></p><p>报错如图，解决方案为：创建公私密钥，输入命令后，直接按两次enter键确认就行了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 /]# ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509093635141-1427855583.png" alt="img"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 /]# ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509093734236-128491269.png" alt="img"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 /]# ssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509093845818-407554589.png" alt="img"></p><p>再次启动SSH服务</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509103116501-1930285777.png" alt="img"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 /]# yum -y install lsof</span><br><span class="line">[root@hadoop1 /]# lsof -i:22</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509094741072-935231545.png" alt="img"></p><h3 id="6-3-修改root密码"><a href="#6-3-修改root密码" class="headerlink" title="6.3　修改root密码"></a>6.3　修改root密码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 /]# passwd</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509094525056-1526638278.png" alt="img"></p><p>测试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 /]# ssh localhost</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509103401359-448686039.png" alt="img"></p><p>上图中可以看到已经登录到本机了，也就说容器中的主机拥有了ssh远程登录其它主机的能力，当然你也可以登录其他主机。要退出的话，输入命令exit即可。</p><h3 id="6-4-宿主机能登录本机（容器中的主机）"><a href="#6-4-宿主机能登录本机（容器中的主机）" class="headerlink" title="6.4　宿主机能登录本机（容器中的主机）"></a>6.4　宿主机能登录本机（容器中的主机）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 ~]# vi /etc/hosts</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509103544191-424697759.png" alt="img"></p><p>得到容器中的主机的ip地址172.17.0.2（可能和你得到的不一样）</p><p>然后在宿主机中开启一个新的终端输入命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[docker@bigdata ~]$ ssh root@172.17.0.2</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509103827941-843699179.png" alt="img"></p><h3 id="6-5-配置ssh无密码登录"><a href="#6-5-配置ssh无密码登录" class="headerlink" title="6.5　配置ssh无密码登录"></a>6.5　配置ssh无密码登录</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 ~]# ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509104000782-1509822811.png" alt="img"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 ~]# cd .ssh/</span><br><span class="line">[root@hadoop1 .ssh]# cat id_rsa.pub &gt;&gt; authorized_keys</span><br></pre></td></tr></table></figure><p>输入完后，这时再输入命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 .ssh]# ssh localhost</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509104156362-385616941.png" alt="img"></p><h2 id="七、上传软件到容器里面"><a href="#七、上传软件到容器里面" class="headerlink" title="七、上传软件到容器里面"></a>七、上传软件到容器里面</h2><p>将JDK上传到Linux系统，，然后将其移动到/home/docker/build文件夹下面，注意：这里需要使用root用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata docker]# mv jdk-8u73-linux-x64.tar.gz build/</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508204211692-1348459794.png" alt="img"></p><p>进入容器里面的/root/build文件夹下面进行查看</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 /]# cd /root/build/</span><br><span class="line">[root@hadoop1 build]# ls</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508210142987-1838746916.png" alt="img"></p><h4 id="5-2-3-安装JDK"><a href="#5-2-3-安装JDK" class="headerlink" title="5.2.3　安装JDK"></a>5.2.3　安装JDK</h4><p>在容器/root下面建一个apps文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 ~]# mkdir apps</span><br></pre></td></tr></table></figure><p>解压JDK的安装包到apps文件夹下面</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 build]# tar -zxvf jdk-8u73-linux-x64.tar.gz -C /root/apps/</span><br></pre></td></tr></table></figure><p>修改环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 ~]# vi .bashrc</span><br><span class="line">#JAVA</span><br><span class="line">export JAVA_HOME=/root/apps/jdk1.8.0_73</span><br><span class="line">export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib </span><br><span class="line">export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH:$HOME/bin</span><br></pre></td></tr></table></figure><p>保存使其立即生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 ~]# source .bashrc</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180508211002462-8312294.png" alt="img"></p><h2 id="八、保存镜像"><a href="#八、保存镜像" class="headerlink" title="八、保存镜像"></a>八、保存镜像</h2><p>基于已有的docker容器，做一新的dokcer image.</p><p>$ docker commit <container_id> <image_name></image_name></container_id></p><p>另开一个窗口</p><p>举例：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180509113403826-1693220830.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mybatis-generator</title>
      <link href="/mybatis-generator.html"/>
      <url>/mybatis-generator.html</url>
      
        <content type="html"><![CDATA[<p>** mybatis-generator：** &lt;Excerpt in index | 首页摘要&gt;<br>mybatis反向生成器，根据数据库表，自动创建pojo，mapper以及mybatis配置文件，能极大的提高开发效率。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="插件介绍"><a href="#插件介绍" class="headerlink" title="插件介绍"></a>插件介绍</h2><p>本插件fork自<a href="http://link" target="_blank" rel="noopener">mybatis-generator-gui</a>,在此基础上加了批量生成表。</p><h2 id="插件特性"><a href="#插件特性" class="headerlink" title="插件特性"></a>插件特性</h2><ol><li>保存数据库配置</li><li>根据表生成pojo，mapper以及mybatis配置文件</li><li>批量生成</li><li>其它功能（待开发）</li></ol><h2 id="插件使用"><a href="#插件使用" class="headerlink" title="插件使用"></a>插件使用</h2><h3 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h3><p>本工具由于使用了Java 8的众多特性，所以要求JDK <strong>1.8.0.60</strong>以上版本，对于JDK版本还没有升级的童鞋表示歉意。</p><h3 id="启动本软件"><a href="#启动本软件" class="headerlink" title="启动本软件"></a>启动本软件</h3><ul><li>方法一: 自助构建</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/maochunguang/mybatis-generator-gui</span><br><span class="line"><span class="built_in">cd</span> mybatis-generator-gui</span><br><span class="line">mvn jfx:jar</span><br><span class="line"><span class="built_in">cd</span> target/jfx/app/</span><br><span class="line">java -jar mybatis-generator-gui.jar</span><br></pre></td></tr></table></figure><ul><li>方法二: IDE中运行Eclipse or IntelliJ IDEA中启动, 找到<code>com.zzg.mybatis.generator.MainUI</code>类并运行就可以了</li></ul><h3 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h3><p>更多详细文档请参考本库的Wiki</p><ul><li><a href="https://github.com/maochunguang/mybatis-generator-gui/wiki" target="_blank" rel="noopener">Usage</a></li></ul><h2 id="截图参考"><a href="#截图参考" class="headerlink" title="截图参考"></a>截图参考</h2><p><img src="http://o7kalf5h3.bkt.clouddn.com/mybatis.png" alt="MainUI"></p>]]></content>
      
      
      <categories>
          
          <category> 开发工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>突破算法第11天-红黑树</title>
      <link href="/suanfa-11.html"/>
      <url>/suanfa-11.html</url>
      
        <content type="html"><![CDATA[<p>** 突破算法第11天-红黑树：** &lt;Excerpt in index | 首页摘要&gt;<br>红黑树</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="红黑树"><a href="#红黑树" class="headerlink" title="红黑树"></a>红黑树</h2><p>本文的主要内容：</p><ol><li>红黑树的基本概念以及最重要的 5 点规则。</li><li>红黑树的左旋转、右旋转、重新着色的原理与 Java 实现；</li><li>红黑树的增加结点、删除结点过程解析；</li></ol><h2 id="红黑树的基本概念与数据结构表示"><a href="#红黑树的基本概念与数据结构表示" class="headerlink" title="红黑树的基本概念与数据结构表示"></a>红黑树的基本概念与数据结构表示</h2><p>首先红黑树来个定义：</p><blockquote><p>红黑树定义：红黑树又称红 - 黑二叉树，它首先是一颗二叉树，它具体二叉树所有的特性。同时红黑树更是一颗自平衡的排序二叉树 (平衡二叉树的一种实现方式)。</p></blockquote><p>我们知道一颗基本的二叉排序树他们都需要满足一个基本性质：即树中的任何节点的值大于它的左子节点，且小于它的右子节点。</p><p>按照这个基本性质使得树的检索效率大大提高。我们知道在生成二叉排序树的过程是非常容易失衡的，最坏的情况就是一边倒（只有右 / 左子树），这样势必会导致二叉树的检索效率大大降低（O(n)），所以为了维持二叉排序树的平衡，大牛们提出了各种平衡二叉树的实现算法，如：AVL，SBT，伸展树，TREAP ，红黑树等等。</p><blockquote><p>平衡二叉树必须具备如下特性：它是一棵空树或它的左右两个子树的高度差的绝对值不超过 1，并且左右两个子树都是一棵平衡二叉树。也就是说该二叉树的任何一个子节点，其左右子树的高度都相近。下面给出平衡二叉树的几个示意图：</p></blockquote><p><img src="http://img.blog.csdn.net/20170110134212154?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="红黑树"></p><p>红黑树顾名思义就是结点是红色或者是黑色的平衡二叉树，它通过颜色的约束来维持着二叉树的平衡。对于一棵有效的红黑树而言我们必须增加如下规则，这也是红黑树最重要的 5 点规则：</p><ol><li>每个结点都只能是红色或者黑色中的一种。</li><li>根结点是黑色的。</li><li>每个叶结点（NIL 节点，空节点）是黑色的。</li><li>如果一个结点是红的，则它两个子节点都是黑的。也就是说在一条路径上不能出现相邻的两个红色结点。</li><li>从任一结点到其每个叶子的所有路径都包含相同数目的黑色结点。</li></ol><p>这些约束强制了红黑树的关键性质: 从根到叶子最长的可能路径不多于最短的可能路径的两倍长。结果是这棵树大致上是平衡的。因为操作比如插入、删除和查找某个值的最坏情况时间都要求与树的高度成比例，这个在高度上的理论上限允许红黑树在最坏情况下都是高效的，而不同于普通的二叉查找树。所以红黑树它是复杂而高效的，其检索效率 O(lg n)。下图为一颗典型的红黑二叉树：<br><img src="http://img.blog.csdn.net/20170110134903553?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><p>上面关于红黑树的概念基本已经说得很清楚了，下面给出红黑树的结点用 Java 表示数据结构：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">boolean</span> RED = <span class="keyword">true</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">boolean</span> BLACK = <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">private</span> Node root;<span class="comment">//二叉查找树的根节点</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//结点数据结构</span></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Key key;<span class="comment">//键</span></span><br><span class="line">    <span class="keyword">private</span> Value value;<span class="comment">//值</span></span><br><span class="line">    <span class="keyword">private</span> Node left, right;<span class="comment">//指向子树的链接:左子树和右子树.</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> N;<span class="comment">//以该节点为根的子树中的结点总数</span></span><br><span class="line">    <span class="keyword">boolean</span> color;<span class="comment">//由其父结点指向它的链接的颜色也就是结点颜色.</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">(Key key, Value value, <span class="keyword">int</span> N, <span class="keyword">boolean</span> color)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.key = key;</span><br><span class="line">        <span class="keyword">this</span>.value = value;</span><br><span class="line">        <span class="keyword">this</span>.N = N;</span><br><span class="line">        <span class="keyword">this</span>.color = color;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取整个二叉查找树的大小</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> size(root);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取某一个结点为根结点的二叉查找树的大小</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> x</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">(Node x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(x == <span class="keyword">null</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> x.N;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isRed</span><span class="params">(Node x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(x == <span class="keyword">null</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> x.color == RED;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="红黑树的三个基本操作"><a href="#红黑树的三个基本操作" class="headerlink" title="红黑树的三个基本操作"></a>红黑树的三个基本操作</h2><p>红黑树在插入，删除过程中可能会破坏原本的平衡条件导致不满足红黑树的性质，这时候一般情况下要通过左旋、右旋和重新着色这个三个操作来使红黑树重新满足平衡化条件。</p><h2 id="旋转"><a href="#旋转" class="headerlink" title="旋转"></a>旋转</h2><p>旋转分为左旋和右旋。在我们实现某些操作中可能会出现红色右链接或则两个连续的红链接，这时候就要通过旋转修复。</p><p>通常左旋操作用于将一个向右倾斜的红色链接 (这个红色链接链连接的两个结点均是红色结点) 旋转为向左链接。对比操作前后，可以看出，该操作实际上是将红线链接的两个结点中的一个较大的结点移动到根结点上。</p><p>左旋的示意图如下：<br><img src="http://img.blog.csdn.net/20170110141248765?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><p><img src="http://img.blog.csdn.net/20170110141309245?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><p>左旋的 Java 实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 左旋转</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> h</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Node <span class="title">rotateLeft</span><span class="params">(Node h)</span></span>&#123;</span><br><span class="line">    Node x = h.right;</span><br><span class="line">    <span class="comment">//把x的左结点赋值给h的右结点</span></span><br><span class="line">    h.right = x.left;</span><br><span class="line">    <span class="comment">//把h赋值给x的左结点</span></span><br><span class="line">    x.left = h;</span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    x.color = h.color;</span><br><span class="line">    h.color = RED;</span><br><span class="line">    x.N = h.N;</span><br><span class="line">    h.N = <span class="number">1</span>+ size(h.left) + size(h.right);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>左旋的动画效果如下：<br><img src="http://img.blog.csdn.net/20170110142027660?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><p>右旋其实就是左旋的逆操作：<br><img src="http://img.blog.csdn.net/20170110142230957?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br><img src="http://img.blog.csdn.net/20170110142252648?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>右旋的代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 右旋转</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> h</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Node <span class="title">rotateRight</span><span class="params">(Node h)</span></span>&#123;</span><br><span class="line">    Node x = h.left;</span><br><span class="line">    h.left = x.right;</span><br><span class="line">    x.right = h;</span><br><span class="line"></span><br><span class="line">    x.color = h.color;</span><br><span class="line">    h.color = RED;</span><br><span class="line">    x.N = h.N;</span><br><span class="line">    h.N = <span class="number">1</span>+ size(h.left) + size(h.right);</span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>右旋的动态示意图：<br><img src="http://img.blog.csdn.net/20170110142410322?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><h2 id="颜色反转"><a href="#颜色反转" class="headerlink" title="颜色反转"></a><a></a>颜色反转</h2><p>当出现一个临时的 4-node 的时候，即一个节点的两个子节点均为红色，如下图：<br><img src="http://img.blog.csdn.net/20170110143015321?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>我们需要将 E 提升至父节点，操作方法很简单，就是把 E 对子节点的连线设置为黑色，自己的颜色设置为红色。颜色反转之后颜色如下：<br><img src="http://img.blog.csdn.net/20170110143225712?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>实现代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 颜色转换</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> h</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">flipColors</span><span class="params">(Node h)</span></span>&#123;</span><br><span class="line">    h.color = RED;<span class="comment">//父结点颜色变红</span></span><br><span class="line">    h.left.color = BLACK;<span class="comment">//子结点颜色变黑</span></span><br><span class="line">    h.right.color = BLACK;<span class="comment">//子结点颜色变黑</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>注意：以上的旋转和颜色反转操作都是针对单一结点的，反转或则颜色反转操作之后可能引起其父结点又不满足平衡性质。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>突破算法第10天-二叉树</title>
      <link href="/suanfa-10.html"/>
      <url>/suanfa-10.html</url>
      
        <content type="html"><![CDATA[<p>** 突破算法第10天-二叉树：** &lt;Excerpt in index | 首页摘要&gt;<br>用java实现算法求出二叉树的高度</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="树"><a href="#树" class="headerlink" title="树"></a>树</h2><ul><li>先序遍历：先访问根结点，然后左节点，最后右节点</li><li>中序遍历：先访问左结点，然后根节点，最后右节点</li><li>后续遍历：先访问左结点，然后右节点，最后根节点</li></ul><h2 id="java实现"><a href="#java实现" class="headerlink" title="java实现"></a>java实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span> </span>&#123;</span><br><span class="line">    TreeNode left;</span><br><span class="line">    TreeNode right;</span><br><span class="line">    <span class="keyword">int</span> val;</span><br><span class="line"></span><br><span class="line">    TreeNode(<span class="keyword">int</span> val) &#123;</span><br><span class="line">        <span class="keyword">this</span>.val = val;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        TreeNode root = <span class="keyword">new</span> TreeNode(<span class="number">1</span>);</span><br><span class="line">        TreeNode left1 = <span class="keyword">new</span> TreeNode(<span class="number">2</span>);</span><br><span class="line">        TreeNode left2 = <span class="keyword">new</span> TreeNode(<span class="number">3</span>);</span><br><span class="line">        TreeNode right1 = <span class="keyword">new</span> TreeNode(<span class="number">4</span>);</span><br><span class="line">        <span class="comment">//创建一棵树</span></span><br><span class="line">        root.left = left1;</span><br><span class="line">        left1.right = left2;</span><br><span class="line">        root.right = right1;</span><br><span class="line">        scanNodes(root);</span><br><span class="line">        System.out.println(<span class="string">"树的深度是："</span> + getDepth(root));</span><br><span class="line">        System.out.println(<span class="string">"非递归深度："</span> + findDeep2(root));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 递归返回二叉树的深度</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getDepth</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> left = getDepth(root.left);</span><br><span class="line">        <span class="keyword">int</span> right = getDepth(root.right);</span><br><span class="line">        <span class="keyword">return</span> left &gt; right ? left + <span class="number">1</span> : right + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">scanNodes</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"><span class="comment">//        System.out.println(root.val); //先序遍历</span></span><br><span class="line">        scanNodes(root.left);</span><br><span class="line"><span class="comment">//        System.out.println(root.val); //中序遍历</span></span><br><span class="line">        scanNodes(root.right);</span><br><span class="line">        System.out.println(root.val); <span class="comment">// 后序遍历</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 非递归求深度</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">findDeep2</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        TreeNode current = <span class="keyword">null</span>;</span><br><span class="line">        LinkedList&lt;TreeNode&gt; queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">        queue.offer(root);</span><br><span class="line">        <span class="keyword">int</span> cur, next;</span><br><span class="line">        <span class="keyword">int</span> level = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (!queue.isEmpty()) &#123;</span><br><span class="line">            cur = <span class="number">0</span>;</span><br><span class="line">            <span class="comment">//当遍历完当前层以后，队列里元素全是下一层的元素，队列的长度是这一层的节点的个数</span></span><br><span class="line">            next = queue.size();</span><br><span class="line">            <span class="keyword">while</span> (cur &lt; next) &#123;</span><br><span class="line">                current = queue.poll();</span><br><span class="line">                cur++;</span><br><span class="line">                <span class="comment">//把当前节点的左右节点入队（如果存在的话）  </span></span><br><span class="line">                <span class="keyword">if</span> (current.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    queue.offer(current.left);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (current.right != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    queue.offer(current.right);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            level++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> level;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="树的变种"><a href="#树的变种" class="headerlink" title="树的变种"></a>树的变种</h2><p>二叉查找树，平衡二叉查找树，红黑树，b树<br>红黑树和平衡二叉树（AVL树）类似，都是在进行插入和删除操作时通过特定操作保持二叉查找树的平衡，从而获得较高的查找性能。红黑树和AVL树的区别在于它使用颜色来标识结点的高度，它所追求的是局部平衡而不是AVL树中的非常严格的平衡。<br>由于二叉树的效率和深度息息相关，于是出现了多路的B树，B+树等等。b树是叶子为n的平衡树。</p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>突破算法第九天-排序算法比较</title>
      <link href="/suanfa-9.html"/>
      <url>/suanfa-9.html</url>
      
        <content type="html"><![CDATA[<p>** 突破算法第九天-排序算法比较：** &lt;Excerpt in index | 首页摘要&gt;<br>排序算法个有千秋，有的性能高，有的性能很低。这就要求我们对常用的排序算法要全面了解，不要用错了算法，导致性能问题。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="排序算法性能比较·"><a href="#排序算法性能比较·" class="headerlink" title="排序算法性能比较·"></a>排序算法性能比较·</h2><p>借一张网路上的比较图。特别直观。<br><img src="http://o7kalf5h3.bkt.clouddn.com/sortcom.jpg" alt="算法比较"></p><h2 id="排序算法总结"><a href="#排序算法总结" class="headerlink" title="排序算法总结"></a>排序算法总结</h2><p>个人看法：</p><ul><li>一般的情况还是以快速排序为主，</li><li>对于多个有序的数组合并的情况使用归并排序</li><li>性能要求快，空间足够，待排序的元素都要在一定的范围内使用桶排序</li></ul>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>突破算法第八天-桶排序</title>
      <link href="/suanfa-8.html"/>
      <url>/suanfa-8.html</url>
      
        <content type="html"><![CDATA[<p>** 突破算法第八天-桶排序：** &lt;Excerpt in index | 首页摘要&gt;<br>桶排序是个神奇的排序，在某些情况下可以达到O(N)的复杂度，快的离谱。但是桶排序是利用空间换时间，在空间充足的情况下，可以用桶排序进行高效的排序。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="桶排序的基本原理"><a href="#桶排序的基本原理" class="headerlink" title="桶排序的基本原理"></a>桶排序的基本原理</h2><p>将阵列分到有限数量的桶子里。每个桶子再个别排序（有可能再使用别的排序算法或是以递回方式继续使用桶排序进行排序）。当要被排序的阵列内的数值是均匀分配的时候，桶排序使用线性时间（Θ（n））。但桶排序并不是 比较排序，他不受到 O(nlogn) 下限的影响， 简单来说，就是把数据分组，放在一个个的桶中，然后对每个桶里面的在进行排序</p><h2 id="桶排序的java实现"><a href="#桶排序的java实现" class="headerlink" title="桶排序的java实现"></a>桶排序的java实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">bucketSort1</span><span class="params">(<span class="keyword">int</span>[] arr)</span></span>&#123;</span><br><span class="line">        <span class="comment">//分桶，这里采用映射函数f(x)=x/10。</span></span><br><span class="line">        <span class="keyword">int</span> bucketCount =<span class="number">10</span>;</span><br><span class="line">        Integer[][] bucket = <span class="keyword">new</span> Integer[bucketCount][arr.length];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;arr.length; i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> quotient = arr[i]/<span class="number">10</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;arr.length; j++)&#123;</span><br><span class="line">                <span class="keyword">if</span> (bucket[quotient][j]==<span class="keyword">null</span>)&#123;</span><br><span class="line">                    bucket[quotient][j]=arr[i];</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//小桶排序</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;bucket.length; i++)&#123;</span><br><span class="line">            <span class="comment">//insertion sort</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;bucket[i].length; ++j)&#123;</span><br><span class="line">                <span class="keyword">if</span>(bucket[i][j]==<span class="keyword">null</span>)&#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">int</span> value = bucket[i][j];</span><br><span class="line">                <span class="keyword">int</span> position=j;</span><br><span class="line">                <span class="keyword">while</span> (position&gt;<span class="number">0</span> &amp;&amp; bucket[i][position-<span class="number">1</span>]&gt;value)&#123;</span><br><span class="line">                    bucket[i][position] = bucket[i][position-<span class="number">1</span>];</span><br><span class="line">                    position--;</span><br><span class="line">                &#125;</span><br><span class="line">                bucket[i][position] = value;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//输出</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>, index=<span class="number">0</span>; i&lt;bucket.length; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;bucket[i].length; j++)&#123;</span><br><span class="line">                <span class="keyword">if</span> (bucket[i][j]!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    arr[index] = bucket[i][j];</span><br><span class="line">                    index++;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span>&#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="算法复杂度"><a href="#算法复杂度" class="headerlink" title="算法复杂度"></a>算法复杂度</h2><p>前面说的几大排序算法 ，大部分时间复杂度都是O（n2），也有部分排序算法时间复杂度是O(nlogn)。而桶式排序却能实现O（n）的时间复杂度。<br>但桶排序的缺点是：</p><ol><li>首先是空间复杂度比较高，需要的额外开销大。排序有两个数组的空间开销，一个存放待排序数组，一个就是所谓的桶，比如待排序值是从0到m-1，那就需要m个桶，这个桶数组就要至少m个空间。</li><li>其次待排序的元素都要在一定的范围内，限制较多。</li></ol>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>突破算法第七天-堆排序</title>
      <link href="/suanfa-7.html"/>
      <url>/suanfa-7.html</url>
      
        <content type="html"><![CDATA[<p>** 突破算法第七天-堆排序：** &lt;Excerpt in index | 首页摘要&gt;<br>堆排序是利用二叉树的原理实现的一种排序，难点在于要构建堆,构建堆一般可以采用下沉或者上浮的算法进行。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="堆排序的基本原理"><a href="#堆排序的基本原理" class="headerlink" title="堆排序的基本原理"></a>堆排序的基本原理</h2><p>初始时把要排序的n个数的序列看作是一棵顺序存储的二叉树（一维数组存储二叉树），调整它们的存储序，使之成为一个堆，将堆顶元素输出，得到n 个元素中最小(或最大)的元素，这时堆的根节点的数最小（或者最大）。然后对前面(n-1)个元素重新调整使之成为堆，输出堆顶元素，得到n 个元素中次小(或次大)的元素。依此类推，直到只有两个节点的堆，并对它们作交换，最后得到有n个节点的有序序列。称这个过程为堆排序。<br>因此，实现堆排序需解决两个问题：</p><ol><li>如何将n 个待排序的数建成堆；</li><li>输出堆顶元素后，怎样调整剩余n-1 个元素，使其成为一个新堆。</li></ol><h2 id="堆排序java实现"><a href="#堆排序java实现" class="headerlink" title="堆排序java实现"></a>堆排序java实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sort</span><span class="params">(<span class="keyword">int</span>[] a)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = a.length;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> k = n / <span class="number">2</span>; k &gt;= <span class="number">1</span>; k--)</span><br><span class="line">            sink(a, k, n);</span><br><span class="line">        <span class="keyword">while</span> (n &gt; <span class="number">1</span>) &#123;</span><br><span class="line">            swap(a, <span class="number">1</span>, n--);</span><br><span class="line">            sink(a, <span class="number">1</span>, n);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sink</span><span class="params">(<span class="keyword">int</span>[] a, <span class="keyword">int</span> k, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">2</span> * k &lt;= n) &#123;</span><br><span class="line">        <span class="keyword">int</span> j = <span class="number">2</span> * k;</span><br><span class="line">        <span class="keyword">if</span> (j &lt; n &amp;&amp; a[j - <span class="number">1</span>] &lt; a[j + <span class="number">1</span> - <span class="number">1</span>]) j++;</span><br><span class="line">        <span class="keyword">if</span> (a[k - <span class="number">1</span>] &gt;= a[j - <span class="number">1</span>]) <span class="keyword">break</span>;</span><br><span class="line">        swap(a, k, j);</span><br><span class="line">        k = j;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>[] a, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> swap = a[i - <span class="number">1</span>];</span><br><span class="line">    a[i - <span class="number">1</span>] = a[j - <span class="number">1</span>];</span><br><span class="line">    a[j - <span class="number">1</span>] = swap;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span>[] arr = &#123;<span class="number">49</span>, <span class="number">38</span>, <span class="number">65</span>, <span class="number">97</span>, <span class="number">76</span>, <span class="number">13</span>, <span class="number">27</span>, <span class="number">4</span>, <span class="number">78</span>, <span class="number">34</span>, <span class="number">12</span>, <span class="number">64</span>, <span class="number">1</span>, <span class="number">8</span>&#125;;</span><br><span class="line">    sort(arr);</span><br><span class="line">    System.out.println(<span class="string">"排序之后："</span>);</span><br><span class="line">    System.out.println(Arrays.toString(arr));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="算法复杂度"><a href="#算法复杂度" class="headerlink" title="算法复杂度"></a>算法复杂度</h2><p>堆排序的平均时间复杂度为Ο(nlogn)</p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>突破算法第六天-冒泡排序</title>
      <link href="/suanfa-6.html"/>
      <url>/suanfa-6.html</url>
      
        <content type="html"><![CDATA[<p>** 突破算法第六天-冒泡排序：** &lt;Excerpt in index | 首页摘要&gt;<br>冒泡排序也非常简单，效率比较低。了解即可。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="冒泡排序的原理"><a href="#冒泡排序的原理" class="headerlink" title="冒泡排序的原理"></a>冒泡排序的原理</h2><p>在要排序的一组数中，对当前还未排好序的范围内的全部数，自上而下对相邻的两个数依次进行比较和调整，让较大的数往下沉，较小的往上冒。即：每当两相邻的数比较后发现它们的排序与排序要求相反时，就将它们互换。</p><p><img src="http://o7kalf5h3.bkt.clouddn.com/bubbleSort.jpg" alt="冒泡排序图"></p><h2 id="冒泡排序的java实现"><a href="#冒泡排序的java实现" class="headerlink" title="冒泡排序的java实现"></a>冒泡排序的java实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">bubbleSort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n - <span class="number">1</span>; ++i) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; n - i - <span class="number">1</span>; ++j) &#123;</span><br><span class="line">            <span class="keyword">if</span> (a[j] &gt; a[j + <span class="number">1</span>]) &#123;</span><br><span class="line">                <span class="keyword">int</span> tmp = a[j];</span><br><span class="line">                a[j] = a[j + <span class="number">1</span>];</span><br><span class="line">                a[j + <span class="number">1</span>] = tmp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="算法复杂度"><a href="#算法复杂度" class="headerlink" title="算法复杂度"></a>算法复杂度</h2><p>冒泡排序的复杂度为O(n^2)</p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>突破算法第五天-选择排序</title>
      <link href="/suanfa-5.html"/>
      <url>/suanfa-5.html</url>
      
        <content type="html"><![CDATA[<p>** 突破算法第五天-选择排序：** &lt;Excerpt in index | 首页摘要&gt;<br>选择排序很简单，属于交换排序算法。通过比较找到最大值或最小值，然后进行交换。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="选择排序的原理"><a href="#选择排序的原理" class="headerlink" title="选择排序的原理"></a>选择排序的原理</h2><p>首先找到数组中最小的元素，与数组第一个元素交换，然后在剩下的元素中选择最小的，与第二个元素交换，以此类推，直到排序完成。<br><img src="http://o7kalf5h3.bkt.clouddn.com/selectSort.jpg" alt="选择排序图"></p><h2 id="选择排序的java实现"><a href="#选择排序的java实现" class="headerlink" title="选择排序的java实现"></a>选择排序的java实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sort</span><span class="params">(<span class="keyword">int</span>[] a)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> len = a.length;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; len; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = i; j &gt; <span class="number">0</span> &amp;&amp; (a[j] &lt; a[j - <span class="number">1</span>]); j--) &#123;</span><br><span class="line">            swap(a, j, j - <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>[] a, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> swap = a[i];</span><br><span class="line">    a[i] = a[j];</span><br><span class="line">    a[j] = swap;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="算法复杂度"><a href="#算法复杂度" class="headerlink" title="算法复杂度"></a>算法复杂度</h2><p>选择排序的算法复杂度是O(n^2)</p><h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><ol><li>每次选择的时候把最大值和最小值都比较出来，双向进行交换排序</li></ol>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>突破算法第一天-归并排序</title>
      <link href="/suanfa-4.html"/>
      <url>/suanfa-4.html</url>
      
        <content type="html"><![CDATA[<p>** 突破算法第一天-归并排序：** &lt;Excerpt in index | 首页摘要&gt;<br>归并排序是利用分治思想进行排序的典型应用，特别是对几个基本有序的子序列合并时，效率最高。在实际应用中，分布式应用，分布式查询排序会比较多应用到归并排序。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="归并排序的原理"><a href="#归并排序的原理" class="headerlink" title="归并排序的原理"></a>归并排序的原理</h2><p>归并（Merge）排序法是将两个（或两个以上）有序表合并成一个新的有序表，即把待排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。<br>归并排序分为两种种，第一种是自底向上的归并。</p><p>第二种是自顶向下的归并。</p><h2 id="自底向上的归并排序java实现"><a href="#自底向上的归并排序java实现" class="headerlink" title="自底向上的归并排序java实现"></a>自底向上的归并排序java实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MergeSortBU</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="keyword">int</span>[] a, <span class="keyword">int</span>[] aux, <span class="keyword">int</span> lo, <span class="keyword">int</span> mid, <span class="keyword">int</span> hi)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 复制到aux[]</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> k = lo; k &lt;= hi; k++) &#123;</span><br><span class="line">            aux[k] = a[k];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 合并回 a[]</span></span><br><span class="line">        <span class="keyword">int</span> i = lo, j = mid + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> k = lo; k &lt;= hi; k++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (i &gt; mid) a[k] = aux[j++];</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (j &gt; hi) a[k] = aux[i++];</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (aux[j] &lt; aux[i]) a[k] = aux[j++];</span><br><span class="line">            <span class="keyword">else</span> a[k] = aux[i++];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">mergeSort</span><span class="params">(<span class="keyword">int</span>[] a)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = a.length;</span><br><span class="line">        <span class="keyword">int</span>[] aux = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> len = <span class="number">1</span>; len &lt; n; len *= <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> lo = <span class="number">0</span>; lo &lt; n - len; lo += len + len) &#123;</span><br><span class="line">                <span class="keyword">int</span> mid = lo + len - <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">int</span> hi = Math.min(lo + len + len - <span class="number">1</span>, n - <span class="number">1</span>);</span><br><span class="line">                merge(a, aux, lo, mid, hi);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = &#123;<span class="number">49</span>, <span class="number">38</span>, <span class="number">65</span>, <span class="number">97</span>, <span class="number">76</span>, <span class="number">13</span>, <span class="number">27</span>, <span class="number">4</span>, <span class="number">78</span>, <span class="number">34</span>, <span class="number">12</span>, <span class="number">64</span>, <span class="number">1</span>, <span class="number">8</span>&#125;;</span><br><span class="line">        mergeSort(arr);</span><br><span class="line">        System.out.println(<span class="string">"排序之后："</span>);</span><br><span class="line">        System.out.println(Arrays.toString(arr));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="自顶向下的归并排序java实现"><a href="#自顶向下的归并排序java实现" class="headerlink" title="自顶向下的归并排序java实现"></a>自顶向下的归并排序java实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sort</span><span class="params">(<span class="keyword">int</span>[] a, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (high &lt;= low) <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">int</span> mid = low + (high - low) / <span class="number">2</span>;</span><br><span class="line">        sort(a, low, mid);</span><br><span class="line">        sort(a, mid + <span class="number">1</span>, high);</span><br><span class="line">        merge(a, low, mid, high);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="keyword">int</span>[] a, <span class="keyword">int</span> lo, <span class="keyword">int</span> mid, <span class="keyword">int</span> hi)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 复制到aux[]</span></span><br><span class="line">        <span class="keyword">int</span>[] aux = <span class="keyword">new</span> <span class="keyword">int</span>[a.length];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> k = lo; k &lt;= hi; k++) &#123;</span><br><span class="line">            aux[k] = a[k];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 合并回 a[]</span></span><br><span class="line">        <span class="keyword">int</span> i = lo, j = mid + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> k = lo; k &lt;= hi; k++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (i &gt; mid) a[k] = aux[j++];</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (j &gt; hi) a[k] = aux[i++];</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (aux[j] &lt; aux[i]) a[k] = aux[j++];</span><br><span class="line">            <span class="keyword">else</span> a[k] = aux[i++];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="算法复杂度"><a href="#算法复杂度" class="headerlink" title="算法复杂度"></a>算法复杂度</h2><p>归并排序的算法复杂度是nlgn</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ol><li>几个基本有序的数组进行排序</li><li>部分有序的数组</li></ol>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>突破算法第三天-希尔排序</title>
      <link href="/suanfa-3.html"/>
      <url>/suanfa-3.html</url>
      
        <content type="html"><![CDATA[<p>** 突破算法第三天-希尔排序：** &lt;Excerpt in index | 首页摘要&gt;<br>希尔排序平常用的比较少，主要是基于插入排序的改进。但是希尔排序的性能很高，数组越大，性能优势越明显。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="希尔排序的基本原理"><a href="#希尔排序的基本原理" class="headerlink" title="希尔排序的基本原理"></a>希尔排序的基本原理</h2><p>基本思想：先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录“基本有序”时，再对全体记录进行依次直接插入排序。<br>操作方法：</p><ol><li>选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1； </li><li>按增量序列个数k，对序列进行k 趟排序； </li><li>每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。</li></ol><p><img src="http://o7kalf5h3.bkt.clouddn.com/shellSort.jpg" alt="希尔排序原理图"></p><h2 id="希尔排序java实现"><a href="#希尔排序java实现" class="headerlink" title="希尔排序java实现"></a>希尔排序java实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">shellSort</span><span class="params">(<span class="keyword">int</span>[] a)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = a.length;</span><br><span class="line">    <span class="keyword">int</span> h = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (h &lt; n/<span class="number">3</span>) h = <span class="number">3</span>*h + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (h &gt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="comment">// h-sort the array</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = h; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = i; j &gt;= h &amp;&amp; (a[j]&lt; a[j-h]); j -= h) &#123;</span><br><span class="line">                swap(a, j, j-h);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        h /= <span class="number">3</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>[] a, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> swap = a[i];</span><br><span class="line">    a[i] = a[j];</span><br><span class="line">    a[j] = swap;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="算法复杂度"><a href="#算法复杂度" class="headerlink" title="算法复杂度"></a>算法复杂度</h2><p>希尔排序时效分析很难，关键码的比较次数与记录移动次数依赖于增量因子序列d的选取，是一个不稳定排序算法</p><h2 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h2>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>突破算法第二天-插入排序</title>
      <link href="/suanfa-2.html"/>
      <url>/suanfa-2.html</url>
      
        <content type="html"><![CDATA[<p>** 突破算法第二天-插入排序：** &lt;Excerpt in index | 首页摘要&gt;<br>今天是突破算法第二天，插入排序，比较简单。效率比较低，但是思想很广泛，应用很广，是很多高级排序算法的一个子过程。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="插入排序的原理"><a href="#插入排序的原理" class="headerlink" title="插入排序的原理"></a>插入排序的原理</h2><p> 将一个记录插入到已排序好的有序表中，从而得到一个新，记录数增1的有序表。即：先将序列的第1个记录<br> 看成是一个有序的子序列，然后从第2个记录逐个进行插入，直至整个序列有序为止。<br> 要点：设立哨兵，作为临时存储和判断数组边界之用</p><p><img src="http://o7kalf5h3.bkt.clouddn.com/insert.jpg" alt="插入排序原理"></p><h2 id="插入排序java实现"><a href="#插入排序java实现" class="headerlink" title="插入排序java实现"></a>插入排序java实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">insertSort</span><span class="params">(<span class="keyword">int</span>[] a, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">           <span class="keyword">for</span> (<span class="keyword">int</span> j = i; j &gt; <span class="number">0</span> &amp;&amp; (a[j]&lt;a[j-<span class="number">1</span>]); j--) &#123;</span><br><span class="line">               swap(a, j, j-<span class="number">1</span>);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>[] a, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> swap = a[i];</span><br><span class="line">       a[i] = a[j];</span><br><span class="line">       a[j] = swap;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="算法复杂度"><a href="#算法复杂度" class="headerlink" title="算法复杂度"></a>算法复杂度</h2><p>插入排序的复杂度为O（n^2）</p><h2 id="改进方法"><a href="#改进方法" class="headerlink" title="改进方法"></a>改进方法</h2><p>希尔排序，其他的插入排序有二分插入排序，2-路插入排序。</p><h2 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h2><p>插入排序比较适合部分有序的数组（以下四种数组）</p><ul><li>数组中每个元素距离它的最终位置都不远</li><li>一个有序的大数组接一个小数组</li><li>数组中只有几个位置不正确</li><li>数组比较小</li></ul>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>突破算法第一天-快速排序</title>
      <link href="/suanfa-1.html"/>
      <url>/suanfa-1.html</url>
      
        <content type="html"><![CDATA[<p>** 突破算法第一天-快速排序：** &lt;Excerpt in index | 首页摘要&gt;<br>30天突破算法是我给自己定的一个学习计划，希望在这30天，每天都能完成计划。第一天学习最重要的快速排序。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="30天突破算法"><a href="#30天突破算法" class="headerlink" title="30天突破算法"></a>30天突破算法</h2><p>算法种类不计其数，说30天突破只是给自己定的学习计划。目的是通过30天的记录熟悉常见的算法，提高自己的算法能力。对以后的工作来说也是打下夯实的基础。</p><h2 id="快速排序的原理"><a href="#快速排序的原理" class="headerlink" title="快速排序的原理"></a>快速排序的原理</h2><p>快速排序也是分治法思想的一种实现，他的思路是使数组中的每个元素与基准值（Pivot，通常是数组的首个值，A[0]）比较，数组中比基准值小的放在基准值的左边，形成左部；大的放在右边，形成右部；接下来将左部和右部分别递归地执行上面的过程：选基准值，小的放在左边，大的放在右边。重复此过程，直到排序结束。步骤如下：</p><ul><li>1.找基准值，设Pivot = a[0] </li><li>2.分区（Partition）：比基准值小的放左边，大的放右边，基准值(Pivot)放左部与右部的之间。</li><li>3.进行左部（a[0] - a[pivot-1]）的递归，以及右部（a[pivot+1] - a[n-1]）的递归，重复上述步骤。</li></ul><p><img src="http://o7kalf5h3.bkt.clouddn.com/quicksort.jpg" alt="快速排序原理图"></p><h2 id="快速排序java实现（递归版）"><a href="#快速排序java实现（递归版）" class="headerlink" title="快速排序java实现（递归版）"></a>快速排序java实现（递归版）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">QuickSort</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] a=&#123;<span class="number">49</span>,<span class="number">38</span>,<span class="number">65</span>,<span class="number">97</span>,<span class="number">76</span>,<span class="number">13</span>,<span class="number">27</span>,<span class="number">49</span>,<span class="number">78</span>,<span class="number">34</span>,<span class="number">12</span>,<span class="number">64</span>,<span class="number">1</span>,<span class="number">8</span>&#125;;</span><br><span class="line">        System.out.println(<span class="string">"排序之前："</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.length; i++) &#123;</span><br><span class="line">            System.out.print(a[i]+<span class="string">" "</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//快速排序</span></span><br><span class="line">        quick(a);</span><br><span class="line">        System.out.println();</span><br><span class="line">        System.out.println(<span class="string">"排序之后："</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.length; i++) &#123;</span><br><span class="line">            System.out.print(a[i]+<span class="string">" "</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">quick</span><span class="params">(<span class="keyword">int</span>[] a)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(a.length&gt;<span class="number">0</span>)&#123;</span><br><span class="line">            quickSort(a,<span class="number">0</span>,a.length-<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(<span class="keyword">int</span>[] a, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(low&lt;high)&#123; <span class="comment">//如果不加这个判断递归会无法退出导致堆栈溢出异常</span></span><br><span class="line">            <span class="keyword">int</span> middle = getMiddle(a,low,high);</span><br><span class="line">            quickSort(a, <span class="number">0</span>, middle-<span class="number">1</span>);</span><br><span class="line">            quickSort(a, middle+<span class="number">1</span>, high);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getMiddle</span><span class="params">(<span class="keyword">int</span>[] a, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> temp = a[low];<span class="comment">//基准元素</span></span><br><span class="line">        <span class="keyword">while</span>(low&lt;high)&#123;</span><br><span class="line">            <span class="comment">//找到比基准元素小的元素位置</span></span><br><span class="line">            <span class="keyword">while</span>(low&lt;high &amp;&amp; a[high]&gt;=temp)&#123;</span><br><span class="line">                high--;</span><br><span class="line">            &#125;</span><br><span class="line">            a[low] = a[high]; </span><br><span class="line">            <span class="keyword">while</span>(low&lt;high &amp;&amp; a[low]&lt;=temp)&#123;</span><br><span class="line">                low++;</span><br><span class="line">            &#125;</span><br><span class="line">            a[high] = a[low];</span><br><span class="line">        &#125;</span><br><span class="line">        a[low] = temp;</span><br><span class="line">        <span class="keyword">return</span> low;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="快速排序三向切分法（改进的实现）"><a href="#快速排序三向切分法（改进的实现）" class="headerlink" title="快速排序三向切分法（改进的实现）"></a>快速排序三向切分法（改进的实现）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">quick3Sort</span><span class="params">(<span class="keyword">int</span>[] a, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (low &gt;= high) <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">int</span> lt = low, gt = high;</span><br><span class="line">    <span class="keyword">int</span> temp = a[low];</span><br><span class="line">    <span class="keyword">int</span> i = low;</span><br><span class="line">    <span class="keyword">while</span> (i &lt;= gt) &#123;</span><br><span class="line">        <span class="keyword">if</span> (a[i] &lt; temp) swap(a, lt++, i++);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (a[i] &gt; temp) swap(a, i, gt--);</span><br><span class="line">        <span class="keyword">else</span> i++;</span><br><span class="line">    &#125;</span><br><span class="line">    quick3Sort(a, low, lt - <span class="number">1</span>);</span><br><span class="line">    quick3Sort(a, gt + <span class="number">1</span>, high);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>[] a, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> swap = a[i];</span><br><span class="line">    a[i] = a[j];</span><br><span class="line">    a[j] = swap;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="快速排序的复杂度"><a href="#快速排序的复杂度" class="headerlink" title="快速排序的复杂度"></a>快速排序的复杂度</h2><p>时间复杂度 nlogn,排序方法中平均性能最好的。但若初始序列按关键码有序或基本有序时，快排序反而蜕化为冒泡排序。快速排序是一个不稳定的排序方法。</p><h2 id="改进方法"><a href="#改进方法" class="headerlink" title="改进方法"></a>改进方法</h2><ol><li>当数组比较小的时候，快速排序比插入排序慢，这个时候用插入排序替换比较好。</li><li>通常以“三者取中法”来选取基准记录，即将排序区间的两个端点与中点三个记录关键码居中的调整为支点记录</li></ol><h2 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h2><ul><li>普通的无序集合排序，使用快速排序。 </li><li>包含很多重复元素的集合排序，使用三向切分的快速排序。</li><li>基本有序的集合使用归并排序。</li></ul>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>学习计划-30天突破算法</title>
      <link href="/study-plan.html"/>
      <url>/study-plan.html</url>
      
        <content type="html"><![CDATA[<p>** 学习计划-30天突破算法：** &lt;Excerpt in index | 首页摘要&gt;<br>作为一个非专业出身的程序员，一直对算法的学习赶紧断断续续，终于下定决心对算法做一次详细总结。30天时间把程序员常用算法逐一突破。这次计划更是对自己的一次挑战，希望自己能坚持到最后！</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="学习排序算法的意义"><a href="#学习排序算法的意义" class="headerlink" title="学习排序算法的意义"></a>学习排序算法的意义</h2><ol><li>学会比较算法的性能的方法</li><li>相关的排序能解决类似的问题</li><li>排序算法很多时候是解决问题的第一步</li></ol><h3 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h3><ol><li>快速排序</li><li>插入排序</li><li>希尔排序</li><li>归并排序</li><li>选择排序</li><li>冒泡排序</li><li>堆排序</li><li>桶排序</li><li>排序算法比较</li></ol><h3 id="树"><a href="#树" class="headerlink" title="树"></a>树</h3><ol><li>二叉树高度和二叉树的遍历</li><li>红黑树</li><li>b树</li></ol><h3 id="查找算法"><a href="#查找算法" class="headerlink" title="查找算法"></a>查找算法</h3><ol><li>二分查找</li><li>二叉查找树</li><li>平衡查找树</li><li>散列表</li></ol><h3 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h3><ol><li>递归（普通递归，尾递归）</li><li>动态规划</li><li>贪婪算法</li><li>分治法</li></ol><h3 id="图的算法"><a href="#图的算法" class="headerlink" title="图的算法"></a>图的算法</h3><ol><li>深度优先</li><li>广度优先</li><li>最小生成树</li><li>最短路径 </li></ol><h3 id="字符串算法"><a href="#字符串算法" class="headerlink" title="字符串算法"></a>字符串算法</h3><ol><li>字符串查找</li><li>单词查找树</li><li>子字符串查找</li></ol><h3 id="典型算法分析"><a href="#典型算法分析" class="headerlink" title="典型算法分析"></a>典型算法分析</h3><ol><li>拓扑排序</li><li>关键路径排序</li><li>遗传算法</li><li>RSA算法</li></ol><h2 id="英语技术文档阅读突破"><a href="#英语技术文档阅读突破" class="headerlink" title="英语技术文档阅读突破"></a>英语技术文档阅读突破</h2><ol><li>熟悉常用技术词汇</li><li>阅读常见的技术文档（官网文档看一遍）</li><li>记住常用的词汇</li><li>阅读英文技术书籍</li></ol>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用java将GBK工程转为uft8</title>
      <link href="/trandsferProject.html"/>
      <url>/trandsferProject.html</url>
      
        <content type="html"><![CDATA[<p>** 用java将GBK工程转为uft8：** &lt;Excerpt in index | 首页摘要&gt;<br>windows下的默认编码为GBK还有gb2312，如何把gbk的java工程转为utf8的呢，如果直接修改工程编码，其实里面的java文件中中文是会乱码的，写了个批量转换java工程的程序，消遣一下。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="为什么要转码？"><a href="#为什么要转码？" class="headerlink" title="为什么要转码？"></a>为什么要转码？</h2><p>有些老的项目，或者朋友的项目之前没注意在windows上不是utf8，而你有需要看注释或者什么，总不能一个文件一个文件的去改编码属性吧。</p><h2 id="本程序试用范围"><a href="#本程序试用范围" class="headerlink" title="本程序试用范围"></a>本程序试用范围</h2><p>gbk的代码，或者gb2312的工程均可以转换</p><h2 id="编码转换的思路"><a href="#编码转换的思路" class="headerlink" title="编码转换的思路"></a>编码转换的思路</h2><p>本来想做成一个通用的会自动检测编码，自动转换的程序。但是由于判断编码类型不准，所以做成了针对GBK的转换。</p><ol><li>制定gbk编码把文件流读进来，加载到内存，转为String类型的内容</li><li>将String内容转为utf8的String</li><li>将String内容写入文件</li></ol><h2 id="核心代码："><a href="#核心代码：" class="headerlink" title="核心代码："></a>核心代码：</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TransferProject</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">transferFile</span><span class="params">(String pathName, <span class="keyword">int</span> depth)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        File dirFile = <span class="keyword">new</span> File(pathName);</span><br><span class="line">        <span class="keyword">if</span> (!isValidFile(dirFile)) <span class="keyword">return</span>;</span><br><span class="line">        <span class="comment">//获取此目录下的所有文件名与目录名</span></span><br><span class="line">        String[] fileList = dirFile.list();</span><br><span class="line">        <span class="keyword">int</span> currentDepth = depth + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; fileList.length; i++) &#123;</span><br><span class="line">            String string = fileList[i];</span><br><span class="line">            File file = <span class="keyword">new</span> File(dirFile.getPath(), string);</span><br><span class="line">            String name = file.getName();</span><br><span class="line">            <span class="comment">//如果是一个目录，搜索深度depth++，输出目录名后，进行递归</span></span><br><span class="line">            <span class="keyword">if</span> (file.isDirectory()) &#123;</span><br><span class="line">                <span class="comment">//递归</span></span><br><span class="line">                transferFile(file.getCanonicalPath(), currentDepth);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (name.contains(<span class="string">".java"</span>) || name.contains(<span class="string">".properties"</span>) || name.contains(<span class="string">".xml"</span>)) &#123;</span><br><span class="line">                    readAndWrite(file);</span><br><span class="line">                    System.out.println(name + <span class="string">" has converted to utf8 "</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isValidFile</span><span class="params">(File dirFile)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (dirFile.exists()) &#123;</span><br><span class="line">            System.out.println(<span class="string">"file exist"</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (dirFile.isDirectory()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (dirFile.isFile()) &#123;</span><br><span class="line">                System.out.println(dirFile.getCanonicalFile());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">readAndWrite</span><span class="params">(File file)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String  content = FileUtils.readFileByEncode(file.getPath(), <span class="string">"GBK"</span>);</span><br><span class="line">        FileUtils.writeByBufferedReader(file.getPath(), <span class="keyword">new</span> String(content.getBytes(<span class="string">"UTF-8"</span>), <span class="string">"UTF-8"</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//程序入口，制定src的path</span></span><br><span class="line">        String path = <span class="string">"/Users/mac/Downloads/unit06_jdbc/src"</span>;</span><br><span class="line">        transferFile(path, <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FileUtils</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeByBufferedReader</span><span class="params">(String path, String content)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            File file = <span class="keyword">new</span> File(path);</span><br><span class="line">            file.delete();</span><br><span class="line">            <span class="keyword">if</span> (!file.exists()) &#123;</span><br><span class="line">                file.createNewFile();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            FileWriter fw = <span class="keyword">new</span> FileWriter(file, <span class="keyword">false</span>);</span><br><span class="line">            BufferedWriter bw = <span class="keyword">new</span> BufferedWriter(fw);</span><br><span class="line">            bw.write(content);</span><br><span class="line">            bw.flush();</span><br><span class="line">            bw.close();</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">readFileByEncode</span><span class="params">(String path, String chatSet)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        InputStream input = <span class="keyword">new</span> FileInputStream(path);</span><br><span class="line">        InputStreamReader in = <span class="keyword">new</span> InputStreamReader(input, chatSet);</span><br><span class="line">        BufferedReader reader = <span class="keyword">new</span> BufferedReader(in);</span><br><span class="line">        StringBuffer sb = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">        String line = reader.readLine();</span><br><span class="line">        <span class="keyword">while</span> (line != <span class="keyword">null</span>) &#123;</span><br><span class="line">            sb.append(line);</span><br><span class="line">            sb.append(<span class="string">"\r\n"</span>);</span><br><span class="line">            line = reader.readLine();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> sb.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>遇到类似的问题，都可以试着用代码来进行实现，给自己的编码带来一些新的乐趣，也增加自己的信心。</p>]]></content>
      
      
      <categories>
          
          <category> 编程语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阿拉伯数字转汉字写法</title>
      <link href="/num2Chinese.html"/>
      <url>/num2Chinese.html</url>
      
        <content type="html"><![CDATA[<p>** 阿拉伯数字转汉字写法：** &lt;Excerpt in index | 首页摘要&gt;<br>找工作时看到“某团”的题目，把一个int的数字转为汉字的读法，比如123，转成一百二十三，限时20分钟。如果二十分钟做不出来，简历就不要投了。说实话，20分钟能调通的人真的不多，感觉某团还是装逼成分太多！</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h2><p>用java实现，把int的数字转为汉字读音，比如123，转成一百二十三，10020转为一万零二十</p><h2 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h2><p>中文计数的特点，以万为小节，万以内的都是以“十百千”为权位单独计数，比如一千百，一千千都是非法的。<br>而“十百千”这样的权位可以与“万”，“亿”进行搭配，二十亿，五千万等等。</p><h2 id="中文数字的零"><a href="#中文数字的零" class="headerlink" title="中文数字的零"></a>中文数字的零</h2><p>中文的零的使用总结起来有三个规则，</p><ul><li>以10000为小节，结尾是0，不使用零，比如1020</li><li>以10000为小节，小节内两个非0数字之间需要零</li><li>小节的千位是0，若小节前无其他数字，不用零，否者用零</li></ul><h2 id="完整代码（参考算法的乐趣第四章）"><a href="#完整代码（参考算法的乐趣第四章）" class="headerlink" title="完整代码（参考算法的乐趣第四章）"></a>完整代码（参考算法的乐趣第四章）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NumberTransfer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> String[] chnNumChar = <span class="keyword">new</span> String[]&#123;<span class="string">"零"</span>, <span class="string">"一"</span>, <span class="string">"二"</span>, <span class="string">"三"</span>, <span class="string">"四"</span>, <span class="string">"五"</span>, <span class="string">"六"</span>, <span class="string">"七"</span>, <span class="string">"八"</span>, <span class="string">"九"</span>&#125;;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> String[] chnUnitSection = <span class="keyword">new</span> String[]&#123;<span class="string">""</span>, <span class="string">"万"</span>, <span class="string">"亿"</span>, <span class="string">"万亿"</span>&#125;;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> String[] chnUnitChar = <span class="keyword">new</span> String[]&#123;<span class="string">""</span>, <span class="string">"十"</span>, <span class="string">"百"</span>, <span class="string">"千"</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testNumberToChinese</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] nums = <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">304</span>, <span class="number">4006</span>, <span class="number">4000</span>, <span class="number">10003</span>, <span class="number">10030</span>, <span class="number">21010011</span>, <span class="number">101101101</span>&#125;;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">            System.out.println(numberToChinese(nums[i]));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">numberToChinese</span><span class="params">(<span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">        String strIns;</span><br><span class="line">        String chnStr = <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">int</span> unitPos = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">boolean</span> needZero = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (num == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"零"</span>;</span><br><span class="line">        <span class="keyword">while</span> (num &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            strIns = <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">int</span> section = num % <span class="number">10000</span>;</span><br><span class="line">            <span class="keyword">if</span> (needZero) &#123;</span><br><span class="line">                chnStr = chnNumChar[<span class="number">0</span>] + chnStr;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 添加节权（万，亿）</span></span><br><span class="line">            strIns += (section != <span class="number">0</span>) ? chnUnitSection[unitPos] : chnUnitSection[<span class="number">0</span>];</span><br><span class="line">            chnStr = strIns + chnStr;</span><br><span class="line">            <span class="comment">// 以万为单位，求万以内的权位</span></span><br><span class="line">            chnStr = sectionToChinese(section, chnStr);</span><br><span class="line">            needZero = (section &lt; <span class="number">1000</span>) &amp;&amp; (section &gt; <span class="number">0</span>);</span><br><span class="line">            num = num / <span class="number">10000</span>;</span><br><span class="line">            unitPos++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> chnStr;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> String <span class="title">sectionToChinese</span><span class="params">(<span class="keyword">int</span> section, String chnStr)</span> </span>&#123;</span><br><span class="line">        String strIns;</span><br><span class="line">        <span class="keyword">int</span> unitPos = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">boolean</span> zero = <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">while</span> (section &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">int</span> v = section % <span class="number">10</span>;</span><br><span class="line">            <span class="keyword">if</span> (v == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (section == <span class="number">0</span> || !zero) &#123;</span><br><span class="line">                    zero = <span class="keyword">true</span>;<span class="comment">// zero确保不会出现多个零</span></span><br><span class="line">                    chnStr = chnNumChar[v] + chnStr;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                zero = <span class="keyword">false</span>;</span><br><span class="line">                strIns = chnNumChar[v]; <span class="comment">// 此位置对应等中文数字</span></span><br><span class="line">                strIns += chnUnitChar[unitPos];<span class="comment">// 此位置对应的权位</span></span><br><span class="line">                chnStr = strIns + chnStr;</span><br><span class="line">            &#125;</span><br><span class="line">            unitPos++;</span><br><span class="line">            section = section / <span class="number">10</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> chnStr;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java面试大全自制版</title>
      <link href="/java-interview.html"/>
      <url>/java-interview.html</url>
      
        <content type="html"><![CDATA[<p>** java面试大全自制版：** &lt;Excerpt in index | 首页摘要&gt;<br>java语言知识点多而杂，面试时很多人找不到重点。这份java面试大全，有部分网络上资源，大多数是从好的文章和书籍里总结出来的知识点。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="本书的目的"><a href="#本书的目的" class="headerlink" title="本书的目的"></a>本书的目的</h2><p>每个java程序员在面试前都不知该准备什么？或者是随便看几个文章就去面试，这样的结果很容易失败！希望本书能给java程序员一个好的指引，让java程序员没有难找的工作！</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p><a href="https://maochunguang.gitbooks.io/java-interview/" target="_blank" rel="noopener">gitbook地址</a><br><img src="http://o7kalf5h3.bkt.clouddn.com/java-tips.png" alt="目录截图"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kobo aura one导出笔记高级配置</title>
      <link href="/kobo-config.html"/>
      <url>/kobo-config.html</url>
      
        <content type="html"><![CDATA[<p>** kobo aura one导出笔记高级配置：** &lt;Excerpt in index | 首页摘要&gt;<br>kobo电子书折腾记，导出笔记，从激活到设置，打补丁实现自定义配置，还是自己折腾起来有意思啊。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h2><p>买电子书是为了阅读和学习，不是天天折腾电子书，一天刷一次机，如果只是看书，做笔记，学个英文什么的<br>原生系统是最好的。如果看pdf为主，不建议买这电子书，看pdf首选电脑，平板，sony dsp系列，用普通的电子书阅读器，体验太差。</p><h2 id="kobo原生系统的功能（推荐原生系统，打上补丁）"><a href="#kobo原生系统的功能（推荐原生系统，打上补丁）" class="headerlink" title="kobo原生系统的功能（推荐原生系统，打上补丁）"></a>kobo原生系统的功能（推荐原生系统，打上补丁）</h2><ol><li>格式支持epub，mobi，cbz漫画，txt，kobo epub格式</li><li>高亮，笔记，导出笔记（需要配置一下）</li><li>字典（英文，中文，法文等多国字典，可以自己修改）</li><li>阅读pocket文章（可以把网页保存到pocket，实用pocket同步到阅读器）</li><li>自动亮度（最大的优点）</li></ol><h2 id="koreader的功能"><a href="#koreader的功能" class="headerlink" title="koreader的功能"></a>koreader的功能</h2><ol><li>格式支持epub，mobi，cbz漫画，txt，kobo epub格式</li><li>扫描版pdf支持重拍，切边（最大特色）</li><li>笔记导出到印象笔记</li><li>字典（强大的字典扩展）</li></ol><h2 id="激活"><a href="#激活" class="headerlink" title="激活"></a>激活</h2><p><strong>说明：wifi激活需要翻墙，可以实用笔记连接vpn，然后共享wifi给kobo</strong></p><ol><li>wifi激活,</li><li>kobo setup desktop激活，去kobo官网下载软件，然后电脑需要翻墙，电子书连接上电脑，用软件登录激活。这个软件很不好用，bug也多，建议使用wifi激活。</li></ol><h2 id="更新固件，打补丁"><a href="#更新固件，打补丁" class="headerlink" title="更新固件，打补丁"></a>更新固件，打补丁</h2><p>kobo的更新固件，更新补丁都是一个模式，把固件或者补丁放到.kobo文件夹，弹出设备就会自动重启</p><h2 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h2><p>电脑连接kobo，在根目录建立一个fonts文件夹，把需要的字体放进去即可</p><h2 id="词典"><a href="#词典" class="headerlink" title="词典"></a>词典</h2><p>下载网上改好的字典，直接放到.kobo文件夹下的dict目录下，然后重启就可以了</p><h2 id="自定义配置"><a href="#自定义配置" class="headerlink" title="自定义配置"></a>自定义配置</h2><ol><li>刷新页数（打补丁）</li><li>上下页宽（打补丁）</li><li>全屏模式（修改配置文件）</li><li>字体高级设置（修改配置文件）</li><li>导出笔记和高亮（修改配置文件）</li></ol><h2 id="kobo高级配置文件详解"><a href="#kobo高级配置文件详解" class="headerlink" title="kobo高级配置文件详解"></a>kobo高级配置文件详解</h2><p>用电脑连接kobo电子书，打开Kobo找到eReader.conf文件，最好用notepad++修改，或者其他文本编辑器。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[FeatureSettings]</span><br><span class="line">#导出笔记</span><br><span class="line">ExportHighlightsEnabled=true</span><br><span class="line">#显示全书的页码，而不是章节的页码</span><br><span class="line">FullBookPageNumbers=true</span><br><span class="line">#用在线等维基百科代替词典查询</span><br><span class="line">OnlineWikipedia=true</span><br><span class="line">#全屏阅读</span><br><span class="line">FullScreenReading=true</span><br><span class="line">#图片缩放</span><br><span class="line">ImageZoom=true</span><br><span class="line">#浏览器全屏</span><br><span class="line">FullScreenBrowser=true</span><br><span class="line">#关机键截图，但是关机键就无法关机了，不要设置这个鸡肋的功能</span><br><span class="line">Screenshots=true</span><br><span class="line">[Reading]</span><br><span class="line">#翻页刷新的页数，20页全刷一次</span><br><span class="line">numPartialUpdatePageTurns=20</span><br><span class="line">#左边距</span><br><span class="line">readingLeftMargin=0</span><br><span class="line">#右边距</span><br><span class="line">readingRightMargin=0</span><br><span class="line">#行高</span><br><span class="line">readingLineHeight=1.4</span><br><span class="line"></span><br><span class="line">[PowerOptions]</span><br><span class="line">#自动关机时间</span><br><span class="line">AutoOffMinutes=60</span><br></pre></td></tr></table></figure><blockquote><p>博客搬家，请访问新博客地址吧! <a href="https://www.duduhuahua.cn" target="_blank" rel="noopener">我的博客</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 开发工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Illegal mix of collations</title>
      <link href="/mysql-collation.html"/>
      <url>/mysql-collation.html</url>
      
        <content type="html"><![CDATA[<p>** mysql排序字符集问题：** &lt;Excerpt in index | 首页摘要&gt;<br>mysql表的每个字段都可以设置单独的排序字符集和文本字符集，如果你创建表的时候不注意，很可能会遇到Illegal mix of collations这个问题。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>用mysql进行两个表的联合查询的时候，出现下面的错误。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Illegal mix of collations (utf8_unicode_ci,IMPLICIT) and (utf8_general_ci,IMPLICIT) for operation &apos;=&apos;</span><br></pre></td></tr></table></figure><h2 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h2><ol><li>通过google搜索找到原因，这个错误是mysql的排序字符集不一致导致的。</li><li>把联合查询的表使用navicat查看字段的设置，发现了有一个关联字段排序字符集的问题，如图：</li><li>这两个表中openid的排序规则不一致，导致出现问题。<br><img src="http://o7kalf5h3.bkt.clouddn.com/openid01.png" alt="user表中opeid"><br><img src="http://o7kalf5h3.bkt.clouddn.com/openid02.png" alt="user_tag表中opeid"></li></ol><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>将user表中的字符集和排序规则设置为默认，保持一致即可。</p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mongodb从入门到精通</title>
      <link href="/mongodb-study.html"/>
      <url>/mongodb-study.html</url>
      
        <content type="html"><![CDATA[<p>** mongodb从入门到精通** &lt;Excerpt in index | 首页摘要&gt;<br>    mongodb日常使用的一些知识，增删改查，索引，分片。<br> <a id="more"></a><br>&lt;The rest of contents | 余下全文&gt;</p><h2 id="mongodb学习"><a href="#mongodb学习" class="headerlink" title="mongodb学习"></a>mongodb学习</h2><h2 id="1-mongodb特性"><a href="#1-mongodb特性" class="headerlink" title="1.mongodb特性"></a>1.mongodb特性</h2><pre><code>1）mongo是一个面向文档的数据库，它集合了nosql和sql数据库两方面的特性。2）所有实体都是在首次使用时创建。3）没有严格的事务特性，但是它保证任何一次数据变更都是原子性的。4）也没有固定的数据模型5）mongo以javascript作为命令行执行引擎，所以利用shell进行复杂的计算和查询时会相当的慢。6）mongo本身支持集群和数据分片7）mongo是c++实现的，支持windows mac linux等主流操作系统8）性能优越，速度快</code></pre><h2 id="2-mongo常用操作"><a href="#2-mongo常用操作" class="headerlink" title="2.mongo常用操作"></a>2.mongo常用操作</h2><h3 id="增删操作"><a href="#增删操作" class="headerlink" title="增删操作"></a>增删操作</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">db.user.insert(&#123;<span class="attr">name</span>:<span class="string">'aaaa'</span>,<span class="attr">age</span>:<span class="number">30</span>&#125;);</span><br><span class="line">db.user.save(&#123;<span class="attr">name</span>:<span class="string">'aaaa'</span>,<span class="attr">age</span>:<span class="number">30</span>&#125;);</span><br><span class="line">db.collection.insertOne(&#123;&#125;);<span class="comment">//(3.2新特性)</span></span><br><span class="line">db.collection.deleteOne(&#123;&#125;,&#123;&#125;);<span class="comment">//(3.2新特性)</span></span><br><span class="line">db.collection.remove(&#123;<span class="attr">name</span>:<span class="string">'aaa'</span>&#125;);</span><br><span class="line">db.collection.remove();<span class="comment">//(删除全部)</span></span><br></pre></td></tr></table></figure><h3 id="更新操作"><a href="#更新操作" class="headerlink" title="更新操作"></a>更新操作</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">db.users.update(&#123;<span class="string">"name"</span>: <span class="string">"joe"</span>&#125;, joe );</span><br><span class="line"><span class="comment">//upsert模式</span></span><br><span class="line">db.users.update(&#123;<span class="string">"name"</span>: <span class="string">"joe"</span>&#125;, joe,  <span class="literal">true</span> );</span><br><span class="line"><span class="comment">//MULTI模式</span></span><br><span class="line">db.users.update(&#123;<span class="string">"name"</span>: <span class="string">"joe"</span>&#125;, joe,  <span class="literal">true</span> ，<span class="literal">true</span>);</span><br></pre></td></tr></table></figure><blockquote><p>update是对文档替换，而不是局部修改默认情况update更新匹配的第一条文档，multi模式更新所有匹配的  </p></blockquote><h3 id="查询操作"><a href="#查询操作" class="headerlink" title="查询操作"></a>查询操作</h3><p>普通查询</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">db.user.find();</span><br><span class="line">db.user.find(&#123;<span class="attr">name</span>:<span class="string">'aaa'</span>&#125;);</span><br><span class="line">db.user.findOne(&#123;<span class="attr">name</span>:<span class="string">'aaa'</span>&#125;);</span><br></pre></td></tr></table></figure><p>模糊查询</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.UserInfo.find(&#123;<span class="attr">userName</span> :<span class="string">'/A/'</span>&#125;) <span class="comment">//（名称%A%）</span></span><br><span class="line">db.UserInfo.find(&#123;<span class="attr">userName</span> :<span class="string">'/^A/'</span>&#125;) <span class="comment">//(名称A%)</span></span><br></pre></td></tr></table></figure><h3 id="操作符"><a href="#操作符" class="headerlink" title="操作符"></a>操作符</h3><ol><li>$lt, $lte,$gt, $gte(&lt;, &lt;=, &gt;, &gt;= )     </li><li>$all    数组中的元素是否完全匹配  db.things.find( { a: { $all: [ 2, 3 ] } } );</li><li>$exists  可选：true，false  db.things.find( { a : { $exists : true } } );</li><li>$mod  取模：a % 10 == 1  db.things.find( { a : { $mod : [ 10 , 1 ] } } );</li><li>$ne 取反：即not equals  db.things.find( { x : { $ne : 3 } } );</li><li>$in 类似于SQL的IN操作  db.things.find({j:{$in: [2,4,6]}});</li><li>$nin $in的反操作，即SQL的  NOT IN  db.things.find({j:{$nin: [2,4,6]}});</li><li>$nor $or的反操作，即不匹配(a或b)  db.things.find( { name : “bob”, $nor : [ { a : 1 },{ b : 2 }]})</li><li>$or Or子句，注意$or不能嵌套使用  db.things.find( { name : “bob” , $or : [ { a : 1 },{ b : 2 }]})</li><li>$size  匹配数组长度  db.things.find( { a : { $size: 1 } } );</li><li>$type  匹配子键的数据类型，详情请看  db.things.find( { a : { $type : 2 } } );</li></ol><h3 id="数组查询"><a href="#数组查询" class="headerlink" title="数组查询"></a>数组查询</h3><p>$size 用来匹配数组长度（即最大下标）<br>// 返回comments包含5个元素的文档<br>db.posts.find({}, {comments:{‘$size’: 5}});<br>// 使用冗余字段来实现<br>db.posts.find({}, {‘commentCount’: { ‘$gt’: 5 }});<br>$slice 操作符类似于子键筛选，只不过它筛选的是数组中的项<br>// 仅返回数组中的前5项<br>db.posts.find({}, {comments:{‘$slice’: 5}});<br>// 仅返回数组中的最后5项<br>db.posts.find({}, {comments:{‘$slice’: -5}});<br>// 跳过数组中的前20项，返回接下来的10项<br>db.posts.find({}, {comments:{‘$slice’: [20, 10]}});<br>// 跳过数组中的最后20项，返回接下来的10项<br>db.posts.find({}, {comments:{‘$slice’: [-20, 10]}});<br>MongoDB 允许在查询中指定数组的下标，以实现更加精确的匹配<br>// 返回comments中第1项的by子键为Abe的所有文档<br>db.posts.find( { “comments.0.by” : “Abe” } );   </p><h2 id="3-索引的使用"><a href="#3-索引的使用" class="headerlink" title="3.索引的使用"></a>3.索引的使用</h2><h3 id="创建索引"><a href="#创建索引" class="headerlink" title="创建索引"></a>创建索引</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">db.things.ensureIndex(&#123;<span class="string">'j'</span>: <span class="number">1</span>&#125;);</span><br><span class="line"><span class="comment">//创建子文档 索引</span></span><br><span class="line">db.things.ensureIndex(&#123;<span class="string">'user.Name'</span> : - <span class="number">1</span>&#125;);</span><br><span class="line"><span class="comment">//创建 复合 索引</span></span><br><span class="line">db.things.ensureIndex(&#123;</span><br><span class="line"><span class="string">'j'</span> : <span class="number">1</span> ,   <span class="comment">//  升序</span></span><br><span class="line"><span class="string">'x'</span> : - <span class="number">1</span>   <span class="comment">//  降序</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>如果 您的 find 操作只用到了一个键，那么索引方向是无关紧要的<br>        当创建复合索引的时候，一定要谨慎斟酌每个键的排序方向</p><h3 id="修改索引"><a href="#修改索引" class="headerlink" title="修改索引"></a>修改索引</h3><p>修改索引，只需要重新 运行索引 命令即可<br>如果索引已经存在则会 重建， 不存在的索引会被 添加  </p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">db.things.ensureIndex (&#123;</span><br><span class="line"><span class="comment">//原来的索引会 重建</span></span><br><span class="line"><span class="string">'user.Name '</span> : - <span class="number">1</span> ,</span><br><span class="line"><span class="comment">//新增一个升序 索引</span></span><br><span class="line"><span class="string">'user.Name '</span> : <span class="number">1</span> ,</span><br><span class="line"><span class="comment">//为 Age 新建降序 索引</span></span><br><span class="line"><span class="string">'user.Age '</span> :  - <span class="number">1</span></span><br><span class="line"><span class="comment">//打开后台执行</span></span><br><span class="line">&#125;,&#123;<span class="string">'background'</span> :   <span class="literal">true</span>&#125;);</span><br><span class="line"><span class="comment">//重建索引</span></span><br><span class="line">db.things.reIndex();</span><br></pre></td></tr></table></figure><h3 id="删除索引"><a href="#删除索引" class="headerlink" title="删除索引"></a>删除索引</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//删除集合中的所有 索引</span></span><br><span class="line">db.things.dropIndexes ();  </span><br><span class="line"><span class="comment">//删除指定键的索引  </span></span><br><span class="line">db.things.dropIndex (&#123;</span><br><span class="line">x :   <span class="number">1</span> ,</span><br><span class="line">y :   - <span class="number">1</span></span><br><span class="line">&#125;);  </span><br><span class="line"><span class="comment">//使用 command 删除指定键的 索引</span></span><br><span class="line">db.runCommand (&#123;</span><br><span class="line">dropIndexes : <span class="string">'foo '</span> ,</span><br><span class="line">index:&#123; <span class="attr">y</span> : <span class="number">1</span> &#125;</span><br><span class="line">&#125;);  </span><br><span class="line"><span class="comment">//使用 command 删除所有 索引</span></span><br><span class="line">db.runCommand (&#123;<span class="attr">dropIndexes</span> : <span class="string">'foo '</span>,<span class="attr">index</span>: <span class="string">'*'</span>&#125;)</span><br></pre></td></tr></table></figure><p>如果是删除集合中所有的文档（remove）则不会影响索引，当有新文档插入时，索引就会重建。</p><h3 id="唯一索引"><a href="#唯一索引" class="headerlink" title="唯一索引"></a>唯一索引</h3><p>创建唯一索引，同时这也是一个符合唯一索引  </p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">db.things.ensureIndex (</span><br><span class="line">&#123;</span><br><span class="line"><span class="string">'firstName '</span> :   <span class="number">1</span> ,</span><br><span class="line"><span class="string">'lastName '</span> :   <span class="number">1</span></span><br><span class="line">&#125;,   &#123;</span><br><span class="line"><span class="comment">//指定为唯一索引</span></span><br><span class="line"><span class="string">'unique'</span>: <span class="literal">true</span> ,</span><br><span class="line"><span class="comment">//删除重复 记录</span></span><br><span class="line"><span class="string">'dropDups'</span>: <span class="literal">true</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="强制使用索引"><a href="#强制使用索引" class="headerlink" title="强制使用索引"></a>强制使用索引</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//强制使用索引 a 和 b</span></span><br><span class="line">db.collection.find(&#123;</span><br><span class="line"><span class="string">'a'</span> :   <span class="number">4</span> ,</span><br><span class="line"><span class="string">'b'</span> :   <span class="number">5</span> ,</span><br><span class="line"><span class="string">'c'</span> :   <span class="number">6</span></span><br><span class="line">&#125;).hint(&#123;</span><br><span class="line"><span class="string">'a'</span> :   <span class="number">1</span> ,</span><br><span class="line"><span class="string">'b'</span> :   <span class="number">1</span></span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">//强制不使用任何 索引</span></span><br><span class="line">db.collection.find().hint(&#123;</span><br><span class="line"><span class="string">'$natural'</span> :   <span class="number">1</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><hr><p>索引总结:</p><ul><li>索引可以加速查询；</li><li>单个索引无需在意其索引方向；</li><li>多键索引需要慎重考虑每个索引的方向；</li><li>做海量数据更新时应当先卸载所有索引，待数据更新完成后再重建索引；</li><li>不要试图为每个键都创建索引，应考虑实际需要，并不是索引越多越好；</li><li>唯一索引可以用来消除重复记录；</li><li>地理空间索引是没有单位的，其内部实现是基本的勾股定理算法</li></ul><h2 id="4-mongo数据库管理"><a href="#4-mongo数据库管理" class="headerlink" title="4.mongo数据库管理"></a>4.mongo数据库管理</h2><h3 id="安全与认证"><a href="#安全与认证" class="headerlink" title="安全与认证"></a>安全与认证</h3><ol><li>默认为无认证，启动用登录 shell ；</li><li>添加账号；</li><li>关闭 shell .关闭 MongoDB ；</li><li>为 MongoDB 增加 — auth 参数；</li><li>重 启 MongoDB ；</li><li>登录 shell ，此时就需要认证了</li></ol><h3 id="冷备份"><a href="#冷备份" class="headerlink" title="冷备份"></a>冷备份</h3><ol><li>关闭MongoDB引擎</li><li>拷贝数据库文件夹及文件</li><li>恢复时反向操作即可        </li></ol><ul><li>优点：可以完全保证数据完整性；</li><li>缺点：需要数据库引擎离线     <h3 id="热备份"><a href="#热备份" class="headerlink" title="热备份"></a>热备份</h3></li></ul><ol><li>保持MongoDB为运行状态</li><li>使用mongodump备份数据</li><li>使用mongorestore恢复数据</li></ol><ul><li>优点：数据库引擎无须离线</li><li>缺点：不能保证数据完整性，操作时会降低MongoDB性能</li></ul><h3 id="主从复制备份"><a href="#主从复制备份" class="headerlink" title="主从复制备份"></a>主从复制备份</h3><ol><li>创建主从复制机制</li><li>配置完成后数据会自动同步</li><li>恢复途径很多</li></ol><ul><li>优点：可以保持MongoDB处于联机状态，不影响性能</li><li>缺点：在数据写入密集的情况下可能无法保证数据完整性</li></ul><h3 id="修复db-repairDatabase"><a href="#修复db-repairDatabase" class="headerlink" title="修复db.repairDatabase();"></a>修复db.repairDatabase();</h3><ul><li>修复数据库还可以起到压缩数据的作用；</li><li>修复数据库的操作相当耗时，万不得已请不要使用；</li><li>建议经常做数据备份；<h2 id="5-mongo复制-集群"><a href="#5-mongo复制-集群" class="headerlink" title="5.mongo复制(集群)"></a>5.mongo复制(集群)</h2></li></ul><ol><li>主从复制<br>选项      说明</li></ol><p>–only  作用是限定仅复制指定的某个数据库<br>–slavedelay  为复制设置操作延迟，单位为秒<br>–fastsync  以主节点的数据快照为基础启动从节点。<br>–autoresync  当主从节点数据不一致时，是否自动重新同步<br>–oplogSize  设定主节点中的oplog的容量，单位是MB</p><ol start="2"><li><p>副本集<br>与普通主从复制集群相比，具有自动检测机制<br>需要使用—replSet 选项指定副本同伴<br>任何时候，副本集当中最多只允许有1个活跃节点</p></li><li><p>读写分离<br>将密集的读取操作分流到从节点上，降低主节点的负载<br>默认情况下，从节点是不允许处理<br>客户端请求的，需要使用—slaveOkay打开<br>不适用于实时性要求非常高的应用</p></li><li><p>工作原理—— OPLOG<br>oplog保存在local数据库中，oplog就在其中的<br>oplog.$main集合内保存。该集合的每个文档都记录了主节点上执行的一个操作，其键定义如下：<br> ts：操作时间戳，占用4字节<br> op：操作类型，占用1字节<br> ns：操作对象的命名空间（或理解为集合全名）<br> o：进一步指定所执行的操作，例如插入</p></li><li><p>工作原理—— 同步<br> 从节点首次启动时，做完整同步<br> 主节点数据发生变化时，做增量同步<br> 从节点与主节点数据严重不一致时，做完整同步</p></li><li><p>复制管理—— 诊断<br>db.printReplicationInfo()<br>在主节点上使用<br> 返回信息是oplog的大小以及各种操作的耗时. 空间占用等数据<br>在从节点上使用<br>db.printSlaveReplicationInfo()<br> 返回信息是从节点的数据源列表. 同步延迟时间等</p></li><li><p>复制管理—— 变更OPLOG 容量<br>在主节点上使用<br> 设定—oplogSize参数<br> 重启MongoDB</p></li><li><p>复制管理—— 复制认证<br>主从节点皆须配置<br> 存储在local.system.users<br> 优先尝试repl用户<br> 主从节点的用户配置必须保持一致</p><h2 id="6-MONGODB分片"><a href="#6-MONGODB分片" class="headerlink" title="6.MONGODB分片"></a>6.MONGODB分片</h2><h3 id="分片与自动分片"><a href="#分片与自动分片" class="headerlink" title="分片与自动分片"></a>分片与自动分片</h3><p>分片是指将数据拆分，分散到不同的实例上进行负载分流的做法。我们常说的“分表”、“分库”、“分区”等概念都属于分片的实际体现。<br>传统分片做法是手工分表、分库。自动分片技术是根据指定的“片键”自动拆分数据并维护数据请求路由的过程。</p></li></ol><ul><li>递增片键–连续 不均匀 写入集中 分流较差</li><li>随机片键–不连续 均匀 写入分散 分流较好</li></ul><h3 id="三个组成部分"><a href="#三个组成部分" class="headerlink" title="三个组成部分"></a>三个组成部分</h3><ul><li>片,保存子集数据的容器</li><li>mongos,MongoDB的路由器进程</li><li>配置服务器,分·片集群的配置信息<h3 id="创建分片"><a href="#创建分片" class="headerlink" title="创建分片"></a>创建分片</h3></li><li>–启动配置服务器,可以创建一个或多个</li><li>–添加片,每个片都应该是副本集</li><li>–物理服务器,性能、安全和稳定性<h3 id="管理分片"><a href="#管理分片" class="headerlink" title="管理分片"></a>管理分片</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//查询分片</span></span><br><span class="line">db.shards.find();</span><br><span class="line"><span class="comment">//数据库</span></span><br><span class="line">db.databases.find();</span><br><span class="line"><span class="comment">//块</span></span><br><span class="line">db.chunks.find();</span><br><span class="line"><span class="comment">//分片状态</span></span><br><span class="line">db.printShardingStatus();</span><br><span class="line"><span class="comment">//删除片</span></span><br><span class="line">db.runCommand(&#123; <span class="attr">removeshard</span> : <span class="string">'ip:port'</span> &#125;);</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mongodb </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo自用黑色主题</title>
      <link href="/hexo-theme.html"/>
      <url>/hexo-theme.html</url>
      
        <content type="html"><![CDATA[<p>** hexo和coding打造静态博客 ：** &lt;Excerpt in index | 首页摘要&gt;<br>使用hexo一年有余，对所有主题都感觉有所缺陷，便修改了一个自用黑色主题，本主题以黑色和蓝色为主，色彩鲜明，主题明确。    </p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="主题图片"><a href="#主题图片" class="headerlink" title="主题图片"></a>主题图片</h2><p><img src="http://o7kalf5h3.bkt.clouddn.com/blog-index.png" alt="主题首页"> </p><h2 id="black-blue主题来源"><a href="#black-blue主题来源" class="headerlink" title="black-blue主题来源"></a>black-blue主题来源</h2><p>本主题修改自<strong>spfk</strong>主题，但之前spfk主题有很多问题，本主题改进如下：</p><ol><li>压缩js，css提高性能</li><li>代码段样式显示更完美</li><li>增加本地搜索</li><li>设置更合适的字体大小</li><li>颜色以黑色和蓝色为主，色彩鲜明</li><li>seo适当优化</li><li>删除多说，有言，增加畅言评论</li><li>删除stylus，全部改用css方便修改</li></ol><h2 id="主题地址"><a href="#主题地址" class="headerlink" title="主题地址"></a>主题地址</h2><p><a href="https://github.com/maochunguang/black-blue" target="_blank" rel="noopener">black-blue</a></p><h2 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h2><p>大家使用主题的时候，把<strong>主题配置文件_config.yml</strong>以下几项必须修改，项目里实用的是我博客的正式代码，请大家修改成自己的！</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">google_analytics:</span> <span class="string">xxx</span></span><br><span class="line"><span class="attr">baidu_analytics:</span> <span class="string">xxxxxxx</span></span><br><span class="line"><span class="attr">disqus:</span></span><br><span class="line"><span class="attr">  on:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  shortname:</span> <span class="string">xxxx</span></span><br><span class="line"><span class="comment"># 畅言评论</span></span><br><span class="line"><span class="attr">changyan:</span></span><br><span class="line"><span class="attr">  on:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  appid:</span> <span class="string">xxxx</span></span><br><span class="line"><span class="attr">  conf:</span> <span class="string">xxxxx</span></span><br></pre></td></tr></table></figure><h2 id="black-blue主题配置"><a href="#black-blue主题配置" class="headerlink" title="black-blue主题配置"></a>black-blue主题配置</h2><h3 id="切换主题"><a href="#切换主题" class="headerlink" title="切换主题"></a>切换主题</h3><p>复制主题到themes目录下<code>cd themes &amp;&amp; git clone https://github.com/maochunguang/black-blue</code>，修改_config.yml <code>theme: black-blue</code></p><h3 id="安装常用插件，建议全部安装"><a href="#安装常用插件，建议全部安装" class="headerlink" title="安装常用插件，建议全部安装"></a>安装常用插件，建议全部安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## rss插件</span></span><br><span class="line">npm install hexo-generator-feed --save</span><br><span class="line"><span class="comment">## 站点sitemap生成插件</span></span><br><span class="line">npm install hexo-generator-sitemap --save</span><br><span class="line">npm install hexo-generator-baidu-sitemap --save</span><br><span class="line"><span class="comment">## 百度url提交</span></span><br><span class="line">npm install hexo-baidu-url-submit --save</span><br><span class="line"><span class="comment">## 本地搜索插件集成</span></span><br><span class="line">npm install hexo-generator-search --save</span><br></pre></td></tr></table></figure><h3 id="博客全局配置，修改根目录下-config-yml"><a href="#博客全局配置，修改根目录下-config-yml" class="headerlink" title="博客全局配置，修改根目录下_config.yml"></a>博客全局配置，修改根目录下_config.yml</h3><p>插件配置</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Plugins:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">hexo-generator-feed</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">hexo-generator-sitemap</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">hexo-generator-baidu-sitemap</span></span><br></pre></td></tr></table></figure><p>rss设置</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">feed:</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">atom</span></span><br><span class="line"><span class="attr">  path:</span> <span class="string">atom.xml</span></span><br><span class="line"><span class="attr">  limit:</span> <span class="number">20</span></span><br></pre></td></tr></table></figure><p>本地搜索配置</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">search:</span></span><br><span class="line"><span class="attr">  path:</span> <span class="string">search.json</span></span><br><span class="line"><span class="attr">  field:</span> <span class="string">post</span></span><br></pre></td></tr></table></figure><p>站点地图，seo搜索引擎需要</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">sitemap:</span></span><br><span class="line"><span class="attr">  path:</span> <span class="string">sitemap.xml</span></span><br><span class="line"><span class="attr">baidusitemap:</span></span><br><span class="line"><span class="attr">  path:</span> <span class="string">baidusitemap.xml</span></span><br></pre></td></tr></table></figure><h3 id="主题配置"><a href="#主题配置" class="headerlink" title="主题配置"></a>主题配置</h3><p>菜单配置</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 添加单独的页面:hexo new page about，about是页面的路径，也是名称</span></span><br><span class="line"><span class="comment">## Tags Cloud Page: `hexo new page tags`</span></span><br><span class="line"><span class="attr">menu:</span></span><br><span class="line">  <span class="comment"># 主页: /archives/</span></span><br><span class="line">  <span class="string">所有文章:</span> <span class="string">/archives/</span></span><br><span class="line">  <span class="string">玩转开发工具:</span> <span class="string">/categories/开发工具/</span></span><br><span class="line">  <span class="string">玩转数码:</span> <span class="string">/categories/digital</span></span><br><span class="line">  <span class="string">认知提升:</span> <span class="string">/categories/cognition</span></span><br><span class="line">  <span class="string">关于我:</span> <span class="string">/about/</span></span><br></pre></td></tr></table></figure><p>评论配置</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 是否开启畅言评论，</span></span><br><span class="line"><span class="attr">changyan:</span></span><br><span class="line"><span class="attr">  on:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  appid:</span> <span class="string">xxxx</span></span><br><span class="line"><span class="attr">  conf:</span> <span class="string">xxxxxxxxxxxx</span></span><br><span class="line"><span class="comment"># 是否开启disqus，</span></span><br><span class="line"><span class="attr">disqus:</span></span><br><span class="line"><span class="attr">  on:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  shortname:</span> <span class="string">mmmmmm</span></span><br></pre></td></tr></table></figure><h3 id="其他配置，详细的配置请下载主题，都有注释"><a href="#其他配置，详细的配置请下载主题，都有注释" class="headerlink" title="其他配置，详细的配置请下载主题，都有注释"></a>其他配置，<strong>详细的配置请下载主题，都有注释</strong></h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数学公式支持</span></span><br><span class="line"><span class="attr">mathjax:</span> <span class="literal">false</span></span><br><span class="line"><span class="comment"># Socail Share | 是否开启分享</span></span><br><span class="line"><span class="attr">baidushare:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># 谷歌分析，百度分析，seo分析很有用</span></span><br><span class="line"><span class="attr">google_analytics:</span> <span class="string">xxxxxx</span></span><br><span class="line"><span class="attr">baidu_analytics:</span> <span class="string">xxcxcxcsdsf</span></span><br></pre></td></tr></table></figure><h2 id="自定义配置（对前端技术有了解即可）"><a href="#自定义配置（对前端技术有了解即可）" class="headerlink" title="自定义配置（对前端技术有了解即可）"></a>自定义配置（对前端技术有了解即可）</h2><h3 id="显示更多和折叠文章"><a href="#显示更多和折叠文章" class="headerlink" title="显示更多和折叠文章"></a>显示更多和折叠文章</h3><p>你的md文件格式需要按下面的来：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">title: 突破算法第11天-红黑树</span><br><span class="line">date: 2017-10-30 22:35:37</span><br><span class="line">tags: 算法</span><br><span class="line">categories: algorithm</span><br><span class="line">---</span><br><span class="line">** &#123;&#123; title &#125;&#125;：** &lt;Excerpt in index | 首页摘要&gt;</span><br><span class="line">红黑树</span><br><span class="line">&lt;!-- more --&gt;</span><br><span class="line">&lt;The rest of contents | 余下全文&gt;</span><br><span class="line">正文……</span><br></pre></td></tr></table></figure><h3 id="头像配置"><a href="#头像配置" class="headerlink" title="头像配置"></a>头像配置</h3><p>在themes/black-blue/source/img/avatar.png,替换此头像即可实现自定义头像</p><h3 id="背景图片配置"><a href="#背景图片配置" class="headerlink" title="背景图片配置"></a>背景图片配置</h3><p>在themes/black-blue/source/background/,替换为自己喜欢的图片，图片名称不能改</p><h3 id="添加评论插件"><a href="#添加评论插件" class="headerlink" title="添加评论插件"></a>添加评论插件</h3><p>比如把畅言替换为有言</p><ol><li><p>先修改themes/black-blue/_config.yml文件</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">changyan:</span></span><br><span class="line"><span class="attr">  on:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  uid:</span> <span class="string">xxxxxxx</span></span><br></pre></td></tr></table></figure></li><li><p>修改themes/black-blue/layout/_partial/comments/changyan.ejs</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;section <span class="class"><span class="keyword">class</span></span>=<span class="string">"changyan"</span> id=<span class="string">"comments"</span>&gt;</span><br><span class="line">&lt;div id=<span class="string">"uyan_frame"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line">&lt;script type=<span class="string">"text/javascript"</span> src=<span class="string">"http://v2.uyan.cc/code/uyan.js?uid=&lt;%= uid%&gt;"</span>&gt;</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">&lt;/</span>section&gt;</span><br></pre></td></tr></table></figure></li><li><p>修改themes/black-blue/layout/_partial/article.ejs</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;%- partial(<span class="string">'comments/changyan'</span>, &#123;</span><br><span class="line">  uid: theme.changyan.uid</span><br><span class="line">&#125;) %&gt;</span><br></pre></td></tr></table></figure></li><li><p>重新生成页面<code>hexo g</code></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 开发工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数码产品选购</title>
      <link href="/digital-info.html"/>
      <url>/digital-info.html</url>
      
        <content type="html"><![CDATA[<p>** 数码产品选购：** &lt;Excerpt in index | 首页摘要&gt;<br>作为一个数码产品控，一出新的的电子产品，我都欣喜若狂。看参数，看评价，感觉合适，就会买。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="我喜欢的电子产品"><a href="#我喜欢的电子产品" class="headerlink" title="我喜欢的电子产品"></a>我喜欢的电子产品</h2><ul><li>电脑（笔记本，台式机，游戏主机，工作站）</li><li>手机（苹果，安卓，其它智能手机）</li><li>平板（安卓平板，ios平板）</li><li>电子书阅读器</li><li>电子手表</li></ul><h2 id="选购的原则"><a href="#选购的原则" class="headerlink" title="选购的原则"></a>选购的原则</h2><ol><li>产品生态，买电子产品虽然不是随大流，但是用户群体一定程度决定了生态。用的人多，相应的资源会比较丰富，遇到问题很快找到解决方案。</li><li>产品价格，性价比在中国，乃至全世界都是很具有吸引力的。物美价廉的都不买的要么是脑残，要么是钱多没地方花。</li><li>产品硬件参数，买电子产品不看参数，肯定是买不到物美价廉的产品。</li><li>产品外观，现在是看脸的时代，新时代的数码产品对外观要求更高，更时尚。</li><li>功能，买电子产品，首要的就是功能，如果功能都不齐全，再漂亮，再便宜都没用。</li><li>买电子产品的目的，没有任何需求就是瞎买。</li></ol><h2 id="电子产品的使用"><a href="#电子产品的使用" class="headerlink" title="电子产品的使用"></a>电子产品的使用</h2><p>我见过很多人买电子产品，比如买电子书阅读器，买一个kobo电子书折腾来折腾去，今天刷这个系统，明天改那个设置，<br>书还没读几本，系统刷了几十次，天天刷固件。这真的是得不偿失，捡了芝麻丢了西瓜。<br>第一，买电子产品是为了用的，买回来之后配置好之后，就不要来回折腾系统和配置了，把时间放到核心功能上。<br>第二，买电子产品不要攀比，就跟买苹果手机一样，如果只是为了装B买，真没必要，结果自己还用不习惯。<br>第三，了解自己的需求，需要什么买什么，</p>]]></content>
      
      
      <categories>
          
          <category> digital </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数码产品 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何写一篇好博客？</title>
      <link href="/bestblog.html"/>
      <url>/bestblog.html</url>
      
        <content type="html"><![CDATA[<p>** 提高自己博客的质量：** &lt;Excerpt in index | 首页摘要&gt;<br>写博客陆陆续续也有一年了，但是一直没有多少访问量，仔细看了很多大神的博客，总结了几点，分享一下。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="好博客，好文章是什么样的？"><a href="#好博客，好文章是什么样的？" class="headerlink" title="好博客，好文章是什么样的？"></a>好博客，好文章是什么样的？</h2><ol><li>文章名称鲜明，一看名称就知道关于什么的内容</li><li>整体结构清晰，把事件或者原理的始末按照‘什么样（what？）’，‘为什么（why）’，‘怎么做（how）’说明</li><li>简明扼要。太啰嗦，没人看。</li><li>难易适中，太高深也没人看</li><li>图文搭配，有句话说的好，<strong>一图胜千文</strong>，好的图片胜过千言万语</li></ol><h2 id="怎么写出好博客？"><a href="#怎么写出好博客？" class="headerlink" title="怎么写出好博客？"></a>怎么写出好博客？</h2><ol><li>定主题和文章名称。如果想写一个关于redis后台启动的文章，名称要准确，就叫redis后台启动，不要起啰嗦的名字，比如redis如何后台启动</li><li>准备资料阶段，熟悉redis配置相关资料，做好功课</li><li>定文章的结构和提纲。还拿这个redis后台启动为例，你得说明什么是后台启动？为什么要后台启动？如何做到后台启动？</li><li>语言表单，简单直白，不用凑字数</li><li>深入主题，比如挖掘更多redis的配置，把参数简要说明</li><li>找一个好图片，如果找不到，自己制作一个最契合自己主题的图片</li><li>把文章发给好友阅读，提出宝贵的意见</li><li>改进博客</li><li>坚持写博客</li></ol>]]></content>
      
      
      <categories>
          
          <category> 个人随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> others </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis后台启动详细配置</title>
      <link href="/redis-config.html"/>
      <url>/redis-config.html</url>
      
        <content type="html"><![CDATA[<p>** redis后台启动详细配置：** &lt;Excerpt in index | 首页摘要&gt;<br>  redis启动的时候有多种模式，后台启动，集群启动等等。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>在开发中一般都是在命令行中直接运行<code>redis-server</code>,但是这样命令行关闭，服务就停止了。<br>如果要在后台运行redis服务，需要制定配置文件。这里以<strong>ubuntu14</strong>为例子</p><h2 id="准备配置文件"><a href="#准备配置文件" class="headerlink" title="准备配置文件"></a>准备配置文件</h2><p>查看‘/etc/redis/redis.conf’,没有可以创建一个，或者下载一个，配置文件位置没有要求</p><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>把daemonize设置为yes，<br>然后<code>redis-server /etc/redis/redis.conf</code>启动服务，</p><h2 id="查看服务"><a href="#查看服务" class="headerlink" title="查看服务"></a>查看服务</h2><p><code>ps -ef|grep redis-server</code>查看是否有redis进程存在</p><h2 id="更多配置，在conf文件有说明"><a href="#更多配置，在conf文件有说明" class="headerlink" title="更多配置，在conf文件有说明"></a>更多配置，在conf文件有说明</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"># 是否以后台daemon方式运行，默认是 no，一般我们会改为 yes</span><br><span class="line">daemonize no</span><br><span class="line">pidfile /var/run/redis.pid</span><br><span class="line"># 只允许本机访问</span><br><span class="line">bind 127.0.0.1</span><br><span class="line"># 端口设置</span><br><span class="line">port 6379</span><br><span class="line">tcp-backlog 511</span><br><span class="line">timeout 0</span><br><span class="line">tcp-keepalive 0</span><br><span class="line">loglevel notice</span><br><span class="line"># 日志文件</span><br><span class="line">logfile &quot;&quot;</span><br><span class="line"># 开启数据库的数量，Redis 是有数据库概念的，默认是 16 个，数字从 0 ~ 15</span><br><span class="line">databases 16</span><br><span class="line">save 900 1</span><br><span class="line">save 300 10</span><br><span class="line">save 60 10000</span><br><span class="line">stop-writes-on-bgsave-error yes</span><br><span class="line">rdbcompression yes</span><br><span class="line">rdbchecksum yes</span><br><span class="line">dbfilename dump.rdb</span><br><span class="line">dir ./</span><br><span class="line">slave-serve-stale-data yes</span><br><span class="line">slave-read-only yes</span><br><span class="line">repl-diskless-sync no</span><br><span class="line">repl-diskless-sync-delay 5</span><br><span class="line">repl-disable-tcp-nodelay no</span><br><span class="line"># 密码设置，需要设置密码打开</span><br><span class="line">requirepass 123455</span><br><span class="line">slave-priority 100</span><br><span class="line">appendonly no</span><br><span class="line">appendfilename &quot;appendonly.aof&quot;</span><br><span class="line">appendfsync everysec</span><br><span class="line">no-appendfsync-on-rewrite no</span><br><span class="line">auto-aof-rewrite-percentage 100</span><br><span class="line">auto-aof-rewrite-min-size 64mb</span><br><span class="line">aof-load-truncated yes</span><br><span class="line">lua-time-limit 5000</span><br><span class="line">slowlog-log-slower-than 10000</span><br><span class="line">slowlog-max-len 128</span><br><span class="line">latency-monitor-threshold 0</span><br><span class="line">notify-keyspace-events &quot;&quot;</span><br><span class="line">hash-max-ziplist-entries 512</span><br><span class="line">hash-max-ziplist-value 64</span><br><span class="line">list-max-ziplist-entries 512</span><br><span class="line">list-max-ziplist-value 64</span><br><span class="line">set-max-intset-entries 512</span><br><span class="line">zset-max-ziplist-entries 128</span><br><span class="line">zset-max-ziplist-value 64</span><br><span class="line">hll-sparse-max-bytes 3000</span><br><span class="line">activerehashing yes</span><br><span class="line">client-output-buffer-limit normal 0 0 0</span><br><span class="line">client-output-buffer-limit slave 256mb 64mb 60</span><br><span class="line">client-output-buffer-limit pubsub 32mb 8mb 60</span><br><span class="line">hz 10</span><br><span class="line">aof-rewrite-incremental-fsync yes</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用koa2.x写下载漫画的爬虫</title>
      <link href="/spider-koa2.html"/>
      <url>/spider-koa2.html</url>
      
        <content type="html"><![CDATA[<p>** 用koa2.x写下载漫画的爬虫：** &lt;Excerpt in index | 首页摘要&gt;<br>使用koa2.x的async ，await解决异步问题，写一个下载漫画的爬虫，代码里有惊喜和福利哦！</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="项目搭建"><a href="#项目搭建" class="headerlink" title="项目搭建"></a>项目搭建</h2><ol><li>安装nodejs&gt;7.6,安装koa-generator</li><li>直接<code>koa2 spider</code>,生成项目</li><li>安装request,request-promise,cheerio,mkdirp</li><li>npm install安装依赖</li></ol><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>图片或者漫画爬虫的思路很简单，首先观察url的规律，把url按规律加入到下载任务，其实就是请求获得html内容，然后对html进行解析，找到下载的图片url（一般都是img标签的src属性值），把url放到数组保存，使用async await控制所有的任务，直到把所有的图片下载完。</p><h2 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h2><p>但是nodejs本身上异步的，如果你直接在for循环里去下载，肯定是不行的，必须控制好异步的执行上关键。<br>爬虫简单，处理好异步难。这里我使用的es7中async，await配合promise解决异步问题，还可以使用async模块，eventproxy，等等异步控制模块来解决。</p><h2 id="核心代码-spider-js"><a href="#核心代码-spider-js" class="headerlink" title="核心代码,spider.js"></a>核心代码,spider.js</h2><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> fs = <span class="built_in">require</span>(<span class="string">'fs'</span>);</span><br><span class="line"><span class="keyword">const</span> request = <span class="built_in">require</span>(<span class="string">"request-promise"</span>);</span><br><span class="line"><span class="keyword">const</span> cheerio = <span class="built_in">require</span>(<span class="string">"cheerio"</span>);</span><br><span class="line"><span class="keyword">const</span> mkdirp = <span class="built_in">require</span>(<span class="string">'mkdirp'</span>);</span><br><span class="line"><span class="keyword">const</span> config = <span class="built_in">require</span>(<span class="string">'../config'</span>);</span><br><span class="line">exports.download = <span class="keyword">async</span> <span class="function"><span class="keyword">function</span>(<span class="params">ctx, next</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">const</span> dir = <span class="string">'images'</span>;</span><br><span class="line">    <span class="comment">// 图片链接地址</span></span><br><span class="line">    <span class="keyword">let</span> links = [];</span><br><span class="line">    <span class="comment">// 创建目录</span></span><br><span class="line">    mkdirp(dir);</span><br><span class="line">    <span class="keyword">var</span> urls = [];</span><br><span class="line">    <span class="keyword">let</span> tasks = [];</span><br><span class="line">    <span class="keyword">let</span> downloadTask = [];</span><br><span class="line">    <span class="keyword">let</span> url = config.url;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">1</span>; i &lt;= config.size; i++) &#123;</span><br><span class="line">        <span class="keyword">let</span> link = url + <span class="string">'_'</span> + i + <span class="string">'.html'</span>;</span><br><span class="line">        <span class="keyword">if</span> (i == <span class="number">1</span>) &#123;</span><br><span class="line">            link = url + <span class="string">'.html'</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        tasks.push(getResLink(i, link))</span><br><span class="line">    &#125;</span><br><span class="line">    links = <span class="keyword">await</span> <span class="built_in">Promise</span>.all(tasks)</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'links=========='</span>, links.length);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; links.length; i++) &#123;</span><br><span class="line">        <span class="keyword">let</span> item = links[i];</span><br><span class="line">        <span class="keyword">let</span> index = item.split(<span class="string">'___'</span>)[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">let</span> src = item.split(<span class="string">'___'</span>)[<span class="number">1</span>];</span><br><span class="line">        downloadTask.push(downloadImg(src, dir, index + links[i].substr(<span class="number">-4</span>, <span class="number">4</span>)));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">await</span> <span class="built_in">Promise</span>.all(downloadTask);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">function</span> <span class="title">downloadImg</span>(<span class="params">url, dir, filename</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'download begin---'</span>, url);</span><br><span class="line">    request.get(url).pipe(fs.createWriteStream(dir + <span class="string">"/"</span> + filename)).on(<span class="string">'close'</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">        <span class="built_in">console</span>.log(<span class="string">'download success'</span>, url);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">function</span> <span class="title">getResLink</span>(<span class="params">index, url</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">const</span> body = <span class="keyword">await</span> request(url);</span><br><span class="line">    <span class="keyword">let</span> urls = [];</span><br><span class="line">    <span class="keyword">var</span> $ = cheerio.load(body);</span><br><span class="line">    $(config.rule).each(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">        <span class="keyword">var</span> src = $(<span class="keyword">this</span>).attr(<span class="string">'src'</span>);</span><br><span class="line">        urls.push(src);</span><br><span class="line">    &#125;);</span><br><span class="line">    <span class="keyword">return</span> index + <span class="string">'___'</span> + urls[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h2><p>由于爬虫的复杂性基于不同的网站，不同的任务很不一样，这里只是把几个常用的变量抽取到了config.js。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">    <span class="comment">//初始url</span></span><br><span class="line">    url: <span class="string">'http://www.xieet.com/meinv/230'</span>,</span><br><span class="line">    size: <span class="number">10</span>,</span><br><span class="line">    <span class="comment">// 选中图片img标签的选择器</span></span><br><span class="line">    rule: <span class="string">'.imgbox a img'</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="运行代码"><a href="#运行代码" class="headerlink" title="运行代码"></a>运行代码</h2><ol><li>下载我上传的代码<a href="https://github.com/maochunguang/koa-spider" target="_blank" rel="noopener">koa-spider</a></li><li>npm install,npm start即可运行</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实无论是写爬虫还是些其他程序，使用nodejs很大一部分都是要处理异步，要学好nodejs必须学好异步处理。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nodejs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>微信公众号开发</title>
      <link href="/wechat-dev.html"/>
      <url>/wechat-dev.html</url>
      
        <content type="html"><![CDATA[<p>** 微信公众号开发：** &lt;Excerpt in index | 首页摘要&gt;<br>微信公众号开发的一些注意事项</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="开发环境搭建"><a href="#开发环境搭建" class="headerlink" title="开发环境搭建"></a>开发环境搭建</h2><ol><li>微信公众号开发者配置，url，token，</li><li>本地调试，使用内网穿透工具，花生壳，或者netapp，买一个可以自定义域名的，内网映射到制定端口，</li><li>项目搭建，express或koa搭建项目，npm有微信的现成包，直接配置</li></ol><h2 id="回复"><a href="#回复" class="headerlink" title="回复"></a>回复</h2><ol><li>回复和发消息并没有什么特别注意的地方，这里不多说</li></ol><h2 id="菜单"><a href="#菜单" class="headerlink" title="菜单"></a>菜单</h2><ol><li>微信菜单有自定义菜单，有个性化菜单，但是个性化菜单优先级高于个性化菜单</li><li>个性化菜单可以根据用户的tag，sex，group等属性进行区分菜单</li><li>注意，我在使用时发现<strong>个性化菜单经常会失效</strong>，不起作用，偶尔会起作用，如果线上打算使用个性化菜单，请慎重并仔细测试</li></ol><h2 id="授权"><a href="#授权" class="headerlink" title="授权"></a>授权</h2><p>授权有网页授权，js sdk授权，<br>网页授权也有两种，一个上静默授权，一个是点击授权，贴一下js sdk调用前认证的代码，要使用sha1加密</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> getSignConfig(originUrl) &#123;</span><br><span class="line">      <span class="keyword">let</span> data = &#123;&#125;</span><br><span class="line">      <span class="keyword">const</span> sha1 = crypto.createHash(<span class="string">'sha1'</span>)</span><br><span class="line">      <span class="keyword">const</span> appId = <span class="keyword">this</span>.app.config.weixin.appID</span><br><span class="line">      <span class="keyword">const</span> jsapi_ticket = <span class="keyword">await</span> <span class="keyword">this</span>.ctx.service.token.getJSApiTicket()</span><br><span class="line">      <span class="keyword">const</span> noncestr = <span class="keyword">this</span>.app.config.jsapi.noncestr</span><br><span class="line">      <span class="keyword">const</span> url = <span class="keyword">this</span>.app.config.domain + originUrl</span><br><span class="line">      <span class="keyword">const</span> timestamp = <span class="built_in">parseInt</span>(<span class="keyword">new</span> <span class="built_in">Date</span>().getTime() / <span class="number">1000</span>)</span><br><span class="line">      <span class="comment">// sha1加密</span></span><br><span class="line">      <span class="keyword">const</span> str = <span class="string">`jsapi_ticket=<span class="subst">$&#123;jsapi_ticket&#125;</span>&amp;noncestr=<span class="subst">$&#123;noncestr&#125;</span>&amp;timestamp=<span class="subst">$&#123;timestamp&#125;</span>&amp;url=<span class="subst">$&#123;url&#125;</span>`</span></span><br><span class="line">      sha1.update(str)</span><br><span class="line">      <span class="keyword">const</span> signature = sha1.digest(<span class="string">'hex'</span>)</span><br><span class="line">      data = &#123; jsapi_ticket, noncestr, timestamp, url, signature, appId &#125;</span><br><span class="line">      <span class="keyword">return</span> data</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>调用js sdk页面上代码</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">wx.config(&#123;</span><br><span class="line">    debug: <span class="literal">false</span>, <span class="comment">// 开启调试模式,</span></span><br><span class="line">    appId: appId, <span class="comment">// 必填，公众号的唯一标识</span></span><br><span class="line">    timestamp: timestamp, <span class="comment">// 必填，生成签名的时间戳</span></span><br><span class="line">    nonceStr: nonceStr, <span class="comment">// 必填，生成签名的随机串</span></span><br><span class="line">    signature:  signature,<span class="comment">// 必填，签名，见附录1</span></span><br><span class="line">    jsApiList: [<span class="string">'closeWindow'</span>] <span class="comment">// 必填，需要使用的JS接口列表，所有JS接口列表见附录2</span></span><br><span class="line">&#125;);</span><br><span class="line">wx.ready(<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    setTimeout(<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">      wx.closeWindow();</span><br><span class="line">    &#125;,<span class="number">2000</span>);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="实用的常识"><a href="#实用的常识" class="headerlink" title="实用的常识"></a>实用的常识</h2><ol><li>tag不能重复创建，但是给用户可以重复打同一个tag</li><li>更改菜单一般五分钟生效，或者重新关注公众号，立马能看到</li><li>如果调用js sdk，务必使用https，防止因为安全问题，导致ios下js下载失败。如果你的服务是https，而引用了https的微信js，在ios下肯定会下载失败，这是ios的安全机制导致的。</li><li>微信关闭窗口的js接口，不管jsconfig验证是否通过，窗口都可以关闭</li><li>微信的token过期时间上2h，但是很多时候30分钟不到可能已经失效，建议<strong>把token过期时间设置为10分钟之内</strong></li></ol><h2 id="常见报错"><a href="#常见报错" class="headerlink" title="常见报错"></a>常见报错</h2><ol><li>创建菜单的时候，菜单长度不合法，仔细检查自己传的json菜单，一般都是<strong>json格式问题</strong>，而不是长度</li><li>redirect_uri不合法，是创建授权菜单的redirect_uri和<strong>网页授权域名</strong>配置不一样</li><li>关注公众号，服务端设置的欢迎消息发不过去，如果自己代码无异常，一般是因为<strong>token过期</strong></li></ol><h2 id="以后遇到其他问题继续补充"><a href="#以后遇到其他问题继续补充" class="headerlink" title="以后遇到其他问题继续补充"></a>以后遇到其他问题继续补充</h2>]]></content>
      
      
      <categories>
          
          <category> javacript </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程语言 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>那些年读的书</title>
      <link href="/mybooks.html"/>
      <url>/mybooks.html</url>
      
        <content type="html"><![CDATA[<p>** 那些年读的书：** &lt;Excerpt in index | 首页摘要&gt;<br>人生漫漫，不知不觉读了好多书，此贴只记录自己读过哪些书，不做多余的分析和总结。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="读过哪些种类的"><a href="#读过哪些种类的" class="headerlink" title="读过哪些种类的"></a>读过哪些种类的</h2><ul><li>编程专业类</li><li>小说类</li><li>励志类</li></ul><h2 id="小说"><a href="#小说" class="headerlink" title="小说"></a>小说</h2><ul><li><p>平凡的世界</p></li><li><p>白鹿原</p></li><li><p>穆斯林的葬礼</p></li><li><p>金庸武侠系列</p></li><li><p>古龙武侠小说</p></li><li><p>梁羽生武侠小说</p></li><li><p>余华作品集</p></li><li><p>雷米小说全集（侦探类）</p></li><li><p>网络小说：</p><ul><li>诛仙，</li><li>盗墓笔记，</li><li>泡沫之夏，</li><li>芈月传，</li></ul></li></ul><h2 id="编程类"><a href="#编程类" class="headerlink" title="编程类"></a>编程类</h2><ul><li><p>java编程思想</p></li><li><p>effective java</p></li><li><p>java并发编程的艺术</p></li><li><p>代码整洁之道</p></li><li><p>黑客与画家</p></li><li><p>深入浅出nodejs</p></li><li><p>nodejs实战</p></li><li><p>js高级程序设计</p></li><li><p>survivejs</p></li><li><p>redux和react中文手册</p></li><li><p>你不知道的javascript</p></li><li><p>算法javascript实现</p></li><li><p>mysql权威指南</p></li><li><p>mongodb权威指南</p></li><li><p>mongodb实战第二版</p></li><li><p>redis入门</p></li></ul><h2 id="经管励志"><a href="#经管励志" class="headerlink" title="经管励志"></a>经管励志</h2><ul><li>时间管理</li><li>一分钟系列</li><li>番茄工作法图解</li></ul>]]></content>
      
      
      <categories>
          
          <category> book </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>免费的开源书籍</title>
      <link href="/free-books.html"/>
      <url>/free-books.html</url>
      
        <content type="html"><![CDATA[<p>** 免费的开源书籍：** &lt;Excerpt in index | 首页摘要&gt;<br>国外程序员在 stackoverflow 推荐的程序员必读书籍，中文版。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><a href="#语言无关">语言无关</a><ul><li><a href="#ide">IDE</a></li><li><a href="#mysql">MySQL</a></li><li><a href="#nosql">NoSQL</a></li><li><a href="#postgresql">PostgreSQL</a></li><li><a href="#web">Web</a></li><li><a href="#web服务器">WEB服务器</a></li><li><a href="#其它">其它</a></li><li><a href="#函数式概念">函数式概念</a></li><li><a href="#分布式系统">分布式系统</a></li><li><a href="#在线教育">在线教育</a></li><li><a href="#大数据">大数据</a></li><li><a href="#操作系统">操作系统</a></li><li><a href="#数据库">数据库</a></li><li><a href="#智能系统">智能系统</a></li><li><a href="#正则表达式">正则表达式</a></li><li><a href="#版本控制">版本控制</a></li><li><a href="#程序员杂谈">程序员杂谈</a></li><li><a href="#管理和监控">管理和监控</a></li><li><a href="#编程艺术">编程艺术</a></li><li><a href="#编译原理">编译原理</a></li><li><a href="#编辑器">编辑器</a></li><li><a href="#计算机图形学">计算机图形学</a></li><li><a href="#设计模式">设计模式</a></li><li><a href="#软件开发方法">软件开发方法</a></li><li><a href="#项目相关">项目相关</a></li></ul></li><li><a href="#语言相关">语言相关</a><ul><li><a href="#android">Android</a></li><li><a href="#awk">AWK</a></li><li><a href="#c">C</a></li><li><a href="#c-sharp">C#</a></li><li><a href="#c-1">C++</a></li><li><a href="#coffeescript">CoffeeScript</a></li><li><a href="#dart">Dart</a></li><li><a href="#elasticsearch">Elasticsearch</a></li><li><a href="#elixir">Elixir</a></li><li><a href="#erlang">Erlang</a></li><li><a href="#fortran">Fortran</a></li><li><a href="#golang">Golang</a></li><li><a href="#haskell">Haskell</a></li><li><a href="#html--css">HTML / CSS</a></li><li><a href="#http">HTTP</a></li><li><a href="#ios">iOS</a></li><li><a href="#java">Java</a></li><li><a href="#javascript">JavaScript</a></li><li><a href="#latex">LaTeX</a></li><li><a href="#lisp">LISP</a></li><li><a href="#lua">Lua</a></li><li><a href="#markdown">Markdown</a></li><li><a href="#nodejs">Node.js</a></li><li><a href="#perl">Perl</a></li><li><a href="#php">PHP</a></li><li><a href="#python">Python</a></li><li><a href="#r">R</a></li><li><a href="#restructuredtext">reStructuredText</a></li><li><a href="#ruby">Ruby</a></li><li><a href="#rust">Rust</a></li><li><a href="#scala">Scala</a></li><li><a href="#scheme">Scheme</a></li><li><a href="#shell">Shell</a></li><li><a href="#swift">Swift</a></li><li><a href="#vim">Vim</a></li><li><a href="#visual-prolog">Visual Prolog</a></li></ul></li></ul><h2 id="语言无关"><a href="#语言无关" class="headerlink" title="语言无关"></a>语言无关</h2><h2 id="IDE"><a href="#IDE" class="headerlink" title="IDE"></a>IDE</h2><ul><li><a href="https://github.com/judasn/IntelliJ-IDEA-Tutorial" target="_blank" rel="noopener">IntelliJ IDEA 简体中文专题教程</a></li></ul><h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><ul><li><a href="http://www.cnblogs.com/mr-wid/archive/2013/05/09/3068229.html" target="_blank" rel="noopener">21分钟MySQL入门教程</a></li><li><a href="http://blog.codinglabs.org/articles/theory-of-mysql-index.html" target="_blank" rel="noopener">MySQL索引背后的数据结构及算法原理</a></li></ul><h2 id="NoSQL"><a href="#NoSQL" class="headerlink" title="NoSQL"></a>NoSQL</h2><ul><li><a href="http://disquebook.com" target="_blank" rel="noopener">Disque 使用教程</a></li><li><a href="http://neo4j.tw" target="_blank" rel="noopener">Neo4j .rb 中文資源</a></li><li><a href="http://docs.neo4j.org.cn" target="_blank" rel="noopener">Neo4j 简体中文手册 v1.8</a></li><li><a href="http://redisdoc.com" target="_blank" rel="noopener">Redis 命令参考</a></li><li><a href="http://redisbook.com" target="_blank" rel="noopener">Redis 设计与实现</a></li><li><a href="https://github.com/justinyhuang/the-little-mongodb-book-cn/blob/master/mongodb.md" target="_blank" rel="noopener">The Little MongoDB Book</a></li><li><a href="https://github.com/JasonLai256/the-little-redis-book/blob/master/cn/redis.md" target="_blank" rel="noopener">The Little Redis Book</a></li><li><a href="https://github.com/huangz1990/annotated_redis_source" target="_blank" rel="noopener">带有详细注释的 Redis 2.6 代码</a></li><li><a href="https://github.com/huangz1990/redis-3.0-annotated" target="_blank" rel="noopener">带有详细注释的 Redis 3.0 代码</a></li></ul><h2 id="PostgreSQL"><a href="#PostgreSQL" class="headerlink" title="PostgreSQL"></a>PostgreSQL</h2><ul><li><a href="http://works.jinbuguo.com/postgresql/menu823/index.html" target="_blank" rel="noopener">PostgreSQL 8.2.3 中文文档</a></li><li><a href="http://www.postgres.cn/docs/9.3/index.html" target="_blank" rel="noopener">PostgreSQL 9.3.1 中文文档</a></li></ul><h2 id="Web"><a href="#Web" class="headerlink" title="Web"></a>Web</h2><ul><li><a href="https://www.gitbook.com/book/juntao/3-web-designs-in-3-weeks/details" target="_blank" rel="noopener">3 Web Designs in 3 Weeks</a></li><li><a href="https://github.com/CN-Chrome-DevTools/CN-Chrome-DevTools" target="_blank" rel="noopener">Chrome 开发者工具中文手册</a></li><li><a href="http://open.chrome.360.cn/extension_dev/overview.html" target="_blank" rel="noopener">Chrome扩展开发文档</a></li><li><a href="https://github.com/phodal/growth-ebook" target="_blank" rel="noopener">Growth: 全栈增长工程师指南</a></li><li><a href="http://www.gruntjs.net" target="_blank" rel="noopener">Grunt中文文档</a></li><li><a href="https://github.com/nimojs/gulp-book" target="_blank" rel="noopener">Gulp 入门指南</a></li><li><a href="http://www.gulpjs.com.cn/docs/" target="_blank" rel="noopener">gulp中文文档</a></li><li><a href="https://github.com/bolasblack/http-api-guide" target="_blank" rel="noopener">HTTP 接口设计指北</a></li><li><a href="http://yuedu.baidu.com/ebook/478d1a62376baf1ffc4fad99?pn=1" target="_blank" rel="noopener">HTTP/2.0 中文翻译</a></li><li><a href="https://www.gitbook.com/book/ye11ow/http2-explained/details" target="_blank" rel="noopener">http2讲解</a></li><li><a href="https://github.com/darcyliu/google-styleguide/blob/master/JSONStyleGuide.md" target="_blank" rel="noopener">JSON风格指南</a></li><li><a href="http://man.lupaworld.com/content/network/wireshark/index.html" target="_blank" rel="noopener">Wireshark用户手册</a></li><li><a href="https://community.emc.com/thread/194901" target="_blank" rel="noopener">一站式学习Wireshark</a></li><li><a href="http://www.20thingsilearned.com/zh-CN/home" target="_blank" rel="noopener">关于浏览器和网络的 20 项须知</a></li><li><a href="http://coderlmn.github.io/code-standards/" target="_blank" rel="noopener">前端代码规范 及 最佳实践</a></li><li><a href="https://github.com/fouber/blog/issues/2" target="_blank" rel="noopener">前端开发体系建设日记</a></li><li><a href="https://github.com/hacke2/hacke2.github.io/issues/1" target="_blank" rel="noopener">前端资源分享（一）</a></li><li><a href="https://github.com/hacke2/hacke2.github.io/issues/3" target="_blank" rel="noopener">前端资源分享（二）</a></li><li><a href="http://deerchao.net/tutorials/regex/regex.htm" target="_blank" rel="noopener">正则表达式30分钟入门教程</a></li><li><a href="http://jinlong.github.io/2013/08/29/devtoolsecrets/" target="_blank" rel="noopener">浏览器开发工具的秘密</a></li><li><a href="https://github.com/AlloyTeam/Mars" target="_blank" rel="noopener">移动Web前端知识库</a></li><li><a href="https://github.com/hoosin/mobile-web-favorites" target="_blank" rel="noopener">移动前端开发收藏夹</a></li></ul><h2 id="WEB服务器"><a href="#WEB服务器" class="headerlink" title="WEB服务器"></a>WEB服务器</h2><ul><li><a href="http://works.jinbuguo.com/apache/menu22/index.html" target="_blank" rel="noopener">Apache 中文手册</a></li><li><a href="http://tengine.taobao.org/book/index.html" target="_blank" rel="noopener">Nginx开发从入门到精通</a> (淘宝团队出品)</li><li><a href="http://www.ttlsa.com/nginx/nginx-stu-pdf/" target="_blank" rel="noopener">Nginx教程从入门到精通</a> (PDF版本，运维生存时间出品)</li></ul><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><ul><li><a href="https://softwaredownload.gitbooks.io/openwrt-fanqiang/content/" target="_blank" rel="noopener">OpenWrt智能、自动、透明翻墙路由器教程</a></li><li><a href="https://community.emc.com/docs/DOC-16067" target="_blank" rel="noopener">SAN 管理入门系列</a></li><li><a href="http://sketchcn.com/sketch-chinese-user-manual.html#introduce" target="_blank" rel="noopener">Sketch 中文手册</a></li><li><a href="http://ifeve.com/perfbook/" target="_blank" rel="noopener">深入理解并行编程</a></li></ul><h2 id="函数式概念"><a href="#函数式概念" class="headerlink" title="函数式概念"></a>函数式概念</h2><ul><li><a href="https://github.com/justinyhuang/Functional-Programming-For-The-Rest-of-Us-Cn" target="_blank" rel="noopener">傻瓜函数编程</a></li></ul><h2 id="分布式系统"><a href="#分布式系统" class="headerlink" title="分布式系统"></a>分布式系统</h2><ul><li><a href="http://dcaoyuan.github.io/papers/pdfs/Scalability.pdf" target="_blank" rel="noopener">走向分布式</a> (PDF)</li></ul><h2 id="在线教育"><a href="#在线教育" class="headerlink" title="在线教育"></a>在线教育</h2><ul><li><a href="http://edu.51cto.com" target="_blank" rel="noopener">51CTO学院</a></li><li><a href="https://www.codecademy.com/?locale_code=zh" target="_blank" rel="noopener">Codecademy</a></li><li><a href="https://www.codeschool.com" target="_blank" rel="noopener">CodeSchool</a></li><li><a href="https://www.coursera.org/courses?orderby=upcoming&lngs=zh" target="_blank" rel="noopener">Coursera</a></li><li><a href="https://learnxinyminutes.com" target="_blank" rel="noopener">Learn X in Y minutes</a> (数十种语言快速入门教程)</li><li><a href="https://www.shiyanlou.com" target="_blank" rel="noopener">shiyanlou</a></li><li><a href="https://teamtreehouse.com" target="_blank" rel="noopener">TeamTreeHouse</a></li><li><a href="https://www.udacity.com" target="_blank" rel="noopener">Udacity</a></li><li><a href="https://www.xuetangx.com" target="_blank" rel="noopener">xuetangX</a></li><li><a href="http://www.imooc.com/course/list" target="_blank" rel="noopener">慕课网</a> (丰富的移动端开发、php开发、web前端、html5教程以及css3视频教程等课程资源)</li><li><a href="http://www.jikexueyuan.com" target="_blank" rel="noopener">极客学院</a></li><li><a href="http://www.jisuanke.com" target="_blank" rel="noopener">计蒜客</a></li></ul><h2 id="大数据"><a href="#大数据" class="headerlink" title="大数据"></a>大数据</h2><ul><li><a href="https://aiyanbo.gitbooks.io/spark-programming-guide-zh-cn/content/" target="_blank" rel="noopener">Spark 编程指南简体中文版</a></li><li><a href="https://code.csdn.net/CODE_Translation/spark_matei_phd" target="_blank" rel="noopener">大型集群上的快速和通用数据处理架构</a></li><li><a href="https://github.com/Flowerowl/Big-Data-Resources" target="_blank" rel="noopener">大数据/数据挖掘/推荐系统/机器学习相关资源</a></li><li><a href="https://github.com/linyiqun/DataMiningAlgorithm" target="_blank" rel="noopener">数据挖掘中经典的算法实现和详细的注释</a></li><li><a href="http://dataminingguide.books.yourtion.com" target="_blank" rel="noopener">面向程序员的数据挖掘指南</a></li></ul><h2 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h2><ul><li><a href="http://man.chinaunix.net/linux/debian/reference/reference.zh-cn.html" target="_blank" rel="noopener">Debian 参考手册 </a></li><li><a href="https://github.com/yeasy/docker_practice" target="_blank" rel="noopener">Docker —— 从入门到实践</a></li><li><a href="https://github.com/widuu/chinese_docker" target="_blank" rel="noopener">Docker中文指南</a></li><li><a href="http://yuedu.baidu.com/ebook/d817967416fc700abb68fca1" target="_blank" rel="noopener">Docker入门实战</a></li><li><a href="http://www.freebsd.org/doc/zh_CN.UTF-8/books/handbook/" target="_blank" rel="noopener">FreeBSD 使用手册</a></li><li><a href="http://freeradius.akagi201.org" target="_blank" rel="noopener">FreeRADIUS新手入门</a></li><li><a href="https://tinylab.gitbooks.io/linux-doc/content/zh-cn/" target="_blank" rel="noopener">Linux Documentation (中文版)</a></li><li><a href="http://happypeter.github.io/LGCB/book/" target="_blank" rel="noopener">Linux Guide for Complete Beginners</a></li><li><a href="http://works.jinbuguo.com/lfs/lfs62/index.html" target="_blank" rel="noopener">Linux 构建指南</a></li><li><a href="http://sourceforge.net/projects/elpi/" target="_blank" rel="noopener">Linux 系统高级编程</a></li><li><a href="https://github.com/me115/linuxtools_rst" target="_blank" rel="noopener">Linux工具快速教程</a></li><li><a href="https://aaaaaashu.gitbooks.io/mac-dev-setup/content/" target="_blank" rel="noopener">Mac 开发配置手册</a></li><li><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/" target="_blank" rel="noopener">Operating Systems: Three Easy Pieces</a></li><li><a href="http://billie66.github.io/TLCL/index.html" target="_blank" rel="noopener">The Linux Command Line</a> (中英文版)</li><li><a href="http://wiki.ubuntu.org.cn/UbuntuManual" target="_blank" rel="noopener">Ubuntu 参考手册 </a></li><li><a href="https://www.gitbook.com/book/objectkuan/ucore-docs/details" target="_blank" rel="noopener">uCore Lab: Operating System Course in Tsinghua University</a></li><li><a href="http://cb.vu/unixtoolbox_zh_CN.xhtml" target="_blank" rel="noopener">UNIX TOOLBOX</a></li><li><a href="https://github.com/jlevy/the-art-of-command-line/blob/master/README-zh.md" target="_blank" rel="noopener">命令行的艺术</a></li><li><a href="https://tinylab.gitbooks.io/elinux/content/zh/" target="_blank" rel="noopener">嵌入式 Linux 知识库 (eLinux.org 中文版)</a></li><li><a href="http://i.linuxtoy.org/docs/guide/index.html" target="_blank" rel="noopener">开源世界旅行手册</a></li><li><a href="http://www.kerneltravel.net/kernel-book/%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90Linux%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81.html" target="_blank" rel="noopener">深入分析Linux内核源码</a></li><li><a href="https://github.com/tobegit3hub/understand_linux_process" target="_blank" rel="noopener">理解Linux进程</a></li><li><a href="http://vbird.dic.ksu.edu.tw/linux_basic/linux_basic.php" target="_blank" rel="noopener">鸟哥的 Linux 私房菜 基础学习篇</a></li><li><a href="http://vbird.dic.ksu.edu.tw/linux_server/" target="_blank" rel="noopener">鸟哥的 Linux 私房菜 服务器架设篇</a></li></ul><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><ul><li><a href="http://redisbook.com" target="_blank" rel="noopener">Redis 设计与实现</a></li><li><a href="https://github.com/justinyhuang/the-little-mongodb-book-cn" target="_blank" rel="noopener">The Little MongoDB Book 中文版</a></li></ul><h2 id="智能系统"><a href="#智能系统" class="headerlink" title="智能系统"></a>智能系统</h2><ul><li><a href="https://github.com/phodal/designiot" target="_blank" rel="noopener">一步步搭建物联网系统</a></li></ul><h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><ul><li><a href="http://deerchao.net/tutorials/regex/regex.htm" target="_blank" rel="noopener">正则表达式30分钟入门教程</a></li></ul><h2 id="版本控制"><a href="#版本控制" class="headerlink" title="版本控制"></a>版本控制</h2><ul><li><a href="http://rogerdudler.github.io/git-guide/index.zh.html" target="_blank" rel="noopener">Git - 简易指南</a></li><li><a href="https://github.com/flyhigher139/Git-Cheat-Sheet" target="_blank" rel="noopener">Git-Cheat-Sheet</a> （感谢 @flyhigher139 翻译了中文版）</li><li><a href="http://gitbook.liuhui998.com" target="_blank" rel="noopener">Git Community Book 中文版</a></li><li><a href="http://danielkummer.github.io/git-flow-cheatsheet/index.zh_CN.html" target="_blank" rel="noopener">git-flow 备忘清单</a></li><li><a href="http://www-cs-students.stanford.edu/~blynn/gitmagic/intl/zh_cn/" target="_blank" rel="noopener">Git magic</a></li><li><a href="http://www-cs-students.stanford.edu/~blynn/gitmagic/intl/zh_cn/" target="_blank" rel="noopener">Git Magic</a></li><li><a href="http://gitref.justjavac.com" target="_blank" rel="noopener">Git 参考手册</a></li><li><a href="https://github.com/waylau/github-help" target="_blank" rel="noopener">Github帮助文档</a></li><li><a href="https://snowdream86.gitbooks.io/github-cheat-sheet/content/zh/" target="_blank" rel="noopener">GitHub秘籍</a></li><li><a href="http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">Git教程</a> （本文由 <a href="http://weibo.com/liaoxuefeng" target="_blank" rel="noopener">@廖雪峰</a> 创作，如果觉得本教程对您有帮助，可以去 <a href="https://itunes.apple.com/cn/app/git-jiao-cheng/id876420437" target="_blank" rel="noopener">iTunes</a> 购买）</li><li><a href="https://github.com/gotgit/gotgithub" target="_blank" rel="noopener">Got GitHub</a></li><li><a href="http://www.worldhello.net/gotgithub/index.html" target="_blank" rel="noopener">GotGitHub</a></li><li><a href="http://bucunzai.net/hginit/" target="_blank" rel="noopener">HgInit (中文版)</a></li><li><a href="https://www.mercurial-scm.org/wiki/ChineseTutorial" target="_blank" rel="noopener">Mercurial 使用教程</a></li><li><a href="https://git-scm.com/book/zh/v2" target="_blank" rel="noopener">Pro Git</a></li><li><a href="https://www.gitbook.com/book/0532/progit/details" target="_blank" rel="noopener">Pro Git 中文版</a> (整理在gitbook上)</li><li><a href="http://svnbook.red-bean.com/nightly/zh/index.html" target="_blank" rel="noopener">svn 手册</a></li><li><a href="http://pcottle.github.io/learnGitBranching/" target="_blank" rel="noopener">学习 Git 分支</a> (点击右下角按钮可切换至简体及正体中文)</li><li><a href="http://igit.linuxtoy.org/index.html" target="_blank" rel="noopener">沉浸式学 Git</a></li><li><a href="http://backlogtool.com/git-guide/cn/" target="_blank" rel="noopener">猴子都能懂的GIT入门</a></li></ul><h2 id="程序员杂谈"><a href="#程序员杂谈" class="headerlink" title="程序员杂谈"></a>程序员杂谈</h2><ul><li><a href="http://www.kancloud.cn/kancloud/a-programmer-prepares" target="_blank" rel="noopener">程序员的自我修养</a></li></ul><h2 id="管理和监控"><a href="#管理和监控" class="headerlink" title="管理和监控"></a>管理和监控</h2><ul><li><a href="https://www.gitbook.com/book/fuxiaopang/learnelasticsearch/details" target="_blank" rel="noopener">ElasticSearch 权威指南</a></li><li><a href="http://es.xiaoleilu.com" target="_blank" rel="noopener">Elasticsearch 权威指南（中文版）</a></li><li><a href="http://kibana.logstash.es" target="_blank" rel="noopener">ELKstack 中文指南</a></li><li><a href="https://github.com/chenryn/logstash-best-practice-cn" target="_blank" rel="noopener">Logstash 最佳实践</a></li><li><a href="http://udn.yyuap.com/doc/mastering-elasticsearch/" target="_blank" rel="noopener">Mastering Elasticsearch(中文版)</a></li><li><a href="http://bbs.konotes.org/workdoc/puppet-27/" target="_blank" rel="noopener">Puppet 2.7 Cookbook 中文版</a></li></ul><h2 id="编程艺术"><a href="#编程艺术" class="headerlink" title="编程艺术"></a>编程艺术</h2><ul><li><a href="http://read.douban.com/ebook/4972883/" target="_blank" rel="noopener">取悦的工序：如何理解游戏</a> (豆瓣阅读，免费书籍)</li><li><a href="http://www.oschina.net/translate/what-every-programmer-should-know-about-memory-part1?print" target="_blank" rel="noopener">每个程序员都应该了解的内存知识(译)</a>【第一部分】</li><li><a href="https://github.com/julycoding/The-Art-Of-Programming-by-July" target="_blank" rel="noopener">程序员编程艺术</a></li><li><a href="http://www.kancloud.cn/kancloud/intro-to-prog/52592" target="_blank" rel="noopener">编程入门指南</a></li></ul><h2 id="编译原理"><a href="#编译原理" class="headerlink" title="编译原理"></a>编译原理</h2><ul><li><a href="https://github.com/DeathKing/Learning-SICP" target="_blank" rel="noopener">《计算机程序的结构和解释》公开课 翻译项目</a></li></ul><h2 id="编辑器"><a href="#编辑器" class="headerlink" title="编辑器"></a>编辑器</h2><ul><li><a href="http://exvim.github.io/docs-zh/intro/" target="_blank" rel="noopener">exvim–vim 改良成IDE项目</a></li><li><a href="https://github.com/vimcn/vimcdoc" target="_blank" rel="noopener">Vim中文文档</a></li><li><a href="https://github.com/yangyangwithgnu/use_vim_as_ide" target="_blank" rel="noopener">所需即所获：像 IDE 一样使用 vim</a></li><li><a href="http://learnvimscriptthehardway.onefloweroneworld.com" target="_blank" rel="noopener">笨方法学Vimscript 中译本</a></li></ul><h2 id="计算机图形学"><a href="#计算机图形学" class="headerlink" title="计算机图形学"></a>计算机图形学</h2><ul><li><a href="https://github.com/zilongshanren/opengl-tutorials" target="_blank" rel="noopener">OpenGL 教程</a></li></ul><h2 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h2><ul><li><a href="http://blog.csdn.net/lovelion/article/details/17517213" target="_blank" rel="noopener">史上最全设计模式导学目录</a></li><li><a href="https://github.com/me115/design_patterns" target="_blank" rel="noopener">图说设计模式</a></li></ul><h2 id="软件开发方法"><a href="#软件开发方法" class="headerlink" title="软件开发方法"></a>软件开发方法</h2><ul><li><a href="https://github.com/justinyhuang/Functional-Programming-For-The-Rest-of-Us-Cn" target="_blank" rel="noopener">傻瓜函数编程</a> (《Functional Programming For The Rest of Us》中文版)</li><li><a href="http://www.infoq.com/cn/minibooks/scrum-xp-from-the-trenches" target="_blank" rel="noopener">硝烟中的 Scrum 和 XP</a></li></ul><h2 id="项目相关"><a href="#项目相关" class="headerlink" title="项目相关"></a>项目相关</h2><ul><li><a href="http://docs.huihoo.com/gnu/linux/gmake.html" target="_blank" rel="noopener">GNU make 指南</a></li><li><a href="https://github.com/waylau/Gradle-2-User-Guide" target="_blank" rel="noopener">Gradle 2 用户指南</a></li><li><a href="http://yuedu.baidu.com/ebook/f23af265998fcc22bcd10da2" target="_blank" rel="noopener">Gradle 中文使用文档</a></li><li><a href="http://local.joelonsoftware.com/wiki/Chinese_(Simplified)" target="_blank" rel="noopener">Joel谈软件</a></li><li><a href="https://github.com/fool2fish/selenium-doc" target="_blank" rel="noopener">selenium 中文文档</a></li><li><a href="http://www.ituring.com.cn/book/1143" target="_blank" rel="noopener">开源软件架构</a></li><li><a href="http://article.yeeyan.org/view/2251/94882" target="_blank" rel="noopener">持续集成（第二版）</a> (译言网)</li><li><a href="http://local.joelonsoftware.com/wiki/%E9%A6%96%E9%A0%81" target="_blank" rel="noopener">約耳談軟體(Joel on Software)</a></li><li><a href="https://github.com/ecomfe/spec" target="_blank" rel="noopener">编码规范</a></li><li><a href="http://www.ibm.com/developerworks/cn/java/j-ap/" target="_blank" rel="noopener">让开发自动化系列专栏</a></li><li><a href="http://www.ibm.com/developerworks/cn/java/j-cq/" target="_blank" rel="noopener">追求代码质量</a></li></ul><h2 id="语言相关"><a href="#语言相关" class="headerlink" title="语言相关"></a>语言相关</h2><h2 id="Android"><a href="#Android" class="headerlink" title="Android"></a>Android</h2><ul><li><a href="http://www.apkbus.com/design/index.html" target="_blank" rel="noopener">Android Design(中文版)</a></li><li><a href="https://github.com/CharonChui/AndroidNote" target="_blank" rel="noopener">Android Note(开发过程中积累的知识点)</a></li><li><a href="http://leanote.com/blog/post/561658f938f41126b2000298?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io" target="_blank" rel="noopener">Android6.0新特性详解</a></li><li><a href="http://stormzhang.github.io/android/2014/07/07/learn-android-from-rookie/" target="_blank" rel="noopener">Android学习之路</a></li><li><a href="https://github.com/bboyfeiyu/android-tech-frontier" target="_blank" rel="noopener">Android开发技术前线(android-tech-frontier)</a></li><li><a href="http://hukai.me/android-training-course-in-chinese/index.html" target="_blank" rel="noopener">Google Android官方培训课程中文版</a></li><li>Google Material Design 正體中文版 (<a href="https://wcc723.gitbooks.io/google_design_translate/content/style-icons.html" target="_blank" rel="noopener">译本一</a> <a href="https://github.com/1sters/material_design_zh" target="_blank" rel="noopener">译本二</a>)</li><li><a href="http://wiki.jikexueyuan.com/project/material-design/" target="_blank" rel="noopener">Material Design 中文版</a></li><li><a href="https://github.com/FX-Max/Point-of-Android" target="_blank" rel="noopener">Point-of-Android</a> Android 一些重要知识点解析整理</li></ul><h2 id="AWK"><a href="#AWK" class="headerlink" title="AWK"></a>AWK</h2><ul><li><a href="http://awk.readthedocs.org/en/latest/index.html" target="_blank" rel="noopener">awk中文指南</a></li><li><a href="https://github.com/wuzhouhui/awk" target="_blank" rel="noopener">awk程序设计语言</a></li></ul><h2 id="C"><a href="#C" class="headerlink" title="C"></a>C</h2><ul><li><a href="http://c-faq-chn.sourceforge.net/ccfaq/ccfaq.html" target="_blank" rel="noopener">C 语言常见问题集</a></li><li><a href="http://doc.lellansin.com" target="_blank" rel="noopener">C/C++ 学习教程</a></li><li><a href="http://docs.linuxtone.org/ebooks/C&CPP/c/" target="_blank" rel="noopener">Linux C 编程一站式学习</a></li><li><a href="https://github.com/limingth/NCCL" target="_blank" rel="noopener">新概念 C 语言教程</a></li></ul><h2 id="C-Sharp"><a href="#C-Sharp" class="headerlink" title="C Sharp"></a>C Sharp</h2><ul><li><a href="http://book.douban.com/subject/24827879/" target="_blank" rel="noopener">精通C#(第6版) </a></li></ul><h2 id="C-1"><a href="#C-1" class="headerlink" title="C++"></a>C++</h2><ul><li><a href="https://github.com/hellogcc/100-gcc-tips/blob/master/src/index.md" target="_blank" rel="noopener">100个gcc小技巧</a></li><li><a href="https://github.com/hellogcc/100-gdb-tips/blob/master/src/index.md" target="_blank" rel="noopener">100个gdb小技巧</a></li><li><a href="https://tinylab.gitbooks.io/cbook/content/" target="_blank" rel="noopener">C 语言编程透视</a></li><li><a href="https://github.com/andycai/cprimer" target="_blank" rel="noopener">C/C++ Primer</a> - @andycai</li><li><a href="http://www.sunistudio.com/cppfaq/" target="_blank" rel="noopener">C++ FAQ LITE(中文版)</a></li><li><a href="https://github.com/Mooophy/Cpp-Primer" target="_blank" rel="noopener">C++ Primer 5th Answers</a></li><li><a href="https://github.com/wuye9036/CppTemplateTutorial" target="_blank" rel="noopener">C++ Template 进阶指南</a></li><li><a href="http://www.prglab.com/cms/" target="_blank" rel="noopener">C++ 基础教程</a></li><li><a href="https://chenxiaowei.gitbooks.io/cpp_concurrency_in_action/content/" target="_blank" rel="noopener">C++ 并发编程(基于C++11)</a></li><li><a href="https://github.com/forhappy/Cplusplus-Concurrency-In-Practice" target="_blank" rel="noopener">C++ 并发编程指南</a></li><li><a href="https://github.com/leeyiw/cgdb-manual-in-chinese" target="_blank" rel="noopener">CGDB中文手册</a></li><li><a href="http://sewm.pku.edu.cn/src/paradise/reference/CMake%20Practice.pdf" target="_blank" rel="noopener">Cmake 实践</a> (PDF版)</li><li><a href="http://docs.huihoo.com/gnu/linux/gmake.html" target="_blank" rel="noopener">GNU make 指南</a></li><li><a href="http://zh-google-styleguide.readthedocs.org/en/latest/google-cpp-styleguide/contents/" target="_blank" rel="noopener">Google C++ 风格指南</a></li><li><a href="http://www.kuqin.com/qtdocument/tutorial.html" target="_blank" rel="noopener">QT 教程</a></li><li><a href="https://github.com/anjuke/zguide-cn" target="_blank" rel="noopener">ZMQ 指南</a></li><li><a href="http://www.ituring.com.cn/book/1203" target="_blank" rel="noopener">像计算机科学家一样思考（C++版)</a> (《How To Think Like a Computer Scientist: C++ Version》中文版)</li><li><a href="http://www.nowamagic.net/librarys/books/contents/c" target="_blank" rel="noopener">简单易懂的C魔法</a></li><li><a href="http://scc.qibebt.cas.cn/docs/linux/base/%B8%FA%CE%D2%D2%BB%C6%F0%D0%B4Makefile-%B3%C2%F0%A9.pdf" target="_blank" rel="noopener">跟我一起写Makefile(PDF)</a> (PDF)</li></ul><h2 id="CoffeeScript"><a href="#CoffeeScript" class="headerlink" title="CoffeeScript"></a>CoffeeScript</h2><ul><li><a href="http://coffee-script.org" target="_blank" rel="noopener">CoffeeScript 中文</a></li><li><a href="https://github.com/elrrrrrrr/coffeescript-style-guide/blob/master/README-ZH.md" target="_blank" rel="noopener">CoffeeScript 编程风格指南</a></li></ul><h2 id="Dart"><a href="#Dart" class="headerlink" title="Dart"></a>Dart</h2><ul><li><a href="http://dart.lidian.info/wiki/Language_Tour" target="_blank" rel="noopener">Dart 语言导览</a></li></ul><h2 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h2><ul><li><a href="https://github.com/looly/elasticsearch-definitive-guide-cn" target="_blank" rel="noopener">Elasticsearch 权威指南</a> （《Elasticsearch the definitive guide》中文版）</li><li><a href="http://kibana.logstash.es" target="_blank" rel="noopener">ELKstack 中文指南</a></li><li><a href="http://udn.yyuap.com/doc/mastering-elasticsearch/" target="_blank" rel="noopener">Mastering Elasticsearch(中文版)</a></li></ul><h2 id="Elixir"><a href="#Elixir" class="headerlink" title="Elixir"></a>Elixir</h2><ul><li><a href="https://github.com/Ljzn/ElixrGettingStartedChinese" target="_blank" rel="noopener">Elixir Getting Started 中文翻译</a></li><li><a href="https://elixirschool.com/cn/" target="_blank" rel="noopener">Elixir 编程语言教程</a> (Elixir School)</li><li><a href="https://github.com/Ljzn/MetaProgrammingInElixirChinese" target="_blank" rel="noopener">Elixir元编程与DSL 中文翻译</a></li><li><a href="https://mydearxym.gitbooks.io/phoenix-doc-in-chinese/content/" target="_blank" rel="noopener">Phoenix 框架中文文档</a></li></ul><h2 id="Erlang"><a href="#Erlang" class="headerlink" title="Erlang"></a>Erlang</h2><ul><li><a href="https://github.com/liancheng/cpie-cn" target="_blank" rel="noopener">Erlang 并发编程</a> (《Concurrent Programming in Erlang (Part I)》中文版)</li></ul><h2 id="Fortran"><a href="#Fortran" class="headerlink" title="Fortran"></a>Fortran</h2><ul><li><a href="http://micro.ustc.edu.cn/Fortran/ZJDing/" target="_blank" rel="noopener">Fortran77和90/95编程入门</a></li></ul><h2 id="Golang"><a href="#Golang" class="headerlink" title="Golang"></a>Golang</h2><ul><li><a href="http://www.hellogcc.org/effective_go.html" target="_blank" rel="noopener">Effective Go</a></li><li><a href="https://github.com/astaxie/build-web-application-with-golang" target="_blank" rel="noopener">Go Web 编程</a></li><li><a href="https://github.com/Unknwon/the-way-to-go_ZH_CN" target="_blank" rel="noopener">Go 入门指南</a> (《The Way to Go》中文版)</li><li><a href="https://github.com/golang-china/golangdoc.translations" target="_blank" rel="noopener">Go 官方文档翻译</a></li><li><a href="http://go-tour-zh.appsp0t.com" target="_blank" rel="noopener">Go 指南</a> (《A Tour of Go》中文版)</li><li><a href="https://github.com/songleo/the-little-go-book_ZH_CN" target="_blank" rel="noopener">Go 简易教程</a> (《<a href="https://github.com/karlseguin/the-little-go-book" target="_blank" rel="noopener">The Little Go Book</a>》中文版)</li><li><a href="https://github.com/Unknwon/go-fundamental-programming" target="_blank" rel="noopener">Go 编程基础</a></li><li><a href="https://github.com/polaris1119/The-Golang-Standard-Library-by-Example" target="_blank" rel="noopener">Go 语言标准库</a></li><li><a href="https://github.com/hyper-carrot/go_command_tutorial" target="_blank" rel="noopener">Go命令教程</a></li><li><a href="https://github.com/astaxie/Go-in-Action" target="_blank" rel="noopener">Go实战开发</a></li><li><a href="https://github.com/achun/Go-Blog-In-Action" target="_blank" rel="noopener">Go语言博客实践</a></li><li><a href="http://blog.csdn.net/dc_726/article/details/46565241" target="_blank" rel="noopener">Java程序员的Golang入门指南</a></li><li><a href="https://github.com/astaxie/NPWG_zh" target="_blank" rel="noopener">Network programming with Go 中文翻译版本</a></li><li><a href="http://gorevel.cn/docs/manual/index.html" target="_blank" rel="noopener">Revel 框架手册</a></li><li><a href="http://mikespook.com/learning-go/" target="_blank" rel="noopener">学习Go语言</a></li></ul><h2 id="Groovy"><a href="#Groovy" class="headerlink" title="Groovy"></a>Groovy</h2><ul><li><a href="http://www.ibm.com/developerworks/cn/java/j-pg/" target="_blank" rel="noopener">实战 Groovy 系列</a></li></ul><h2 id="Haskell"><a href="#Haskell" class="headerlink" title="Haskell"></a>Haskell</h2><ul><li><a href="http://learnyoua.haskell.sg" target="_blank" rel="noopener">Haskell 趣学指南</a></li><li><a href="http://cnhaskell.com" target="_blank" rel="noopener">Real World Haskell 中文版</a></li></ul><h2 id="HTML-CSS"><a href="#HTML-CSS" class="headerlink" title="HTML / CSS"></a>HTML / CSS</h2><ul><li><a href="https://github.com/waylau/css3-tutorial" target="_blank" rel="noopener">CSS3 Tutorial 《CSS3 教程》</a></li><li><a href="http://css.doyoe.com" target="_blank" rel="noopener">CSS参考手册</a></li><li><a href="http://yanxyz.github.io/emmet-docs/" target="_blank" rel="noopener">Emmet 文档</a></li><li><a href="http://www.w3school.com.cn/html5/" target="_blank" rel="noopener">HTML5 教程</a></li><li><a href="http://codeguide.bootcss.com" target="_blank" rel="noopener">HTML和CSS编码规范</a></li><li><a href="http://sass-guidelin.es/zh/" target="_blank" rel="noopener">Sass Guidelines 中文</a></li><li><a href="http://alloyteam.github.io/CodeGuide/" target="_blank" rel="noopener">前端代码规范</a> (腾讯 AlloyTeam 团队)</li><li><a href="http://zh.learnlayout.com" target="_blank" rel="noopener">学习CSS布局</a></li><li><a href="https://github.com/chadluo/CSS-Guidelines/blob/master/README.md" target="_blank" rel="noopener">通用 CSS 笔记、建议与指导</a></li></ul><h2 id="iOS"><a href="#iOS" class="headerlink" title="iOS"></a>iOS</h2><ul><li><a href="http://nilsun.github.io/apple-watch/" target="_blank" rel="noopener">Apple Watch开发初探</a></li><li><a href="http://zh-google-styleguide.readthedocs.org/en/latest/google-objc-styleguide/" target="_blank" rel="noopener">Google Objective-C Style Guide 中文版</a></li><li><a href="http://isux.tencent.com/ios-human-interface-guidelines-ui-design-basics-ios7.html" target="_blank" rel="noopener">iOS7人机界面指南</a></li><li><a href="https://github.com/qinjx/30min_guides/blob/master/ios.md" target="_blank" rel="noopener">iOS开发60分钟入门</a></li><li><a href="http://wileam.com/iphone-6-screen-cn/" target="_blank" rel="noopener">iPhone 6 屏幕揭秘</a></li><li><a href="https://github.com/jkyin/Subtitle" target="_blank" rel="noopener">网易斯坦福大学公开课：iOS 7应用开发字幕文件</a></li></ul><h2 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h2><ul><li><a href="https://github.com/waylau/activiti-5.x-user-guide" target="_blank" rel="noopener">Activiti 5.x 用户指南</a></li><li><a href="https://github.com/waylau/apache-mina-2.x-user-guide" target="_blank" rel="noopener">Apache MINA 2 用户指南</a></li><li><a href="https://github.com/waylau/apache-shiro-1.2.x-reference" target="_blank" rel="noopener">Apache Shiro 用户指南</a></li><li><a href="http://www.hawstein.com/posts/google-java-style.html" target="_blank" rel="noopener">Google Java编程风格指南</a></li><li><a href="https://github.com/waylau/h2-database-doc" target="_blank" rel="noopener">H2 Database 教程</a></li><li><a href="https://github.com/waylau/servlet-3.1-specification" target="_blank" rel="noopener">Java Servlet 3.1 规范</a></li><li><a href="https://github.com/waylau/java-code-conventions" target="_blank" rel="noopener">Java 编码规范</a></li><li><a href="https://github.com/waylau/Jersey-2.x-User-Guide" target="_blank" rel="noopener">Jersey 2.x 用户指南</a></li><li><a href="https://github.com/waylau/jsse-reference-guide" target="_blank" rel="noopener">JSSE 参考指南</a></li><li><a href="http://mybatis.github.io/mybatis-3/zh/index.html" target="_blank" rel="noopener">MyBatis中文文档</a></li><li><a href="https://github.com/waylau/netty-4-user-guide" target="_blank" rel="noopener">Netty 4.x 用户指南</a></li><li><a href="https://github.com/waylau/essential-netty-in-action" target="_blank" rel="noopener">Netty 实战(精髓)</a></li><li><a href="https://github.com/waylau/rest-in-action" target="_blank" rel="noopener">REST 实战</a></li><li><a href="https://github.com/qibaoguang/Spring-Boot-Reference-Guide" target="_blank" rel="noopener">Spring Boot参考指南</a> (翻译中)</li><li><a href="https://github.com/waylau/spring-framework-4-reference" target="_blank" rel="noopener">Spring Framework 4.x参考文档</a></li><li><a href="https://github.com/waylau/RestDemo" target="_blank" rel="noopener">用jersey构建REST服务</a></li></ul><h2 id="Javascript"><a href="#Javascript" class="headerlink" title="Javascript"></a>Javascript</h2><ul><li><a href="https://github.com/adamlu/javascript-style-guide" target="_blank" rel="noopener">Airbnb JavaScript 规范</a></li><li>AngularJS<ul><li><a href="https://github.com/peiransun/angularjs-cn" target="_blank" rel="noopener">AngularJS中译本</a></li><li><a href="https://github.com/zensh/AngularjsTutorial_cn" target="_blank" rel="noopener">AngularJS入门教程</a></li><li><a href="https://github.com/mgechev/angularjs-style-guide/blob/master/README-zh-cn.md" target="_blank" rel="noopener">AngularJS最佳实践和风格指南</a></li><li><a href="http://www.waylau.com/build-angularjs-app-with-yeoman-in-windows/" target="_blank" rel="noopener">在Windows环境下用Yeoman构建AngularJS项目</a></li><li><a href="https://github.com/xufei/Make-Your-Own-AngularJS/blob/master/01.md" target="_blank" rel="noopener">构建自己的AngularJS</a></li></ul></li><li>backbone.js<ul><li><a href="http://www.css88.com/doc/backbone/" target="_blank" rel="noopener">backbone.js中文文档</a></li><li><a href="http://www.the5fire.com/backbone-js-tutorials-pdf-download.html" target="_blank" rel="noopener">backbone.js入门教程</a> (PDF)</li><li><a href="https://github.com/the5fire/backbonejs-learning-note" target="_blank" rel="noopener">Backbone.js入门教程第二版</a></li><li><a href="http://feliving.github.io/developing-backbone-applications" target="_blank" rel="noopener">Developing Backbone.js Applications(中文版)</a></li></ul></li><li><a href="http://www.ituring.com.cn/minibook/950" target="_blank" rel="noopener">Chrome扩展及应用开发</a></li><li>CoffeeScript<ul><li><a href="https://github.com/geekplux/coffeescript-style-guide" target="_blank" rel="noopener">CoffeeScript 编码风格指南</a></li></ul></li><li>D3.js<ul><li><a href="http://www.ourd3js.com/wordpress/?cat=2" target="_blank" rel="noopener">D3.js 入门系列</a> (还有进阶、高级等系列)</li><li><a href="https://github.com/mbostock/d3/wiki/API--%E4%B8%AD%E6%96%87%E6%89%8B%E5%86%8C" target="_blank" rel="noopener">官方API文档</a></li><li><a href="http://blog.csdn.net/zhang__tianxu/article/category/1623437" target="_blank" rel="noopener">张天旭的D3教程</a></li><li><a href="http://www.cnblogs.com/winleisure/tag/D3.js/" target="_blank" rel="noopener">楚狂人的D3教程</a></li></ul></li><li><a href="http://es6.ruanyifeng.com" target="_blank" rel="noopener">ECMAScript 6 入门</a> (作者：阮一峰)</li><li>ExtJS<ul><li><a href="http://extjs-doc-cn.github.io/ext4api/" target="_blank" rel="noopener">Ext4.1.0 中文文档</a></li></ul></li><li><a href="http://bq69.com/blog/articles/script/868/google-javascript-style-guide.html" target="_blank" rel="noopener">Google JavaScript 代码风格指南</a></li><li><a href="https://github.com/darcyliu/google-styleguide/blob/master/JSONStyleGuide.md" target="_blank" rel="noopener">Google JSON 风格指南</a></li><li>impress.js<ul><li><a href="https://github.com/kokdemo/impress.js-tutorial-in-Chinese" target="_blank" rel="noopener">impress.js的中文教程</a></li></ul></li><li><a href="http://liubin.github.io/promises-book/" target="_blank" rel="noopener">JavaScript Promise迷你书</a></li><li><a href="http://typeof.net/s/jsmech/" target="_blank" rel="noopener">Javascript 原理</a></li><li><a href="http://javascript.ruanyifeng.com" target="_blank" rel="noopener">JavaScript 标准参考教程（alpha）</a></li><li><a href="https://github.com/jayli/javascript-patterns" target="_blank" rel="noopener">《JavaScript 模式》</a> “JavaScript patterns”中译本</li><li><a href="https://github.com/justjavac/12-javascript-quirks" target="_blank" rel="noopener">javascript 的 12 个怪癖</a></li><li><a href="http://bonsaiden.github.io/JavaScript-Garden/zh/" target="_blank" rel="noopener">JavaScript 秘密花园</a></li><li><a href="http://icodeit.org/jsccp/" target="_blank" rel="noopener">JavaScript核心概念及实践</a> (PDF) (此书已由人民邮电出版社出版发行，但作者依然免费提供PDF版本，希望开发者们去购买，支持作者)</li><li><a href="http://pij.robinqu.me" target="_blank" rel="noopener">Javascript编程指南</a> (<a href="https://github.com/RobinQu/Programing-In-Javascript" target="_blank" rel="noopener">源码</a>)</li><li>jQuery<ul><li><a href="http://i5ting.github.io/How-to-write-jQuery-plugin/build/jquery.plugin.html" target="_blank" rel="noopener">How to write jQuery plugin</a></li><li><a href="http://www.nowamagic.net/librarys/books/contents/jquery" target="_blank" rel="noopener">简单易懂的JQuery魔法</a></li></ul></li><li>Meteor<ul><li><a href="http://zh.discovermeteor.com" target="_blank" rel="noopener">Discover Meteor</a></li></ul></li><li>Node.js<ul><li><a href="http://expressjs.jser.us" target="_blank" rel="noopener">express.js 中文文档</a></li><li><a href="http://javascript.ruanyifeng.com/nodejs/express.html" target="_blank" rel="noopener">Express框架</a></li><li><a href="https://github.com/guo-yu/koa-guide" target="_blank" rel="noopener">koa 中文文档</a></li><li><a href="https://www.npmjs.com/package/learnyounode-zh-cn" target="_blank" rel="noopener">Learn You The Node.js For Much Win! (中文版)</a></li><li><a href="http://i5ting.github.io/node-debug-tutorial/" target="_blank" rel="noopener">Node debug 三法三例</a></li><li><a href="https://github.com/jollen/nodejs-fullstack-lessons" target="_blank" rel="noopener">Node.js Fullstack《從零到一的進撃》</a></li><li><a href="https://github.com/alsotang/node-lessons" target="_blank" rel="noopener">Node.js 包教不包会</a></li><li><a href="https://github.com/nodejs-tw/nodejs-wiki-book" target="_blank" rel="noopener">Nodejs Wiki Book</a> (繁体中文)</li><li><a href="https://www.gitbook.com/book/0532/nodejs/details" target="_blank" rel="noopener">nodejs中文文档</a></li><li><a href="http://www.nodebeginner.org/index-zh-cn.html" target="_blank" rel="noopener">Node入门</a></li><li><a href="http://nqdeng.github.io/7-days-nodejs/" target="_blank" rel="noopener">七天学会NodeJS</a></li><li><a href="https://github.com/nswbmw/N-blog" target="_blank" rel="noopener">使用 Express + MongoDB 搭建多人博客</a></li></ul></li><li>React.js<ul><li><a href="https://github.com/theJian/build-a-hn-front-page" target="_blank" rel="noopener">Learn React &amp; Webpack by building the Hacker News front page</a></li><li><a href="http://wiki.jikexueyuan.com/project/react-native/" target="_blank" rel="noopener">React Native 中文文档(含最新Android内容)</a></li><li><a href="https://github.com/fakefish/react-webpack-cookbook" target="_blank" rel="noopener">React webpack-cookbook</a></li><li><a href="http://fraserxu.me/intro-to-react/" target="_blank" rel="noopener">React 入门教程</a></li><li><a href="http://reactjs.cn" target="_blank" rel="noopener">React.js 中文文档</a></li></ul></li><li>underscore.js<ul><li><a href="http://learningcn.com/underscore/" target="_blank" rel="noopener">Underscore.js中文文档</a></li></ul></li><li><a href="https://github.com/getify/You-Dont-Know-JS" target="_blank" rel="noopener">You-Dont-Know-JS</a> (深入JavaScript语言核心机制的系列图书)</li><li>Zepto.js<ul><li><a href="http://mweb.baidu.com/zeptoapi/" target="_blank" rel="noopener">Zepto.js 中文文档</a></li></ul></li><li><a href="http://justjavac.com/named-function-expressions-demystified.html" target="_blank" rel="noopener">命名函数表达式探秘</a>  (注:原文由<a href="http://www.cn-cuckoo.com" target="_blank" rel="noopener">为之漫笔</a> 翻译，原始地址无法打开，所以此处地址为我博客上的备份)</li><li><a href="http://www.oschina.net/translate/learning-javascript-design-patterns" target="_blank" rel="noopener">学用 JavaScript 设计模式</a> (开源中国)</li><li><a href="http://www.cnblogs.com/TomXu/archive/2011/12/15/2288411.html" target="_blank" rel="noopener">深入理解JavaScript系列</a></li></ul><h2 id="LaTeX"><a href="#LaTeX" class="headerlink" title="LaTeX"></a>LaTeX</h2><ul><li><a href="http://www.dralpha.com/zh/tech/tech.htm" target="_blank" rel="noopener">LaTeX 笔记</a></li><li><a href="http://ctan.org/pkg/lshort-zh-cn" target="_blank" rel="noopener">一份不太简短的 LaTeX2ε 介绍</a></li><li><a href="https://github.com/49951331/graduate-project-102pj/blob/master/docs/latex123.pdf" target="_blank" rel="noopener">大家來學 LaTeX</a> (PDF)</li></ul><h2 id="LISP"><a href="#LISP" class="headerlink" title="LISP"></a>LISP</h2><ul><li><a href="http://acl.readthedocs.org/en/latest/" target="_blank" rel="noopener">ANSI Common Lisp 中文翻译版</a></li><li><a href="http://www.ituring.com.cn/minibook/862" target="_blank" rel="noopener">Common Lisp 高级编程技术</a> (《On Lisp》中文版)</li></ul><h2 id="Lua"><a href="#Lua" class="headerlink" title="Lua"></a>Lua</h2><ul><li><a href="http://www.w3cschool.cc/manual/lua53doc/contents.html" target="_blank" rel="noopener">Lua 5.3 参考手册</a></li></ul><h2 id="Markdown"><a href="#Markdown" class="headerlink" title="Markdown"></a>Markdown</h2><ul><li><a href="http://wowubuntu.com/markdown/basic.html" target="_blank" rel="noopener">Markdown 快速入门</a></li><li><a href="http://www.jianshu.com/p/7bd23251da0a" target="_blank" rel="noopener">Markdown 简明教程</a></li><li><a href="http://wowubuntu.com/markdown/" target="_blank" rel="noopener">Markdown 语法说明</a></li><li><a href="http://www.jianshu.com/p/q81RER" target="_blank" rel="noopener">献给写作者的 Markdown 新手指南</a></li></ul><h2 id="Node-js"><a href="#Node-js" class="headerlink" title="Node.js"></a>Node.js</h2><ul><li><a href="http://www.nodebeginner.org/index-zh-cn.html" target="_blank" rel="noopener">Node 入门</a></li><li><a href="https://www.gitbook.com/book/0532/nodejs/details" target="_blank" rel="noopener">The NodeJS 中文文档</a>（社区翻译）</li><li><a href="http://nqdeng.github.io/7-days-nodejs/" target="_blank" rel="noopener">七天学会NodeJS</a> 阿里出品，很好的入门资料</li></ul><h2 id="Perl"><a href="#Perl" class="headerlink" title="Perl"></a>Perl</h2><ul><li><a href="https://github.com/fayland/chinese-perl-book" target="_blank" rel="noopener">Master Perl Today</a></li><li><a href="https://github.com/horus/modern_perl_book" target="_blank" rel="noopener">《Modern Perl》中文版</a></li><li><a href="http://www.cbi.pku.edu.cn/chinese/documents/perl/index.htm" target="_blank" rel="noopener">Perl 5 教程</a></li><li><a href="http://www.yiibai.com/perl" target="_blank" rel="noopener">Perl 教程</a></li></ul><h2 id="PHP"><a href="#PHP" class="headerlink" title="PHP"></a>PHP</h2><ul><li><a href="http://wulijun.github.io/php-the-right-way/" target="_blank" rel="noopener">PHP 之道</a></li><li><a href="http://php.net/manual/zh/" target="_blank" rel="noopener">PHP5中文手册</a></li><li><a href="http://www.walu.cc/phpbook/preface.md" target="_blank" rel="noopener">PHP扩展开发及内核应用</a></li><li><a href="https://wusuopu.gitbooks.io/symfony2_tutorial/content" target="_blank" rel="noopener">Symfony2 实例教程</a></li><li><a href="http://www.php-internals.com/book/" target="_blank" rel="noopener">深入理解 PHP 内核</a></li></ul><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><ul><li><a href="http://djangobook.py3k.cn/2.0/" target="_blank" rel="noopener">Django book 2.0</a></li><li><a href="http://docspy3zh.readthedocs.org/en/latest/" target="_blank" rel="noopener">Python 3 文档(简体中文) 3.2.2 documentation</a></li><li><a href="http://www.pythondoc.com" target="_blank" rel="noopener">Python 中文学习大本营</a></li><li><a href="https://github.com/jiechic/diveintopython3" target="_blank" rel="noopener">深入 Python 3</a></li><li><a href="http://old.sebug.net/paper/books/LearnPythonTheHardWay/" target="_blank" rel="noopener">笨办法学 Python</a></li></ul><h2 id="R"><a href="#R" class="headerlink" title="R"></a>R</h2><ul><li><a href="http://cran.r-project.org/doc/contrib/Liu-FAQ.pdf" target="_blank" rel="noopener">153分钟学会 R</a> (PDF)</li><li><a href="http://www.biosino.org/R/R-doc/files/R4beg_cn_2.0.pdf" target="_blank" rel="noopener">《R for beginners》中文版</a> (PDF)</li><li><a href="http://cran.r-project.org/doc/contrib/Ding-R-intro_cn.pdf" target="_blank" rel="noopener">R 导论</a> (《An Introduction to R》中文版) (PDF)</li><li><a href="http://yanping.me/shiny-tutorial/" target="_blank" rel="noopener">用 R 构建 Shiny 应用程序</a> (《Building ‘Shiny’ Applications with R》中文版)</li><li><a href="http://cran.r-project.org/doc/contrib/Xu-Statistics_and_R.pdf" target="_blank" rel="noopener">统计学与 R 读书笔记</a> (PDF)</li></ul><h2 id="reStructuredText"><a href="#reStructuredText" class="headerlink" title="reStructuredText"></a>reStructuredText</h2><ul><li><a href="http://www.pythondoc.com/sphinx/rest.html" target="_blank" rel="noopener">reStructuredText 入门</a></li><li><a href="http://jwch.sdut.edu.cn/book/rst.html" target="_blank" rel="noopener">reStructuredText 简明教程</a></li></ul><h2 id="Ruby"><a href="#Ruby" class="headerlink" title="Ruby"></a>Ruby</h2><ul><li><a href="https://github.com/JuanitoFatas/rails-style-guide/blob/master/README-zhCN.md" target="_blank" rel="noopener">Rails 风格指南</a></li><li><a href="http://railstutorial-china.org" target="_blank" rel="noopener">Ruby on Rails Tutorial 原书第 2 版</a></li><li><a href="https://ihower.tw/rails4/" target="_blank" rel="noopener">Ruby on Rails 实战圣经</a></li><li><a href="https://github.com/JuanitoFatas/ruby-style-guide/blob/master/README-zhCN.md" target="_blank" rel="noopener">Ruby 风格指南</a></li><li><a href="http://lrthw.github.io" target="_blank" rel="noopener">笨方法学 Ruby</a></li></ul><h2 id="Rust"><a href="#Rust" class="headerlink" title="Rust"></a>Rust</h2><ul><li><a href="https://github.com/KaiserY/rust-book-chinese" target="_blank" rel="noopener">Rust 官方教程</a></li><li><a href="https://github.com/photino/rust-notes" target="_blank" rel="noopener">Rust 语言学习笔记</a></li><li><a href="https://github.com/rustcc/RustPrimer" target="_blank" rel="noopener">RustPrimer</a></li><li><a href="https://github.com/rustcc/rust-by-example/" target="_blank" rel="noopener">通过例子学习 Rust</a></li></ul><h2 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h2><ul><li><a href="http://twitter.github.io/effectivescala/index-cn.html" target="_blank" rel="noopener">Effective Scala</a></li><li><a href="https://www.gitbook.com/book/windor/beginners-guide-to-scala/details" target="_blank" rel="noopener">Scala 初学者指南</a> (The Neophyte’s Guide to Scala)</li><li><a href="http://twitter.github.io/scala_school/zh_cn/index.html" target="_blank" rel="noopener">Scala 课堂</a> (Twitter的Scala中文教程)</li></ul><h2 id="Scheme"><a href="#Scheme" class="headerlink" title="Scheme"></a>Scheme</h2><ul><li><a href="http://deathking.github.io/yast-cn/" target="_blank" rel="noopener">Scheme 入门教程</a> (《Yet Another Scheme Tutorial》中文版)</li></ul><h2 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h2><ul><li><a href="http://wiki.ubuntu.org.cn/Shell%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80" target="_blank" rel="noopener">Shell 编程基础</a></li><li><a href="https://github.com/qinjx/30min_guides/blob/master/shell.md" target="_blank" rel="noopener">Shell 脚本编程30分钟入门</a></li><li><a href="http://billie66.github.io/TLCL/book/zh" target="_blank" rel="noopener">The Linux Command Line 中文版</a></li></ul><h2 id="Swift"><a href="#Swift" class="headerlink" title="Swift"></a>Swift</h2><ul><li><a href="https://www.gitbook.com/book/numbbbbb/-the-swift-programming-language-/details" target="_blank" rel="noopener">《The Swift Programming Language》中文版</a></li></ul><h2 id="Vim"><a href="#Vim" class="headerlink" title="Vim"></a>Vim</h2><ul><li><a href="http://man.chinaunix.net/newsoft/vi/doc/help.html" target="_blank" rel="noopener">Vim Manual(中文版)</a></li><li><a href="http://www.study-area.org/tips/vim/index.html" target="_blank" rel="noopener">大家來學 VIM</a></li></ul><h2 id="Visual-Prolog"><a href="#Visual-Prolog" class="headerlink" title="Visual Prolog"></a>Visual Prolog</h2><ul><li><a href="http://wiki.visual-prolog.com/index.php?title=A_Beginners_Guide_to_Visual_Prolog_in_Chinese" target="_blank" rel="noopener">Visual Prolog 7初学指南</a></li><li><a href="http://wiki.visual-prolog.com/index.php?title=Visual_Prolog_for_Tyros_in_Chinese" target="_blank" rel="noopener">Visual Prolog 7边练边学</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 资源分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> book </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo配置和优化记录</title>
      <link href="/hexo-config.html"/>
      <url>/hexo-config.html</url>
      
        <content type="html"><![CDATA[<p>** hexo配置和优化高级篇：** &lt;Excerpt in index | 首页摘要&gt;<br>本文章不讲解hexo的基础配置，只针对hexo的高级配置，性能优化，seo配置进行讲解。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>仔细想想，使用hexo搭建博客也有半年多了，但是发现访问量一直几乎没有，特别是经历几次迁移之后，之前从github到coding，<br>现在迁移到了云服务器，研究了一下如何进行seo和网站性能优化，便有了这篇文章。</p><h2 id="实用的功能"><a href="#实用的功能" class="headerlink" title="实用的功能"></a>实用的功能</h2><ol><li>站内搜索（百度的）</li><li>本地搜索（本地插件）</li><li>网站统计</li><li>留言功能</li><li>rss订阅功能</li></ol><h2 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h2><ol><li>html压缩</li><li>css压缩</li><li>js压缩·</li><li>img压缩</li><li>nginx代理，开启gzip压缩</li><li>cdn代理css和图·片</li><li>删除主题无用的js和css</li></ol><h2 id="seo优化"><a href="#seo优化" class="headerlink" title="seo优化"></a>seo优化</h2><ol><li>sitemap</li><li>对于没有价值的外链a标签添加<code>rel=&quot;external nofollow&quot;</code></li><li>使用meta标签</li><li>使用robots文件</li><li>主动提交sitemap到搜索引擎</li><li>添加外链和内链</li></ol>]]></content>
      
      
      <categories>
          
          <category> 开发工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ubuntu服务器详细配置</title>
      <link href="/server-config.html"/>
      <url>/server-config.html</url>
      
        <content type="html"><![CDATA[<p>** ubuntu服务器私人定制：** &lt;Excerpt in index | 首页摘要&gt;<br>把ubuntu服务器打造成自己的个性服务器，装逼必备！！！</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>##　说明<br><strong>此教程针对Ubuntu14,其他版本仅作参考</strong></p><p>##　用户密码管理<br><code>sudo passwd root</code></p><ol><li><p>添加一个用户组并指定id为1002<br><code>sudo groupadd －g 1002 www</code></p></li><li><p>添加一个用户到www组并指定id为1003<br><code>sudo useradd wyx -g 1002 -u 1003 -m</code></p></li><li><p>修改用户的密码<br><code>sudo passwd wyx</code></p></li><li><p>删除一个用户<br><code>sudo userdel wyx</code></p></li><li><p>为该用户添加sudo权限</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -a -G adm wyx</span><br><span class="line">sudo usermod -a -G sudo wyx</span><br></pre></td></tr></table></figure><ol start="6"><li>查看所有用户和用户组：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/passwd</span><br><span class="line">cat /etc/group</span><br></pre></td></tr></table></figure></li></ol><h2 id="安装nodejs"><a href="#安装nodejs" class="headerlink" title="安装nodejs"></a>安装nodejs</h2><ol><li>安装nvm<code>curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.32.1/install.sh | bash</code></li><li>安装node<code>nvm install v4.4.4</code>,安装<code>nvm install v6.9.1</code></li><li>设置默认的node版本<code>nvm alias default v4.4.4</code></li><li>安装npm3  <code>npm install -g npm@3</code></li><li>设置淘宝的cnpm源  <code>npm install -g cnpm --registry=https://registry.npm.taobao.org</code></li><li>验证安装<code>node -v,npm -v,cnpm -v</code><h2 id="安装node常用包"><a href="#安装node常用包" class="headerlink" title="安装node常用包"></a>安装node常用包</h2></li><li>安装pm2<code>cnpm install -g pm2</code></li><li>安装hexo博客<code>cnpm install -g hexo-cli</code></li><li>安装同步插件rsync<code>cnpm install -g rsync</code></li></ol><h2 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h2><ol><li>apt安装</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y docker.io</span><br><span class="line">sudo ln -sf /usr/bin/docker.io /usr/<span class="built_in">local</span>/bin/docker</span><br><span class="line">sudo sed -i <span class="string">'$acomplete -F _docker docker'</span> /etc/bash_completion.d/docker.io</span><br></pre></td></tr></table></figure><ol start="2"><li>源码安装最新版本</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install apt-transport-https</span><br><span class="line">sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9</span><br><span class="line">sudo bash -c <span class="string">"echo deb https://get.docker.io/ubuntu docker main &gt; /etc/apt/sources.list.d/docker.list"</span></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install lxc-docker</span><br></pre></td></tr></table></figure><ol start="3"><li>验证安装版本<br><code>docker -v</code></li></ol><h2 id="安装nginx"><a href="#安装nginx" class="headerlink" title="安装nginx"></a>安装nginx</h2><p><code>sudo apt-get install nginx</code><br>启动和配置nginx</p><h2 id="安装redis"><a href="#安装redis" class="headerlink" title="安装redis"></a>安装redis</h2><p><code>sudo apt-get install redis-server</code><br>启动和配置文件:</p><h2 id="安装mongodb"><a href="#安装mongodb" class="headerlink" title="安装mongodb"></a>安装mongodb</h2><ol><li>安装3.0</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"deb http://repo.mongodb.org/apt/debian wheezy/mongodb-org/3.0 main"</span> | sudo tee /etc/apt/sources.list.d/mongodb-org-3.0.list</span><br><span class="line">apt-get update  </span><br><span class="line">apt-get install mongodb-org</span><br></pre></td></tr></table></figure><ol start="2"><li>安装3.2最新版</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"deb http://repo.mongodb.org/apt/ubuntu "</span>$(lsb_release -sc)<span class="string">"/mongodb-org/3.2 multiverse"</span> | sudo tee /etc/apt/sources.list.d/mongodb.list</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install mongodb-org</span><br></pre></td></tr></table></figure><ol start="3"><li><p>制定版本<br><code>apt-get install mongodb-org=3.2.0 mongodb-org-server=3.2.0 mongodb-org-shell=3.2.0 mongodb-org-mongos=3.2.0 mongodb-org-tools=3.2.0</code></p></li><li><p>启动服务</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo service mongod start</span><br><span class="line">sudo service mongod stop</span><br></pre></td></tr></table></figure><ol start="5"><li>验证安装<br><code>mongod --version</code></li></ol><p>配置</p><h2 id="安装jdk"><a href="#安装jdk" class="headerlink" title="安装jdk"></a>安装jdk</h2><p>安装jdk1.7<code>sudo apt-get install openjdk-7-jdk</code><br>源码安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /usr/lib/jvm</span><br><span class="line">sudo tar zxvf jdk-7u21-linux-i586.tar.gz -C /usr/lib/jvm</span><br><span class="line"><span class="built_in">cd</span> /usr/lib/jvm</span><br><span class="line">sudo mv jdk1.7.0_21 java</span><br><span class="line"></span><br><span class="line">sudo vim ~/.bashrc</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java</span><br><span class="line"><span class="built_in">export</span> JRE_HOME=<span class="variable">$&#123;JAVA_HOME&#125;</span>/jre  </span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$&#123;JAVA_HOME&#125;</span>/lib:<span class="variable">$&#123;JRE_HOME&#125;</span>/lib  </span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure><h2 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h2><p>实用ubuntu自带的工具下载<br><code>sudo apt-get install mysql-server</code></p><h2 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h2><p>常见的方法有两种。</p><ol><li><p>在用户主目录下有一个 .bashrc 文件，可以在此文件中加入 PATH 的设置如下：<br><code>export PATH=”$PATH:/your path1/:/your path2/…..”</code></p></li><li><p>在 /etc/profile中增加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PATH=<span class="string">"<span class="variable">$PATH</span>:/home/zhengb66/bin"</span> </span><br><span class="line"><span class="built_in">export</span> PATH</span><br></pre></td></tr></table></figure></li></ol><h2 id="开机自启动"><a href="#开机自启动" class="headerlink" title="开机自启动"></a>开机自启动</h2><ol><li><p>方法一，编辑rc.loacl脚本<br>Ubuntu开机之后会执行/etc/rc.local文件中的脚本，<br>所以我们可以直接在/etc/rc.local中添加启动脚本。<br>当然要添加到语句：exit 0 前面才行。代码如下:<br><code>sudo vi /etc/rc.local</code><br>然后在 exit 0 前面添加好脚本代码。</p></li><li><p>方法二，添加一个Ubuntu的开机启动服务。<br>如果要添加为开机启动执行的脚本文件，<br>可先将脚本复制或者软连接到/etc/init.d/目录下，<br>然后用：update-rc.d xxx defaults NN命令(NN为启动顺序)，<br>将脚本添加到初始化执行的队列中去。<br>注意如果脚本需要用到网络，则NN需设置一个比较大的数字，如99。<br>1) 将你的启动脚本复制到 /etc/init.d目录下<br>以下假设你的脚本文件名为 test。<br>2) 设置脚本文件的权限</p></li></ol><p>代码如下:<br><code>sudo chmod 755 /etc/init.d/test</code><br>3) 执行如下命令将脚本放到启动脚本中去：<br>代码如下:<br><code>cd /etc/init.d</code>  <code>sudo update-rc.d test defaults 95</code><br> 注：其中数字95是脚本启动的顺序号，按照自己的需要相应修改即可。在你有多个启动脚本，而它们之间又有先后启动的依赖关系时你就知道这个数字的具体作用了。该命令的输出信息参考如下：<br>卸载启动脚本的方法：<br>代码如下:<br><code>cd /etc/init.d</code><br><code>sudo update-rc.d -f test remove</code></p><h2 id="定时任务"><a href="#定时任务" class="headerlink" title="定时任务"></a>定时任务</h2><p>在Ubuntu下，cron是被默认安装并启动的。通过查看/etc/crontab<br>推荐使用crontab -e命令添加自定义的任务（编辑的是/var/spool/cron下对应用户的cron文件，在/var/spool/cron下的crontab文件 不可以直接创建或者直接修改，crontab文件是通过crontab命令得到的）。<br><code>crontab -e</code></p><ol><li><p>直接执行命令行<br>每2分钟打印一个字符串“Hello World”，保存至文件/home/laigw/cron/HelloWorld.txt中，cron 格式如下：<br><code>*/2 * * * * echo “Hello World.” &gt;&gt; /home/HelloWorld.txt</code></p></li><li><p>shell 文件<br>每3分钟调用一次 /home/laigw/cron/test.sh 文件，cron 格式如下：<br><code>*/3 * * * * /home/laigw/cron/test.sh</code></p><h2 id="ftp和rsync配置"><a href="#ftp和rsync配置" class="headerlink" title="ftp和rsync配置"></a>ftp和rsync配置</h2></li></ol><h2 id="持续集成环境"><a href="#持续集成环境" class="headerlink" title="持续集成环境"></a>持续集成环境</h2><ol><li>jenkens配置</li><li>gitlab配置</li><li>git服务器</li></ol>]]></content>
      
      
      <categories>
          
          <category> 开发工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mac开发环境配置</title>
      <link href="/mac-dev.html"/>
      <url>/mac-dev.html</url>
      
        <content type="html"><![CDATA[<p>** mac开发环境配置：** &lt;Excerpt in index | 首页摘要&gt;<br>工欲善其事，必先利其器，做好开发者，先搞好开发环境啊。针对mac开发者的开发配置，把mac打造成最具生产力工具！</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="软件下载说明"><a href="#软件下载说明" class="headerlink" title="软件下载说明"></a>软件下载说明</h2><p>下面所提到的软件，有很多需要付费或者破解版，为了方便大家使用，会在网盘分享给大家，只需在评论的地方留下自己的<strong>百度云账号</strong>！！！</p><h2 id="软件分类说明"><a href="#软件分类说明" class="headerlink" title="软件分类说明"></a>软件分类说明</h2><ol><li>通用（开发者必备的软件）</li><li>java类（java开发者必不可少）</li><li>前端类（偏前端和nodejs）</li><li>python类</li><li>数据库类</li><li>其他（php，ruby等等）</li></ol><h2 id="通用软件"><a href="#通用软件" class="headerlink" title="通用软件"></a>通用软件</h2><ol><li>Alfred</li><li>dash</li><li>homebrew</li><li>zsh（oh my zsh）</li><li>sublime text3,</li><li>vscode</li><li>paste(剪切板工具)</li><li>BetterSnapTool(分屏软件)</li><li>cornerstone(svn)</li><li>tower(git)</li><li>alternote()</li><li>paw</li><li>chrome</li><li>firefox</li><li>pdf expert</li><li>CheatSheet</li><li>snippetslab</li></ol><h2 id="java软件"><a href="#java软件" class="headerlink" title="java软件"></a>java软件</h2><ol><li>jdk</li><li>idea</li><li>eclipse</li><li>maven</li><li>zookeeper,dubbo</li><li>tomcat</li><li>apache</li></ol><h2 id="前端必备"><a href="#前端必备" class="headerlink" title="前端必备"></a>前端必备</h2><ol><li>nvm(nodejs,npm,cnpm)</li><li>webpack</li><li>yo</li><li>webstorm</li></ol><h2 id="python必备"><a href="#python必备" class="headerlink" title="python必备"></a>python必备</h2><ol><li>pycharm</li><li>sublime text（插件）</li></ol><h2 id="数据库类"><a href="#数据库类" class="headerlink" title="数据库类"></a>数据库类</h2><ol><li>mysql</li><li>mongodb</li><li>sqllite</li><li>navicate</li><li>robomongo</li><li>redis</li></ol><h2 id="其他软件"><a href="#其他软件" class="headerlink" title="其他软件"></a>其他软件</h2><ol><li>office</li><li>keynote,pages,number</li><li>photoshop</li></ol><p>文章长期更新，请收藏</p>]]></content>
      
      
      <categories>
          
          <category> 开发工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mac </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>atom中最好的js代码补全</title>
      <link href="/best-js-snippet.html"/>
      <url>/best-js-snippet.html</url>
      
        <content type="html"><![CDATA[<p>** atom中最好的js代码补全：** &lt;Excerpt in index | 首页摘要&gt;<br>    这或许是atom中最好的js代码补全,包含了express,nodejs,es6,目前仍在继续更新</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="best-js-snippets"><a href="#best-js-snippets" class="headerlink" title="best-js-snippets"></a>best-js-snippets</h2><p>这个package的名字就叫 <strong>best-js-snippets</strong> ,用atom的可以下载使用一下,提出建议,我会尽快修改<br><img src="http://o7kalf5h3.bkt.clouddn.com/snippets.png" alt="best-js-snippets"></p><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><ol><li>express补全</li><li>es6补全</li><li>js补全(string,dom操作)</li><li>nodejs补全(fs,event,util,module,class,assert)</li></ol><h2 id="如何安装"><a href="#如何安装" class="headerlink" title="如何安装"></a>如何安装</h2><ol><li>atom编辑器中找到设置,搜索package,安装即可.</li><li>重启atom,享受吧!</li></ol>]]></content>
      
      
      <categories>
          
          <category> 开发工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> others </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用排序算法学习</title>
      <link href="/sort-study.html"/>
      <url>/sort-study.html</url>
      
        <content type="html"><![CDATA[<p>** 常用排序算法学习：** &lt;Excerpt in index | 首页摘要&gt;<br>    程序员各种排序算法，算法的实现和分析<br> <a id="more"></a><br>&lt;The rest of contents | 余下全文&gt;</p><h2 id="排序算法的分类"><a href="#排序算法的分类" class="headerlink" title="排序算法的分类"></a>排序算法的分类</h2><ol><li>排序分内排序和外排序。</li><li>内排序:指在排序期间数据对象全部存放在内存的排序。</li><li>外排序:指在排序期间全部对象个数太多,不能同时存放在内存,必须根据排序过程的要求,不断在内、外存之间移动的排序。</li><li>内排序的方法有许多种,按所用策略不同,可归纳为五类:插入排序、选择排序、交换排序、归并排序、分配排序和计数排序。</li><li>插入排序主要包括直接插入排序，折半插入排序和希尔排序两种;</li><li>选择排序主要包括直接选择排序和堆排序;</li><li>交换排序主要包括冒泡排序和快速排序;</li><li>归并排序主要包括二路归并(常用的归并排序)和自然归并。</li><li>分配排序主要包括箱排序和基数排序</li></ol><h2 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h2><ul><li>冒泡排序就是把小的元素往前调或者把大的元素往后调。比较是相邻的两个元素比较，交换也发生在这两个元素之间。所以，如果两个元素相等，是不用交换的；如果两个相等的元素没有相邻，那么即使通过前面的两两交换把两个相邻起来，这时候也不会交换，所以相同元素的前后顺序并没有改变，所以冒泡排序是一种稳定排序算法<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// js代码</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">sort</span>(<span class="params">arr</span>) </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (arr.length == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> [];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> length = arr.length;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; length; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">var</span> j = <span class="number">0</span>; j &lt; length - i - <span class="number">1</span>; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (arr[j] &gt; arr[j + <span class="number">1</span>]) &#123;</span><br><span class="line">                <span class="keyword">var</span> temp = arr[j];</span><br><span class="line">                arr[j] = arr[j + <span class="number">1</span>];</span><br><span class="line">                arr[j + <span class="number">1</span>] = temp;</span><br><span class="line">                <span class="built_in">console</span>.log(arr);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><ul><li>快速排序是对冒泡排序的一种改进。它的基本思想是：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列.</li><li>时间复杂度：O（n<em>lgn）最坏：O（n^2）空间复杂度：O（n</em>lgn）</li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// js递归实现</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">quickSort</span>(<span class="params">arr</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (arr.length == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> [];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">var</span> left = [];</span><br><span class="line">    <span class="keyword">var</span> right = [];</span><br><span class="line">    <span class="keyword">var</span> pivot = arr[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">1</span>; i &lt; arr.length; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (arr[i] &lt; pivot) &#123;</span><br><span class="line">            left.push(arr[i]);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            right.push(arr[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> quickSort(left).concat(pivot, quickSort(right));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> a = [];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; ++i) &#123;</span><br><span class="line">    a[i] = <span class="built_in">Math</span>.floor((<span class="built_in">Math</span>.random() * <span class="number">100</span>) + <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">console</span>.log(a);</span><br><span class="line"><span class="built_in">console</span>.log(quickSort(a));</span><br></pre></td></tr></table></figure><h2 id="直接插入排序"><a href="#直接插入排序" class="headerlink" title="直接插入排序"></a>直接插入排序</h2><ul><li>直接插入排序(straight insertion sort)的作法是：每次从无序表中取出第一个元素，把它插入到有序表的合适位置，使有序表仍然有序.</li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">insertionSort</span>(<span class="params">arr</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> temp, inner;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">var</span> outer = <span class="number">1</span>; outer &lt;= arr.length - <span class="number">1</span>; ++outer) &#123;</span><br><span class="line">        temp = arr[outer];</span><br><span class="line">        inner = outer;</span><br><span class="line">        <span class="keyword">while</span> (inner &gt; <span class="number">0</span> &amp;&amp; (arr[inner - <span class="number">1</span>] &gt;= temp)) &#123;</span><br><span class="line">            arr[inner] = arr[inner - <span class="number">1</span>];</span><br><span class="line">            --inner;</span><br><span class="line">        &#125;</span><br><span class="line">        arr[inner] = temp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> arr;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> a = [];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; ++i) &#123;</span><br><span class="line">    a[i] = <span class="built_in">Math</span>.floor((<span class="built_in">Math</span>.random() * <span class="number">100</span>) + <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">console</span>.log(a);</span><br><span class="line"><span class="built_in">console</span>.log(insertionSort(a));</span><br></pre></td></tr></table></figure><h2 id="折半插入排序"><a href="#折半插入排序" class="headerlink" title="折半插入排序"></a>折半插入排序</h2><ul><li>折半插入排序算法的具体操作为：在将一个新元素插入已排好序的数组的过程中，寻找插入点时，将待插入区域的首元素设置为a[low],末元素设置为 a[high]，则轮比较时将待插入元素与a[m],其中m=(low+high)/2相比较,如果比参考元素小，则选择a[low]到a[m-1]为新 的插入区域(即high=m-1)，否则选择a[m+1]到a[high]为新的插入区域（即low=m+1），如此直至low&lt;=high不成 立，即将此位置之后所有元素后移一位，并将新元素插入a[high+1]</li></ul><h2 id="希尔排序"><a href="#希尔排序" class="headerlink" title="希尔排序"></a>希尔排序</h2><ul><li>先取一个小于n的整数d1作为第一个增量，把文件的全部记录分成d1个组。所有距离为dl的倍数的记录放在同一个组中。先在各组内进行直接插入 排序；然后，取第二个增量d2&lt;d1重复上述的分组和排序，直至所取的增量dt=1(dt&lt;dt-l&lt;…&lt;d2&lt;d1)， 即所有记录放在同一组中进行直接插入排序为止。</li><li>该方法实质上是一种分组插入方法。插入排序（Insertion Sort）的一个重要的特点是，如果原始数据的大部分元素已经排序，那么插入排序的速度很快（因为需要移动的元素很少）。从这个事实我们可以想到，如果原 始数据只有很少元素，那么排序的速度也很快。－－希尔排序就是基于这两点对插入排序作出了改进。</li></ul><h2 id="直接选择排序"><a href="#直接选择排序" class="headerlink" title="直接选择排序"></a>直接选择排序</h2><ul><li>直接选择排序是给每个位置选择当前元素最小的，比如给第一个位置选择最小的，在剩余元素里面给第二个元素选择第二小的，依次类推，直到第n-1个元素，第n个 元素不用选择了，因为只剩下它一个最大的元素了。那么，在一趟选择，如果当前元素比一个元素小，而该小的元素又出现在一个和当前元素相等的元素后面，那么 交换后稳定性就被破坏了。比较拗口，举个例子，序列5 8 5 2 9，我们知道第一遍选择第1个元素5会和2交换，那么原序列中2个5的相对前后顺序就被破坏了，所以选择排序不是一个稳定的排序算法。时间复杂度是O(n^2)</li></ul><h2 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h2><ul><li>我们知道堆的结构是节点i的孩子为2<em>i和2</em>i+1节点，大顶堆要求父节点大于等于其2个子节点，小顶堆要求父节点小于等于其2个子节点。在一个长为n 的序列，堆排序的过程是从第n/2开始和其子节点共3个值选择最大(大顶堆)或者最小(小顶堆),这3个元素之间的选择当然不会破坏稳定性。但当为n /2-1, n/2-2, …1这些个父节点选择元素时，就会破坏稳定性。有可能第n/2个父节点交换把后面一个元素交换过去了，而第n/2-1个父节点把后面一个相同的元素没 有交换，那么这2个相同的元素之间的稳定性就被破坏了。所以，堆排序不是稳定的排序算法。</li></ul><h2 id="二路归并排序"><a href="#二路归并排序" class="headerlink" title="二路归并排序"></a>二路归并排序</h2><ul><li>归并排序是把序列递归地分成短序列，递归出口是短序列只有1个元素(认为直接有序)或者2个序列(1次比较和交换),然后把各个有序的段序列合并成一个有 序的长序列，不断合并直到原序列全部排好序。可以发现，在1个或2个元素时，1个元素不会交换，2个元素如果大小相等也没有人故意交换，这不会破坏稳定 性。那么，在短的有序序列合并的过程中，稳定是是否受到破坏？没有，合并过程中我们可以保证如果两个当前元素相等时，我们把处在前面的序列的元素保存在结 果序列的前面，这样就保证了稳定性。所以，归并排序也是稳定的排序算法。</li></ul>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql优化的常用方法</title>
      <link href="/mysql-optimize.html"/>
      <url>/mysql-optimize.html</url>
      
        <content type="html"><![CDATA[<p>** mysql优化：** &lt;Excerpt in index | 首页摘要&gt;<br>    mysql的优化措施，从sql优化做起</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="优化sql的一般步骤"><a href="#优化sql的一般步骤" class="headerlink" title="优化sql的一般步骤"></a>优化sql的一般步骤</h2><ol><li>通过show status了解各种sql的执行频率</li><li>定位执行效率低的sql语句</li><li>通过explain分析效率低的sql</li><li>通过show profile分析sql</li><li>通过trace分析优化器如何选择执行计划</li><li>确定问题，采取措施优化</li></ol><h2 id="索引优化措施"><a href="#索引优化措施" class="headerlink" title="索引优化措施"></a>索引优化措施</h2><ol><li><p>mysql中使用索引的典型场景</p><ol><li>匹配全值，条件所有列都在索引中而且是等值匹配</li><li>匹配值的范围查找，字段必须在索引中</li><li>匹配最左前缀，复合索引只会根据最左列进行查找</li><li>仅仅对索引进行查询，即查询的所有字段都在索引上</li><li>匹配列前缀，比如like ‘ABC%’,如果是like ‘%aaa’就不可以</li><li>如果列名是索引，使用column is null会使用索引</li></ol></li><li><p>存在索引但不会使用索引的典型场景</p><ol><li>以%开头的like查询不能使用b树索引</li><li>数据类型出现隐式转换不能使用索引</li><li>复合索引，查询条件不符合最左列原则</li><li>用or分割的条件，如果前面的条件有索引，而后面的条件没有索引</li></ol></li><li><p>查看索引使用的情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show status like &apos;Handler_read%&apos;;</span><br></pre></td></tr></table></figure></li></ol><p>如果Handler_read_rnd_next的值比较高，说明索引不正确或者查询没有使用到索引</p><h2 id="简单实用的优化方法"><a href="#简单实用的优化方法" class="headerlink" title="简单实用的优化方法"></a>简单实用的优化方法</h2><ol><li>定期检查表和分析表<br>分析表语法：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">analyze table 表名；</span><br></pre></td></tr></table></figure></li></ol><p>检查表语法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">check table 表名；</span><br></pre></td></tr></table></figure><ol start="2"><li>定期优化表<ul><li>对于字节大小不固定的字段，数据更新和删除会造成磁盘空间不释放，这时候就行优化表，可以整理磁盘碎片，提高性能<br>语法如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimize table user(表名)；</span><br></pre></td></tr></table></figure></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mac下mysql5.6字符集设置</title>
      <link href="/mac-mysql-unicode.html"/>
      <url>/mac-mysql-unicode.html</url>
      
        <content type="html"><![CDATA[<p>** mac下mysql5.6字符集设置：** &lt;Excerpt in index | 首页摘要&gt;<br>    在mac下设置mysql5.6字符集时踩过的坑，百分百保证有效</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="为什么要设置字符集"><a href="#为什么要设置字符集" class="headerlink" title="为什么要设置字符集"></a>为什么要设置字符集</h2><ol><li>设置字符集主要是解决乱码问题，由于中文和英文编码不同导致，中文出现乱码，所以一般都设置为utf8格式</li><li>不同的字符集和编码占用的字节不同，选择适合的编码会提高数据库性能</li></ol><h2 id="mac下设置"><a href="#mac下设置" class="headerlink" title="mac下设置"></a>mac下设置</h2><ul><li>在/etc/my.cnf文件进行设置，如果没有此文件可以从/usr/local/mysql/support-files/拷贝，命令如下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/mysql/support-files</span><br><span class="line">sudo cp my.cnf /etc/my.cnf</span><br></pre></td></tr></table></figure></li></ul><p>查看文件的读写权限，如果为644（rw- r– r–）则改为(664) (rw- rw- r–)<br>如果改为(666)(rw- rw- rw-)则修改以后配置文件不会生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod 664 /etc/my.cnf</span><br></pre></td></tr></table></figure><ul><li>my.cnf设置如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[client]</span><br><span class="line">default-character-set=utf8</span><br><span class="line">[mysqld]</span><br><span class="line">collation-server = utf8_unicode_ci</span><br><span class="line">init-connect=&apos;SET NAMES utf8&apos;</span><br><span class="line">character-set-server = utf8</span><br><span class="line">[mysql]</span><br><span class="line">default-character-set=utf8</span><br></pre></td></tr></table></figure></li></ul><h2 id="查看设置是否成功"><a href="#查看设置是否成功" class="headerlink" title="查看设置是否成功"></a>查看设置是否成功</h2><p>在命令行输入mysql，如果提示没有命令的话，在bash或者zsh的文件里修改，我用的是zsh，设置~/.zshrc,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export MYSQL=&quot;/usr/local/mysql/bin/&quot;</span><br><span class="line">export PATH=&quot;/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:$MYSQL&quot;</span><br></pre></td></tr></table></figure><p>在命令行输入mysql,进入mysql命令行后，输入<code>status;</code>或者<code>show variables like &#39;%char%&#39;;</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">| character_set_client     | utf8                                                    |</span><br><span class="line">| character_set_connection | utf8                                                    |</span><br><span class="line">| character_set_database   | utf8                                                    |</span><br><span class="line">| character_set_filesystem | binary                                                  |</span><br><span class="line">| character_set_results    | utf8                                                    |</span><br><span class="line">| character_set_server     | utf8                                                    |</span><br><span class="line">| character_set_system     | utf8                                                    |</span><br><span class="line">| character_sets_dir       | /usr/local/mysql-5.6.30-osx10.11-x86_64/share/charsets/</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql学习笔记</title>
      <link href="/mysql-study.html"/>
      <url>/mysql-study.html</url>
      
        <content type="html"><![CDATA[<p>** mysql学习笔记：** &lt;Excerpt in index | 首页摘要&gt;<br>    mysql学习，基础的增删改查，数据库优化，索引，分片，集群搭建等等。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="mysql的特点"><a href="#mysql的特点" class="headerlink" title="mysql的特点"></a>mysql的特点</h2><ol><li>关系型数据库，免费使用，</li><li>插入式存储引擎，</li><li>性能高，</li></ol><h2 id="基础的增删改查"><a href="#基础的增删改查" class="headerlink" title="基础的增删改查"></a>基础的增删改查</h2><ol><li><p>ddl语句，数据定义语句</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">create database test1;</span><br><span class="line">drop database test1;</span><br><span class="line">use test1;</span><br><span class="line">create table emp(ename varchar(10),hiredate date,sal decimal(10,2),deptno int(2));</span><br><span class="line">drop table emp;</span><br><span class="line">alter table emp modify ename varchar(20);</span><br><span class="line">alter table emp add column age int(3);</span><br><span class="line">alter table emp drop column age;</span><br><span class="line">alter table emp change age age1 int(4);</span><br><span class="line">alter table emp add birth date after ename;</span><br><span class="line">alter table emp modify age int(3) first;</span><br><span class="line">alter table emp rename emp1;</span><br></pre></td></tr></table></figure></li><li><p>dml语句，数据操纵语句</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">insert into emp(ename,hiredate,sal,deptno) values(&apos;zzx1&apos;,&apos;2000-10-11&apos;,2000,1);</span><br><span class="line">insert into emp values(&apos;lisa&apos;,&apos;2004-05-09&apos;,3000,2);</span><br><span class="line">insert into dept values(5,&apos;dept5&apos;),(6,&apos;dept6&apos;);</span><br><span class="line">update emp set sal=4000 where ename=&apos;lisa&apos;;</span><br><span class="line">update emp a,dept b set a.sal=a.sal*b.deptno,b.deptname=a.ename where a.deptno=b.deptno;</span><br><span class="line">delete from emp where ename=&apos;dony&apos;;</span><br><span class="line">delete a,b from emp a,dept b where a.deptno=b.deptno and a.deptno=3;</span><br><span class="line">select * from emp where ename=&apos;lisa&apos;;</span><br><span class="line">select distinct deptno from emp;</span><br><span class="line">select * from emp order by sal(desc);</span><br><span class="line">select * from emp order by sal limit 5;</span><br><span class="line">select * from emp order by sal limit 1,5;ss</span><br></pre></td></tr></table></figure></li><li><p>dcl语句，数据控制语句</p></li></ol><h2 id="sql优化"><a href="#sql优化" class="headerlink" title="sql优化"></a>sql优化</h2><ol><li>尽量使用 prepareStatement(java)，利用预处理功能。</li><li>在进行多条记录的增加、修改、删除时，建议使用批处理功能，批处理的次数以整<br>个 SQL 语句不超过相应数据库的 SQL 语句大小的限制为准。</li><li>建议每条 SQL 语句中 in 中的元素个数在 200 以下，如果个数超过时，应拆分为多<br>条 SQL 语句。禁止使用 xx in(‘’,’’….) or xx in(‘’,’’,’’)。 ★</li><li>禁止使用 or 超过 200，如 xx =’123’ or xx=’456’。 ★</li><li>尽量不使用外连接。</li><li>禁止使用 not in 语句，建议用 not exist。 ★</li><li>禁止使用 Union, 如果有业务需要，请拆分为两个查询。 ★</li><li>禁止在一条 SQL 语句中使用 3 层以上的嵌套查询，如果有，请考虑使用临时表或<br>中间结果集。</li><li>尽量避免在一条 SQL 语句中从&gt;= 4 个表中同时取数， 对于仅是作为过滤条件关联，<br>但不涉及取数的表，不参与表个数计算</li><li>查询条件里任何对列的操作都将导致表扫描，所以应尽量将数据库函数、计算表达<br>式写在逻辑操作符右边。</li><li>在对 char 类型比较时,建议不要使用 rtrim()函数,应该在程序中将不足的长度补<br>齐。</li><li>用多表连接代替 EXISTS 子句。</li><li>如果有多表连接时， 应该有主从之分， 并尽量从一个表取数， 如 select a.col1, a.col2<br>from a join b on a.col3=b.col4 where b.col5 = ‘a’。</li><li>在使用 Like 时，建议 Like 的一边是字符串，表列在一边出现。</li><li>不允许将 where 子句的条件放到 having 中。</li><li>将更新操作放到事务的最后执行。如</li><li>一个事务需更新多个对象时，需保证更新的顺序一致以避免死锁的发生。如总是先<br>更新子表再更新主表，根据存货档案批量更新现存量时，对传入的存货档案 PK 进<br>行排序，再做更新处理等。</li><li>禁止随意使用临时表，在临时数据不超过 200 行的情况下禁止使用临时表。</li><li>禁止随意使用 distinct，避免造成不必要的排序。</li></ol><h2 id="索引优化"><a href="#索引优化" class="headerlink" title="索引优化"></a>索引优化</h2><ol><li><p>创建索引，删除索引</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create index cityname on city(city(10));</span><br><span class="line">drop index cityname on city;</span><br></pre></td></tr></table></figure></li><li><p>搜索的索引列最好在where的字句或者连接子句</p></li><li><p>使用唯一索引</p></li><li><p>使用短索引，对于较长的字段，使用其前缀做索引</p></li><li><p>不要过度使用索引，索引引起额外的性能开销和维护</p></li></ol><h2 id="高级优化措施"><a href="#高级优化措施" class="headerlink" title="高级优化措施"></a>高级优化措施</h2><h2 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h2>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nodejs开发规范</title>
      <link href="/node-develop.html"/>
      <url>/node-develop.html</url>
      
        <content type="html"><![CDATA[<p>** nodejs开发规范：** &lt;Excerpt in index | 首页摘要&gt;<br>    nodejs开发中应当遵循的规范，以及最佳实践</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="node开发需要编程规范吗？"><a href="#node开发需要编程规范吗？" class="headerlink" title="node开发需要编程规范吗？"></a>node开发需要编程规范吗？</h2><ol><li>js的灵活性非常大，如果开发人员每个人都按自己的习惯随意编写，js的代码会非常混乱不堪。js程序员需要更强的自律性和规范，才能写出易读性，易维护的代码。</li><li>随着前端mvc的崛起，前端的js代码会更加庞大难以管理，如果没有统一的规范，后期维护会比登天还难。</li></ol><h2 id="编码规范"><a href="#编码规范" class="headerlink" title="编码规范"></a>编码规范</h2><ol><li><p>缩进<br>采用两个空格缩进，在编辑器中设置tab为两个空格</p></li><li><p>变量声明</p></li></ol><ul><li>用var声明变量<br>var assert = require(‘assert’);<br>var fork = require(‘child_process’).fork;<br>var net = require(‘net’);</li></ul><p>错误实例：<br>var assert = require(‘assert’)<br>, fork = require(‘child_process’).fork<br>, net = require(‘net’)；</p><ul><li><p>用字面量声明方式<br>var num = 123;<br>var aaa = {};<br>var arr = [];<br>var isAdmin = true;</p></li><li><p>避免使用：<br>var obj =new Object();<br>var arr = new Array();<br>var test  =new String(“”);<br>var size = new Number();</p></li><li><p>不要在for循环等循环里声明var变量<br>首先var是函数作用域，在循环声明以后只有等函数声明周期结束这些资源才会释放</p></li></ul><ol start="3"><li><p>空格<br>在操作符前后需要加上空格,= 、% 、* 、- 、+ 前后都应该加一个空格<br>比如：var foo = ‘bar’ + baz;<br>错误实例：var foo=’bar’+baz;</p></li><li><p>单双引号的使用<br>在node中尽量使用单引号，<br>var html = ‘<a href="http://cnodejs.org" target="_blank" rel="noopener">CNode</a>‘;<br>在json中使用双引号</p></li><li><p>分号<br>给表达式结尾加分号，尽管js会自动在行尾加上分号，但是会产生一些误解</p></li></ol><h2 id="命名规范"><a href="#命名规范" class="headerlink" title="命名规范"></a>命名规范</h2><p>在编码中，命名是重头戏。好的命名可以使代码赏心悦目，具有良好的维护性。</p><ol><li><p>变量命名<br>变量名采用小驼峰命名，单词之间没有任何符号如：<br>var adminUser = {};<br>var callNum = 2134323;</p></li><li><p>方法命名<br>也是采用小驼峰命名，与变量不同的是采用动词或判断行词汇，如：<br>var getUser = function(){};<br>var isAdmin = function(){};<br>var findUser = function(){};</p></li><li><p>类命名<br>类名采用大驼峰，所有单词首字母大写，如：<br>function User{}</p></li><li><p>常量命名<br>作为常量，单词所有字母大写，用下划线分割，如：<br>var PINK_COLOR = “PINK”;</p></li><li><p>文件命名<br>命名文件时，尽量使用下划线分割单词，比如child_process.js和string_decode.js</p></li><li><p>包名<br>在包名中尽量不要包含js和node的字样，应当适当短并且有意义</p></li></ol><h2 id="其它要点"><a href="#其它要点" class="headerlink" title="其它要点"></a>其它要点</h2><ol><li><p>作用域<br>慎用with和eval（），容易引起作用域混乱</p></li><li><p>比较操作<br>尽量使用===代替==,否则会遇到下面的情况，’0’==0;//true;<br>‘’==0;//true;<br>‘0’===’’//false;</p></li><li><p>严格模式<br>在node后台中尽量全使用严格模式<br>‘use strict’;</p></li><li><p>对象和数组遍历<br>数组遍历使用普通for循环，避免使用for in对数组遍历，<br>对象的遍历使用for in</p></li></ol><h2 id="项目中实践"><a href="#项目中实践" class="headerlink" title="项目中实践"></a>项目中实践</h2><ol><li><p>sublime和webstorm都有JSLint,JSHint这样的代码质量工具，在配置文件中制定好模板规范即可</p></li><li><p>在版本控制工具中设置hook，在precommit的脚本中设置，如果代码不符合标准，就无法提交</p></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li>深入浅出nodejs</li><li>js秘密花园</li><li>js高级编程</li></ol>]]></content>
      
      
      <categories>
          
          <category> 编程语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> node </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis学习笔记</title>
      <link href="/redis-study.html"/>
      <url>/redis-study.html</url>
      
        <content type="html"><![CDATA[<p>** redis学习笔记：** &lt;Excerpt in index | 首页摘要&gt;<br>    redis数据库的基本操作，增删改查</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="keys"><a href="#keys" class="headerlink" title="keys"></a>keys</h2><p>redis本质上是一个key-value数据库</p><ol><li>设置：set key value</li><li>获取：get key</li><li>判断存在：exists key</li><li>删除：del key        del  test:fan:age</li><li>重命名：rename  oldkey newkey        </li><li>数量：dbsize  返回数据</li><li>获取所有key（通配符）：<code>Keys test:*:age</code><br><code>Keys test:?:age</code></li><li>清空：flushdb    flushall</li><li>设置有效时间：expire test:fan:age 30</li><li>查询有效时间：ttl test:fan:age</li></ol><h2 id="String类型"><a href="#String类型" class="headerlink" title="String类型"></a>String类型</h2><ol><li>设置：<br> set key value<br> setnx ky value(nx是not exist)<br> mset key1 value1 keyN valueN<br> msetnx key1 value1 keyN valueN</li><li>获取：<br> get            不存在返回nil<br> getset        设置key的值，并返回key的旧值，不存在返回nil<br> mget        </li><li>自增减：<br> incr key   对key的值进行++操作，返回新的值<br> decr key<br> incrby key integer        对key加上一个数值<br> decrby key integer</li><li>截取：<br> substr key indexStart indexEnd             下标从0开始</li><li>追加：<br> append key value</li></ol><h2 id="list类型"><a href="#list类型" class="headerlink" title="list类型"></a>list类型</h2><p>redis的list其实就是一个每个元素都是string 的双向链表，所以push和pop的时间复杂度都是O（1）</p><ol><li>添加<br> lpush key string         在头部添加<br> rpush key string        在尾部添加</li><li>修改<br> lset key index value  修改指定下标的key的值</li><li>删除<br> lpop key     从头部返回删除<br> rpop key  从尾部<br> lrem key count value  删除count个相同的value，count为0删除全部<br> blpop key …keyN timeout<br> brpop 从尾部删除</li><li>获取<br> lrange key indexStart indexEnd</li><li>数量<br> llen key        返回key对应的list长度</li><li>截取<br> ltrim key start end</li><li>转移<br> rpoplpush key1 key2    从key1尾部移到key2头部</li></ol><h2 id="set集合"><a href="#set集合" class="headerlink" title="set集合"></a>set集合</h2><p>redis的set就是String的无序集合，通过hashtable实现</p><ol><li>添加<br> sadd key member</li><li>删除<br> srem key member        移除指定的元素<br> spop key                     删除并返回一个随机的</li><li>获取<br> smembers key            返回所有<br> srandmember            随机取一个不删除</li><li>判断存在<br> sismember key member</li><li>数量<br> scard key                     返回元素个数</li><li>转移<br> smove srckey dstkey member</li><li>取交集<br> sinter key1 key2 keyN<br> sinterstore dstkey key1 keyN        将交集存在dstkey</li><li>取并集<br> sunion key1 key2 keyN<br> sunionstore dstkey key1 keyN    将并集存在dstkey</li><li>取差集<br> sdiff key1 key2 keyN<br> sdiffstore dstkey key1 keyN        将差集存在dstkey</li></ol><h2 id="有序set类型"><a href="#有序set类型" class="headerlink" title="有序set类型"></a>有序set类型</h2><p>和set一样，不同的是每个元素关联一个double类型的score，根据score排序，sorted set的实现由skip list和hashtable</p><ol><li>添加<br> zadd key score member</li><li>删除<br> zrem key member<br> zremrangebyrank key min max<br> zremrangebyscore key min max     删除集合score在给定区间的元素</li><li>获取<br> zrange key start end<br> zrevrange    key start end            按score的逆序<br> zrangebyscore key min max        </li><li>判断存在<br> zrank key member        返回下标<br> zrerank key member        返回逆序的下标</li><li>数量<br> zcard key                        总数<br> zcount key min max         区间的数量</li><li>修改<br> zincrby key incr member    增加member的score值并排序</li></ol><h2 id="hash类型"><a href="#hash类型" class="headerlink" title="hash类型"></a>hash类型</h2><p>redis的hash是一个string类型的field和value的映射表，hash特别适合存储对象，</p><ol><li>设置：<br> hset key field value<br> hmset key field1 value1 field2 value2</li><li>获取：<br> hget key field<br> hmget key field1 field2</li><li>判断存在<br> hexists key field</li><li>删除<br> hdel key field</li><li>查找<br> hkeys key            返回所有 field<br> hvals key            返回所有的value<br> hgetall key        返回所有field和value</li><li>数量<br> hlen key</li><li>值加减<br> hincrby key field integer    将指定的hash field加上定值</li></ol>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git比svn的优势</title>
      <link href="/git-svn.html"/>
      <url>/git-svn.html</url>
      
        <content type="html"><![CDATA[<p>** git比svn的优势：** &lt;Excerpt in index | 首页摘要&gt;<br>    主要介绍svn和git在使用的时候一些区别</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="合并操作时对提交过程的保留"><a href="#合并操作时对提交过程的保留" class="headerlink" title="合并操作时对提交过程的保留"></a>合并操作时对提交过程的保留</h2><ul><li>git:合并操作保留原有的提交过程</li><li>svn:多个提交合并为一个提交</li><li>不用因为合并操作而导致追踪的困难</li></ul><h2 id="修正提交"><a href="#修正提交" class="headerlink" title="修正提交"></a>修正提交</h2><ul><li>git：可以修正提交。<br>使用功能分支工作流，在自己的分支可以方便修正提交而不会影响大家。</li><li>svn：一旦提交就到服务器上，实际使用中就是不能修改<br>（svn可以在服务器上修改，因为过程复杂需要权限实际上从不会这样做）</li></ul><h2 id="本地分支"><a href="#本地分支" class="headerlink" title="本地分支"></a>本地分支</h2><ul><li>git可以方便的创建本地分支,创建时间极短,分支可以是本地的,不会存在svn中目录权限的问题</li></ul><h2 id="强大的合并能力"><a href="#强大的合并能力" class="headerlink" title="强大的合并能力"></a>强大的合并能力</h2><ul><li><p>git：重命名（无论文件还有目录）提交 可以合并上 文件重命名前的这些文件的提交</p></li><li><p>svn：重命名（无论文件还有目录）提交后，你本地/或是分支上 有文件重命名前的这些文件的修改或提交，在做合并操作时,你会碰上传说中难搞的<strong><em>树冲突</em></strong>！</p></li><li><p>这就导致在调整目录名称和类名调整的时候比较繁琐,需要告诉大家,我修改完以后你再修改</p></li></ul><h2 id="tag的支持"><a href="#tag的支持" class="headerlink" title="tag的支持"></a>tag的支持</h2><ul><li>svn在模型上是没有分支和tag的。tag是通过目录权限限制（对开发只读）来保证不变。</li><li>git模型上一等公民支持tag，保证只读。</li></ul><h2 id="速度优势"><a href="#速度优势" class="headerlink" title="速度优势"></a>速度优势</h2><ul><li>git的提交是个本地提交,相对svn来说如闪电一般</li><li>git提供了暂存区,可以方便制定提交内容,而不是全部内容</li></ul><h2 id="日志查看"><a href="#日志查看" class="headerlink" title="日志查看"></a>日志查看</h2><ul><li>git：本地包含了完整的日志，闪电的速度（并且无需网络)</li><li>svn：需要从服务拉取。</li><li>一旦用了git后，等待svn日志过程简直让我发狂</li></ul>]]></content>
      
      
      <categories>
          
          <category> 开发工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> others </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java和javascript日期详解</title>
      <link href="/java-date.html"/>
      <url>/java-date.html</url>
      
        <content type="html"><![CDATA[<p>** java，js日期转换：** &lt;Excerpt in index | 首页摘要&gt;<br>    java的各种日期转换</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="日期表示类型"><a href="#日期表示类型" class="headerlink" title="日期表示类型"></a>日期表示类型</h2><ol><li><p>获取long类型的日期格式</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> time = System.currentTimeMillis();</span><br><span class="line">System.out.printf(time+<span class="string">""</span>);</span><br><span class="line">Date date =<span class="keyword">new</span> Date();</span><br><span class="line">System.out.println(date.getTime());</span><br></pre></td></tr></table></figure></li><li><p>获取制定格式的日期</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SimpleDateFormat sdf = <span class="keyword">new</span> SimpleDateFormat(<span class="string">"yyyy-MM-dd hh:mm:ss"</span>);</span><br><span class="line">Date date =<span class="keyword">new</span> Date();</span><br><span class="line">System.out.println(sdf.format(date) );</span><br></pre></td></tr></table></figure></li><li><p>把制定格式的日期转为date或者毫秒值</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SimpleDateFormat sdf = <span class="keyword">new</span> SimpleDateFormat(<span class="string">"yyyy-MM-dd hh:mm:ss"</span>);</span><br><span class="line">Date date = sdf.parse(<span class="string">"2016-05-22 10:15:21"</span>);</span><br><span class="line"><span class="keyword">long</span> mills = date.getTime();</span><br></pre></td></tr></table></figure></li></ol><ul><li>说明:System.currentTimeMillis()并不能精确到1ms的级别,它取决于运行的系统,你再windows,mac,linux精确的范围都有差异,对于有高精度时间的要求,不能使用这个</li></ul><h2 id="日期计算"><a href="#日期计算" class="headerlink" title="日期计算"></a>日期计算</h2><ol><li>最方便的方式是将时间转为毫秒值进行计算<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Date from =<span class="keyword">new</span> Date();</span><br><span class="line">Thread.sleep(<span class="number">200</span>);<span class="comment">//线程休眠2ms</span></span><br><span class="line">Date to =<span class="keyword">new</span> Date();</span><br><span class="line">System.out.println(to.getTime()-from.getTime());</span><br></pre></td></tr></table></figure></li></ol><h2 id="高精度时间"><a href="#高精度时间" class="headerlink" title="高精度时间"></a>高精度时间</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> time1 =System.nanoTime();</span><br><span class="line">System.out.printf(time1+<span class="string">""</span>);</span><br></pre></td></tr></table></figure><ul><li>说明:System.nanoTime()提高了ns级别的精度,1ms=1000000ns,</li></ul><h2 id="javascript日期"><a href="#javascript日期" class="headerlink" title="javascript日期"></a>javascript日期</h2><ol><li><p>获取时间的毫秒值，获取月份，时间</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> myDate = <span class="keyword">new</span> <span class="built_in">Date</span>();</span><br><span class="line">myDate.getYear(); <span class="comment">//获取当前年份(2位)</span></span><br><span class="line">myDate.getFullYear(); <span class="comment">//获取完整的年份(4位,1970-????)</span></span><br><span class="line">myDate.getMonth(); <span class="comment">//获取当前月份(0-11,0代表1月)</span></span><br><span class="line">myDate.getDate(); <span class="comment">//获取当前日(1-31)</span></span><br><span class="line">myDate.getDay(); <span class="comment">//获取当前星期X(0-6,0代表星期天)</span></span><br><span class="line">myDate.getTime(); <span class="comment">//获取当前时间(从1970.1.1开始的毫秒数)</span></span><br><span class="line">myDate.getHours(); <span class="comment">//获取当前小时数(0-23)</span></span><br><span class="line">myDate.getMinutes(); <span class="comment">//获取当前分钟数(0-59)</span></span><br><span class="line">myDate.getSeconds(); <span class="comment">//获取当前秒数(0-59)</span></span><br><span class="line">myDate.getMilliseconds(); <span class="comment">//获取当前毫秒数(0-999)</span></span><br><span class="line">myDate.toLocaleDateString(); <span class="comment">//获取当前日期</span></span><br><span class="line"><span class="keyword">var</span> mytime=myDate.toLocaleTimeString(); <span class="comment">//获取当前时间</span></span><br><span class="line">myDate.toLocaleString( ); <span class="comment">//获取日期与时间</span></span><br></pre></td></tr></table></figure></li><li><p>时间戳获取<br>注意，java，php等生成的时间戳是秒，不是毫秒，所以需要签名时间戳的时候，需要转为秒时间戳</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> time = <span class="keyword">new</span> <span class="built_in">Date</span>();</span><br><span class="line"><span class="keyword">var</span> timestamp = <span class="built_in">parseInt</span>(time.getTime()/<span class="number">1000</span>);</span><br></pre></td></tr></table></figure></li><li><p>格式化时间</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取当前时间，格式YYYY-MM-DD</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">getNowFormatDate</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> date = <span class="keyword">new</span> <span class="built_in">Date</span>();</span><br><span class="line">    <span class="keyword">var</span> seperator1 = <span class="string">"-"</span>;</span><br><span class="line">    <span class="keyword">var</span> year = date.getFullYear();</span><br><span class="line">    <span class="keyword">var</span> month = date.getMonth() + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">var</span> strDate = date.getDate();</span><br><span class="line">    <span class="keyword">if</span> (month &gt;= <span class="number">1</span> &amp;&amp; month &lt;= <span class="number">9</span>) &#123;</span><br><span class="line">        month = <span class="string">"0"</span> + month;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (strDate &gt;= <span class="number">0</span> &amp;&amp; strDate &lt;= <span class="number">9</span>) &#123;</span><br><span class="line">        strDate = <span class="string">"0"</span> + strDate;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">var</span> currentdate = year + seperator1 + month + seperator1 + strDate;</span><br><span class="line">    <span class="keyword">return</span> currentdate;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> 编程语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>制定学习目标和计划</title>
      <link href="/study-goals.html"/>
      <url>/study-goals.html</url>
      
        <content type="html"><![CDATA[<p>** 制定学习目标和计划：** &lt;Excerpt in index | 首页摘要&gt;<br>    近期的学习目标和学习重点,提高自己的能力<br> <a id="more"></a><br>&lt;The rest of contents | 余下全文&gt;</p><h2 id="找到自己的兴趣"><a href="#找到自己的兴趣" class="headerlink" title="找到自己的兴趣"></a>找到自己的兴趣</h2><ul><li>自己主动学习一定要基于自己的兴趣,不要看什么框架流行,什么语言火,就去学,学的不温不火,然后放弃.</li><li>一定看自己的兴趣,比如你对色彩,对布局,对特效比较痴迷,那你去css3,html5做出特酷的效果,肯定能让你肯定自己,<br>收获知识和自信.</li><li>没有兴趣的时候,可以适当的多接触一些东西,在最短的时间多接触一些领域,让自己的心去做选择,</li></ul><h2 id="制定目标"><a href="#制定目标" class="headerlink" title="制定目标"></a>制定目标</h2><ol><li>为什么要制定目标?</li></ol><ul><li>制定目标是对自己学习能力的检验,同时也是提高学习效率的关键,而不是自己没有目的的瞎看,</li></ul><ol start="2"><li>如何制定目标?</li></ol><ul><li>结合自身的能力,定制比自己能力稍高的目标,这样自己通过一定程度的努力可以实现目标.这样自己的能力能一次一次提高.</li></ul><h2 id="及时反馈"><a href="#及时反馈" class="headerlink" title="及时反馈"></a>及时反馈</h2><ul><li>古人说的好,吾日三省吾身,对待学习目标也是一样,要时不时的看自己的目标完成的如何,进度如何,是不是需要调整,不能闷着头蛮干,方向错了,再多的努力也是白搭了.</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>我在刚开始学编程的时候,每天都给自己定制了目标,一天完成多少课时,完成多少练习,都是按量完成,在最初的几个月收到了立竿见影的效果,让我也在短短三个月的时间学会了java,所以,目标的制定对于结果的影响是非常大.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 个人随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> others </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用ghost搭建个人博客</title>
      <link href="/ghost-blog.html"/>
      <url>/ghost-blog.html</url>
      
        <content type="html"><![CDATA[<p>** 使用ghost搭建个人博客：** &lt;Excerpt in index | 首页摘要&gt;<br>    使用ghost搭建个人博客</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="ghost简介"><a href="#ghost简介" class="headerlink" title="ghost简介"></a>ghost简介</h2><ul><li>ghost是轻量级的博客建站工具,使用起来简单,功能强大,适合个人搭建小型网站,个人博客,或者个人展示的网站</li><li>ghost基于nodejs,对于熟悉js的前端小伙伴来说,入手起来也是简单不少.</li></ul><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ol><li>安装nodejs</li><li>安转git</li><li>配置ssh</li><li>下载ghost</li><li>购买域名</li></ol><h2 id="搭建博客"><a href="#搭建博客" class="headerlink" title="搭建博客"></a>搭建博客</h2><h2 id="定制个人博客"><a href="#定制个人博客" class="headerlink" title="定制个人博客"></a>定制个人博客</h2><h2 id="享受吧"><a href="#享受吧" class="headerlink" title="享受吧"></a>享受吧</h2>]]></content>
      
      
      <categories>
          
          <category> 开发工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> others </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git学习笔记</title>
      <link href="/git-config-study.html"/>
      <url>/git-config-study.html</url>
      
        <content type="html"><![CDATA[<p>** git学习笔记：** &lt;Excerpt in index | 首页摘要&gt;<br>    git的常用操作，高级技巧都要哦</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="安装git"><a href="#安装git" class="headerlink" title="安装git"></a>安装git</h2><ol><li>下载安装包 ￼下载地址￼</li><li>安装git</li><li>进入命令行,输入git看看是否成功</li></ol><h2 id="配置git"><a href="#配置git" class="headerlink" title="配置git"></a>配置git</h2><ol><li>配置全局用户名和密码<br> `git config –global user.name “John Doe”<br> git config –global user.email <a href="mailto:johndoe@example.com" target="_blank" rel="noopener">johndoe@example.com</a><br> `</li><li>配置ssh公钥<br> <code>cd ~/.ssh</code> 然后<code>ls</code><br> 如果没有,直接生成,一路点击enter<br> ```<br> ssh-keygen<br> cat ~/.ssh/id_rsa.pub<br> ```<br> 把公钥配置到github的个人设置</li></ol><h2 id="常用的命令"><a href="#常用的命令" class="headerlink" title="常用的命令"></a>常用的命令</h2><ol><li><p>repository操作</p><ul><li>检出（clone）仓库代码：<code>git clone repository-url</code> / <code>git clone repository-url local-directoryname</code><ul><li>例如，clone jquery 仓库到本地： <code>git clone git://github.com/jquery/jquery.git</code></li><li>clone jquery 仓库到本地，并且重命名为 my-jquery ：<code>git clone git://github.com/jquery/jquery.git my-jquery</code></li></ul></li><li>查看远程仓库：<code>git remote -v</code></li><li>添加远程仓库：<code>git remote add [name] [repository-url]</code></li><li>删除远程仓库：<code>git remote rm [name]</code></li><li>修改远程仓库地址：<code>git remote set-url origin new-repository-url</code></li><li>拉取远程仓库： <code>git pull [remoteName] [localBranchName]</code></li><li>推送远程仓库： <code>git push [remoteName] [localBranchName]</code></li></ul></li><li><p>提交/拉取/合并/删除</p><ul><li><p>添加文件到暂存区（staged）：<code>git add filename</code> / <code>git stage filename</code></p></li><li><p>将所有修改文件添加到暂存区（staged）： <code>git add --all</code> / <code>git add -A</code></p></li><li><p>提交修改到暂存区（staged）：<code>git commit -m &#39;commit message&#39;</code> / <code>git commit -a -m &#39;commit message&#39;</code> 注意理解 -a 参数的意义</p></li><li><p>从Git仓库中删除文件：<code>git rm filename</code></p></li><li><p>从Git仓库中删除文件，但本地文件保留：<code>git rm --cached filename</code></p></li><li><p>重命名某个文件：<code>git mv filename newfilename</code> 或者直接修改完毕文件名 ，进行<code>git add -A &amp;&amp; git commit -m &#39;commit message&#39;</code> Git会自动识别是重命名了文件</p></li><li><p>获取远程最新代码到本地：<code>git pull (origin branchname)</code> 可以指定分支名，也可以忽略。pull 命令自动 fetch 远程代码并且 merge，如果有冲突，会显示在状态栏，需要手动处理。更推荐使用：<code>git fetch</code> 之后 <code>git merge --no-ff origin branchname</code> 拉取最新的代码到本地仓库，并手动 merge 。</p></li></ul></li><li><p>日志查看</p><ul><li>查看日志：<code>git log</code></li><li>查看日志，并查看每次的修改内容：<code>git log -p</code></li><li>查看日志，并查看每次文件的简单修改状态：<code>git log --stat</code></li><li>一行显示日志：<code>git log --pretty=oneline</code> / <code>git log --pretty=&#39;format:&quot;%h - %an, %ar : %s&#39;</code></li><li>查看日志范围：<ul><li>查看最近10条日志：<code>git log -10</code></li><li>查看2周前：<code>git log --until=2week</code> 或者指定2周的明确日期，比如：<code>git log --until=2015-08-12</code></li><li>查看最近2周内：<code>git log --since=2week</code> 或者指定2周明确日志，比如：<code>git log --since=2015-08-12</code></li><li>只查看某个用户的提交：<code>git log --committer=user.name</code> / <code>git log --author=user.name</code></li></ul></li></ul></li><li><p>取消操作</p><ul><li>上次提交msg错误/有未提交的文件应该同上一次一起提交，需要重新提交备注：<code>git commit --amend -m &#39;new msg&#39;</code></li><li>一次<code>git add -A</code>后，需要将某个文件撤回到工作区，即：某个文件不应该在本次commit中：<code>git reset HEAD filename</code></li><li>撤销某些文件的修改内容：<code>git checkout -- filename</code> 注意：一旦执行，所有的改动都没有了，谨慎！谨慎！谨慎！</li><li>将工作区内容回退到远端的某个版本：<code>git reset --hard &lt;sha1-of-commit&gt;</code><ul><li><code>--hard</code>：reset stage and working directory ,<commitid> 以来所有的变更全部丢弃，并将 HEAD 指向<commitid></commitid></commitid></li><li><code>--soft</code>：nothing changed to stage and working directory ,仅仅将HEAD指向<commitid> ，所有变更显示在”changed to be committed”中</commitid></li><li><code>--mixed</code>：default,reset stage ,nothing to working directory ，这也就是第二个例子的原因</li></ul></li></ul></li><li><p>比较差异</p><ul><li>查看工作区（working directory）和暂存区（staged）之间差异：<code>git diff</code></li><li>查看工作区（working directory）与当前仓库版本（repository）HEAD版本差异：<code>git diff HEAD</code></li><li>查看暂存区（staged）与当前仓库版本（repository）差异：<code>git diff --cached</code> / <code>git diff --staged</code></li></ul></li><li><p>合并操作</p><ul><li>解决冲突后/获取远程最新代码后合并代码：<code>git merge branchname</code></li><li>保留该存在版本合并log：<code>git merge --no-ff branchname</code> 参数<code>--no-ff</code>防止 fast-forward 的提交</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> others </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ubuntu16服务器环境配置</title>
      <link href="/ubuntu-dev-config.html"/>
      <url>/ubuntu-dev-config.html</url>
      
        <content type="html"><![CDATA[<p>** ubuntu开发环境配置：** &lt;Excerpt in index | 首页摘要&gt;<br>    ubuntu16下node,java开发环境配置<br> <a id="more"></a><br>&lt;The rest of contents | 余下全文&gt;</p><h2 id="ubuntu14升级到ubuntu16"><a href="#ubuntu14升级到ubuntu16" class="headerlink" title="ubuntu14升级到ubuntu16"></a>ubuntu14升级到ubuntu16</h2><ol><li>终端下执行命令<br><code>sudo apt-get update &amp;&amp; sudo apt-get dist-upgrade</code></li><li>重启系统以完成更新的安装<br><code>sudo init 6</code></li><li>用命令安装更新管理器核心update-manager-core，如果服务器已安装则可以跳过<br><code>sudo apt-get install update-manager-core</code></li><li>编辑/etc/update-manager/release-upgrades配置文件，设置Prompt=lts<br><code>sudo vi /etc/update-manager/release-upgrades</code></li><li>启动升级进程<br><code>sudo do-release-upgrade -d</code></li></ol><h2 id="安装系统软件"><a href="#安装系统软件" class="headerlink" title="安装系统软件"></a>安装系统软件</h2><ol><li><p>更新系统和软件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get upgade</span><br></pre></td></tr></table></figure></li><li><p>谷歌浏览器，火狐浏览器，atom编辑器，sublime编辑器，webstome,idea,eclipse</p></li><li><p>安装搜狗输入法（官网），安装fcitx配置搜狗输入法</p><h2 id="安装jdk"><a href="#安装jdk" class="headerlink" title="安装jdk"></a>安装jdk</h2></li><li><p>下载jdk并新建一个文件夹</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /usr/lib/jvm</span><br></pre></td></tr></table></figure></li><li><p>解压文件</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo tar zxvf jdk-7u71-linux-x64.tar.gz -C /usr/lib/jvm/jdk1.7</span><br></pre></td></tr></table></figure></li><li><p>设置环境变量,设置~/.zshrc文件,或者编辑/etc/profile（全局）文件</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/lib/jvm/jdk1.7</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre  </span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib  </span><br><span class="line">export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure></li><li><p>检查是否安装成功<br> 打开shell,</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java --version</span><br></pre></td></tr></table></figure></li></ol><h2 id="安装nodejs"><a href="#安装nodejs" class="headerlink" title="安装nodejs"></a>安装nodejs</h2><ol><li><p>nodejs版本迭代较快，有时候需要检查在不同版本下的兼容性问题，用nvm来控制版本</p></li><li><p>安装nvm,source的时候根据自己的shell版本，~/.bashrc, ~/.profile, 或者 ~/.zshrc</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.31.0/install.sh | bash</span><br><span class="line">export NVM_DIR=&quot;$HOME/.nvm&quot;</span><br><span class="line">[ -s &quot;$NVM_DIR/nvm.sh&quot; ] &amp;&amp; . &quot;$NVM_DIR/nvm.sh&quot; # This loads nvm</span><br><span class="line">source ~/.profile</span><br></pre></td></tr></table></figure></li><li><p>安装不同版本的nodejs<br>　　<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nvm ls-remote</span><br><span class="line">nvm install v0.12.9</span><br><span class="line">nvm install 5.0</span><br><span class="line">nvm use 0.12.9</span><br><span class="line">nvm alias default 0.12.9</span><br></pre></td></tr></table></figure></p></li></ol><h2 id="安装mongodb"><a href="#安装mongodb" class="headerlink" title="安装mongodb"></a>安装mongodb</h2><ol><li><p>配置公钥</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"deb http://repo.mongodb.org/apt/ubuntu "</span>$(lsb_release -sc)<span class="string">"/mongodb-org/3.0 multiverse"</span> | sudo tee /etc/apt/sources.list.d/mongodb-org-3.0.list</span><br></pre></td></tr></table></figure></li><li><p>更新软件列表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y mongodb-org</span><br></pre></td></tr></table></figure></li><li><p>完成上面的安装步骤配置mongodb的数据库的位置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mongod --dbpath /data/db</span><br></pre></td></tr></table></figure></li><li><p>启动mongod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo service mongod start</span><br><span class="line">sudo service mongod stop</span><br><span class="line">sudo service mongod restart</span><br></pre></td></tr></table></figure></li></ol><h2 id="安装redis"><a href="#安装redis" class="headerlink" title="安装redis"></a>安装redis</h2><ol><li><p>下载软件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://download.redis.io/releases/redis-2.8.11.tar.gz</span><br></pre></td></tr></table></figure></li><li><p>解压安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar xvfz redis-2.8.11.tar.gz</span><br><span class="line"><span class="built_in">cd</span> redis-2.8.11 &amp;&amp; sudo make &amp;&amp; sudo make install</span><br></pre></td></tr></table></figure></li><li><p>配置使用</p><ol><li><p>下载配置文件和init启动脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/ijonas/dotfiles/raw/master/etc/init.d/redis-server</span><br><span class="line">wget https://github.com/ijonas/dotfiles/raw/master/etc/redis.conf</span><br><span class="line">sudo mv redis-server /etc/init.d/redis-server</span><br><span class="line">sudo chmod +x /etc/init.d/redis-server</span><br><span class="line">sudo mv redis.conf /etc/redis.conf</span><br></pre></td></tr></table></figure></li><li><p>初始化用户和日志路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo useradd redis</span><br><span class="line">sudo mkdir -p /var/lib/redis</span><br><span class="line">sudo mkdir -p /var/<span class="built_in">log</span>/redis</span><br><span class="line">sudo chown redis.redis /var/lib/redis</span><br><span class="line">sudo chown redis.redis /var/<span class="built_in">log</span>/redis</span><br></pre></td></tr></table></figure></li><li><p>设置开机自动启动，关机自动关闭</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo update-rc.d redis-server defaults</span><br></pre></td></tr></table></figure></li></ol></li></ol><h2 id="环境变量配置"><a href="#环境变量配置" class="headerlink" title="环境变量配置"></a>环境变量配置</h2><ol><li>认识环境变量相关的文件</li></ol><ul><li>/etc/profile —— 此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行.并从/etc/profile.d目录的配置文件中搜集shell的设置；</li><li>/etc/environment —— 在登录时操作系统使用的第二个文件,系统在读取你自己的profile前,设置环境文件的环境变量；</li><li>/etc/bashrc —— 为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取；</li><li>~/.profile —— 每个用户都可使用该文件输入专用于自己使用的shell信息，当用户登录时，该文件仅仅执行一次！默认情况下,它设置一些环境变量,执行用户的.bashrc文件；</li><li>~/.bashrc —— 该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该文件被读取；</li></ul><ol start="2"><li>配置环境变量</li></ol><ul><li><p>在Ubuntu14.04的~/.bashrc中添加的环境变量,在文件添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/home/qtcreator-2.6.1/bin</span><br></pre></td></tr></table></figure></li><li><p>修改profile文件,vim编辑/etc/profile</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/profile</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure></li></ul><h2 id="安装开发工具"><a href="#安装开发工具" class="headerlink" title="安装开发工具"></a>安装开发工具</h2><ol><li>zsh命令行工具</li><li>mysql客户端workbench，mongo客户端工具robomongo</li><li>安装git,svn版本控制工具<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install git</span><br><span class="line">sudo apt-get install subversion</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> 开发工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我的梦想</title>
      <link href="/dream.html"/>
      <url>/dream.html</url>
      
        <content type="html"><![CDATA[<p>** 我的梦想：** &lt;Excerpt in index | 首页摘要&gt;<br>    一个人如果活着没有梦想,那和咸鱼有什么区别?</p><a id="more"></a>请问你的梦想是什么?<p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="我的梦想是什么"><a href="#我的梦想是什么" class="headerlink" title="我的梦想是什么?"></a>我的梦想是什么?</h2><ul><li><p>刚开始接触编程的时候,感觉代码是个神器的世界,在这里你可以为所欲为,然后看到很多大神的框架,软件,在使用别人好的框架,好的软件,那一刻我感觉 <strong>“我的梦想就是用代码改变世界!”</strong></p></li><li><p>感觉自己迷失了好久,找不到方向,曾经的激情不知道去了哪里?</p></li></ul><h2 id="开始追梦"><a href="#开始追梦" class="headerlink" title="开始追梦"></a>开始追梦</h2><ul><li>有了梦想,我开始了疯狂的奋斗,每天休息4,5个小时,全身心去学习编程,努力还是很快得到了回报,我用了3个月就入门学好了java,然后找了java程序员的工作,就这样开始了我程序员的追梦之旅!</li></ul><h2 id="初级程序员"><a href="#初级程序员" class="headerlink" title="初级程序员"></a>初级程序员</h2><ul><li>虽然入门了,但是刚开始的工作并不是一帆风顺的.我还记得第一份任务,老大让我写一个稍微复杂的接口,客户专用的接口,使用springmvc,还要提交到git上,对我而言,这一切都是新东西,经过我几天的努力,还是搞砸了,就这样第一个任务以失败告终!</li><li>虽然第一个任务失败了,但是工作还在继续,我还是继续努力的工作,我必须承认我不是编程的天才,可能别人一个小时完成的任务,我需要一个半小时,但是我必须做好,因为我有梦想!</li></ul><h2 id="中级程序员"><a href="#中级程序员" class="headerlink" title="中级程序员"></a>中级程序员</h2><ul><li><p>在工作的时候就感觉时间飞逝,一天天很快过去.晚上睡觉的时候,我就会问自己,我今天到底做了什么功能?我收获了哪些技能?曾经有段时间每天都是该页面,我几乎烦的崩溃,感觉每天都在做无用的东西,后来发现,无论是前段后端,其实都是必不可少的技能,我的心态应该调整,让自己去喜欢前段,同时保持后端的热情.</p></li><li><p>一个成熟的程序员和菜鸟最大的区别应该是心态!</p></li></ul><h2 id="高级程序员"><a href="#高级程序员" class="headerlink" title="高级程序员"></a>高级程序员</h2><ul><li>不再是代码搬运工，根据业务和需求自己随便造个轮子什么的。强大的代码能力，考虑事情应该全面，深刻</li></ul><h2 id="架构师"><a href="#架构师" class="headerlink" title="架构师"></a>架构师</h2><ul><li>未完待续</li></ul>]]></content>
      
      
      <categories>
          
          <category> 个人随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> others </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>程序员入门指南</title>
      <link href="/coder-study.html"/>
      <url>/coder-study.html</url>
      
        <content type="html"><![CDATA[<p>** 程序员入门指南 ：** &lt;Excerpt in index | 首页摘要&gt;<br>        程序员入门必须了解的一些知识，个人经验，不喜勿喷！</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="程序员的入门规划"><a href="#程序员的入门规划" class="headerlink" title="程序员的入门规划"></a>程序员的入门规划</h2><h2 id="1-我该学习什么语言？"><a href="#1-我该学习什么语言？" class="headerlink" title="1.我该学习什么语言？"></a>1.我该学习什么语言？</h2><ul><li><p>这个问题困扰了几乎所有的程序员，比如java应用广好就业，比如php入门简单，ios待遇高，  python是万能语言，HTML和js前端缺人才等等</p></li><li><p>个人见解：先学习难度小，大众化的编程语言，比如java，php，python，javascript,c/c++,这几个学哪一种其实差不多，入门以后看自己兴趣在进行其它语言的学习。</p></li></ul><h2 id="2-我该怎么学习编程？"><a href="#2-我该怎么学习编程？" class="headerlink" title="2.我该怎么学习编程？"></a>2.我该怎么学习编程？</h2><p>这个问题是所有的程序员都有的，我也经常会疑问，到底该怎么学习呢？</p><p>个人见解：</p><ol><li>先了解语言的特性，适用的范围场景，比如是适合web开发，还是适合客户端程序，有的适合并发多线程，有的适合异步，还有的比较稳定，适合构建大型项目，有的开发效率高，等等。</li><li>了解语言的语法和常用api的使用，比如变量的声明，循环的使用，io的读取，http服务的创建，把这些基本的语法搞清楚，在进行下一步的学习。</li><li>学习web开发之前的准备，数据库的学习，http协议的学习，html，css和javacript的常用知识了解</li><li>学习常用框架，比如java学习常用的ssh三大框架，node的学习express，一定要做2个项目练习，把自己的之前学习的知识都巩固一下，</li><li>总结一下自己学习的过程，明白编程的思想在哪里，思路在哪里，学习编程，首先应该培养的是编程的思维和思想，有个正确的思维后面都简单多了。</li><li>养成写博客或者学习笔记的习惯，推荐写博客，</li><li>熟悉项目管理工具，svn，git之类的必须要会，工作中这些都是必须的</li><li>准备面试，通过面试题进一步巩固自己的知识，夯实基础。</li></ol><h2 id="3-我应该去哪里学习编程？"><a href="#3-我应该去哪里学习编程？" class="headerlink" title="3.我应该去哪里学习编程？"></a>3.我应该去哪里学习编程？</h2><p>其实这个看个人，如果自学能力强，自控能力强，自学挺好的，下面我列举几个程序员常用的网站  </p><ol><li><a href="http://study.163.com/" target="_blank" rel="noopener">网易云课堂</a>，很多免费的视频课程，适合入门学习</li><li><a href="http://www.imooc.com/" target="_blank" rel="noopener">慕课网</a>，很多it入门教学视频，资源也不错</li><li><a href="http://www.jikexueyuan.com/" target="_blank" rel="noopener">极客学院</a>，和前两个网站差不多，</li><li><a href="http://www.ibeifeng.com/" target="_blank" rel="noopener">北风网</a>，类似的教学网站，其它的就不说了</li></ol><h2 id="4-编程遇到问题怎么办？"><a href="#4-编程遇到问题怎么办？" class="headerlink" title="4.编程遇到问题怎么办？"></a>4.编程遇到问题怎么办？</h2><ul><li>百度或者谷歌看看网上有没有类似的问题，一回生，二回熟，很快就明白了</li><li>去官网查看api文档查找原因</li><li>自己要学会debug代码，查找原因</li><li>去各大论坛逛逛，说不定早有人提问此类问题了</li></ul><h2 id="5-我想看编程的书籍去哪找呢？"><a href="#5-我想看编程的书籍去哪找呢？" class="headerlink" title="5.我想看编程的书籍去哪找呢？"></a>5.我想看编程的书籍去哪找呢？</h2><p>经典书籍还是买纸质的，买正版的，支持正版！</p><ol><li><a href="http://vdisk.weibo.com/" target="_blank" rel="noopener">新浪微盘</a>，非常多的it书籍  </li><li><a href="http://www.jb51.net/" target="_blank" rel="noopener">脚本之家</a>，非常多的pdf书籍，可惜大多数不是文字版pdf  </li><li><a href="http://www.salttiger.com/" target="_blank" rel="noopener">英文原版书籍</a>，都是高清文字版pdf，强烈推荐，都是英文原版的  </li><li><a href="http://bestcbooks.com/" target="_blank" rel="noopener">计算机书控</a>，都是免费的pdf文档，大多数不是文字版pdf</li></ol><h2 id="6-代码资源"><a href="#6-代码资源" class="headerlink" title="6.代码资源"></a>6.代码资源</h2><ol><li>最好的代码仓库 <a href="https://github.com/" target="_blank" rel="noopener">github</a></li><li><a href="https://code.csdn.net/" target="_blank" rel="noopener">csdn代码仓库</a></li><li><a href="https://gist.github.com/" target="_blank" rel="noopener">gist</a></li><li><a href="http://www.phpxs.com/code/" target="_blank" rel="noopener">代码片段之家</a></li></ol><h2 id="7-学习心态"><a href="#7-学习心态" class="headerlink" title="7.学习心态"></a>7.学习心态</h2><ol><li>不要老是折腾工具，ide工具和文本编辑器一样一个就够了</li><li>不要自满，编程的东西学一辈子也学不会，要谦虚好学</li><li>不要急躁，既然知识学不完，我们应该掌握学习方法，指定计划去学习</li><li>要持之以恒，学习是一辈子的事，如果你没有这个打算，还是不要做程序员的好</li><li>切忌眼高手低，必须要敲代码才能达到效果</li></ol><h2 id="8-编程进阶之路"><a href="#8-编程进阶之路" class="headerlink" title="8.编程进阶之路"></a>8.编程进阶之路</h2><p>当有了一定的编程基础之后,最大的问题是确定自己的方向,这个时候最容易迷茫和困惑,学习什么技术?怎么去学,这些真的很难</p><ul><li>个人建议如下:<br>1.技术型方向:提高自己的编程能力和语言造诣,最有效的是”造轮子”,量变引起质变</li><li>写插件,写框架,写爬虫,写数据库,自制编程语言,等等.<br>2.业务型方向:提高自己的业务能力,和客户的沟通能力,分析需求,解决客户的难题</li><li>多出去见客户,去现场,了解需求,分析需求,</li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> others </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo和github打造个人博客</title>
      <link href="/hexo-githup-blog.html"/>
      <url>/hexo-githup-blog.html</url>
      
        <content type="html"><![CDATA[<p>** hexo和github打造个人博客 ：** &lt;Excerpt in index | 首页摘要&gt;<br>    使用hexo和github打造属于自己的静态博客，展示自己的作品，思想……</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>##说明<br>    自己在使用hexo搭建静态博客的时候踩了许多坑,最终去官网看教程搞定了,<br>    建议用hexo搭建个人博客的时候,最好看清教程的日期和使用的版本,这样就<br>    不会因为版本的不同导致的问题了.建议先去hexo官网了解一下<br>   <a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener"><strong>hexo官网</strong></a></p><h2 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1.准备工作"></a>1.准备工作</h2><ol><li>安装nodejs<ul><li>去官网下载nodejs安装(推荐安装4.x),安装之后在命令行 node -v,如果成功说明node环境ok,不成功就去环境变量配置一下.</li></ul></li><li>安装hexo<ul><li>使用命令 npm install hexo -g,执行hexo -v 查看版本,本教程适合<strong>3.1.1以上</strong>版本</li></ul></li><li>安装git<ul><li>去官网下载git安装,不会自行百度</li></ul></li><li>配置git<ul><li>配置ssh私钥,上传到github上</li></ul></li></ol><h2 id="2-github-pages的说明"><a href="#2-github-pages的说明" class="headerlink" title="2.github-pages的说明"></a>2.github-pages的说明</h2><ol><li>github有两种主页,一种是github-page(个人主页),一种是项目主页,本教程针对个人主页</li><li>github-page需要将hexo博客发布到repository的master(主干)即可</li><li>github的个人主页要求repository的名称和username一致，加入username是tom，则repository的名称为tom.github.io</li></ol><h2 id="3-使用hexo写博客"><a href="#3-使用hexo写博客" class="headerlink" title="3.使用hexo写博客"></a>3.使用hexo写博客</h2><pre><code>- 新建一个文件夹myblog,- 右键git bash here使用git的shell- 在shell中输入hexo init,回车执行- 在shell中输入hexo g ,回车- 在shell中hexo s,回车- 去浏览器访问http://localhost:4000,访问到主页,然后在shell中ctrl c停止- 在shell中hexo new &quot;first-blog&quot;,回车- 在shell中hexo g ,回车- 在shell中hexo s ,回车,在访问- ok,在本地测试就没问题了</code></pre><h2 id="4-发布到github"><a href="#4-发布到github" class="headerlink" title="4.发布到github"></a>4.发布到github</h2><p>打开项目根部录下的.config.yml,找到deploy,修改如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line"> - type: git</span><br><span class="line">   repo: git@github.com:yourname/yourname.github.io.git,master</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: &lt;repository url&gt;</span><br><span class="line">  branch: [branch]</span><br><span class="line">  message: [message]</span><br></pre></td></tr></table></figure><p>访问地址就是 <a href="http://tom.github.io/" target="_blank" rel="noopener">http://tom.github.io/</a></p><h2 id="5-常用命令"><a href="#5-常用命令" class="headerlink" title="5.常用命令"></a>5.常用命令</h2><pre><code>命令的简写为：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hexo n == hexo new</span><br><span class="line">hexo g == hexo generate</span><br><span class="line">hexo s == hexo server</span><br><span class="line">hexo d == hexo deploy</span><br><span class="line">hexo clean  删除public文件夹</span><br></pre></td></tr></table></figure></code></pre><h2 id="6-常见问题"><a href="#6-常见问题" class="headerlink" title="6.常见问题"></a>6.常见问题</h2><ol><li>部署时出现git not found<ul><li>npm install hexo-deployer-git –save  安装依赖包</li></ul></li></ol><h2 id="7-详细设置"><a href="#7-详细设置" class="headerlink" title="7.详细设置"></a>7.详细设置</h2><pre><code>每个人对自己的博客都有不一样的要求，比如主题，分类，标签，评论插件的选择，  这些对程序员的你来说，都是小菜一碟，下面是官网教程：</code></pre><p>   <a href="https://hexo.io/docs/" target="_blank" rel="noopener">hexo官方文档</a></p><p>博客效果可以看我的个人博客     <a href="https://www.duduhuahua.cn" target="_blank" rel="noopener">我的个人博客</a></p>]]></content>
      
      
      <categories>
          
          <category> 开发工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>node学习</title>
      <link href="/node-study.html"/>
      <url>/node-study.html</url>
      
        <content type="html"><![CDATA[<p>** node学习： ** &lt;Excerpt in index | 首页摘要&gt;<br>    nodejs学习的方法，进阶路线<br> <a id="more"></a><br>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一-学习内容"><a href="#一-学习内容" class="headerlink" title="一 学习内容"></a>一 学习内容</h2><ol><li>node的常用模块,buffer,fs,http,net等.</li><li>node常用框架express,mongoose,koa,mocha,should</li><li>部署上线,pm2,grunt,</li></ol><h2 id="二-学习要点"><a href="#二-学习要点" class="headerlink" title="二 学习要点"></a>二 学习要点</h2><ol><li>了解node的特性和语法</li><li>编写扩展node模块</li><li>用异步的思想编程</li><li>常用框架的使用</li><li>回调的解决方案(promise)</li></ol><h2 id="三-入门实战"><a href="#三-入门实战" class="headerlink" title="三 入门实战"></a>三 入门实战</h2><ol><li>参照nodejs实战上的微博系统,使用express4.x+ mongoose实现</li><li>使用socket.io实现一个简单的即时聊天的系统</li><li>使用mongoose+express+node开发一个论坛系统</li><li>使用koa+mongoose做一个简单的cms或者权限系统</li></ol><h2 id="四-学习方法"><a href="#四-学习方法" class="headerlink" title="四 学习方法"></a>四 学习方法</h2><ul><li>建议有基础的直接开始入门实战,在练习中熟悉node的api,做完一个项目再去看书</li><li>不要一直看书,没什么效果的,实战永远是最有效的</li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程语言 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> node </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模板</title>
      <link href="/2015-08-15-%E6%A8%A1%E6%9D%BF.html"/>
      <url>/2015-08-15-%E6%A8%A1%E6%9D%BF.html</url>
      
        <content type="html"><![CDATA[<p>** 模板：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （一）Spark初识</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
