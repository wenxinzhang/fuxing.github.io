<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>福星</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zhangfuxin.cn/"/>
  <updated>2019-09-18T16:01:41.367Z</updated>
  <id>http://zhangfuxin.cn/</id>
  
  <author>
    <name>福 星</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Parquet文件存储格式</title>
    <link href="http://zhangfuxin.cn/2019-09-18-Parquet%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F.html"/>
    <id>http://zhangfuxin.cn/2019-09-18-Parquet文件存储格式.html</id>
    <published>2019-09-18T02:30:04.000Z</published>
    <updated>2019-09-18T16:01:41.367Z</updated>
    
    <content type="html"><![CDATA[<p>** Parquet文件存储格式：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Parquet文件存储格式</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h1 id="一、Parquet的组成"><a href="#一、Parquet的组成" class="headerlink" title="一、Parquet的组成"></a>一、<strong>Parquet的组成</strong></h1><p>Parquet仅仅是一种存储格式，它是语言、平台无关的，并且不需要和任何一种数据处理框架绑定，目前能够和Parquet适配的组件包括下面这些，可以看出基本上通常使用的查询引擎和计算框架都已适配，并且可以很方便的将其它序列化工具生成的数据转换成Parquet格式。</p><ul><li>查询引擎: Hive, Impala, Pig, Presto, Drill, Tajo, HAWQ, IBM Big SQL</li><li>计算框架: MapReduce, Spark, Cascading, Crunch, Scalding, Kite</li><li>数据模型: Avro, Thrift, Protocol Buffers, POJOs</li></ul><h2 id="项目组成"><a href="#项目组成" class="headerlink" title="项目组成"></a>项目组成</h2><p>Parquet项目由以下几个子项目组成:</p><ul><li><a href="https://github.com/apache/parquet-format" target="_blank" rel="noopener">parquet-format</a>项目由java实现，它定义了所有Parquet元数据对象，Parquet的元数据是使用Apache Thrift进行序列化并存储在Parquet文件的尾部。</li><li><a href="https://github.com/apache/parquet-mr" target="_blank" rel="noopener">parquet-format</a>项目由java实现，它包括多个模块，包括实现了读写Parquet文件的功能，并且提供一些和其它组件适配的工具，例如Hadoop Input/Output Formats、Hive Serde(目前Hive已经自带Parquet了)、Pig loaders等。</li><li><a href="https://github.com/Parquet/parquet-compatibility" target="_blank" rel="noopener">parquet-compatibility</a>项目，包含不同编程语言之间(JAVA和C/C++)读写文件的测试代码。</li><li><a href="https://github.com/apache/parquet-cpp" target="_blank" rel="noopener">parquet-cpp</a>项目，它是用于用于读写Parquet文件的C++库。</li></ul><p>下图展示了Parquet各个组件的层次以及从上到下交互的方式。</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017105343302-1717603971.png" alt="img"></p><ul><li>数据存储层定义了Parquet的文件格式，其中元数据在parquet-format中定义，包括Parquet原始类型定义、Page类型、编码类型、压缩类型等等。</li><li>对象转换层完成其他对象模型与Parquet内部数据模型的映射和转换，Parquet的编码方式使用的是striping and assembly算法。</li><li>对象模型层定义了如何读取Parquet文件的内容，这一层转换包括Avro、Thrift、PB等序列化格式、Hive serde等的适配。并且为了帮助大家理解和使用，Parquet提供了org.apache.parquet.example包实现了java对象和Parquet文件的转换。</li></ul><h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a><strong>数据模型</strong></h2><p>Parquet支持嵌套的数据模型，类似于Protocol Buffers，每一个数据模型的schema包含多个字段，每一个字段又可以包含多个字段，每一个字段有三个属性：重复数、数据类型和字段名，重复数可以是以下三种：required(出现1次)，repeated(出现0次或多次)，optional(出现0次或1次)。每一个字段的数据类型可以分成两种：group(复杂类型)和primitive(基本类型)。例如Dremel中提供的Document的schema示例，它的定义如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">message <span class="type">Document</span> &#123;</span><br><span class="line">    required int64 <span class="type">DocId</span>;</span><br><span class="line">    optional group <span class="type">Links</span> &#123;</span><br><span class="line">        repeated int64 <span class="type">Backward</span>;</span><br><span class="line">        repeated int64 <span class="type">Forward</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    repeated group <span class="type">Name</span> &#123;</span><br><span class="line">        repeated group <span class="type">Language</span> &#123;</span><br><span class="line">            required string <span class="type">Code</span>;</span><br><span class="line">            optional string <span class="type">Country</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        optional string <span class="type">Url</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以把这个Schema转换成树状结构，根节点可以理解为repeated类型，如下图: </p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017105738912-789367511.png" alt="img"></p><p>可以看出在Schema中所有的基本类型字段都是叶子节点，在这个Schema中一共存在6个叶子节点，如果把这样的Schema转换成扁平式的关系模型，就可以理解为该表包含六个列。Parquet中没有Map、Array这样的复杂数据结构，但是可以通过repeated和group组合来实现这样的需求。在这个包含6个字段的表中有以下几个字段和每一条记录中它们可能出现的次数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DocId                 int64    只能出现一次 </span><br><span class="line">Links.Backward        int64    可能出现任意多次，但是如果出现0次则需要使用NULL标识 </span><br><span class="line">Links.Forward         int64    同上 </span><br><span class="line">Name.Language.Code    string   同上 </span><br><span class="line">Name.Language.Country string   同上 </span><br><span class="line">Name.Url              string   同上</span><br></pre></td></tr></table></figure><p>由于在一个表中可能存在出现任意多次的列，对于这些列需要标示出现多次或者等于NULL的情况，它是由Striping/Assembly算法实现的。</p><h2 id="Striping-Assembly算法"><a href="#Striping-Assembly算法" class="headerlink" title="Striping/Assembly算法"></a>Striping/Assembly算法</h2><p>上文介绍了Parquet的数据模型，在Document中存在多个非required列，由于Parquet一条记录的数据分散的存储在不同的列中，如何组合不同的列值组成一条记录是由Striping/Assembly算法决定的，在该算法中列的每一个值都包含三部分：value、repetition level和definition level。</p><h3 id="Repetition-Levels"><a href="#Repetition-Levels" class="headerlink" title="Repetition Levels"></a><strong>Repetition Levels</strong></h3><p>为了支持repeated类型的节点，在写入的时候该值等于它和前面的值在哪一层节点是不共享的。在读取的时候根据该值可以推导出哪一层上需要创建一个新的节点，例如对于这样的一个schema和两条记录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">message nested &#123;</span><br><span class="line">     repeated group leve1 &#123;</span><br><span class="line">          repeated string leve2;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line">r1:[[a,b,c,] , [d,e,f,g]]</span><br><span class="line">r2:[[h] , [i,j]]</span><br></pre></td></tr></table></figure><p>计算repetition level值的过程如下：</p><ul><li>value=a是一条记录的开始，和前面的值(已经没有值了)在根节点(第0层)上是不共享的，所以repeated level=0.</li><li>value=b它和前面的值共享了level1这个节点，但是level2这个节点上是不共享的，所以repeated level=2.</li><li>同理value=c, repeated level=2.</li><li>value=d和前面的值共享了根节点(属于相同记录)，但是在level1这个节点上是不共享的，所以repeated level=1.</li><li>value=h和前面的值不属于同一条记录，也就是不共享任何节点，所以repeated level=0.</li></ul><p>根据以上的分析每一个value需要记录的repeated level值如下：</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017111957865-335321751.png" alt="img"></p><p>在读取的时候，顺序的读取每一个值，然后根据它的repeated level创建对象，当读取value=a时repeated level=0，表示需要创建一个新的根节点(新记录)，value=b时repeated level=2，表示需要创建一个新的level2节点，value=d时repeated level=1，表示需要创建一个新的level1节点，当所有列读取完成之后可以创建一条新的记录。本例中当读取文件构建每条记录的结果如下：</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017112045865-798020169.png" alt="img"></p><p>可以看出repeated level=0表示一条记录的开始，并且repeated level的值只是针对路径上的repeated类型的节点，因此在计算该值的时候可以忽略非repeated类型的节点，在写入的时候将其理解为该节点和路径上的哪一个repeated节点是不共享的，读取的时候将其理解为需要在哪一层创建一个新的repeated节点，这样的话每一列最大的repeated level值就等于路径上的repeated节点的个数（不包括根节点）。减小repeated level的好处能够使得在存储使用更加紧凑的编码方式，节省存储空间。</p><h3 id="Definition-Levels"><a href="#Definition-Levels" class="headerlink" title="Definition Levels"></a><strong>Definition Levels</strong></h3><p>有了repeated level我们就可以构造出一个记录了，为什么还需要definition levels呢？由于repeated和optional类型的存在，可能一条记录中某一列是没有值的，假设我们不记录这样的值就会导致本该属于下一条记录的值被当做当前记录的一部分，从而造成数据的错误，因此对于这种情况需要一个占位符标示这种情况。</p><p>definition level的值仅仅对于空值是有效的，表示在该值的路径上第几层开始是未定义的，对于非空的值它是没有意义的，因为非空值在叶子节点是定义的，所有的父节点也肯定是定义的，因此它总是等于该列最大的definition levels。例如下面的schema。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">message <span class="type">ExampleDefinitionLevel</span> &#123;</span><br><span class="line">  optional group a &#123;</span><br><span class="line">    optional group b &#123;</span><br><span class="line">      optional string c;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>它包含一个列a.b.c，这个列的的每一个节点都是optional类型的，当c被定义时a和b肯定都是已定义的，当c未定义时我们就需要标示出在从哪一层开始时未定义的，如下面的值：</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017112121037-646635962.png" alt="img"></p><p>由于definition level只需要考虑未定义的值，而对于repeated类型的节点，只要父节点是已定义的，该节点就必须定义（例如Document中的DocId，每一条记录都该列都必须有值，同样对于Language节点，只要它定义了Code必须有值），所以计算definition level的值时可以忽略路径上的required节点，这样可以减小definition level的最大值，优化存储。</p><h3 id="一个完整的例子"><a href="#一个完整的例子" class="headerlink" title="一个完整的例子"></a><strong>一个完整的例子</strong></h3><p>本节我们使用Dremel论文中给的Document示例和给定的两个值r1和r2展示计算repeated level和definition level的过程，这里把未定义的值记录为NULL，使用R表示repeated level，D表示definition level。</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017112320177-629116810.png" alt="img"></p><p>首先看DocuId这一列，对于r1，DocId=10，由于它是记录的开始并且是已定义的，所以R=0，D=0，同样r2中的DocId=20，R=0，D=0。</p><p>对于Links.Forward这一列，在r1中，它是未定义的但是Links是已定义的，并且是该记录中的第一个值，所以R=0，D=1，在r1中该列有两个值，value1=10，R=0(记录中该列的第一个值)，D=2(该列的最大definition level)。</p><p>对于Name.Url这一列，r1中它有三个值，分别为url1=’<a href="http://a/" target="_blank" rel="noopener">http://A</a>‘，它是r1中该列的第一个值并且是定义的，所以R=0，D=2；value2=’<a href="http://b/" target="_blank" rel="noopener">http://B</a>‘，和上一个值value1在Name这一层是不相同的，所以R=1，D=2；value3=NULL，和上一个值value2在Name这一层是不相同的，所以R=1，但它是未定义的，而Name这一层是定义的，所以D=1。r2中该列只有一个值value3=’<a href="http://c/" target="_blank" rel="noopener">http://C</a>‘，R=0，D=2.</p><p>最后看一下Name.Language.Code这一列，r1中有4个值，value1=’en-us’，它是r1中的第一个值并且是已定义的，所以R=0，D=2(由于Code是required类型，这一列repeated level的最大值等于2)；value2=’en’，它和value1在Language这个节点是不共享的，所以R=2，D=2；value3=NULL，它是未定义的，但是它和前一个值在Name这个节点是不共享的，在Name这个节点是已定义的，所以R=1，D=1；value4=’en-gb’，它和前一个值在Name这一层不共享，所以R=1，D=2。在r2中该列有一个值，它是未定义的，但是Name这一层是已定义的，所以R=0，D=1.</p><h2 id="Parquet文件格式"><a href="#Parquet文件格式" class="headerlink" title="Parquet文件格式"></a>Parquet文件格式</h2><p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。在HDFS文件系统和Parquet文件中存在如下几个概念。</p><ul><li>HDFS块(Block)：它是HDFS上的最小的副本单位，HDFS会把一个Block存储在本地的一个文件并且维护分散在不同的机器上的多个副本，通常情况下一个Block的大小为256M、512M等。</li><li>HDFS文件(File)：一个HDFS的文件，包括数据和元数据，数据分散存储在多个Block中。</li><li>行组(Row Group)：按照行将数据物理上划分为多个单元，每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，Parquet读写的时候会将整个行组缓存在内存中，所以如果每一个行组的大小是由内存大的小决定的，例如记录占用空间比较小的Schema可以在每一个行组中存储更多的行。</li><li>列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。</li><li>页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。</li></ul><h3 id="文件格式"><a href="#文件格式" class="headerlink" title="文件格式"></a><strong>文件格式</strong></h3><p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017112835787-354192287.png" alt="img"></p><p>上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页，但是在后面的版本中增加。</p><p>在执行MR任务的时候可能存在多个Mapper任务的输入是同一个Parquet文件的情况，每一个Mapper通过InputSplit标示处理的文件范围，如果多个InputSplit跨越了一个Row Group，Parquet能够保证一个Row Group只会被一个Mapper任务处理。</p><h3 id="映射下推-Project-PushDown"><a href="#映射下推-Project-PushDown" class="headerlink" title="映射下推(Project PushDown)"></a><strong>映射下推(Project PushDown)</strong></h3><p>说到列式存储的优势，映射下推是最突出的，它意味着在获取表中原始数据时只需要扫描查询中需要的列，由于每一列的所有值都是连续存储的，所以分区取出每一列的所有值就可以实现TableScan算子，而避免扫描整个表文件内容。</p><p>在Parquet中原生就支持映射下推，执行查询的时候可以通过Configuration传递需要读取的列的信息，这些列必须是Schema的子集，映射每次会扫描一个Row Group的数据，然后一次性得将该Row Group里所有需要的列的Cloumn Chunk都读取到内存中，每次读取一个Row Group的数据能够大大降低随机读的次数，除此之外，Parquet在读取的时候会考虑列是否连续，如果某些需要的列是存储位置是连续的，那么一次读操作就可以把多个列的数据读取到内存。</p><h3 id="谓词下推-Predicate-PushDown"><a href="#谓词下推-Predicate-PushDown" class="headerlink" title="谓词下推(Predicate PushDown)"></a><strong>谓词下推(Predicate PushDown)</strong></h3><p>在数据库之类的查询系统中最常用的优化手段就是谓词下推了，通过将一些过滤条件尽可能的在最底层执行可以减少每一层交互的数据量，从而提升性能，例如”select count(1) from A Join B on A.id = B.id where A.a &gt; 10 and B.b &lt; 100”SQL查询中，在处理Join操作之前需要首先对A和B执行TableScan操作，然后再进行Join，再执行过滤，最后计算聚合函数返回，但是如果把过滤条件A.a &gt; 10和B.b &lt; 100分别移到A表的TableScan和B表的TableScan的时候执行，可以大大降低Join操作的输入数据。</p><p>无论是行式存储还是列式存储，都可以在将过滤条件在读取一条记录之后执行以判断该记录是否需要返回给调用者，在Parquet做了更进一步的优化，优化的方法时对每一个Row Group的每一个Column Chunk在存储的时候都计算对应的统计信息，包括该Column Chunk的最大值、最小值和空值个数。通过这些统计值和该列的过滤条件可以判断该Row Group是否需要扫描。另外Parquet未来还会增加诸如Bloom Filter和Index等优化数据，更加有效的完成谓词下推。</p><p>在使用Parquet的时候可以通过如下两种策略提升查询性能：1、类似于关系数据库的主键，对需要频繁过滤的列设置为有序的，这样在导入数据的时候会根据该列的顺序存储数据，这样可以最大化的利用最大值、最小值实现谓词下推。2、减小行组大小和页大小，这样增加跳过整个行组的可能性，但是此时需要权衡由于压缩和编码效率下降带来的I/O负载。</p><h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a><strong>性能</strong></h2><p>相比传统的行式存储，Hadoop生态圈近年来也涌现出诸如RC、ORC、Parquet的列式存储格式，它们的性能优势主要体现在两个方面：1、更高的压缩比，由于相同类型的数据更容易针对不同类型的列使用高效的编码和压缩方式。2、更小的I/O操作，由于映射下推和谓词下推的使用，可以减少一大部分不必要的数据扫描，尤其是表结构比较庞大的时候更加明显，由此也能够带来更好的查询性能</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017122313865-1783718133.png" alt="img"></p><p>上图是展示了使用不同格式存储TPC-H和TPC-DS数据集中两个表数据的文件大小对比，可以看出Parquet较之于其他的二进制文件存储格式能够更有效的利用存储空间，而新版本的Parquet(2.0版本)使用了更加高效的页存储方式，进一步的提升存储空间</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017122414756-63812379.png" alt="img"></p><p>上图展示了Twitter在Impala中使用不同格式文件执行TPC-DS基准测试的结果，测试结果可以看出Parquet较之于其他的行式存储格式有较明显的性能提升。</p><p><img src="https://images2017.cnblogs.com/blog/400827/201710/400827-20171017122448099-998445606.png" alt="img"></p><p>上图展示了criteo公司在Hive中使用ORC和Parquet两种列式存储格式执行TPC-DS基准测试的结果，测试结果可以看出在数据存储方面，两种存储格式在都是用snappy压缩的情况下量中存储格式占用的空间相差并不大，查询的结果显示Parquet格式稍好于ORC格式，两者在功能上也都有优缺点，Parquet原生支持嵌套式数据结构，而ORC对此支持的较差，这种复杂的Schema查询也相对较差；而Parquet不支持数据的修改和ACID，但是ORC对此提供支持，但是在OLAP环境下很少会对单条数据修改，更多的则是批量导入。</p><h2 id="项目发展"><a href="#项目发展" class="headerlink" title="项目发展"></a><strong>项目发展</strong></h2><p>自从2012年由Twitter和Cloudera共同研发Parquet开始，该项目一直处于高速发展之中，并且在项目之初就将其贡献给开源社区，2013年，Criteo公司加入开发并且向Hive社区提交了向hive集成Parquet的patch(HIVE-5783)，在Hive 0.13版本之后正式加入了Parquet的支持；之后越来越多的查询引擎对此进行支持，也进一步带动了Parquet的发展。</p><p>目前Parquet正处于向2.0版本迈进的阶段，在新的版本中实现了新的Page存储格式，针对不同的类型优化编码算法，另外丰富了支持的原始类型，增加了Decimal、Timestamp等类型的支持，增加更加丰富的统计信息，例如Bloon Filter，能够尽可能得将谓词下推在元数据层完成。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>本文介绍了一种支持嵌套数据模型对的列式存储系统Parquet，作为大数据系统中OLAP查询的优化方案，它已经被多种查询引擎原生支持，并且部分高性能引擎将其作为默认的文件存储格式。通过数据编码和压缩，以及映射下推和谓词下推功能，Parquet的性能也较之其它文件格式有所提升，可以预见，随着数据模型的丰富和Ad hoc查询的需求，Parquet将会被更广泛的使用。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Parquet文件存储格式：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Parquet文件存储格式&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>数据集网站汇总</title>
    <link href="http://zhangfuxin.cn/2019-09-17-%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BD%91%E7%AB%99%E6%B1%87%E6%80%BB.html"/>
    <id>http://zhangfuxin.cn/2019-09-17-数据集网站汇总.html</id>
    <published>2019-09-17T02:30:04.000Z</published>
    <updated>2019-09-17T01:29:13.803Z</updated>
    
    <content type="html"><![CDATA[<p>** 数据集网站汇总：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        如果用一个句子总结学习数据科学的本质，那就是： 学习数据科学的最佳方法就是应用数据科学。 如果你是一个初学者，你每完成一个新项目后自身能力都会有极大的提高，如果你是一个有经验的数据科学专家，你已经知道这里所蕴含的价值。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/GO1fHyfFYWF82xt3de5wIJs2RUMBRe1YicaOF4Ck0W6cVhfZUfcjbiaUwN0z3WAwia5Mw26cicQCDO7ChQFgI6r9aw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>一.如何使用这些资源?</strong></p><p>如何使用这些数据源是没有限制的，应用和使用只受到您的创造力和实际应用。使用它们最简单的方法是进行数据项目并在网站上发布它们。这不仅能提高你的数据和可视化技能，还能改善你的结构化思维。</p><p>另一方面，如果你正在考虑/处理基于数据的产品，这些数据集可以通过提供额外的/新的输入数据来增加您的产品的功能。所以，继续在这些项目上工作吧，与更大的世界分享它们，以展示你的数据能力!我们已经在不同的部分中划分了这些数据源，以帮助你根据应用程序对数据源进行分类。</p><p>我们从简单、通用和易于处理数据集开始，然后转向大型/行业相关数据集。然后，我们为特定的目的——文本挖掘、图像分类、推荐引擎等提供数据集的链接。这将为您提供一个完整的数据资源列表。如果你能想到这些数据集的任何应用，或者知道我们漏掉了什么流行的资源，请在下面的评论中与我们分享。(部分可能需要翻墙)</p><p><strong>二.由简单和通用的数据集开始</strong></p><p><strong>1.data.gov ( <a href="https://www.data.gov/" target="_blank" rel="noopener">https://www.data.gov/</a> )</strong> 这是美国政府公开数据的所在地，该站点包含了超过19万的数据点。这些数据集不同于气候、教育、能源、金融和更多领域的数据。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/fYsJGst9n2SFp5EEPia57ExkNWwXWsicrnJTTCUmh7E71ULmu3QO5RMv88CEoCDYZwenOrOtZqQ1shvY7VnibicCAg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>2.data.gov.in ( <a href="https://data.gov.in/" target="_blank" rel="noopener">https://data.gov.in/</a> )</strong> 这是印度政府公开数据的所在地，通过各种行业、气候、医疗保健等来寻找数据，你可以在这里找到一些灵感。根据你居住的国家的不同，你也可以从其他一些网站上浏览类似的网站。</p><p><strong>3.World Bank( <a href="http://data.worldbank.org/" target="_blank" rel="noopener">http://data.worldbank.org/</a> )</strong> 世界银行的开放数据。该平台提供 Open Data Catalog，世界发展指数，教育指数等几个工具。</p><p><strong>4.RBI ( <a href="https://rbi.org.in/Scripts/Statistics.aspx" target="_blank" rel="noopener">https://rbi.org.in/Scripts/Statistics.aspx</a> )</strong> 印度储备银行提供的数据。这包括了货币市场操作、收支平衡、银行使用和一些产品的几个指标。</p><p><strong>5.Five Thirty Eight Datasets ( <a href="https://github.com/fivethirtyeight/data" target="_blank" rel="noopener">https://github.com/fivethirtyeight/data</a> ) Five Thirty Eight，</strong>亦称作 538，专注与民意调查分析，政治，经济与体育的博客。该数据集为 Five Thirty Eight Datasets 使用的数据集。每个数据集包括数据，解释数据的字典和Five Thirty Eight 文章的链接。如果你想学习如何创建数据故事，没有比这个更好。</p><p><strong>三.大型数据集</strong></p><p><strong>1.Amazon Web Services(AWS)datasets ( <a href="https://aws.amazon.com/cn/datasets/" target="_blank" rel="noopener">https://aws.amazon.com/cn/datasets/</a> )</strong>Amazon提供了一些大数据集，可以在他们的平台上使用，也可以在本地计算机上使用。您还可以通过EMR使用EC2和Hadoop来分析云中的数据。在亚马逊上流行的数据集包括完整的安然电子邮件数据集，Google Books n-gram，NASA NEX 数据集，百万歌曲数据集等。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/fYsJGst9n2SFp5EEPia57ExkNWwXWsicrnDT397LtcdcHrCTC3ktMqELeEGypv5YeUq5lN21WcJSr7V5fXoicPMMQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>2.Google datasets ( <a href="https://cloud.google.com/bigquery/public-data/" target="_blank" rel="noopener">https://cloud.google.com/bigquery/public-data/</a> )</strong> Google 提供了一些数据集作为其 Big Query 工具的一部分。包括 GitHub 公共资料库的数据，Hacker News 的所有故事和评论。</p><p><strong>3.Youtube labeled Video Dataset ( <a href="https://research.google.com/youtube8m/" target="_blank" rel="noopener">https://research.google.com/youtube8m/</a> )</strong> 几个月前，谷歌研究小组发布了YouTube上的“数据集”，它由800万个YouTube视频id和4800个视觉实体的相关标签组成。它来自数十亿帧的预先计算的，最先进的视觉特征。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/fYsJGst9n2SFp5EEPia57ExkNWwXWsicrnJTzJa7jgNoyANvhLkHMBeBj9Q35gTJVgEgxn3HryJqILaFcwRDiadQA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>四.预测建模与机器学习数据集</strong></p><p><strong>1.UCI Machine Learning Repository ( <a href="https://archive.ics.uci.edu/ml/datasets.html" target="_blank" rel="noopener">https://archive.ics.uci.edu/ml/datasets.html</a> )</strong>UCI机器学习库显然是最著名的数据存储库。如果您正在寻找与机器学习存储库相关的数据集，通常是首选的地方。这些数据集包括了各种各样的数据集，从像Iris和泰坦尼克这样的流行数据集到最近的贡献，比如空气质量和GPS轨迹。存储库包含超过350个与域名类似的数据集(分类/回归)。您可以使用这些过滤器来确定您需要的数据集。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/fYsJGst9n2SFp5EEPia57ExkNWwXWsicrnibJib29micqeLe5P2s97EKaKsK7icibTFssTU1vA6CSKrSYUeZG6VVZTW2g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>2.Kaggle ( <a href="https://www.kaggle.com/datasets" target="_blank" rel="noopener">https://www.kaggle.com/datasets</a> )</strong> Kaggle提出了一个平台，人们可以贡献数据集，其他社区成员可以投票并运行内核/脚本。他们总共有超过350个数据集——有超过200个特征数据集。虽然一些最初的数据集通常出现在其他地方，但我在平台上看到了一些有趣的数据集，而不是在其他地方出现。与新的数据集一起，界面的另一个好处是，您可以在相同的界面上看到来自社区成员的脚本和问题。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/fYsJGst9n2SFp5EEPia57ExkNWwXWsicrn8GYRYjzkIDAcpeG3nVeMlp1WMicVgoZrYEFv6oWFvxpPibkAkofUqRVQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>3.Analytics Vidhya (<a href="https://datahack.analyticsvidhya.com/contest/all/" target="_blank" rel="noopener">https://datahack.analyticsvidhya.com/contest/all/</a> )</strong> 您可以从我们的实践问题和黑客马拉松问题中参与和下载数据集。问题数据集基于真实的行业问题，并且相对较小，因为它们意味着2 - 7天的黑客马拉松。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/fYsJGst9n2SFp5EEPia57ExkNWwXWsicrnZ0VAM13phGYueXFS3x02bMbS5PAwSJfEaP5a4qBZLMKjQoLVUnsfIA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>4.Quandl ( <a href="https://www.quandl.com/" target="_blank" rel="noopener">https://www.quandl.com/</a> )</strong> Quandl 通过起网站、API 或一些工具的直接集成提供了不同来源的财务、经济和替代数据。他们的数据集分为开放和付费。所有开放数据集为免费，但高级数据集需要付费。通过搜索仍然可以在平台上找到优质数据集。例如，来自印度的证券交易所数据是免费的。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/fYsJGst9n2SFp5EEPia57ExkNWwXWsicrnc7mnwosDNnYiaSUdIZrAOAdh0lgBlaf8H9Orelt6o535sRFC5aGyMMQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>5.Past KDD Cups ( <a href="http://www.kdd.org/kdd-cup" target="_blank" rel="noopener">http://www.kdd.org/kdd-cup</a> )</strong> KDD Cup 是 ACM Special Interest Group 组织的年度数据挖掘和知识发现竞赛。</p><p><strong>6.Driven Data ( <a href="https://www.drivendata.org/" target="_blank" rel="noopener">https://www.drivendata.org/</a> )</strong> Driven Data 发现运用数据科学带来积极社会影响的现实问题。然后，他们为数据科学家组织在线模拟竞赛，从而开发出最好的模型来解决这些问题。</p><p><strong>五.图像分类数据集</strong></p><p><strong>1.The MNIST Database ( <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/</a> )</strong> 最流行的图像识别数据集，使用手写数字。它包括6万个示例和1万个示例的测试集。这通常是第一个进行图像识别的数据集。</p><p><strong>2.Chars74K (<a href="http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/" target="_blank" rel="noopener">http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/</a> )</strong> 这里是下一阶段的进化，如果你已经通过了手写的数字。该数据集包括自然图像中的字符识别。数据集包含74,000个图像，因此数据集的名称。</p><p><strong>3.Frontal Face Images (<a href="http://vasc.ri.cmu.edu//idb/html/face/frontal_images/index.html" target="_blank" rel="noopener">http://vasc.ri.cmu.edu//idb/html/face/frontal_images/index.html</a> )</strong> 如果你已经完成了前两个项目，并且能够识别数字和字符，这是图像识别中的下一个挑战级别——正面人脸图像。这些图像是由CMU &amp; MIT收集的，排列在四个文件夹中。</p><p><strong>4.ImageNet ( <a href="http://image-net.org/" target="_blank" rel="noopener">http://image-net.org/</a> )</strong> 现在是时候构建一些通用的东西了。根据WordNet层次结构组织的图像数据库(目前仅为名词)。层次结构的每个节点都由数百个图像描述。目前，该集合平均每个节点有超过500个图像(而且还在增加)。</p><p><strong>六.文本分类数据集</strong></p><p><strong>1.Spam – Non Spam (<a href="http://www.esp.uem.es/jmgomez/smsspamcorpus/" target="_blank" rel="noopener">http://www.esp.uem.es/jmgomez/smsspamcorpus/</a>)</strong> 区分短信是否为垃圾邮件是一个有趣的问题。你需要构建一个分类器将短信进行分类。</p><p><strong>2.Twitter Sentiment Analysis (<a href="http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/" target="_blank" rel="noopener">http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/</a>)</strong> 该数据集包含 1578627 个分类推文，每行被标记为1的积极情绪，0位负面情绪。数据依次基于 Kaggle 比赛和 Nick Sanders 的分析。</p><p><strong>3.Movie Review Data (<a href="http://www.cs.cornell.edu/People/pabo/movie-review-data/" target="_blank" rel="noopener">http://www.cs.cornell.edu/People/pabo/movie-review-data/</a>)</strong> 这个网站提供了一系列的电影评论文件，这些文件标注了他们的总体情绪极性(正面或负面)或主观评价(例如，“两个半明星”)和对其主观性地位(主观或客观)或极性的标签。</p><p><strong>七.推荐引擎数据集</strong></p><p><strong>1.MovieLens ( <a href="https://grouplens.org/" target="_blank" rel="noopener">https://grouplens.org/</a> ) MovieLens</strong> 是一个帮助人们查找电影的网站。它有成千上万的注册用户。他们进行自动内容推荐，推荐界面，基于标签的推荐页面等在线实验。这些数据集可供下载，可用于创建自己的推荐系统。</p><p><strong>2.Jester (<a href="http://www.ieor.berkeley.edu/~goldberg/jester-data/" target="_blank" rel="noopener">http://www.ieor.berkeley.edu/~goldberg/jester-data/</a>)</strong> 在线笑话推荐系统。</p><p><strong>八.各种来源的数据集网站</strong></p><p><strong>1.KDNuggets (<a href="http://www.kdnuggets.com/datasets/index.html" target="_blank" rel="noopener">http://www.kdnuggets.com/datasets/index.html</a>)</strong> KDNuggets 的数据集页面一直是人们搜索数据集的参考。列表全面，但是某些来源不再提供数据集。因此，需要谨慎选择数据集和来源。</p><p><strong>2.Awesome Public Datasets (<a href="https://github.com/caesar0301/awesome-public-datasets" target="_blank" rel="noopener">https://github.com/caesar0301/awesome-public-datasets</a>)</strong> 一个GitHub存储库，它包含一个由域分类的完整的数据集列表。数据集被整齐地分类在不同的领域，这是非常有用的。但是，对于存储库本身的数据集没有描述，这可能使它非常有用。</p><p><strong>3.Reddit Datasets Subreddit (<a href="https://www.reddit.com/r/datasets/" target="_blank" rel="noopener">https://www.reddit.com/r/datasets/</a>)</strong> 由于这是一个社区驱动的论坛，它可能会遇到一些麻烦(与之前的两个来源相比)。但是，您可以通过流行/投票来对数据集进行排序，以查看最流行的数据集。另外，它还有一些有趣的数据集和讨论。</p><p><strong>九.结尾的话</strong></p><p>我们希望这一资源清单对于那些想项目的人来说是非常有用的。这绝对是一个金矿，好好加以利用吧!</p><p>转自：<a href="https://mp.weixin.qq.com/s?__biz=MzI2MjM2MDEzNQ==&amp;mid=2247489072&amp;idx=1&amp;sn=2ac46ef358be4eef43f3de8670086746&amp;chksm=ea4d0b18dd3a820ef82122648806c8516970e8e7323efb5475aa0db1da1752d22ee8c38ec604&amp;mpshare=1&amp;scene=23&amp;srcid=042625ULmfK6xU66wcmkCf1G#rd" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzI2MjM2MDEzNQ==&amp;mid=2247489072&amp;idx=1&amp;sn=2ac46ef358be4eef43f3de8670086746&amp;chksm=ea4d0b18dd3a820ef82122648806c8516970e8e7323efb5475aa0db1da1752d22ee8c38ec604&amp;mpshare=1&amp;scene=23&amp;srcid=042625ULmfK6xU66wcmkCf1G#rd</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 数据集网站汇总：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        如果用一个句子总结学习数据科学的本质，那就是： 学习数据科学的最佳方法就是应用数据科学。 如果你是一个初学者，你每完成一个新项目后自身能力都会有极大的提高，如果你是一个有经验的数据科学专家，你已经知道这里所蕴含的价值。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Vim常用按键大全</title>
    <link href="http://zhangfuxin.cn/2019-09-16-Vim%E5%B8%B8%E7%94%A8%E6%8C%89%E9%94%AE%E5%A4%A7%E5%85%A8.html"/>
    <id>http://zhangfuxin.cn/2019-09-16-Vim常用按键大全.html</id>
    <published>2019-09-16T02:30:04.000Z</published>
    <updated>2019-09-18T16:07:02.309Z</updated>
    
    <content type="html"><![CDATA[<p>** Vim常用按键大全：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Vim常用按键大全</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>Vim完全可以用键盘进行操作。本文将常用的按键归纳总结。</p><h4 id="第一部分：一般模式可用的按钮，如光标移动、复制粘贴、查找替换等"><a href="#第一部分：一般模式可用的按钮，如光标移动、复制粘贴、查找替换等" class="headerlink" title="第一部分：一般模式可用的按钮，如光标移动、复制粘贴、查找替换等"></a>第一部分：一般模式可用的按钮，如光标移动、复制粘贴、查找替换等</h4><h5 id="移动光标的方法"><a href="#移动光标的方法" class="headerlink" title="移动光标的方法"></a>移动光标的方法</h5><table><thead><tr><th>h, j, k, l</th><th>光标向左，下，上，右移动</th></tr></thead><tbody><tr><td>Ctrl + f / b</td><td>屏幕向下/上移动</td></tr><tr><td>Ctrl + d / u</td><td>屏幕向下/上移动半页</td></tr><tr><td>0</td><td>移动到一行的最前面</td></tr><tr><td>$</td><td>移动到一行的最后面字符</td></tr><tr><td>H / M / L</td><td>移动到屏幕最上方/中央/最下方那一行的第一个字符</td></tr><tr><td>G</td><td>移动到文件的最后一行</td></tr><tr><td>nG / ngg</td><td>移动到文件的第n行</td></tr><tr><td>gg</td><td>移动到文件的第一行</td></tr><tr><td>n[Enter]</td><td>向下移动n行</td></tr></tbody></table><h4 id="查找与替换"><a href="#查找与替换" class="headerlink" title="查找与替换"></a>查找与替换</h4><table><thead><tr><th>/word</th><th>向下查找word字符串</th></tr></thead><tbody><tr><td>?word</td><td>向上查找word字符串</td></tr><tr><td>n</td><td>代表重复前一个查找动作</td></tr><tr><td>N</td><td>代表反向重复前一个查找动作</td></tr><tr><td>: s/old/new</td><td>将第一个old替换为new</td></tr><tr><td>: s/old/new/g</td><td>将一行中所有的old替换为new</td></tr><tr><td>:n1, n2s/word1/word2/g</td><td>将行n1与n2之间的word1替换为word2</td></tr><tr><td>:%s/old/new/g</td><td>将文件所有的old替换为new</td></tr><tr><td>:%s/old/new/gc</td><td>替换前要求确认</td></tr></tbody></table><h5 id="删除、复制与粘贴"><a href="#删除、复制与粘贴" class="headerlink" title="删除、复制与粘贴"></a>删除、复制与粘贴</h5><table><thead><tr><th>x/X</th><th>向后/前删除一个字符</th></tr></thead><tbody><tr><td>nx</td><td>连续删除n个字符</td></tr><tr><td>dd</td><td>删除整行</td></tr><tr><td>ndd</td><td>删除n行</td></tr><tr><td>d1G</td><td>删除光标所在到第一行数据</td></tr><tr><td>dG</td><td>删除光标所在到最后一行数据</td></tr><tr><td>d$</td><td>删除光标所在到该行最后一个字符</td></tr><tr><td>d0</td><td>删除光标所在到该行最前面一个字符</td></tr><tr><td>yy</td><td>复制光标所在的一行</td></tr><tr><td>nyy</td><td>向下复制n行</td></tr><tr><td>y1G</td><td>复制光标所在到第一行数据</td></tr><tr><td>yG</td><td>复制光标所在到最后一行数据</td></tr><tr><td>y$</td><td>复制光标所在到该行最后一个字符</td></tr><tr><td>y0</td><td>复制光标所在到该行最前面一个字符</td></tr><tr><td>p/P</td><td>粘贴数据在光标下/上一行</td></tr><tr><td>J</td><td>将光标所在行与下一行数据结合成同一行</td></tr><tr><td>u</td><td>回撤前一操作</td></tr><tr><td>Ctrl + r</td><td>重做前一操作</td></tr><tr><td>.</td><td>重复前一个操作</td></tr></tbody></table><h4 id="第二部分：一般模式切换到编辑模式"><a href="#第二部分：一般模式切换到编辑模式" class="headerlink" title="第二部分：一般模式切换到编辑模式"></a>第二部分：一般模式切换到编辑模式</h4><h4 id="进入插入或替换的编辑模式"><a href="#进入插入或替换的编辑模式" class="headerlink" title="进入插入或替换的编辑模式"></a>进入插入或替换的编辑模式</h4><table><thead><tr><th>i, I</th><th>进入插入模式： i从当前光标所在处插入，I在目前所在行的第一个非空格符处插入</th></tr></thead><tbody><tr><td>a, A</td><td>进入插入模式： a从当前光标所在的下一个字符插入，A从光标所在行的最后一个字符后插入</td></tr><tr><td>o, O</td><td>进入插入模式： o从当前光标所在行的下一行插入新的一行；O正好相反，从上一行插入新行</td></tr><tr><td>r, R</td><td>进入替换模式： r只会替换光标所在的那一个字符一次；R会一直替换光标所在文字，直到Esc</td></tr></tbody></table><h4 id="块选择"><a href="#块选择" class="headerlink" title="块选择"></a>块选择</h4><table><thead><tr><th>v</th><th>字符选择，将光标经过的地方反白选择</th></tr></thead><tbody><tr><td>V</td><td>行选择，将光标经过的行反白选择</td></tr><tr><td>Ctrl + v</td><td>块选择，可以用长方形选择数据</td></tr><tr><td>y</td><td>将反白的地方复制</td></tr><tr><td>d</td><td>删除反白的地方</td></tr></tbody></table><h4 id="多窗口"><a href="#多窗口" class="headerlink" title="多窗口"></a>多窗口</h4><table><thead><tr><th>：sp filename</th><th>打开新窗口，如果有加filename,新窗口打开新文件，否则打开相同文件</th></tr></thead><tbody><tr><td>Ctrl + w + s/v</td><td>水平/垂直分割打开新窗口</td></tr><tr><td>Ctrl + w + h/j/k/l</td><td>光标移动到左/下/上/右窗口</td></tr><tr><td>Ctrl + w + q</td><td>退出窗口</td></tr></tbody></table><h4 id="vim常用命令示意图"><a href="#vim常用命令示意图" class="headerlink" title="vim常用命令示意图"></a>vim常用命令示意图</h4><p><a href="http://images2015.cnblogs.com/blog/435059/201512/435059-20151225151800734-1593095043.jpg" target="_blank" rel="noopener"><img src="https://images2015.cnblogs.com/blog/435059/201512/435059-20151225151801609-170255026.jpg" alt="vim-commands"></a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Vim常用按键大全：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Vim常用按键大全&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://zhangfuxin.cn/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://zhangfuxin.cn/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>CM+CDH离线安装</title>
    <link href="http://zhangfuxin.cn/CDH-hadoop.html"/>
    <id>http://zhangfuxin.cn/CDH-hadoop.html</id>
    <published>2019-09-05T17:30:04.000Z</published>
    <updated>2019-09-06T00:12:24.698Z</updated>
    
    <content type="html"><![CDATA[<p>** CM+CDH离线安装：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Cloudera Manager可以轻松管理任何生产规模的Hadoop部署。通过直观的用户界面快速部署，配置和监控群集 - 完成滚动升级，备份和灾难恢复以及可定制警报。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1-Cloudera-简介"><a href="#1-1-Cloudera-简介" class="headerlink" title="1.1 Cloudera 简介"></a>1.1 Cloudera 简介</h2><h3 id="1-1-1Cloudera-简介"><a href="#1-1-1Cloudera-简介" class="headerlink" title="1.1.1Cloudera 简介"></a>1.1.1Cloudera 简介</h3><p>官网：<a href="https://www.cloudera.com/" target="_blank" rel="noopener">https://www.cloudera.com/</a></p><p>文档：<a href="https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_intro.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_intro.html</a></p><p>​        CDH是Apache Hadoop和相关项目中最完整，经过测试和最流行的发行版。CDH提供了Hadoop的核心元素 - 可扩展存储和分布式计算 - 以及基于Web的用户界面和重要的企业功能。CDH是Apache许可的开源软件，是唯一提供统一批处理，交互式SQL和交互式搜索以及基于角色的访问控制的Hadoop解决方案。</p><p>CDH提供：</p><ul><li><p>灵活性 - 存储任何类型的数据并使用各种不同的计算框架对其进行操作，包括批处理，交互式SQL，自由文本搜索，机器学习和统计计算。</p></li><li><p>集成 - 在完整的Hadoop平台上快速启动和运行，该平台可与各种硬件和软件解决方案配合使用。</p></li><li><p>安全 - 处理和控制敏感数据。</p></li><li><p>可扩展性 - 支持广泛的应用程序，并扩展和扩展它们以满足您的要求。</p></li><li><p>高可用性 - 充满信心地执行任务关键型业务任务。</p></li><li><p>兼容性 - 利用您现有的IT基础架构和投资。</p><p><img src="https://www.cloudera.com/documentation/enterprise/latest/images/cdh.png" alt="img"></p></li></ul><h3 id="1-1-2Hadoop起源"><a href="#1-1-2Hadoop起源" class="headerlink" title="1.1.2Hadoop起源"></a>1.1.2Hadoop起源</h3><p>​        2004年，Cutting和同为程序员出身的Mike Cafarella决定开发一款可以代替当时的主流搜索产品的开源搜索引擎，这个项目被命名为Nutch。Nutch基于Lucence实现的搜索引擎，能够从互联网上抓取网页数据。抓取来的海量数据的存储问题。但是，这些海量数据都是非结构化数据，不能存在关系型数据库里。如果连数据的存储和管理都解决不了化，就谈不上后续为用户提供搜索服务，包括通过算法去优化检索速度。那时的Cutting正处在Freelancer的生涯 中，所以他希望自己的项目能通过一种低开销的方式来构建网页中的大量算法。幸运的是，Google这时正好发布了一项研究报告，报告中介绍了两款Google为支持自家的搜索引擎而开发的软件平台。这两个平台一个是GFS（Google File System），用于存储不同设备所产生的海量数据；另一个是MapReduce，它运行在GFS之上，负责分布式大规模数据。</p><p>​        2006年cutting根据《Google File System》设计了Nutch的HDFS,hadoop distributed file system。</p><p>Hadoop最开始是nutch的子项目，目的是解决nutch的海量数据存储问题。在nutch 0.8版本之后，Hadoop独立处理，成为一个独立的项目。后来，又根据《Google MapReduce》设计了基于HDFS的MapRedce计算框架。</p><p>2006年之后，cutting带着Hadoop去了雅虎，当时有100多人的团队共同帮cutting完善hadoop。后来yahoo把Hadoop贡献了Apache。所以，现在Hadoop是Apache的顶级项目。</p><h2 id="2-1Hadoop搭建"><a href="#2-1Hadoop搭建" class="headerlink" title="2.1Hadoop搭建"></a>2.1Hadoop搭建</h2><p><strong>Hadoop的三种运行模式</strong> ：</p><ol><li><p>独立（本地）运行模式：无需任何守护进程，所有的程序都运行在同一个JVM上执行。在独立模式下调试MR程序非常高效方便。所以一般该模式主要是在学习或者开发阶段调试使用 。</p></li><li><p>伪分布式模式：  Hadoop守护进程运行在本地机器上，模拟一个小规模的集群，换句话说，可以配置一台机器的Hadoop集群,伪分布式是完全分布式的一个特例。</p></li><li><p>完全分布式模式：Hadoop守护进程运行在一个集群上。</p></li></ol><h2 id="3-1-单机伪分布模式"><a href="#3-1-单机伪分布模式" class="headerlink" title="3.1 单机伪分布模式"></a>3.1 单机伪分布模式</h2><p>​    只支持MapReduce，不支持HDFS。这种模式一般用于调试MapReduce任务用的。</p><h3 id="3-1-1-准备Linux环境，最低的工作内存1G"><a href="#3-1-1-准备Linux环境，最低的工作内存1G" class="headerlink" title="3.1.1 准备Linux环境，最低的工作内存1G"></a>3.1.1 准备Linux环境，最低的工作内存1G</h3><p>内容详见：Vmware安装Centos6.9文档</p><h3 id="3-1-2-关闭防火墙"><a href="#3-1-2-关闭防火墙" class="headerlink" title="3.1.2  关闭防火墙"></a>3.1.2  关闭防火墙</h3><p>临时关闭防火墙：service iptables stop</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br></pre></td></tr></table></figure><p>永久关闭防火墙：chkconfig iptables off </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font>永久修改防火墙需要重启，永久和临时同时执行，可以不用重启。</p><h3 id="3-1-3-配置主机名"><a href="#3-1-3-配置主机名" class="headerlink" title="3.1.3 配置主机名"></a>3.1.3 配置主机名</h3><p>查询主机名称：hostname</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname</span><br></pre></td></tr></table></figure><p>临时修改主机名：hostname  <strong><name></name></strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname &lt;name&gt;</span><br></pre></td></tr></table></figure><p>永久修改主机名：vim /etc/sysconfig/network</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><h3 id="3-1-4-配置hosts文件"><a href="#3-1-4-配置hosts文件" class="headerlink" title="3.1.4 配置hosts文件"></a>3.1.4 配置hosts文件</h3><p>执行: vim /etc/hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><ol><li>不要删除前两行内容。</li><li>IP在前，主机名在后。</li></ol><h3 id="3-1-5-配置免密码登录"><a href="#3-1-5-配置免密码登录" class="headerlink" title="3.1.5 配置免密码登录"></a>3.1.5 配置免密码登录</h3><h4 id="3-1-5-1-免密登陆原理"><a href="#3-1-5-1-免密登陆原理" class="headerlink" title="3.1.5.1 免密登陆原理"></a>3.1.5.1 免密登陆原理</h4><ol><li><p>A机器生成公钥和私钥</p></li><li><p>机器A发送自己的公钥到机器B，这个时候机器B有了机器A的公钥</p></li><li><p>机器B发送一个随机的字符串向机器A</p></li><li><p>机器A利用自己的私钥把字符串加密</p></li><li><p>机器A把加密后的字符串再次发送给机器B</p></li><li><p>机器B利用公钥解密字符串，如果和原来的一样，则OK。</p></li></ol><h4 id="3-1-5-1-免密登陆实现"><a href="#3-1-5-1-免密登陆实现" class="headerlink" title="3.1.5.1 免密登陆实现"></a>3.1.5.1 免密登陆实现</h4><ol><li><p>生成自己的公钥和私钥  ssh-keygen</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure></li><li><p>把生成的公钥copy到远程机器上 ssh-copy-id root@hadoop01</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id root@hadoop01</span><br></pre></td></tr></table></figure></li></ol><p><font color="red">注意：</font>如果是单机的伪分布式环境，自己节点也需要配置免密登录。</p><h3 id="3-1-6-安装和配置jdk"><a href="#3-1-6-安装和配置jdk" class="headerlink" title="3.1.6 安装和配置jdk"></a>3.1.6 安装和配置jdk</h3><ol><li><p>执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>在尾行添加 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASS_PATH</span><br></pre></td></tr></table></figure></li></ol><p>保存退出  :wq</p><ol start="3"><li><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>java -version 查看JDK版本信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-7-上传和安装hadoop"><a href="#3-1-7-上传和安装hadoop" class="headerlink" title="3.1.7 上传和安装hadoop"></a>3.1.7 上传和安装hadoop</h3><p>下载地址：<a href="http://hadoop.apache.org/releases.html" target="_blank" rel="noopener">http://hadoop.apache.org/releases.html</a></p><p><font color="red">注意：</font></p><p>source表示源码</p><p>binary表示二级制包（安装包）</p><h4 id="3-1-7-1-解压Hadoop文件包"><a href="#3-1-7-1-解压Hadoop文件包" class="headerlink" title="3.1.7.1 解压Hadoop文件包"></a>3.1.7.1 解压Hadoop文件包</h4><p>执行：tar -zxvf hadoop-2.7.1_64bit.tar.gz</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-2.7.1_64bit.tar.gz</span><br></pre></td></tr></table></figure><h4 id="3-1-7-2-Hadoop目录说明"><a href="#3-1-7-2-Hadoop目录说明" class="headerlink" title="3.1.7.2 Hadoop目录说明"></a>3.1.7.2 Hadoop目录说明</h4><p>bin目录：命令脚本</p><p>etc/hadoop:存放hadoop的配置文件</p><p>lib目录：hadoop运行的依赖jar包</p><p>sbin目录：启动和关闭hadoop等命令都在这里</p><p>libexec目录：存放的也是hadoop命令，但一般不常用</p><p><font color="red">注意：</font>最常用的就是bin和etc目录。</p><h3 id="3-1-8-配置hadoop配置文件"><a href="#3-1-8-配置hadoop配置文件" class="headerlink" title="3.1.8 配置hadoop配置文件"></a>3.1.8 配置hadoop配置文件</h3><p>Hadoop目录下<strong>/home/hadoop-2.7.1/etc/hadoop/</strong>目录下<strong>6个文件</strong></p><h4 id="3-1-8-1-hadoop-env-sh"><a href="#3-1-8-1-hadoop-env-sh" class="headerlink" title="3.1.8.1 hadoop-env.sh"></a>3.1.8.1 hadoop-env.sh</h4><p>执行：vim hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br></pre></td></tr></table></figure><p> 修改：修改java_home路径和hadoop_conf_dir 路径  25行  33行</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#25行</span><br><span class="line">export JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">#33行</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop-2.7.1/etc/hadoop</span><br></pre></td></tr></table></figure><p> 然后执行：source hadoop-env.sh编译文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source hadoop-env.sh</span><br></pre></td></tr></table></figure><h4 id="3-1-8-2-core-site-xml"><a href="#3-1-8-2-core-site-xml" class="headerlink" title="3.1.8.2 core-site.xml"></a>3.1.8.2 core-site.xml</h4><p>命令行执行：vim core-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim core-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hdfs的老大，namenode的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://tedu:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hadoop运行时产生文件的存放目录--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop-2.7.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-3-hdfs-site-xml"><a href="#3-1-8-3-hdfs-site-xml" class="headerlink" title="3.1.8.3 hdfs-site .xml"></a>3.1.8.3 hdfs-site .xml</h4><p>命令行执行：vim hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hdfs-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定hdfs保存数据副本的数量，包括自己，默认值是3--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--如果是伪分布模式，此值是1--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--设置hdfs的操作权限，false表示任何用户都可以在hdfs上操作文件--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-4-mapred-site-xml"><a href="#3-1-8-4-mapred-site-xml" class="headerlink" title="3.1.8.4 mapred-site.xml"></a>3.1.8.4 mapred-site.xml</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line"></span><br><span class="line">vim mapred-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定mapreduce运行在yarn上--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-5-yarn-site-xml"><a href="#3-1-8-5-yarn-site-xml" class="headerlink" title="3.1.8.5 yarn-site.xml"></a>3.1.8.5 yarn-site.xml</h4><p>命令行执行：vim yarn-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim yarn-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定yarn的老大 resoucemanager的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>tedu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--NodeManager获取数据的方式--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-  services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-6-slaves"><a href="#3-1-8-6-slaves" class="headerlink" title="3.1.8.6 slaves"></a>3.1.8.6 slaves</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves</span><br></pre></td></tr></table></figure><p><strong>修改主机名</strong></p><h3 id="3-1-9-配置hadoop的环境变量"><a href="#3-1-9-配置hadoop的环境变量" class="headerlink" title="3.1.9 配置hadoop的环境变量"></a>3.1.9 配置hadoop的环境变量</h3><ol><li><p>文件最后追加文件</p><p><strong>HADOOP_HOME=/home/hadoop-2.7.1</strong></p><p><strong>export HADOOP_HOME</strong></p></li><li><p>source /etc/profile 使更改的配置立即生效。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">HADOOP_HOME=/home/hadoop-2.7.1</span><br><span class="line">PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASSPATH HADOOP_HOME</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-10-格式化Namenode"><a href="#3-1-10-格式化Namenode" class="headerlink" title="3.1.10 格式化Namenode"></a>3.1.10 格式化Namenode</h3><p>执行：hdfs namenode -format</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>如果不好使，可以重启linux</p><p>当出现：successfully，证明格式化成功。</p><h3 id="3-1-11-启动Hadoop"><a href="#3-1-11-启动Hadoop" class="headerlink" title="3.1.11 启动Hadoop"></a>3.1.11 启动Hadoop</h3><p>在/home/hadoop-2.7.1/sbin目录下</p><p>执行:./start-all.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><h3 id="3-1-12-验证启动成功"><a href="#3-1-12-验证启动成功" class="headerlink" title="3.1.12 验证启动成功"></a>3.1.12 验证启动成功</h3><p>可以访问网址： <a href="http://192.168.220.128:50070" target="_blank" rel="noopener">http://192.168.220.128:50070</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** CM+CDH离线安装：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Cloudera Manager可以轻松管理任何生产规模的Hadoop部署。通过直观的用户界面快速部署，配置和监控群集 - 完成滚动升级，备份和灾难恢复以及可定制警报。&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://zhangfuxin.cn/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://zhangfuxin.cn/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop伪分布式搭建</title>
    <link href="http://zhangfuxin.cn/hadoop-single.html"/>
    <id>http://zhangfuxin.cn/hadoop-single.html</id>
    <published>2019-08-29T17:30:04.000Z</published>
    <updated>2019-08-29T17:28:47.951Z</updated>
    
    <content type="html"><![CDATA[<p>** Hadoop伪分布式搭建：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        大数据（Big Data），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。<br>​        大数据的定义是4Vs：数据量大、处理速度快、数据源多样、真实性。用中文简单描述就是大、快、多、真。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1Hadoop简介"><a href="#1-1Hadoop简介" class="headerlink" title="1.1Hadoop简介"></a>1.1Hadoop简介</h2><h3 id="1-1-1Hadoop创始人"><a href="#1-1-1Hadoop创始人" class="headerlink" title="1.1.1Hadoop创始人"></a>1.1.1Hadoop创始人</h3><p>​        1985年，<strong>Doug Cutting</strong>毕业于美国斯坦福大学。他并不是一开始就决心投身IT行业的，在大学时代的头两年，Cutting学习了诸如物理、地理等常规课程。因为学费的压力，Cutting开始意识到，自己必须学习一些更加实用、有趣的技能。这样，一方面可以帮助自己还清贷款，另一方面，也是为自己未来的生活做打算。因为斯坦福大学座落在IT行业的“圣地”硅谷，所以学习软件对年轻人来说是再自然不过的事情了。 1997年底，Cutting开始以每周两天的时间投入，在家里试着用Java把这个想法变成现实，不久之后，Lucene诞生了。作为第一个提供全文文本搜索的开源函数库，Lucene的伟大自不必多言。</p><p>Doug Cutting是<strong>Lucence,Nutch,Hadoop</strong>的创始人。</p><h3 id="1-1-2Hadoop起源"><a href="#1-1-2Hadoop起源" class="headerlink" title="1.1.2Hadoop起源"></a>1.1.2Hadoop起源</h3><p>​        2004年，Cutting和同为程序员出身的Mike Cafarella决定开发一款可以代替当时的主流搜索产品的开源搜索引擎，这个项目被命名为Nutch。Nutch基于Lucence实现的搜索引擎，能够从互联网上抓取网页数据。抓取来的海量数据的存储问题。但是，这些海量数据都是非结构化数据，不能存在关系型数据库里。如果连数据的存储和管理都解决不了化，就谈不上后续为用户提供搜索服务，包括通过算法去优化检索速度。那时的Cutting正处在Freelancer的生涯 中，所以他希望自己的项目能通过一种低开销的方式来构建网页中的大量算法。幸运的是，Google这时正好发布了一项研究报告，报告中介绍了两款Google为支持自家的搜索引擎而开发的软件平台。这两个平台一个是GFS（Google File System），用于存储不同设备所产生的海量数据；另一个是MapReduce，它运行在GFS之上，负责分布式大规模数据。</p><p>​        2006年cutting根据《Google File System》设计了Nutch的HDFS,hadoop distributed file system。</p><p>Hadoop最开始是nutch的子项目，目的是解决nutch的海量数据存储问题。在nutch 0.8版本之后，Hadoop独立处理，成为一个独立的项目。后来，又根据《Google MapReduce》设计了基于HDFS的MapRedce计算框架。</p><p>2006年之后，cutting带着Hadoop去了雅虎，当时有100多人的团队共同帮cutting完善hadoop。后来yahoo把Hadoop贡献了Apache。所以，现在Hadoop是Apache的顶级项目。</p><h2 id="2-1Hadoop搭建"><a href="#2-1Hadoop搭建" class="headerlink" title="2.1Hadoop搭建"></a>2.1Hadoop搭建</h2><p><strong>Hadoop的三种运行模式</strong> ：</p><ol><li><p>独立（本地）运行模式：无需任何守护进程，所有的程序都运行在同一个JVM上执行。在独立模式下调试MR程序非常高效方便。所以一般该模式主要是在学习或者开发阶段调试使用 。</p></li><li><p>伪分布式模式：  Hadoop守护进程运行在本地机器上，模拟一个小规模的集群，换句话说，可以配置一台机器的Hadoop集群,伪分布式是完全分布式的一个特例。</p></li><li><p>完全分布式模式：Hadoop守护进程运行在一个集群上。</p></li></ol><h2 id="3-1-单机伪分布模式"><a href="#3-1-单机伪分布模式" class="headerlink" title="3.1 单机伪分布模式"></a>3.1 单机伪分布模式</h2><p>​    只支持MapReduce，不支持HDFS。这种模式一般用于调试MapReduce任务用的。</p><h3 id="3-1-1-准备Linux环境，最低的工作内存1G"><a href="#3-1-1-准备Linux环境，最低的工作内存1G" class="headerlink" title="3.1.1 准备Linux环境，最低的工作内存1G"></a>3.1.1 准备Linux环境，最低的工作内存1G</h3><p>内容详见：Vmware安装Centos6.9文档</p><h3 id="3-1-2-关闭防火墙"><a href="#3-1-2-关闭防火墙" class="headerlink" title="3.1.2  关闭防火墙"></a>3.1.2  关闭防火墙</h3><p>临时关闭防火墙：service iptables stop</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br></pre></td></tr></table></figure><p>永久关闭防火墙：chkconfig iptables off </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font>永久修改防火墙需要重启，永久和临时同时执行，可以不用重启。</p><h3 id="3-1-3-配置主机名"><a href="#3-1-3-配置主机名" class="headerlink" title="3.1.3 配置主机名"></a>3.1.3 配置主机名</h3><p>查询主机名称：hostname</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname</span><br></pre></td></tr></table></figure><p>临时修改主机名：hostname  <strong><name></name></strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname &lt;name&gt;</span><br></pre></td></tr></table></figure><p>永久修改主机名：vim /etc/sysconfig/network</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><h3 id="3-1-4-配置hosts文件"><a href="#3-1-4-配置hosts文件" class="headerlink" title="3.1.4 配置hosts文件"></a>3.1.4 配置hosts文件</h3><p>执行: vim /etc/hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><ol><li>不要删除前两行内容。</li><li>IP在前，主机名在后。</li></ol><h3 id="3-1-5-配置免密码登录"><a href="#3-1-5-配置免密码登录" class="headerlink" title="3.1.5 配置免密码登录"></a>3.1.5 配置免密码登录</h3><h4 id="3-1-5-1-免密登陆原理"><a href="#3-1-5-1-免密登陆原理" class="headerlink" title="3.1.5.1 免密登陆原理"></a>3.1.5.1 免密登陆原理</h4><ol><li><p>A机器生成公钥和私钥</p></li><li><p>机器A发送自己的公钥到机器B，这个时候机器B有了机器A的公钥</p></li><li><p>机器B发送一个随机的字符串向机器A</p></li><li><p>机器A利用自己的私钥把字符串加密</p></li><li><p>机器A把加密后的字符串再次发送给机器B</p></li><li><p>机器B利用公钥解密字符串，如果和原来的一样，则OK。</p></li></ol><h4 id="3-1-5-1-免密登陆实现"><a href="#3-1-5-1-免密登陆实现" class="headerlink" title="3.1.5.1 免密登陆实现"></a>3.1.5.1 免密登陆实现</h4><ol><li><p>生成自己的公钥和私钥  ssh-keygen</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure></li><li><p>把生成的公钥copy到远程机器上 ssh-copy-id root@hadoop01</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id root@hadoop01</span><br></pre></td></tr></table></figure></li></ol><p><font color="red">注意：</font>如果是单机的伪分布式环境，自己节点也需要配置免密登录。</p><h3 id="3-1-6-安装和配置jdk"><a href="#3-1-6-安装和配置jdk" class="headerlink" title="3.1.6 安装和配置jdk"></a>3.1.6 安装和配置jdk</h3><ol><li><p>执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>在尾行添加 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASS_PATH</span><br></pre></td></tr></table></figure></li></ol><p>保存退出  :wq</p><ol start="3"><li><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>java -version 查看JDK版本信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-7-上传和安装hadoop"><a href="#3-1-7-上传和安装hadoop" class="headerlink" title="3.1.7 上传和安装hadoop"></a>3.1.7 上传和安装hadoop</h3><p>下载地址：<a href="http://hadoop.apache.org/releases.html" target="_blank" rel="noopener">http://hadoop.apache.org/releases.html</a></p><p><font color="red">注意：</font></p><p>source表示源码</p><p>binary表示二级制包（安装包）</p><h4 id="3-1-7-1-解压Hadoop文件包"><a href="#3-1-7-1-解压Hadoop文件包" class="headerlink" title="3.1.7.1 解压Hadoop文件包"></a>3.1.7.1 解压Hadoop文件包</h4><p>执行：tar -zxvf hadoop-2.7.1_64bit.tar.gz</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-2.7.1_64bit.tar.gz</span><br></pre></td></tr></table></figure><h4 id="3-1-7-2-Hadoop目录说明"><a href="#3-1-7-2-Hadoop目录说明" class="headerlink" title="3.1.7.2 Hadoop目录说明"></a>3.1.7.2 Hadoop目录说明</h4><p>bin目录：命令脚本</p><p>etc/hadoop:存放hadoop的配置文件</p><p>lib目录：hadoop运行的依赖jar包</p><p>sbin目录：启动和关闭hadoop等命令都在这里</p><p>libexec目录：存放的也是hadoop命令，但一般不常用</p><p><font color="red">注意：</font>最常用的就是bin和etc目录。</p><h3 id="3-1-8-配置hadoop配置文件"><a href="#3-1-8-配置hadoop配置文件" class="headerlink" title="3.1.8 配置hadoop配置文件"></a>3.1.8 配置hadoop配置文件</h3><p>Hadoop目录下<strong>/home/hadoop-2.7.1/etc/hadoop/</strong>目录下<strong>6个文件</strong></p><h4 id="3-1-8-1-hadoop-env-sh"><a href="#3-1-8-1-hadoop-env-sh" class="headerlink" title="3.1.8.1 hadoop-env.sh"></a>3.1.8.1 hadoop-env.sh</h4><p>执行：vim hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br></pre></td></tr></table></figure><p> 修改：修改java_home路径和hadoop_conf_dir 路径  25行  33行</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#25行</span><br><span class="line">export JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">#33行</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop-2.7.1/etc/hadoop</span><br></pre></td></tr></table></figure><p> 然后执行：source hadoop-env.sh编译文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source hadoop-env.sh</span><br></pre></td></tr></table></figure><h4 id="3-1-8-2-core-site-xml"><a href="#3-1-8-2-core-site-xml" class="headerlink" title="3.1.8.2 core-site.xml"></a>3.1.8.2 core-site.xml</h4><p>命令行执行：vim core-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim core-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hdfs的老大，namenode的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://tedu:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hadoop运行时产生文件的存放目录--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop-2.7.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-3-hdfs-site-xml"><a href="#3-1-8-3-hdfs-site-xml" class="headerlink" title="3.1.8.3 hdfs-site .xml"></a>3.1.8.3 hdfs-site .xml</h4><p>命令行执行：vim hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hdfs-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定hdfs保存数据副本的数量，包括自己，默认值是3--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--如果是伪分布模式，此值是1--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--设置hdfs的操作权限，false表示任何用户都可以在hdfs上操作文件--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-4-mapred-site-xml"><a href="#3-1-8-4-mapred-site-xml" class="headerlink" title="3.1.8.4 mapred-site.xml"></a>3.1.8.4 mapred-site.xml</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line"></span><br><span class="line">vim mapred-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定mapreduce运行在yarn上--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-5-yarn-site-xml"><a href="#3-1-8-5-yarn-site-xml" class="headerlink" title="3.1.8.5 yarn-site.xml"></a>3.1.8.5 yarn-site.xml</h4><p>命令行执行：vim yarn-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim yarn-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定yarn的老大 resoucemanager的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>tedu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--NodeManager获取数据的方式--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-  services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-6-slaves"><a href="#3-1-8-6-slaves" class="headerlink" title="3.1.8.6 slaves"></a>3.1.8.6 slaves</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves</span><br></pre></td></tr></table></figure><p><strong>修改主机名</strong></p><h3 id="3-1-9-配置hadoop的环境变量"><a href="#3-1-9-配置hadoop的环境变量" class="headerlink" title="3.1.9 配置hadoop的环境变量"></a>3.1.9 配置hadoop的环境变量</h3><ol><li><p>文件最后追加文件</p><p><strong>HADOOP_HOME=/home/hadoop-2.7.1</strong></p><p><strong>export HADOOP_HOME</strong></p></li><li><p>source /etc/profile 使更改的配置立即生效。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">HADOOP_HOME=/home/hadoop-2.7.1</span><br><span class="line">PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASSPATH HADOOP_HOME</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-10-格式化Namenode"><a href="#3-1-10-格式化Namenode" class="headerlink" title="3.1.10 格式化Namenode"></a>3.1.10 格式化Namenode</h3><p>执行：hdfs namenode -format</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>如果不好使，可以重启linux</p><p>当出现：successfully，证明格式化成功。</p><h3 id="3-1-11-启动Hadoop"><a href="#3-1-11-启动Hadoop" class="headerlink" title="3.1.11 启动Hadoop"></a>3.1.11 启动Hadoop</h3><p>在/home/hadoop-2.7.1/sbin目录下</p><p>执行:./start-all.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><h3 id="3-1-12-验证启动成功"><a href="#3-1-12-验证启动成功" class="headerlink" title="3.1.12 验证启动成功"></a>3.1.12 验证启动成功</h3><p>可以访问网址： <a href="http://192.168.220.128:50070" target="_blank" rel="noopener">http://192.168.220.128:50070</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Hadoop伪分布式搭建：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        大数据（Big Data），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。&lt;br&gt;​        大数据的定义是4Vs：数据量大、处理速度快、数据源多样、真实性。用中文简单描述就是大、快、多、真。&lt;/p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://zhangfuxin.cn/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://zhangfuxin.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>如何选购合适的电脑</title>
    <link href="http://zhangfuxin.cn/buy-computer.html"/>
    <id>http://zhangfuxin.cn/buy-computer.html</id>
    <published>2019-08-28T16:30:04.000Z</published>
    <updated>2019-08-28T17:10:49.787Z</updated>
    
    <content type="html"><![CDATA[<p>** 购买合适的电脑：** &lt;Excerpt in index | 首页摘要&gt;<br>随着时代的发展，人们生活水平的提高，计算机也成为了我们工作必不可少的生产力，办公，打游戏，看电影，购物等等。市面上电脑种类根据用途又可以分为很多类，台式机、笔记本、IPAD（平板）。购买一台适合自己的电脑工作时可以如虎添翼，电脑用着舒服，心情也会舒畅许多。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="买电脑主要需求"><a href="#买电脑主要需求" class="headerlink" title="买电脑主要需求"></a>买电脑主要需求</h2><ol><li>看电影，上网（购物）   </li><li>打游戏</li><li>办公（移动办公）</li><li>平面设计（CAD）</li><li>UI(影视剪辑)</li><li>编程</li><li>其他</li></ol><h2 id="电脑配置说明"><a href="#电脑配置说明" class="headerlink" title="电脑配置说明"></a>电脑配置说明</h2><p>目前电脑配置的CPU（绝对过剩），内存Win10最低要8个G，显卡要根据自己需求一般显卡基本够用，电脑最大的瓶颈都是在硬盘上，所以现在买电脑带不带固态硬盘是我首选的配置（我对固态硬盘定义最低要128G,512G固态才是标配，毕竟固态大小会影响到一定的读写速率，还有为了保证固态寿命做系统时会留出10%的空间不划分到分区中），至于买笔记本还是台式机需要根据不同应用场景来定。台式机性能肯定远超同价位笔记本，这个是毋庸置疑的。</p><h2 id="看电影，上网（购物）"><a href="#看电影，上网（购物）" class="headerlink" title="看电影，上网（购物）"></a>看电影，上网（购物）</h2><p>对于这方面需求的一般一女生居多，看电影上网，对电脑配置要求比较低的，一般普通双核CPU，AMD、酷睿i3都可以（最好是i5），内存8G就够了（win7的话4G就够，但是Win7现在不支持更新了）。要是女生最重要的是漂亮，这里推荐DELL或者HP相对性价比会比较合适。毕竟要是要以轻薄、美观为主。要是资金充足可以考虑各家品牌的超级本。要是父母的需求的话其实买笔记本或者台式机都可以。这里不推荐苹果笔记本，因为用苹果看电影会容易热，要是妹子是苹果控或者周边产品都是苹果产品，苹果笔记本也可在考虑之列。</p><h2 id="打游戏"><a href="#打游戏" class="headerlink" title="打游戏"></a>打游戏</h2><p>游戏主机两个最主要的要求配置和扩展性，主要是CPU和显卡，我们又称之为“双烧”，建议买台式机。要是需要便携的话，外星人品牌是一个不错的考虑，笔记本显卡最好不要超过GTX2070以上，也许你会问为什么不买笔记本GTX2080的本子，一方面是贵，价格会差很多。还有就是散热问题。为了更好体验还是台式机加水冷。</p><ul><li>一般的主流网游：i5或i7处理器，内存16G，中端显卡就可以了，硬盘128G固态+1T机械起</li><li>大型单机：i7或i9处理器（水冷），内存16-32G，，显卡中高端GTX1060起，要是玩刺客信条奥德赛GTX2080Ti不用犹豫，硬盘512三星固态+1T机械（最好在配置1T的固态，毕竟游戏不小）起</li><li>发烧友：i9处理器（水冷），内存32G-64G，显卡高端GTX2080或者是多显交火，硬盘512G（三星固态PRO系列）+1T固态</li></ul><h2 id="办公"><a href="#办公" class="headerlink" title="办公"></a>办公</h2><p>用于办公的大多是商务人士，对笔记本的性能要求一般，最主要的是便携性，各大品牌的超极本都很合适，还能衬托气质，最推荐的还是联想的thinkpad系列，没钱买个E系类（基本三年就会坏），要是有资金充裕T系列或者X系列是首选配置（尤其是X系列）。</p><h2 id="平面设计（CAD）"><a href="#平面设计（CAD）" class="headerlink" title="平面设计（CAD）"></a>平面设计（CAD）</h2><p>这个是专业领域的需求，对CPU、显卡和内存、显示器都较高，能好一点就好一点。    </p><h2 id="UI-影视剪辑"><a href="#UI-影视剪辑" class="headerlink" title="UI(影视剪辑)"></a>UI(影视剪辑)</h2><p>苹果的Macbookpro 16G，512SSD（固态太小用久了会后悔的），i7处理器 最为合适。没有比苹果更适合做平面设计的电脑。Windows系统和苹果系统没得比。</p><h2 id="编程"><a href="#编程" class="headerlink" title="编程"></a>编程</h2><p>苹果的Macbookpro 16G、512SSD、i7处理器。个人推荐MAC的笔记本做编程，一用就停不下来，会上瘾。Windows系统用来打游戏就好了。<br>推荐配置：Macbookpro 16G、i7处理器（i9也是阉割版没意义）、512SSD（固态真的不能太小，512G就不大，考虑到价格没办法）、最好是能带键盘灯、Air pods耳机还是要有一个的，用了就知道不亏。经济允许最好是配置一个IPAD PRO做分屏开发可以调高效率。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>IPAD我对它的定义就是一台游戏机，不建议用IPAD看电影（用久了手会麻）。因为我不做UI我也没有体会到那只笔的好处。</p><p>还有一个设备一点光要说一下就是亚马逊的Kindle，要是你经常看小说，或者是看英文，建议有一个（前期是你不是必须要纸质书）还是很方便的，尤其是书多了的时候。IPAD优势在于pdf文档做笔记。用了就会知道两个不一样。Kindle看电子书是生活品质提升的表现。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 购买合适的电脑：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;br&gt;随着时代的发展，人们生活水平的提高，计算机也成为了我们工作必不可少的生产力，办公，打游戏，看电影，购物等等。市面上电脑种类根据用途又可以分为很多类，台式机、笔记本、IPAD（平板）。购买一台适合自己的电脑工作时可以如虎添翼，电脑用着舒服，心情也会舒畅许多。&lt;/p&gt;
    
    </summary>
    
      <category term="others" scheme="http://zhangfuxin.cn/categories/others/"/>
    
    
      <category term="数码产品" scheme="http://zhangfuxin.cn/tags/%E6%95%B0%E7%A0%81%E4%BA%A7%E5%93%81/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （十九）SparkSQL的自定义函数UDF</title>
    <link href="http://zhangfuxin.cn/2019-06-19-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89SparkSQL%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0UDF.html"/>
    <id>http://zhangfuxin.cn/2019-06-19-Spark学习之路 （十九）SparkSQL的自定义函数UDF.html</id>
    <published>2019-06-19T02:30:04.000Z</published>
    <updated>2019-09-17T04:15:17.246Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （十九）SparkSQL的自定义函数UDF：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （十九）SparkSQL的自定义函数UDF</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>在Spark中，也支持Hive中的自定义函数。自定义函数大致可以分为三种：</p><ul><li>UDF(User-Defined-Function)，即最基本的自定义函数，类似to_char,to_date等</li><li>UDAF（User- Defined Aggregation Funcation），用户自定义聚合函数，类似在group by之后使用的sum,avg等</li><li>UDTF(User-Defined Table-Generating Functions),用户自定义生成函数，有点像stream里面的flatMap</li></ul><p>自定义一个UDF函数需要继承UserDefinedAggregateFunction类，并实现其中的8个方法</p><p>示例</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">DataType</span>, <span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GetDistinctCityUDF</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span></span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 输入的数据类型</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"status"</span>,<span class="type">StringType</span>,<span class="literal">true</span>) :: <span class="type">Nil</span></span><br><span class="line">  )</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 缓存字段类型</span></span><br><span class="line"><span class="comment">    * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">StructType</span>(</span><br><span class="line">      <span class="type">Array</span>(</span><br><span class="line">        <span class="type">StructField</span>(<span class="string">"buffer_city_info"</span>,<span class="type">StringType</span>,<span class="literal">true</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 输出结果类型</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">StringType</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 输入类型和输出类型是否一致</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 对辅助字段进行初始化</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer.update(<span class="number">0</span>,<span class="string">""</span>)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *修改辅助字段的值</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//获取最后一次的值</span></span><br><span class="line">    <span class="keyword">var</span> last_str = buffer.getString(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">//获取当前的值</span></span><br><span class="line">    <span class="keyword">val</span> current_str = input.getString(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">//判断最后一次的值是否包含当前的值</span></span><br><span class="line">    <span class="keyword">if</span>(!last_str.contains(current_str))&#123;</span><br><span class="line">      <span class="comment">//判断是否是第一个值，是的话走if赋值，不是的话走else追加</span></span><br><span class="line">      <span class="keyword">if</span>(last_str.equals(<span class="string">""</span>))&#123;</span><br><span class="line">        last_str = current_str</span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        last_str += <span class="string">","</span> + current_str</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    buffer.update(<span class="number">0</span>,last_str)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *对分区结果进行合并</span></span><br><span class="line"><span class="comment">  * buffer1是机器hadoop1上的结果</span></span><br><span class="line"><span class="comment">  * buffer2是机器Hadoop2上的结果</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> buf1 = buffer1.getString(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> buf2 = buffer2.getString(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">//将buf2里面存在的数据而buf1里面没有的数据追加到buf1</span></span><br><span class="line">    <span class="comment">//buf2的数据按照，进行切分</span></span><br><span class="line">    <span class="keyword">for</span>(s &lt;- buf2.split(<span class="string">","</span>))&#123;</span><br><span class="line">      <span class="keyword">if</span>(!buf1.contains(s))&#123;</span><br><span class="line">        <span class="keyword">if</span>(buf1.equals(<span class="string">""</span>))&#123;</span><br><span class="line">          buf1 = s</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">          buf1 += s</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    buffer1.update(<span class="number">0</span>,buf1)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 最终的计算结果</span></span><br><span class="line"><span class="comment">  * */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    buffer.getString(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注册自定义的UDF函数为临时函数</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 第一步 创建程序入口</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"AralHotProductSpark"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> hiveContext = <span class="keyword">new</span> <span class="type">HiveContext</span>(sc)　　<span class="comment">//注册成为临时函数</span></span><br><span class="line">    hiveContext.udf.register(<span class="string">"get_distinct_city"</span>,<span class="type">GetDistinctCityUDF</span>)</span><br><span class="line">　　<span class="comment">//注册成为临时函数</span></span><br><span class="line">    hiveContext.udf.register(<span class="string">"get_product_status"</span>,(str:<span class="type">String</span>) =&gt;&#123;</span><br><span class="line">      <span class="keyword">var</span> status = <span class="number">0</span></span><br><span class="line">      <span class="keyword">for</span>(s &lt;- str.split(<span class="string">","</span>))&#123;</span><br><span class="line">        <span class="keyword">if</span>(s.contains(<span class="string">"product_status"</span>))&#123;</span><br><span class="line">          status = s.split(<span class="string">":"</span>)(<span class="number">1</span>).toInt</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （十九）SparkSQL的自定义函数UDF：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （十九）SparkSQL的自定义函数UDF&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>RDD、DataFrame和DataSet的区别是什么</title>
    <link href="http://zhangfuxin.cn/2019-06-18-DataSet%E5%92%8CDataFrame%E5%8C%BA%E5%88%AB%E5%92%8C%E8%BD%AC%E6%8D%A2.html"/>
    <id>http://zhangfuxin.cn/2019-06-18-DataSet和DataFrame区别和转换.html</id>
    <published>2019-06-18T03:30:04.000Z</published>
    <updated>2019-09-17T04:14:51.186Z</updated>
    
    <content type="html"><![CDATA[<p>** RDD、DataFrame和DataSet的区别是什么：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        RDD、DataFrame和DataSet是容易产生混淆的概念，必须对其相互之间对比，才可以知道其中异同：DataFrame多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合。DataFrame是分布式的Row对象的集合。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>RDD、DataFrame和DataSet是容易产生混淆的概念，必须对其相互之间对比，才可以知道其中异同。</p><h2 id="RDD和DataFrame"><a href="#RDD和DataFrame" class="headerlink" title="RDD和DataFrame"></a>RDD和DataFrame</h2><p><img src="https://upload-images.jianshu.io/upload_images/2160494-08d7d2c7495fd300.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/607/format/webp" alt="img"></p><p>RDD-DataFrame</p><p>上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解 Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。</p><h3 id="提升执行效率"><a href="#提升执行效率" class="headerlink" title="提升执行效率"></a>提升执行效率</h3><p>RDD API是函数式的，强调不变性，在大部分场景下倾向于创建新对象而不是修改老对象。这一特点虽然带来了干净整洁的API，却也使得Spark应用程序在运行期倾向于创建大量临时对象，对GC造成压力。在现有RDD API的基础之上，我们固然可以利用mapPartitions方法来重载RDD单个分片内的数据创建方式，用复用可变对象的方式来减小对象分配和GC的开销，但这牺牲了代码的可读性，而且要求开发者对Spark运行时机制有一定的了解，门槛较高。另一方面，Spark SQL在框架内部已经在各种可能的情况下尽量重用对象，这样做虽然在内部会打破了不变性，但在将数据返回给用户时，还会重新转为不可变数据。利用 DataFrame API进行开发，可以免费地享受到这些优化效果。</p><h3 id="减少数据读取"><a href="#减少数据读取" class="headerlink" title="减少数据读取"></a>减少数据读取</h3><p>分析大数据，最快的方法就是 ——忽略它。这里的“忽略”并不是熟视无睹，而是根据查询条件进行恰当的剪枝。</p><p>上文讨论分区表时提到的分区剪 枝便是其中一种——当查询的过滤条件中涉及到分区列时，我们可以根据查询条件剪掉肯定不包含目标数据的分区目录，从而减少IO。</p><p>对于一些“智能”数据格 式，Spark SQL还可以根据数据文件中附带的统计信息来进行剪枝。简单来说，在这类数据格式中，数据是分段保存的，每段数据都带有最大值、最小值、null值数量等 一些基本的统计信息。当统计信息表名某一数据段肯定不包括符合查询条件的目标数据时，该数据段就可以直接跳过(例如某整数列a某段的最大值为100，而查询条件要求a &gt; 200)。</p><p>此外，Spark SQL也可以充分利用RCFile、ORC、Parquet等列式存储格式的优势，仅扫描查询真正涉及的列，忽略其余列的数据。</p><h3 id="执行优化"><a href="#执行优化" class="headerlink" title="执行优化"></a>执行优化</h3><p><img src="https://upload-images.jianshu.io/upload_images/2160494-c2423230fcc3841d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/571/format/webp" alt="img"></p><p>人口数据分析示例</p><p>为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter 下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。</p><p>得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。</p><p>对于普通开发者而言，查询优化 器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。</p><h2 id="RDD和DataSet"><a href="#RDD和DataSet" class="headerlink" title="RDD和DataSet"></a>RDD和DataSet</h2><p>DataSet以Catalyst逻辑执行计划表示，并且数据以编码的二进制形式被存储，不需要反序列化就可以执行sorting、shuffle等操作。</p><p>DataSet创立需要一个显式的Encoder，把对象序列化为二进制，可以把对象的scheme映射为SparkSQl类型，然而RDD依赖于运行时反射机制。</p><p>通过上面两点，DataSet的性能比RDD的要好很多。</p><h2 id="DataFrame和DataSet"><a href="#DataFrame和DataSet" class="headerlink" title="DataFrame和DataSet"></a>DataFrame和DataSet</h2><p>DataSet跟DataFrame还是有挺大区别的，DataFrame开发都是写sql，但是DataSet是使用类似RDD的API。主要区别是Dataset每一个record存储的是一个强类型值而不是一个Row。</p><h3 id="1-相同点："><a href="#1-相同点：" class="headerlink" title="(1)相同点："></a>(1)相同点：</h3><p>都是分布式数据集</p><p>DataFrame底层是RDD，但是DataSet不是，不过他们最后都是转换成RDD运行</p><p>DataSet和DataFrame的相同点都是有数据特征、数据类型的分布式数据集(schema)</p><h3 id="2-不同点："><a href="#2-不同点：" class="headerlink" title="(2)不同点："></a>(2)不同点：</h3><p><strong>(a)schema信息：</strong></p><p>RDD中的数据是没有数据类型的</p><p>DataFrame中的数据是<strong>弱数据类型</strong>，不会做数据类型检查</p><p>虽然有schema规定了数据类型，但是编译时是不会报错的，运行时才会报错</p><p>DataSet中的数据类型是<strong>强数据类型</strong></p><p><strong>(b)序列化机制：</strong></p><p>RDD和DataFrame默认的序列化机制是java的序列化，可以修改为Kyro的机制</p><p>DataSet使用自定义的数据编码器进行序列化和反序列化</p><h2 id="创建方式："><a href="#创建方式：" class="headerlink" title="创建方式："></a>创建方式：</h2><h3 id="1-要使用toDS之前"><a href="#1-要使用toDS之前" class="headerlink" title="(1)要使用toDS之前"></a>(1)要使用toDS之前</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br></pre></td></tr></table></figure><h3 id="2-将内存中的数据转换成DataSet"><a href="#2-将内存中的数据转换成DataSet" class="headerlink" title="(2)将内存中的数据转换成DataSet"></a>(2)将内存中的数据转换成DataSet</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Encoders for most common types are automatically provided by importing</span></span><br><span class="line"></span><br><span class="line">sqlContext.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).toDS()</span><br><span class="line">ds.map(_ + <span class="number">1</span>).collect() <span class="comment">// Returns: Array(2, 3, 4)</span></span><br></pre></td></tr></table></figure><p>其中：</p><p>collect()：返回一个Array，包含所有行信息</p><p>Returns an array that contains all rows in this Dataset.</p><h3 id="3-可以直接把case-class对象转化成DataSet"><a href="#3-可以直接把case-class对象转化成DataSet" class="headerlink" title="(3)可以直接把case class对象转化成DataSet"></a>(3)可以直接把case class对象转化成DataSet</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Encoders are also created for case classes.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">ds</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Andy"</span>, <span class="number">32</span>)).toDS()</span><br></pre></td></tr></table></figure><h3 id="4-将DataFrame转换成DataSet，不过要求是DataFrame的数据类型必须是case-class"><a href="#4-将DataFrame转换成DataSet，不过要求是DataFrame的数据类型必须是case-class" class="headerlink" title="(4)将DataFrame转换成DataSet，不过要求是DataFrame的数据类型必须是case class"></a>(4)将DataFrame转换成DataSet，不过要求是DataFrame的数据类型必须是case class</h3><p>并且要求DataFrame的数据类型必须和case class一致(顺序也必须一致)</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> _0729DF</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Dataset</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//import org.apache.spark</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Dataset</span> <span class="keyword">extends</span> <span class="title">App</span></span>&#123;</span><br><span class="line"><span class="comment">// import spark.implicits._</span></span><br><span class="line"><span class="comment">// val ds = Seq(1, 2, 3).toDS()</span></span><br><span class="line"><span class="comment">// ds.map(_ + 1).collect() // Returns: Array(2, 3, 4)</span></span><br><span class="line"><span class="comment">// // Encoders are also created for case classes.</span></span><br><span class="line"><span class="comment">// case class Person(name: String, age: Long)</span></span><br><span class="line"><span class="comment">// val ds = Seq(Person("Andy", 32)).toDS()</span></span><br><span class="line"><span class="comment">// ds.show</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> session = <span class="type">SparkSession</span>.builder()</span><br><span class="line">.appName(<span class="string">"app"</span>)</span><br><span class="line">.master(<span class="string">"local"</span>)</span><br><span class="line">.getOrCreate()</span><br><span class="line"><span class="keyword">val</span> sqlContext = session.sqlContext</span><br><span class="line"><span class="keyword">val</span> wcDs = sqlContext.read.textFile(<span class="string">"datas/halibote.txt"</span>)</span><br><span class="line"><span class="comment">// 导入隐式转换</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> session.implicits._</span><br><span class="line"><span class="keyword">val</span> wordData=wcDs.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">wordData.createTempView(<span class="string">"t_word"</span>)</span><br><span class="line">wordData.show()</span><br><span class="line">    </span><br><span class="line"><span class="comment">//wordData.printSchema()</span></span><br><span class="line"><span class="comment">// Encoders for most common types are automatically provided by importing sqlContext.implicits._</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds=<span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>).toDS()</span><br><span class="line">ds.map(_ + <span class="number">1</span>).collect() <span class="comment">// Returns: Array(2, 3, 4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// // Encoders are also created for case classes.</span></span><br><span class="line"><span class="comment">// case class Person(name: String, age: Long)</span></span><br><span class="line"><span class="comment">// val ds = Seq(Person("Andy", 32)).toDS()</span></span><br><span class="line"><span class="comment">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">age:<span class="type">Long</span>,name:<span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">path</span> </span>= <span class="string">"datas/people.json"</span></span><br><span class="line"><span class="keyword">val</span> people: <span class="type">Dataset</span>[<span class="type">Person</span>] = sqlContext.read.json(path).as[<span class="type">Person</span>]</span><br><span class="line">people.show()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>用wordcount举例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DataFrame</span></span><br><span class="line"><span class="comment">// Load a text file and interpret each line as a java.lang.String</span></span><br><span class="line"><span class="keyword">val</span> ds = sqlContext.read.text(<span class="string">"/home/spark/1.6/lines"</span>).as[<span class="type">String</span>]</span><br><span class="line"><span class="keyword">val</span> result = ds</span><br><span class="line">  .flatMap(_.split(<span class="string">" "</span>))              <span class="comment">// Split on whitespace</span></span><br><span class="line">  .filter(_ != <span class="string">""</span>)                    <span class="comment">// Filter empty words</span></span><br><span class="line">  .toDF()                              <span class="comment">// Convert to DataFrame to perform aggregation / sorting</span></span><br><span class="line">  .groupBy($<span class="string">"value"</span>)                  <span class="comment">// Count number of occurences of each word</span></span><br><span class="line">  .agg(count(<span class="string">"*"</span>) as <span class="string">"numOccurances"</span>)</span><br><span class="line">  .orderBy($<span class="string">"numOccurances"</span> desc)      <span class="comment">// Show most common words first</span></span><br></pre></td></tr></table></figure><p>后面版本DataFrame会继承DataSet，DataFrame是面向Spark SQL的接口。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DataSet,完全使用scala编程，不要切换到DataFrame</span></span><br><span class="line"><span class="keyword">val</span> wordCount = </span><br><span class="line">ds.flatMap(.split(<span class="string">" "</span>))</span><br><span class="line">  .filter( != <span class="string">""</span>)</span><br><span class="line">  .groupBy(_.toLowerCase())  <span class="comment">// Instead of grouping on a column expression (i.e. $"value") we pass a lambda function</span></span><br><span class="line">  .count()</span><br></pre></td></tr></table></figure><p>DataFrame和DataSet可以相互转化， df.as[ElementType] 这样可以把DataFrame转化为DataSet， ds.toDF() 这样可以把DataSet转化为DataFrame。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** RDD、DataFrame和DataSet的区别是什么：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        RDD、DataFrame和DataSet是容易产生混淆的概念，必须对其相互之间对比，才可以知道其中异同：DataFrame多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合。DataFrame是分布式的Row对象的集合。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （十八）SparkSQL简单使用</title>
    <link href="http://zhangfuxin.cn/2019-06-18-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89SparkSQL%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8.html"/>
    <id>http://zhangfuxin.cn/2019-06-18-Spark学习之路 （十八）SparkSQL简单使用.html</id>
    <published>2019-06-18T02:30:04.000Z</published>
    <updated>2019-09-17T03:39:50.782Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （十八）SparkSQL简单使用：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （十八）SparkSQL简单使用</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、SparkSQL的进化之路"><a href="#一、SparkSQL的进化之路" class="headerlink" title="一、SparkSQL的进化之路"></a>一、SparkSQL的进化之路</h2><p>1.0以前：    Shark</p><p>1.1.x：         SparkSQL(只是测试性的)  SQL</p><p>1.3.x:            SparkSQL(正式版本)+Dataframe</p><p>1.5.x:            SparkSQL 钨丝计划</p><p>1.6.x：         SparkSQL+DataFrame+DataSet(测试版本)</p><p>2.x   :            SparkSQL+DataFrame+DataSet(正式版本)</p><p>​                      SparkSQL:还有其他的优化</p><p>​                      StructuredStreaming(DataSet)</p><h2 id="二、认识SparkSQL"><a href="#二、认识SparkSQL" class="headerlink" title="二、认识SparkSQL"></a>二、认识SparkSQL</h2><h3 id="2-1-什么是SparkSQL"><a href="#2-1-什么是SparkSQL" class="headerlink" title="2.1　什么是SparkSQL?"></a>2.1　什么是SparkSQL?</h3><p>spark SQL是spark的一个模块，主要用于进行结构化数据的处理。它提供的最核心的编程抽象就是DataFrame。</p><h3 id="2-2-SparkSQL的作用"><a href="#2-2-SparkSQL的作用" class="headerlink" title="2.2　SparkSQL的作用"></a>2.2　SparkSQL的作用</h3><p>提供一个编程抽象（DataFrame） 并且作为分布式 SQL 查询引擎</p><p>DataFrame：它可以根据很多源进行构建，包括：<strong>结构化的数据文件，hive中的表，外部的关系型数据库，以及RDD</strong></p><h3 id="2-3-运行原理"><a href="#2-3-运行原理" class="headerlink" title="2.3　运行原理"></a>2.3　运行原理</h3><p>将 Spark SQL 转化为 RDD， 然后提交到集群执行</p><h3 id="2-4-特点"><a href="#2-4-特点" class="headerlink" title="2.4　特点"></a>2.4　特点</h3><p>（1）容易整合</p><p>（2）统一的数据访问方式</p><p>（3）兼容 Hive</p><p>（4）标准的数据连接</p><h3 id="2-5-SparkSession"><a href="#2-5-SparkSession" class="headerlink" title="2.5　SparkSession"></a>2.5　SparkSession</h3><p>SparkSession是Spark 2.0引如的新概念。SparkSession为用户提供了统一的切入点，来让用户学习spark的各项功能。<br>  在spark的早期版本中，SparkContext是spark的主要切入点，由于RDD是主要的API，我们通过sparkcontext来创建和操作RDD。对于每个其他的API，我们需要使用不同的context。例如，对于Streming，我们需要使用StreamingContext；对于sql，使用sqlContext；对于Hive，使用hiveContext。但是随着DataSet和DataFrame的API逐渐成为标准的API，就需要为他们建立接入点。所以在spark2.0中，引入SparkSession作为DataSet和DataFrame API的切入点，SparkSession封装了SparkConf、SparkContext和SQLContext。为了向后兼容，SQLContext和HiveContext也被保存下来。<br>　　<br>　　SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。</p><p>特点：</p><p>　　 <strong>—-</strong> <strong>为用户提供一个统一的切入点使用Spark 各项功能</strong></p><p>​        <strong>—-</strong> <strong>允许用户通过它调用 DataFrame 和 Dataset 相关 API 来编写程序</strong></p><p>​        <strong>—-</strong> <strong>减少了用户需要了解的一些概念，可以很容易的与 Spark 进行交互</strong></p><p>​        <strong>—-</strong> <strong>与 Spark 交互之时不需要显示的创建 SparkConf, SparkContext 以及 SQlContext，这些对象已经封闭在 SparkSession 中</strong></p><h3 id="2-7-DataFrames"><a href="#2-7-DataFrames" class="headerlink" title="2.7　DataFrames"></a><strong>2.7　DataFrames</strong></h3><p>在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503195056657-1280315007.png" alt="img"></p><h2 id="三、RDD转换成为DataFrame"><a href="#三、RDD转换成为DataFrame" class="headerlink" title="三、RDD转换成为DataFrame"></a>三、RDD转换成为DataFrame</h2><p>使用spark1.x版本的方式</p><p>测试数据目录：/home/hadoop/apps/spark/examples/src/main/resources（spark的安装目录里面）</p><p>people.txt</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503195656260-525846269.png" alt="img"></p><h3 id="3-1-方式一：通过-case-class-创建-DataFrames（反射）"><a href="#3-1-方式一：通过-case-class-创建-DataFrames（反射）" class="headerlink" title="3.1　方式一：通过 case class 创建 DataFrames（反射）"></a>3.1　方式一：<strong>通过 case class 创建 DataFrames（反射）</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">//定义case class，相当于表结构</span><br><span class="line">case class People(var name:String,var age:Int)</span><br><span class="line">object TestDataFrame1 &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setAppName("RDDToDataFrame").setMaster("local")</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val context = new SQLContext(sc)</span><br><span class="line">    // 将本地的数据读入 RDD， 并将 RDD 与 case class 关联</span><br><span class="line">    val peopleRDD = sc.textFile("E:\\666\\people.txt")</span><br><span class="line">      .map(line =&gt; People(line.split(",")(0), line.split(",")(1).trim.toInt))</span><br><span class="line">    import context.implicits._</span><br><span class="line">    // 将RDD 转换成 DataFrames</span><br><span class="line">    val df = peopleRDD.toDF</span><br><span class="line">    //将DataFrames创建成一个临时的视图</span><br><span class="line">    df.createOrReplaceTempView("people")</span><br><span class="line">    //使用SQL语句进行查询</span><br><span class="line">    context.sql("select * from people").show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503202629907-1000361533.png" alt="img"></p><h3 id="3-2-方式二：通过-structType-创建-DataFrames（编程接口）"><a href="#3-2-方式二：通过-structType-创建-DataFrames（编程接口）" class="headerlink" title="3.2　方式二：通过 structType 创建 DataFrames（编程接口）"></a>3.2　方式二：<strong>通过 structType 创建 DataFrames（编程接口）</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">object TestDataFrame2 &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setAppName("TestDataFrame2").setMaster("local")</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val sqlContext = new SQLContext(sc)</span><br><span class="line">    val fileRDD = sc.textFile("E:\\666\\people.txt")</span><br><span class="line">    // 将 RDD 数据映射成 Row，需要 import org.apache.spark.sql.Row</span><br><span class="line">    val rowRDD: RDD[Row] = fileRDD.map(line =&gt; &#123;</span><br><span class="line">      val fields = line.split(",")</span><br><span class="line">      Row(fields(0), fields(1).trim.toInt)</span><br><span class="line">    &#125;)</span><br><span class="line">    // 创建 StructType 来定义结构</span><br><span class="line">    val structType: StructType = StructType(</span><br><span class="line">      //字段名，字段类型，是否可以为空</span><br><span class="line">      StructField("name", StringType, true) ::</span><br><span class="line">        StructField("age", IntegerType, true) :: Nil</span><br><span class="line">    )</span><br><span class="line">    /**</span><br><span class="line">      * rows: java.util.List[Row],</span><br><span class="line">      * schema: StructType</span><br><span class="line">      * */</span><br><span class="line">    val df: DataFrame = sqlContext.createDataFrame(rowRDD,structType)</span><br><span class="line">    df.createOrReplaceTempView("people")</span><br><span class="line">    sqlContext.sql("select * from people").show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503202905865-1517500300.png" alt="img"></p><h3 id="3-3-方式三：通过-json-文件创建-DataFrames"><a href="#3-3-方式三：通过-json-文件创建-DataFrames" class="headerlink" title="3.3　方式三：通过 json 文件创建 DataFrames"></a>3.3　方式三：<strong>通过 json 文件创建 DataFrames</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">object TestDataFrame3 &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setAppName("TestDataFrame2").setMaster("local")</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val sqlContext = new SQLContext(sc)</span><br><span class="line">    val df: DataFrame = sqlContext.read.json("E:\\666\\people.json")</span><br><span class="line">    df.createOrReplaceTempView("people")</span><br><span class="line">    sqlContext.sql("select * from people").show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503203759442-1628348320.png" alt="img"></p><h2 id="四、DataFrame的read和save和savemode"><a href="#四、DataFrame的read和save和savemode" class="headerlink" title="四、DataFrame的read和save和savemode"></a>四、DataFrame的read和save和savemode</h2><h3 id="4-1-数据的读取"><a href="#4-1-数据的读取" class="headerlink" title="4.1　数据的读取"></a>4.1　数据的读取</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">object TestRead &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setAppName("TestDataFrame2").setMaster("local")</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val sqlContext = new SQLContext(sc)</span><br><span class="line">    //方式一</span><br><span class="line">    val df1 = sqlContext.read.json("E:\\666\\people.json")</span><br><span class="line">    val df2 = sqlContext.read.parquet("E:\\666\\users.parquet")</span><br><span class="line">    //方式二</span><br><span class="line">    val df3 = sqlContext.read.format("json").load("E:\\666\\people.json")</span><br><span class="line">    val df4 = sqlContext.read.format("parquet").load("E:\\666\\users.parquet")</span><br><span class="line">    //方式三，默认是parquet格式</span><br><span class="line">    val df5 = sqlContext.load("E:\\666\\users.parquet")</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-2-数据的保存"><a href="#4-2-数据的保存" class="headerlink" title="4.2　数据的保存"></a>4.2　数据的保存</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">object TestSave &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setAppName("TestDataFrame2").setMaster("local")</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val sqlContext = new SQLContext(sc)</span><br><span class="line">    val df1 = sqlContext.read.json("E:\\666\\people.json")</span><br><span class="line">    //方式一</span><br><span class="line">    df1.write.json("E:\\111")</span><br><span class="line">    df1.write.parquet("E:\\222")</span><br><span class="line">    //方式二</span><br><span class="line">    df1.write.format("json").save("E:\\333")</span><br><span class="line">    df1.write.format("parquet").save("E:\\444")</span><br><span class="line">    //方式三</span><br><span class="line">    df1.write.save("E:\\555")</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-3-数据的保存模式"><a href="#4-3-数据的保存模式" class="headerlink" title="4.3　数据的保存模式"></a>4.3　数据的保存模式</h3><p>使用mode</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1.write.format(&quot;parquet&quot;).mode(SaveMode.Ignore).save(&quot;E:\\444&quot;)</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503211638036-705055493.png" alt="img"></p><h2 id="五、数据源"><a href="#五、数据源" class="headerlink" title="五、数据源"></a>五、数据源</h2><h3 id="5-1-数据源只json"><a href="#5-1-数据源只json" class="headerlink" title="5.1　数据源只json"></a>5.1　数据源只json</h3><p>参考4.1</p><h3 id="5-2-数据源之parquet"><a href="#5-2-数据源之parquet" class="headerlink" title="5.2　数据源之parquet"></a>5.2　数据源之parquet</h3><p>参考4.1</p><h3 id="5-3-数据源之Mysql"><a href="#5-3-数据源之Mysql" class="headerlink" title="5.3　数据源之Mysql"></a>5.3　数据源之Mysql</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestMysql</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"TestMysql"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> url = <span class="string">"jdbc:mysql://192.168.123.102:3306/hivedb"</span></span><br><span class="line">    <span class="keyword">val</span> table = <span class="string">"dbs"</span></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.setProperty(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">    properties.setProperty(<span class="string">"password"</span>,<span class="string">"root"</span>)</span><br><span class="line">    <span class="comment">//需要传入Mysql的URL、表明、properties（连接数据库的用户名密码）</span></span><br><span class="line">    <span class="keyword">val</span> df = sqlContext.read.jdbc(url,table,properties)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"dbs"</span>)</span><br><span class="line">    sqlContext.sql(<span class="string">"select * from dbs"</span>).show()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503215248558-1665335084.png" alt="img"></p><h3 id="5-4-数据源之Hive"><a href="#5-4-数据源之Hive" class="headerlink" title="5.4　数据源之Hive"></a>5.4　数据源之Hive</h3><h4 id="（1）准备工作"><a href="#（1）准备工作" class="headerlink" title="（1）准备工作"></a>（1）准备工作</h4><p>在pom.xml文件中添加依赖</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- https:<span class="comment">//mvnrepository.com/artifact/org.apache.spark/spark-hive --&gt;</span></span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-hive_2<span class="number">.11</span>&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;<span class="number">2.3</span><span class="number">.0</span>&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>开发环境则把resource文件夹下添加hive-site.xml文件，集群环境把hive的配置文件要发到$SPARK_HOME/conf目录下</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180504184547333-1552631388.png" alt="img"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;jdbc:mysql://localhost:3306/hivedb?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">                &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">                &lt;!-- 如果 mysql 和 hive 在同一个服务器节点，那么请更改 hadoop02 为 localhost --&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">                &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">                &lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/hive/warehouse&lt;/value&gt;</span><br><span class="line">                &lt;description&gt;hive default warehouse, if nessecory, change it&lt;/description&gt;</span><br><span class="line">        &lt;/property&gt;  </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="（2）测试代码"><a href="#（2）测试代码" class="headerlink" title="（2）测试代码"></a>（2）测试代码</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">object TestHive &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setMaster("local").setAppName(this.getClass.getSimpleName)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val sqlContext = new HiveContext(sc)</span><br><span class="line">    sqlContext.sql("select * from myhive.student").show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180504192745282-1160176093.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （十八）SparkSQL简单使用：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （十八）SparkSQL简单使用&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （十七）Spark分区</title>
    <link href="http://zhangfuxin.cn/2019-06-17-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89Spark%E5%88%86%E5%8C%BA.html"/>
    <id>http://zhangfuxin.cn/2019-06-17-Spark学习之路 （十七）Spark分区.html</id>
    <published>2019-06-17T02:30:04.000Z</published>
    <updated>2019-09-17T03:30:58.175Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （十七）Spark分区：** &lt;Excerpt in index | 首页摘要&gt;</p><p>　　分区是RDD内部并行计算的一个计算单元，RDD的数据集在逻辑上被划分为多个分片，每一个分片称为分区，分区的格式决定了并行计算的粒度，而每个分区的数值计算都是在一个任务中进行的，因此任务的个数，也是由RDD(准确来说是作业最后一个RDD)的分区数决定。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、为什么要进行分区"><a href="#一、为什么要进行分区" class="headerlink" title="一、为什么要进行分区"></a>一、为什么要进行分区</h2><p>　　数据分区，在分布式集群里，网络通信的代价很大，减少网络传输可以极大提升性能。mapreduce框架的性能开支主要在io和网络传输，io因为要大量读写文件，它是不可避免的，但是网络传输是可以避免的，把大文件压缩变小文件，   从而减少网络传输，但是增加了cpu的计算负载。</p><p>　　<strong>Spark</strong>里面io也是不可避免的，但是网络传输spark里面进行了优化：</p><p>　　Spark把rdd进行分区（分片），放在集群上并行计算。同一个rdd分片100个，10个节点，平均一个节点10个分区，当进行sum型的计算的时候，先进行每个分区的sum，然后把sum值shuffle传输到主程序进行全局sum，所以进行sum型计算对网络传输非常小。但对于进行join型的计算的时候，需要把数据本身进行shuffle，网络开销很大。</p><p>spark是如何优化这个问题的呢？</p><p>　　Spark把key－value rdd通过key的hashcode进行分区，而且保证相同的key存储在同一个节点上，这样对改rdd进行key聚合时，就不需要shuffle过程，我们进行mapreduce计算的时候为什么要进行shuffle？，就是说mapreduce里面网络传输主要在shuffle阶段，<strong>shuffle的根本原因是相同的key存在不同的节点上，按key进行聚合的时候不得不进行shuffle</strong>。shuffle是非常影响网络的，它要把所有的数据混在一起走网络，然后它才能把相同的key走到一起。<strong>要进行shuffle是存储决定的。</strong></p><p>　　Spark从这个教训中得到启发，spark会把key进行分区，也就是key的hashcode进行分区，相同的key，hashcode肯定是一样的，所以它进行分区的时候100t的数据分成10分，每部分10个t，它能确保相同的key肯定在一个分区里面，而且它能保证存储的时候相同的key能够存在同一个节点上。比如一个rdd分成了100份，集群有10个节点，所以每个节点存10份，每一分称为每个分区，spark能保证相同的key存在同一个节点上，实际上相同的key存在同一个分区。</p><p>　　key的分布不均决定了有的分区大有的分区小。没法分区保证完全相等，但它会保证在一个接近的范围。所以mapreduce里面做的某些工作里边，spark就不需要shuffle了，spark解决网络传输这块的根本原理就是这个。</p><p>　　进行join的时候是两个表，不可能把两个表都分区好，通常情况下是把用的频繁的大表事先进行分区，小表进行关联它的时候小表进行shuffle过程。</p><p>　　大表不需要shuffle。　　</p><p>　　需要在工作节点间进行数据混洗的转换极大地受益于分区。这样的转换是  cogroup，groupWith，join，leftOuterJoin，rightOuterJoin，groupByKey，reduceByKey，combineByKey 和lookup。</p><p>　　<strong>分区是可配置的，只要RDD是基于键值对的即可</strong>。</p><h2 id="二、Spark分区原则及方法"><a href="#二、Spark分区原则及方法" class="headerlink" title="二、Spark分区原则及方法"></a>二、Spark分区原则及方法</h2><p>RDD分区的一个<strong>分区原则：尽可能是得分区的个数等于集群核心数目</strong></p><p>无论是本地模式、Standalone模式、YARN模式或Mesos模式，我们都可以<strong>通过spark.default.parallelism来配置其默认分区个数</strong>，若没有设置该值，则根据不同的集群环境确定该值</p><h3 id="2-1-本地模式"><a href="#2-1-本地模式" class="headerlink" title="2.1　本地模式"></a>2.1　本地模式</h3><h4 id="（1）默认方式"><a href="#（1）默认方式" class="headerlink" title="（1）默认方式"></a>（1）默认方式</h4><p>以下这种默认方式就一个分区</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503184117132-933712151.png" alt="img"></p><p>结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503184211834-340940238.png" alt="img"></p><h4 id="（2）手动设置"><a href="#（2）手动设置" class="headerlink" title="（2）手动设置"></a>（2）手动设置</h4><p>设置了几个分区就是几个分区</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503184315304-1438737967.png" alt="img"></p><p>结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503184417483-992690743.png" alt="img"></p><h4 id="（3）跟local-n-有关"><a href="#（3）跟local-n-有关" class="headerlink" title="（3）跟local[n] 有关"></a>（3）跟local[n] 有关</h4><p>n等于几默认就是几个分区</p><p>如果n=* 那么分区个数就等于cpu core的个数</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503184602132-1762283216.png" alt="img"></p><p>结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503184628115-1042310352.png" alt="img"></p><p>本机电脑查看cpu core，我的电脑–》右键管理–》设备管理器–》处理器</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503184724600-1103843046.png" alt="img"></p><h4 id="（4）参数控制"><a href="#（4）参数控制" class="headerlink" title="（4）参数控制"></a>（4）参数控制</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503185007050-446009891.png" alt="img"></p><p>结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503185028293-1238150539.png" alt="img"></p><h3 id="2-2-YARN模式"><a href="#2-2-YARN模式" class="headerlink" title="2.2　YARN模式"></a>2.2　YARN模式</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503190552013-885991110.png" alt="img"></p><p> 进入defaultParallelism方法</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503190651046-910979790.png" alt="img"></p><p>继续进入defaultParallelism方法</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503190749812-1860089737.png" alt="img"></p><p>这个一个trait，其实现类是（Ctrl+h）</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503190853986-1549098382.png" alt="img"></p><p>进入TaskSchedulerImpl类找到defaultParallelism方法</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503190937719-848606303.png" alt="img"></p><p>继续进入defaultParallelism方法，又是一个trait，看其实现类</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503191109902-1222100636.png" alt="img"></p><p>Ctrl+h看SchedulerBackend类的实现类</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503191213525-716329324.png" alt="img"></p><p>进入CoarseGrainedSchedulerBackend找到defaultParallelism</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180503191320589-992961865.png" alt="img"></p><p><strong>totalCoreCount.get()是所有executor使用的core总数，和2比较去较大值</strong></p><p><strong>如果正常的情况下，那你设置了多少就是多少</strong></p><h2 id="四、分区器"><a href="#四、分区器" class="headerlink" title="四、分区器"></a>四、分区器</h2><p>（1）如果是从HDFS里面读取出来的数据，不需要分区器。因为HDFS本来就分好区了。</p><p>　　  分区数我们是可以控制的，但是没必要有分区器。</p><p>（2）非key-value RDD分区，没必要设置分区器</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">al testRDD = sc.textFile(<span class="string">"C:\\Users\\Administrator\\IdeaProjects\\myspark\\src\\main\\hello.txt"</span>)</span><br><span class="line">  .flatMap(line =&gt; line.split(<span class="string">","</span>))</span><br><span class="line">  .map(word =&gt; (word, <span class="number">1</span>)).partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">　　没必要设置，但是非要设置也行。</span><br></pre></td></tr></table></figure><p>（3）Key-value形式的时候，我们就有必要了。</p><p><strong>HashPartitioner</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resultRDD = testRDD.reduceByKey(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>),(x:<span class="type">Int</span>,y:<span class="type">Int</span>) =&gt; x+ y)</span><br><span class="line"><span class="comment">//如果不设置默认也是HashPartitoiner，分区数跟spark.default.parallelism一样</span></span><br><span class="line">println(resultRDD.partitioner)</span><br><span class="line">println(<span class="string">"resultRDD"</span>+resultRDD.getNumPartitions)</span><br></pre></td></tr></table></figure><p><strong>RangePartitioner</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resultRDD = testRDD.reduceByKey((x:<span class="type">Int</span>,y:<span class="type">Int</span>) =&gt; x+ y)</span><br><span class="line"><span class="keyword">val</span> newresultRDD=resultRDD.partitionBy(<span class="keyword">new</span> <span class="type">RangePartitioner</span>[<span class="type">String</span>,<span class="type">Int</span>](<span class="number">3</span>,resultRDD))</span><br><span class="line">println(newresultRDD.partitioner)</span><br><span class="line">println(<span class="string">"newresultRDD"</span>+newresultRDD.getNumPartitions)</span><br><span class="line">注：按照范围进行分区的，如果是字符串，那么就按字典顺序的范围划分。如果是数字，就按数据自的范围划分。</span><br></pre></td></tr></table></figure><p><strong>自定义分区</strong></p><p><strong>需要实现2个方法</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartitoiner</span>(<span class="params">val numParts:<span class="type">Int</span></span>) <span class="keyword">extends</span>  <span class="title">Partitioner</span></span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = numParts</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> domain = <span class="keyword">new</span> <span class="type">URL</span>(key.toString).getHost</span><br><span class="line">    <span class="keyword">val</span> code = (domain.hashCode % numParts)</span><br><span class="line">    <span class="keyword">if</span> (code &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      code + numParts</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      code</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DomainNamePartitioner</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"word count"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> urlRDD = sc.makeRDD(<span class="type">Seq</span>((<span class="string">"http://baidu.com/test"</span>, <span class="number">2</span>),</span><br><span class="line">      (<span class="string">"http://baidu.com/index"</span>, <span class="number">2</span>), (<span class="string">"http://ali.com"</span>, <span class="number">3</span>), (<span class="string">"http://baidu.com/tmmmm"</span>, <span class="number">4</span>),</span><br><span class="line">      (<span class="string">"http://baidu.com/test"</span>, <span class="number">4</span>)))</span><br><span class="line">    <span class="comment">//Array[Array[(String, Int)]]</span></span><br><span class="line">    <span class="comment">// = Array(Array(),</span></span><br><span class="line">    <span class="comment">// Array((http://baidu.com/index,2), (http://baidu.com/tmmmm,4),</span></span><br><span class="line">    <span class="comment">// (http://baidu.com/test,4), (http://baidu.com/test,2), (http://ali.com,3)))</span></span><br><span class="line">    <span class="keyword">val</span> hashPartitionedRDD = urlRDD.partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">    hashPartitionedRDD.glom().collect()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用spark-shell --jar的方式将这个partitioner所在的jar包引进去，然后测试下面的代码</span></span><br><span class="line">    <span class="comment">// spark-shell --master spark://master:7077 --jars spark-rdd-1.0-SNAPSHOT.jar</span></span><br><span class="line">    <span class="keyword">val</span> partitionedRDD = urlRDD.partitionBy(<span class="keyword">new</span> <span class="type">MyPartitoiner</span>(<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">val</span> array = partitionedRDD.glom().collect()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （十七）Spark分区：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;　　分区是RDD内部并行计算的一个计算单元，RDD的数据集在逻辑上被划分为多个分片，每一个分片称为分区，分区的格式决定了并行计算的粒度，而每个分区的数值计算都是在一个任务中进行的，因此任务的个数，也是由RDD(准确来说是作业最后一个RDD)的分区数决定。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本</title>
    <link href="http://zhangfuxin.cn/2019-06-16-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89SparkCore%E7%9A%84%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%BA%8C%EF%BC%89spark-submit%E6%8F%90%E4%BA%A4%E8%84%9A%E6%9C%AC.html"/>
    <id>http://zhangfuxin.cn/2019-06-16-Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本.html</id>
    <published>2019-06-16T02:30:04.000Z</published>
    <updated>2019-09-17T03:17:31.537Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>上一篇主要是介绍了spark启动的一些脚本，这篇主要分析一下Spark源码中提交任务脚本的处理逻辑，从spark-submit一步步深入进去看看任务提交的整体流程,首先看一下整体的流程概要图：<br><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180502085945492-811482545.png" alt="img" style="zoom:200%;"></p><h2 id="二、源码解读"><a href="#二、源码解读" class="headerlink" title="二、源码解读"></a>二、源码解读</h2><p>2.1　spark-submit</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> -z是检查后面变量是否为空（空则真） shell可以在双引号之内引用变量，单引号不可</span></span><br><span class="line"><span class="meta">#</span><span class="bash">这一步作用是检查SPARK_HOME变量是否为空，为空则执行<span class="keyword">then</span>后面程序</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">source</span>命令： <span class="built_in">source</span> filename作用在当前bash环境下读取并执行filename中的命令</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="variable">$0</span>代表shell脚本文件本身的文件名，这里即使spark-submit</span></span><br><span class="line"><span class="meta">#</span><span class="bash">dirname用于取得脚本文件所在目录 dirname <span class="variable">$0</span>取得当前脚本文件所在目录</span></span><br><span class="line"><span class="meta">#</span><span class="bash">$(命令)表示返回该命令的结果</span></span><br><span class="line"><span class="meta">#</span><span class="bash">故整个<span class="keyword">if</span>语句的含义是：如果SPARK_HOME变量没有设置值，则执行当前目录下的find-spark-home脚本文件，设置SPARK_HOME值</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">disable</span> randomized <span class="built_in">hash</span> <span class="keyword">for</span> string <span class="keyword">in</span> Python 3.3+</span></span><br><span class="line">export PYTHONHASHSEED=0</span><br><span class="line"><span class="meta">#</span><span class="bash">执行spark-class脚本，传递参数org.apache.spark.deploy.SparkSubmit 和<span class="string">"<span class="variable">$@</span>"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">这里<span class="variable">$@</span>表示之前spark-submit接收到的全部参数</span></span><br><span class="line">exec "$&#123;SPARK_HOME&#125;"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"</span><br></pre></td></tr></table></figure><p>所以spark-submit脚本的整体逻辑就是：<br>首先 检查SPARK_HOME是否设置；if 已经设置 执行spark-class文件 否则加载执行find-spark-home文件 </p><h3 id="2-2-find-spark-home"><a href="#2-2-find-spark-home" class="headerlink" title="2.2　find-spark-home"></a>2.2　find-spark-home</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">定义一个变量用于后续判断是否存在定义SPARK_HOME的python脚本文件</span></span><br><span class="line">FIND_SPARK_HOME_PYTHON_SCRIPT="$(cd "$(dirname "$0")"; pwd)/find_spark_home.py"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Short cirtuit <span class="keyword">if</span> the user already has this <span class="built_in">set</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#如果SPARK_HOME为不为空值，成功退出程序</span></span></span><br><span class="line">if [ ! -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">   exit 0</span><br><span class="line"><span class="meta">#</span><span class="bash"> -f用于判断这个文件是否存在并且是否为常规文件，是的话为真，这里不存在为假，执行下面语句，给SPARK_HOME变量赋值</span></span><br><span class="line">elif [ ! -f "$FIND_SPARK_HOME_PYTHON_SCRIPT" ]; then</span><br><span class="line"><span class="meta">  #</span><span class="bash"> If we are not <span class="keyword">in</span> the same directory as find_spark_home.py we are not pip installed so we don<span class="string">'t</span></span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> need to search the different Python directories <span class="keyword">for</span> a Spark installation.</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> Note only that, <span class="keyword">if</span> the user has pip installed PySpark but is directly calling pyspark-shell or</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> spark-submit <span class="keyword">in</span> another directory we want to use that version of PySpark rather than the</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> pip installed version of PySpark.</span></span><br><span class="line">  export SPARK_HOME="$(cd "$(dirname "$0")"/..; pwd)"</span><br><span class="line">else</span><br><span class="line"><span class="meta">  #</span><span class="bash"> We are pip installed, use the Python script to resolve a reasonable SPARK_HOME</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> Default to standard python interpreter unless told otherwise</span></span><br><span class="line">  if [[ -z "$PYSPARK_DRIVER_PYTHON" ]]; then</span><br><span class="line">     PYSPARK_DRIVER_PYTHON="$&#123;PYSPARK_PYTHON:-"python"&#125;"</span><br><span class="line">  fi</span><br><span class="line">  export SPARK_HOME=$($PYSPARK_DRIVER_PYTHON "$FIND_SPARK_HOME_PYTHON_SCRIPT")</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>可以看到，如果事先用户没有设定SPARK_HOME的值，这里程序也会自动设置并且将其注册为环境变量，供后面程序使用</p><p>当SPARK_HOME的值设定完成之后，就会执行Spark-class文件，这也是我们分析的重要部分，源码如下：</p><h3 id="2-3-spark-class"><a href="#2-3-spark-class" class="headerlink" title="2.3　spark-class"></a>2.3　spark-class</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/usr/bin/env bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">依旧是检查设置SPARK_HOME的值</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">执行load-spark-env.sh脚本文件，主要目的在于加载设定一些变量值</span></span><br><span class="line"><span class="meta">#</span><span class="bash">设定spark-env.sh中的变量值到环境变量中，供后续使用</span></span><br><span class="line"><span class="meta">#</span><span class="bash">设定scala版本变量值</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;"/bin/load-spark-env.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find the java binary</span></span><br><span class="line"><span class="meta">#</span><span class="bash">检查设定java环境值</span></span><br><span class="line"><span class="meta">#</span><span class="bash">-n代表检测变量长度是否为0，不为0时候为真</span></span><br><span class="line"><span class="meta">#</span><span class="bash">如果已经安装Java没有设置JAVA_HOME,<span class="built_in">command</span> -v java返回的值为<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin/java</span></span><br><span class="line">if [ -n "$&#123;JAVA_HOME&#125;" ]; then</span><br><span class="line">  RUNNER="$&#123;JAVA_HOME&#125;/bin/java"</span><br><span class="line">else</span><br><span class="line">  if [ "$(command -v java)" ]; then</span><br><span class="line">    RUNNER="java"</span><br><span class="line">  else</span><br><span class="line">    echo "JAVA_HOME is not set" &gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find Spark jars.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">-d检测文件是否为目录，若为目录则为真</span></span><br><span class="line"><span class="meta">#</span><span class="bash">设置一些关联Class文件</span></span><br><span class="line">if [ -d "$&#123;SPARK_HOME&#125;/jars" ]; then</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/jars"</span><br><span class="line">else</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ ! -d "$SPARK_JARS_DIR" ] &amp;&amp; [ -z "$SPARK_TESTING$SPARK_SQL_TESTING" ]; then</span><br><span class="line">  echo "Failed to find Spark jars directory ($SPARK_JARS_DIR)." 1&gt;&amp;2</span><br><span class="line">  echo "You need to build Spark with the target \"package\" before running this program." 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">else</span><br><span class="line">  LAUNCH_CLASSPATH="$SPARK_JARS_DIR/*"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Add the launcher build dir to the classpath <span class="keyword">if</span> requested.</span></span><br><span class="line">if [ -n "$SPARK_PREPEND_CLASSES" ]; then</span><br><span class="line">  LAUNCH_CLASSPATH="$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> For tests</span></span><br><span class="line">if [[ -n "$SPARK_TESTING" ]]; then</span><br><span class="line">  unset YARN_CONF_DIR</span><br><span class="line">  unset HADOOP_CONF_DIR</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The launcher library will <span class="built_in">print</span> arguments separated by a NULL character, to allow arguments with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> characters that would be otherwise interpreted by the shell. Read that <span class="keyword">in</span> a <span class="keyword">while</span> loop, populating</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> an array that will be used to <span class="built_in">exec</span> the final <span class="built_in">command</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The <span class="built_in">exit</span> code of the launcher is appended to the output, so the parent shell removes it from the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">command</span> array and checks the value to see <span class="keyword">if</span> the launcher succeeded.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">执行类文件org.apache.spark.launcher.Main，返回解析后的参数</span></span><br><span class="line">build_command() &#123;</span><br><span class="line">  "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"</span><br><span class="line">  printf "%d\0" $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Turn off posix mode since it does not allow process substitution</span></span><br><span class="line"><span class="meta">#</span><span class="bash">将build_command方法解析后的参数赋给CMD</span></span><br><span class="line">set +o posix</span><br><span class="line">CMD=()</span><br><span class="line">while IFS= read -d '' -r ARG; do</span><br><span class="line">  CMD+=("$ARG")</span><br><span class="line">done &lt; &lt;(build_command "$@")</span><br><span class="line"></span><br><span class="line">COUNT=$&#123;#CMD[@]&#125;</span><br><span class="line">LAST=$((COUNT - 1))</span><br><span class="line">LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Certain JVM failures result <span class="keyword">in</span> errors being printed to stdout (instead of stderr), <span class="built_in">which</span> causes</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the code that parses the output of the launcher to get confused. In those cases, check <span class="keyword">if</span> the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">exit</span> code is an <span class="built_in">integer</span>, and <span class="keyword">if</span> it<span class="string">'s not, handle it as a special error case.</span></span></span><br><span class="line">if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then</span><br><span class="line">  echo "$&#123;CMD[@]&#125;" | head -n-1 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $LAUNCHER_EXIT_CODE != 0 ]; then</span><br><span class="line">  exit $LAUNCHER_EXIT_CODE</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">CMD=("$&#123;CMD[@]:0:$LAST&#125;")</span><br><span class="line"><span class="meta">#</span><span class="bash">执行CMD中的某个参数类org.apache.spark.deploy.SparkSubmit</span></span><br><span class="line">exec "$&#123;CMD[@]&#125;"</span><br></pre></td></tr></table></figure><p>spark-class文件的执行逻辑稍显复杂，总体上应该是这样的：</p><p>检查SPARK_HOME的值—-》执行load-spark-env.sh文件，设定一些需要用到的环境变量，如scala环境值，这其中也加载了spark-env.sh文件——-》检查设定java的执行路径变量值——-》寻找spark jars,设定一些引用相关类的位置变量——》执行类文件org.apache.spark.launcher.Main，返回解析后的参数给CMD——-》判断解析参数是否正确（代表了用户设置的参数是否正确）——–》正确的话执行org.apache.spark.deploy.SparkSubmit这个类</p><h3 id="2-4-SparkSubmit"><a href="#2-4-SparkSubmit" class="headerlink" title="2.4　SparkSubmit"></a>2.4　SparkSubmit</h3><p>2.1最后提交语句，D:\src\spark-2.3.0\core\src\main\scala\org\apache\spark\deploy\SparkSubmit.scala</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exec "$&#123;SPARK_HOME&#125;"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Initialize logging if it hasn't been done yet. Keep track of whether logging needs to</span></span><br><span class="line">    <span class="comment">// be reset before the application starts.</span></span><br><span class="line">    <span class="keyword">val</span> uninitLog = initializeLogIfNecessary(<span class="literal">true</span>, silent = <span class="literal">true</span>)</span><br><span class="line">    <span class="comment">//拿到submit脚本传入的参数</span></span><br><span class="line">    <span class="keyword">val</span> appArgs = <span class="keyword">new</span> <span class="type">SparkSubmitArguments</span>(args)</span><br><span class="line">    <span class="keyword">if</span> (appArgs.verbose) &#123;</span><br><span class="line">      <span class="comment">// scalastyle:off println</span></span><br><span class="line">      printStream.println(appArgs)</span><br><span class="line">      <span class="comment">// scalastyle:on println</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//根据传入的参数匹配对应的执行方法</span></span><br><span class="line">    appArgs.action <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="comment">//根据传入的参数提交命令</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">SUBMIT</span> =&gt; submit(appArgs, uninitLog)</span><br><span class="line">        <span class="comment">//只有standalone和mesos集群模式才触发</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">KILL</span> =&gt; kill(appArgs)</span><br><span class="line">      <span class="comment">//只有standalone和mesos集群模式才触发</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">REQUEST_STATUS</span> =&gt; requestStatus(appArgs)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h4 id="2-4-1-submit十分关键，主要分为两步骤"><a href="#2-4-1-submit十分关键，主要分为两步骤" class="headerlink" title="2.4.1　submit十分关键，主要分为两步骤"></a>2.4.1　submit十分关键，主要分为两步骤</h4><p>（1）调用prepareSubmitEnvironment</p><p>（2）调用doRunMain</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201805/1228818-20180502185922203-694329056.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本</title>
    <link href="http://zhangfuxin.cn/2019-06-15-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89SparkCore%E7%9A%84%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%B8%80%EF%BC%89%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC.html"/>
    <id>http://zhangfuxin.cn/2019-06-15-Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本.html</id>
    <published>2019-06-15T02:30:04.000Z</published>
    <updated>2019-09-17T04:21:07.772Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、启动脚本分析"><a href="#一、启动脚本分析" class="headerlink" title="一、启动脚本分析"></a>一、启动脚本分析</h2><p>独立部署模式下，主要由master和slaves组成，master可以利用zk实现高可用性，其driver，work，app等信息可以持久化到zk上；slaves由一台至多台主机构成。Driver通过向Master申请资源获取运行环境。</p><p>启动master和slaves主要是执行/usr/dahua/spark/sbin目录下的start-master.sh和start-slaves.sh，或者执行</p><p>start-all.sh，其中star-all.sh本质上就是调用start-master.sh和start-slaves.sh</p><h3 id="1-1-start-all-sh"><a href="#1-1-start-all-sh" class="headerlink" title="1.1　start-all.sh"></a>1.1　start-all.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见以下分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3.执行<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/start-master.sh，见以下分析</span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin"/start-master.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4.执行<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/start-slaves.sh，见以下分析</span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin"/star`t-slaves.sh</span><br></pre></td></tr></table></figure><p>其中start-master.sh和start-slave.sh分别调用的是</p><p>org.apache.spark.deploy.master.Master和org.apache.spark.deploy.worker.Worker</p><h3 id="1-2-start-master-sh"><a href="#1-2-start-master-sh" class="headerlink" title="1.2　start-master.sh"></a>1.2　start-master.sh</h3><p>start-master.sh调用了spark-daemon.sh，注意这里指定了启动的类</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> NOTE: This exact class name is matched downstream by SparkSubmit.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Any changes need to be reflected there.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">2.设置CLASS=<span class="string">"org.apache.spark.deploy.master.Master"</span></span></span><br><span class="line">CLASS="org.apache.spark.deploy.master.Master"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3.如果参数结尾包含--<span class="built_in">help</span>或者-h则打印帮助信息，并退出</span></span><br><span class="line">if [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then</span><br><span class="line">  echo "Usage: ./sbin/start-master.sh [options]"</span><br><span class="line">  pattern="Usage:"</span><br><span class="line">  pattern+="\|Using Spark's default log4j profile:"</span><br><span class="line">  pattern+="\|Registered signal handlers for"</span><br><span class="line"></span><br><span class="line">  "$&#123;SPARK_HOME&#125;"/bin/spark-class $CLASS --help 2&gt;&amp;1 | grep -v "$pattern" 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4.设置ORIGINAL_ARGS为所有参数</span></span><br><span class="line">ORIGINAL_ARGS="$@"</span><br><span class="line"><span class="meta">#</span><span class="bash">5.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">6.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">7.SPARK_MASTER_PORT为空则赋值7077</span></span><br><span class="line">if [ "$SPARK_MASTER_PORT" = "" ]; then</span><br><span class="line">  SPARK_MASTER_PORT=7077</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">8.SPARK_MASTER_HOST为空则赋值本主机名(hostname)</span></span><br><span class="line">if [ "$SPARK_MASTER_HOST" = "" ]; then</span><br><span class="line">  case `uname` in</span><br><span class="line">      (SunOS)</span><br><span class="line">      SPARK_MASTER_HOST="`/usr/sbin/check-hostname | awk '&#123;print $NF&#125;'`"</span><br><span class="line">      ;;</span><br><span class="line">      (*)</span><br><span class="line">      SPARK_MASTER_HOST="`hostname -f`"</span><br><span class="line">      ;;</span><br><span class="line">  esac</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">9.SPARK_MASTER_WEBUI_PORT为空则赋值8080</span></span><br><span class="line">if [ "$SPARK_MASTER_WEBUI_PORT" = "" ]; then</span><br><span class="line">  SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">10.执行脚本</span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start $CLASS 1 \</span><br><span class="line">  --host $SPARK_MASTER_HOST --port $SPARK_MASTER_PORT --webui-port $SPARK_MASTER_WEBUI_PORT \</span><br><span class="line"><span class="meta">  $</span><span class="bash">ORIGINAL_ARGS</span></span><br></pre></td></tr></table></figure><p>其中10肯定是重点，分析之前我们看看5，6都干了些啥，最后直译出最后一个脚本</p><h3 id="1-3-spark-config-sh-1-2的第5步"><a href="#1-3-spark-config-sh-1-2的第5步" class="headerlink" title="1.3　spark-config.sh(1.2的第5步)"></a>1.3　spark-config.sh(1.2的第5步)</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">SPARK_CONF_DIR存在就用此目录，不存在用<span class="variable">$&#123;SPARK_HOME&#125;</span>/conf</span></span><br><span class="line">export SPARK_CONF_DIR="$&#123;SPARK_CONF_DIR:-"$&#123;SPARK_HOME&#125;/conf"&#125;"</span><br><span class="line"><span class="meta">#</span><span class="bash"> Add the PySpark classes to the PYTHONPATH:</span></span><br><span class="line">if [ -z "$&#123;PYSPARK_PYTHONPATH_SET&#125;" ]; then</span><br><span class="line">  export PYTHONPATH="$&#123;SPARK_HOME&#125;/python:$&#123;PYTHONPATH&#125;"</span><br><span class="line">  export PYTHONPATH="$&#123;SPARK_HOME&#125;/python/lib/py4j-0.10.6-src.zip:$&#123;PYTHONPATH&#125;"</span><br><span class="line">  export PYSPARK_PYTHONPATH_SET=1</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="1-4-load-spark-env-sh-1-2的第6步"><a href="#1-4-load-spark-env-sh-1-2的第6步" class="headerlink" title="1.4　load-spark-env.sh(1.2的第6步)"></a>1.4　load-spark-env.sh(1.2的第6步)</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">2.判断SPARK_ENV_LOADED是否有值，没有将其设置为1</span></span><br><span class="line">if [ -z "$SPARK_ENV_LOADED" ]; then</span><br><span class="line">  export SPARK_ENV_LOADED=1</span><br><span class="line"><span class="meta">#</span><span class="bash">3.设置user_conf_dir为SPARK_CONF_DIR或SPARK_HOME/conf</span></span><br><span class="line">  export SPARK_CONF_DIR="$&#123;SPARK_CONF_DIR:-"$&#123;SPARK_HOME&#125;"/conf&#125;"</span><br><span class="line"><span class="meta">#</span><span class="bash">4.执行<span class="string">"<span class="variable">$&#123;user_conf_dir&#125;</span>/spark-env.sh"</span> [注：<span class="built_in">set</span> -/+a含义再做研究]</span></span><br><span class="line">  if [ -f "$&#123;SPARK_CONF_DIR&#125;/spark-env.sh" ]; then</span><br><span class="line">    # Promote all variable declarations to environment (exported) variables</span><br><span class="line">    set -a</span><br><span class="line">    . "$&#123;SPARK_CONF_DIR&#125;/spark-env.sh"</span><br><span class="line">    set +a</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Setting SPARK_SCALA_VERSION <span class="keyword">if</span> not already <span class="built_in">set</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">5.选择scala版本，2.11和2.12都存在的情况下，优先选择2.11</span></span><br><span class="line">if [ -z "$SPARK_SCALA_VERSION" ]; then</span><br><span class="line"></span><br><span class="line">  ASSEMBLY_DIR2="$&#123;SPARK_HOME&#125;/assembly/target/scala-2.11"</span><br><span class="line">  ASSEMBLY_DIR1="$&#123;SPARK_HOME&#125;/assembly/target/scala-2.12"</span><br><span class="line"></span><br><span class="line">  if [[ -d "$ASSEMBLY_DIR2" &amp;&amp; -d "$ASSEMBLY_DIR1" ]]; then</span><br><span class="line">    echo -e "Presence of build for multiple Scala versions detected." 1&gt;&amp;2</span><br><span class="line">    echo -e 'Either clean one of them or, export SPARK_SCALA_VERSION in spark-env.sh.' 1&gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line"></span><br><span class="line">  if [ -d "$ASSEMBLY_DIR2" ]; then</span><br><span class="line">    export SPARK_SCALA_VERSION="2.11"</span><br><span class="line">  else</span><br><span class="line">    export SPARK_SCALA_VERSION="2.12"</span><br><span class="line">  fi</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="1-5-spark-env-sh"><a href="#1-5-spark-env-sh" class="headerlink" title="1.5　spark-env.sh"></a>1.5　spark-env.sh</h3><p>列举很多种模式的选项配置</p><h3 id="1-6-spark-daemon-sh"><a href="#1-6-spark-daemon-sh" class="headerlink" title="1.6　spark-daemon.sh"></a>1.6　spark-daemon.sh</h3><p>回过头来看看<strong>1.2第10步</strong>中需要直译出的最后一个脚本,如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/spark-daemon.sh start org.apache.spark.deploy.master.Master 1 --host hostname --port 7077 --webui-port 8080</span><br></pre></td></tr></table></figure><p>上面搞了半天只是设置了变量，最终才进入主角，继续分析spark-daemon.sh脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.参数个数小于等于1，打印帮助</span></span><br><span class="line">if [ $# -le 1 ]; then</span><br><span class="line">  echo $usage</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">2.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">3.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见上述分析 [类似脚本是否有重复？原因是有的人是直接用spark-daemon.sh启动的服务，反正重复设置下变量不需要什么代价]</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> get arguments</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Check <span class="keyword">if</span> --config is passed as an argument. It is an optional parameter.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Exit <span class="keyword">if</span> the argument is not a directory.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4.判断第一个参数是否是--config,如果是取空格后一个字符串，然后判断该目录是否存在，不存在则打印错误信息并退出，存在设置SPARK_CONF_DIR为该目录,<span class="built_in">shift</span>到下一个参数<span class="comment">#[注：--config只能用在第一参数上]</span></span></span><br><span class="line">if [ "$1" == "--config" ]</span><br><span class="line">then</span><br><span class="line">  shift</span><br><span class="line">  conf_dir="$1"</span><br><span class="line">  if [ ! -d "$conf_dir" ]</span><br><span class="line">  then</span><br><span class="line">    echo "ERROR : $conf_dir is not a directory"</span><br><span class="line">    echo $usage</span><br><span class="line">    exit 1</span><br><span class="line">  else</span><br><span class="line">    export SPARK_CONF_DIR="$conf_dir"</span><br><span class="line">  fi</span><br><span class="line">  shift</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">5.分别设置option、<span class="built_in">command</span>、instance为后面的三个参数(如：option=start,<span class="built_in">command</span>=org.apache.spark.deploy.master.Master,instance=1)<span class="comment">#[注：很多人用spark-daemon.sh启动服务不成功的原因是名字不全]</span></span></span><br><span class="line">option=$1</span><br><span class="line">shift</span><br><span class="line">command=$1</span><br><span class="line">shift</span><br><span class="line">instance=$1</span><br><span class="line">shift</span><br><span class="line"><span class="meta">#</span><span class="bash">6.日志回滚函数，主要用于更改日志名，如<span class="built_in">log</span>--&gt;log.1等，略过</span></span><br><span class="line">spark_rotate_log ()</span><br><span class="line">&#123;</span><br><span class="line">    log=$1;</span><br><span class="line">    num=5;</span><br><span class="line">    if [ -n "$2" ]; then</span><br><span class="line">    num=$2</span><br><span class="line">    fi</span><br><span class="line">    if [ -f "$log" ]; then # rotate logs</span><br><span class="line">    while [ $num -gt 1 ]; do</span><br><span class="line">        prev=`expr $num - 1`</span><br><span class="line">        [ -f "$log.$prev" ] &amp;&amp; mv "$log.$prev" "$log.$num"</span><br><span class="line">        num=$prev</span><br><span class="line">    done</span><br><span class="line">    mv "$log" "$log.$num";</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">7.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">8.判断SPARK_IDENT_STRING是否有值，没有将其设置为<span class="variable">$USER</span>(linux用户)</span></span><br><span class="line">if [ "$SPARK_IDENT_STRING" = "" ]; then</span><br><span class="line">  export SPARK_IDENT_STRING="$USER"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">9.设置SPARK_PRINT_LAUNCH_COMMAND=1</span></span><br><span class="line">export SPARK_PRINT_LAUNCH_COMMAND="1"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> get <span class="built_in">log</span> directory</span></span><br><span class="line"><span class="meta">#</span><span class="bash">10.判断SPARK_LOG_DIR是否有值，没有将其设置为<span class="variable">$&#123;SPARK_HOME&#125;</span>/logs，并创建改目录，测试创建文件，修改权限</span></span><br><span class="line">if [ "$SPARK_LOG_DIR" = "" ]; then</span><br><span class="line">  export SPARK_LOG_DIR="$&#123;SPARK_HOME&#125;/logs"</span><br><span class="line">fi</span><br><span class="line">mkdir -p "$SPARK_LOG_DIR"</span><br><span class="line">touch "$SPARK_LOG_DIR"/.spark_test &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">TEST_LOG_DIR=$?</span><br><span class="line">if [ "$&#123;TEST_LOG_DIR&#125;" = "0" ]; then</span><br><span class="line">  rm -f "$SPARK_LOG_DIR"/.spark_test</span><br><span class="line">else</span><br><span class="line">  chown "$SPARK_IDENT_STRING" "$SPARK_LOG_DIR"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">11.判断SPARK_PID_DIR是否有值，没有将其设置为/tmp</span></span><br><span class="line">if [ "$SPARK_PID_DIR" = "" ]; then</span><br><span class="line">  SPARK_PID_DIR=/tmp</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> some variables</span></span><br><span class="line"><span class="meta">#</span><span class="bash">12.设置<span class="built_in">log</span>和pid</span></span><br><span class="line">log="$SPARK_LOG_DIR/spark-$SPARK_IDENT_STRING-$command-$instance-$HOSTNAME.out"</span><br><span class="line">pid="$SPARK_PID_DIR/spark-$SPARK_IDENT_STRING-$command-$instance.pid"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Set default scheduling priority</span></span><br><span class="line"><span class="meta">#</span><span class="bash">13.判断SPARK_NICENESS是否有值，没有将其设置为0 [注：调度优先级，见后面]</span></span><br><span class="line">if [ "$SPARK_NICENESS" = "" ]; then</span><br><span class="line">    export SPARK_NICENESS=0</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">14.execute_command()函数，暂且略过，调用时再作分析</span></span><br><span class="line">execute_command() &#123;</span><br><span class="line">  if [ -z $&#123;SPARK_NO_DAEMONIZE+set&#125; ]; then</span><br><span class="line">      nohup -- "$@" &gt;&gt; $log 2&gt;&amp;1 &lt; /dev/null &amp;</span><br><span class="line">      newpid="$!"</span><br><span class="line"></span><br><span class="line">      echo "$newpid" &gt; "$pid"</span><br><span class="line"></span><br><span class="line">      # Poll for up to 5 seconds for the java process to start</span><br><span class="line">      for i in &#123;1..10&#125;</span><br><span class="line">      do</span><br><span class="line">        if [[ $(ps -p "$newpid" -o comm=) =~ "java" ]]; then</span><br><span class="line">           break</span><br><span class="line">        fi</span><br><span class="line">        sleep 0.5</span><br><span class="line">      done</span><br><span class="line"></span><br><span class="line">      sleep 2</span><br><span class="line">      # Check if the process has died; in that case we'll tail the log so the user can see</span><br><span class="line">      if [[ ! $(ps -p "$newpid" -o comm=) =~ "java" ]]; then</span><br><span class="line">        echo "failed to launch: $@"</span><br><span class="line">        tail -10 "$log" | sed 's/^/  /'</span><br><span class="line">        echo "full log in $log"</span><br><span class="line">      fi</span><br><span class="line">  else</span><br><span class="line">      "$@"</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">15.进入<span class="keyword">case</span>语句，判断option值，进入该分支，我们以start为例</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   执行run_command class <span class="string">"<span class="variable">$@</span>"</span>，其中<span class="variable">$@</span>此时为空，经验证，启动带上此参数后，关闭也需，不然关闭不了，后面再分析此参数作用</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   我们正式进入run_command()函数，分析</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   I.设置mode=class,创建SPARK_PID_DIR，上面的pid文件是否存在，</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   II.SPARK_MASTER不为空，同步删除某些文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   III.回滚<span class="built_in">log</span>日志</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   IV.进入<span class="keyword">case</span>，<span class="built_in">command</span>=org.apache.spark.deploy.master.Master，最终执行</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       nohup nice -n <span class="string">"<span class="variable">$SPARK_NICENESS</span>"</span> <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/spark-class <span class="variable">$command</span> <span class="string">"<span class="variable">$@</span>"</span> &gt;&gt; <span class="string">"<span class="variable">$log</span>"</span> 2&gt;&amp;1 &lt; /dev/null &amp;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       newpid=<span class="string">"$!"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">       <span class="built_in">echo</span> <span class="string">"<span class="variable">$newpid</span>"</span> &gt; <span class="string">"<span class="variable">$pid</span>"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">   重点转向bin/spark-class org.apache.spark.deploy.master.Master</span></span><br><span class="line">run_command() &#123;</span><br><span class="line">  mode="$1"</span><br><span class="line">  shift</span><br><span class="line"></span><br><span class="line">  mkdir -p "$SPARK_PID_DIR"</span><br><span class="line"></span><br><span class="line">  if [ -f "$pid" ]; then</span><br><span class="line">    TARGET_ID="$(cat "$pid")"</span><br><span class="line">    if [[ $(ps -p "$TARGET_ID" -o comm=) =~ "java" ]]; then</span><br><span class="line">      echo "$command running as process $TARGET_ID.  Stop it first."</span><br><span class="line">      exit 1</span><br><span class="line">    fi</span><br><span class="line">  fi</span><br><span class="line"></span><br><span class="line">  if [ "$SPARK_MASTER" != "" ]; then</span><br><span class="line">    echo rsync from "$SPARK_MASTER"</span><br><span class="line">    rsync -a -e ssh --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' "$SPARK_MASTER/" "$&#123;SPARK_HOME&#125;"</span><br><span class="line">  fi</span><br><span class="line"></span><br><span class="line">  spark_rotate_log "$log"</span><br><span class="line">  echo "starting $command, logging to $log"</span><br><span class="line"></span><br><span class="line">  case "$mode" in</span><br><span class="line">    (class)</span><br><span class="line">      execute_command nice -n "$SPARK_NICENESS" "$&#123;SPARK_HOME&#125;"/bin/spark-class "$command" "$@"</span><br><span class="line">      ;;</span><br><span class="line"></span><br><span class="line">    (submit)</span><br><span class="line">      execute_command nice -n "$SPARK_NICENESS" bash "$&#123;SPARK_HOME&#125;"/bin/spark-submit --class "$command" "$@"</span><br><span class="line">      ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">      echo "unknown mode: $mode"</span><br><span class="line">      exit 1</span><br><span class="line">      ;;</span><br><span class="line">  esac</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $option in</span><br><span class="line"></span><br><span class="line">  (submit)</span><br><span class="line">    run_command submit "$@"</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">  (start)</span><br><span class="line">    run_command class "$@"</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">  (stop)</span><br><span class="line"></span><br><span class="line">    if [ -f $pid ]; then</span><br><span class="line">      TARGET_ID="$(cat "$pid")"</span><br><span class="line">      if [[ $(ps -p "$TARGET_ID" -o comm=) =~ "java" ]]; then</span><br><span class="line">        echo "stopping $command"</span><br><span class="line">        kill "$TARGET_ID" &amp;&amp; rm -f "$pid"</span><br><span class="line">      else</span><br><span class="line">        echo "no $command to stop"</span><br><span class="line">      fi</span><br><span class="line">    else</span><br><span class="line">      echo "no $command to stop"</span><br><span class="line">    fi</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">  (status)</span><br><span class="line"></span><br><span class="line">    if [ -f $pid ]; then</span><br><span class="line">      TARGET_ID="$(cat "$pid")"</span><br><span class="line">      if [[ $(ps -p "$TARGET_ID" -o comm=) =~ "java" ]]; then</span><br><span class="line">        echo $command is running.</span><br><span class="line">        exit 0</span><br><span class="line">      else</span><br><span class="line">        echo $pid file is present but $command not running</span><br><span class="line">        exit 1</span><br><span class="line">      fi</span><br><span class="line">    else</span><br><span class="line">      echo $command not running.</span><br><span class="line">      exit 2</span><br><span class="line">    fi</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">  (*)</span><br><span class="line">    echo $usage</span><br><span class="line">    exit 1</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h3 id="1-7-spark-class"><a href="#1-7-spark-class" class="headerlink" title="1.7　spark-class"></a>1.7　spark-class</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;"/bin/load-spark-env.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find the java binary</span></span><br><span class="line"><span class="meta">#</span><span class="bash">3.判断JAVA_HOME是否为NULL，不是则设置RUNNER=<span class="string">"<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin/java"</span>，否则找系统自带，在没有则报未设置，并退出</span></span><br><span class="line">if [ -n "$&#123;JAVA_HOME&#125;" ]; then</span><br><span class="line">  RUNNER="$&#123;JAVA_HOME&#125;/bin/java"</span><br><span class="line">else</span><br><span class="line">  if [ "$(command -v java)" ]; then</span><br><span class="line">    RUNNER="java"</span><br><span class="line">  else</span><br><span class="line">    echo "JAVA_HOME is not set" &gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find Spark jars.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">4.查找SPARK_JARS_DIR，若<span class="variable">$&#123;SPARK_HOME&#125;</span>/RELEASE文件存在，则SPARK_JARS_DIR=<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/jars"</span>，否则</span></span><br><span class="line"><span class="meta">#</span><span class="bash">SPARK_JARS_DIR=<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/assembly/target/scala-<span class="variable">$SPARK_SCALA_VERSION</span>/jars"</span></span></span><br><span class="line">if [ -d "$&#123;SPARK_HOME&#125;/jars" ]; then</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/jars"</span><br><span class="line">else</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5.若SPARK_JARS_DIR不存在且<span class="variable">$SPARK_TESTING</span><span class="variable">$SPARK_SQL_TESTING</span>有值[注：一般我们不设置这两变量]，报错退出，否则LAUNCH_CLASSPATH=<span class="string">"<span class="variable">$SPARK_JARS_DIR</span>/*"</span></span></span><br><span class="line">if [ ! -d "$SPARK_JARS_DIR" ] &amp;&amp; [ -z "$SPARK_TESTING$SPARK_SQL_TESTING" ]; then</span><br><span class="line">  echo "Failed to find Spark jars directory ($SPARK_JARS_DIR)." 1&gt;&amp;2</span><br><span class="line">  echo "You need to build Spark with the target \"package\" before running this program." 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">else</span><br><span class="line">  LAUNCH_CLASSPATH="$SPARK_JARS_DIR/*"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Add the launcher build dir to the classpath <span class="keyword">if</span> requested.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">6.SPARK_PREPEND_CLASSES不是NULL，则LAUNCH_CLASSPATH=<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/launcher/target/scala-<span class="variable">$SPARK_SCALA_VERSION</span>/classes:<span class="variable">$LAUNCH_CLASSPATH</span>"</span>，<span class="comment">#添加编译相关至LAUNCH_CLASSPATH</span></span></span><br><span class="line">if [ -n "$SPARK_PREPEND_CLASSES" ]; then</span><br><span class="line">  LAUNCH_CLASSPATH="$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> For tests</span></span><br><span class="line"><span class="meta">#</span><span class="bash">7.SPARK_TESTING不是NULL，则<span class="built_in">unset</span> YARN_CONF_DIR和<span class="built_in">unset</span> HADOOP_CONF_DIR，暂且当做是为了某种测试</span></span><br><span class="line">if [[ -n "$SPARK_TESTING" ]]; then</span><br><span class="line">  unset YARN_CONF_DIR</span><br><span class="line">  unset HADOOP_CONF_DIR</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">8.build_command函数，略过</span></span><br><span class="line">build_command() &#123;</span><br><span class="line">  "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"</span><br><span class="line">  printf "%d\0" $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Turn off posix mode since it does not allow process substitution</span></span><br><span class="line">set +o posix</span><br><span class="line">CMD=()</span><br><span class="line">while IFS= read -d '' -r ARG; do</span><br><span class="line">  CMD+=("$ARG")</span><br><span class="line"><span class="meta">  #</span><span class="bash">9.最终调用<span class="string">"<span class="variable">$RUNNER</span>"</span> -Xmx128m -cp <span class="string">"<span class="variable">$LAUNCH_CLASSPATH</span>"</span> org.apache.spark.launcher.Main <span class="string">"<span class="variable">$@</span>"</span>，</span></span><br><span class="line"><span class="meta">  #</span><span class="bash">直译：java -Xmx128m -cp <span class="string">"<span class="variable">$LAUNCH_CLASSPATH</span>"</span> org.apache.spark.launcher.Main <span class="string">"<span class="variable">$@</span>"</span></span></span><br><span class="line"><span class="meta">  #</span><span class="bash">转向java类org.apache.spark.launcher.Main，这就是java入口类</span></span><br><span class="line">done &lt; &lt;(build_command "$@")</span><br><span class="line"></span><br><span class="line">COUNT=$&#123;#CMD[@]&#125;</span><br><span class="line">LAST=$((COUNT - 1))</span><br><span class="line">LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Certain JVM failures result <span class="keyword">in</span> errors being printed to stdout (instead of stderr), <span class="built_in">which</span> causes</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the code that parses the output of the launcher to get confused. In those cases, check <span class="keyword">if</span> the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">exit</span> code is an <span class="built_in">integer</span>, and <span class="keyword">if</span> it<span class="string">'s not, handle it as a special error case.</span></span></span><br><span class="line">if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then</span><br><span class="line">  echo "$&#123;CMD[@]&#125;" | head -n-1 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $LAUNCHER_EXIT_CODE != 0 ]; then</span><br><span class="line">  exit $LAUNCHER_EXIT_CODE</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">CMD=("$&#123;CMD[@]:0:$LAST&#125;")</span><br><span class="line">exec "$&#123;CMD[@]&#125;"</span><br></pre></td></tr></table></figure><h3 id="1-8-start-slaves-sh"><a href="#1-8-start-slaves-sh" class="headerlink" title="1.8　start-slaves.sh"></a>1.8　start-slaves.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find the port number <span class="keyword">for</span> the master</span></span><br><span class="line"><span class="meta">#</span><span class="bash">4.SPARK_MASTER_PORT为空则设置为7077</span></span><br><span class="line">if [ "$SPARK_MASTER_PORT" = "" ]; then</span><br><span class="line">  SPARK_MASTER_PORT=7077</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5.SPARK_MASTER_HOST为空则设置为`hostname`</span></span><br><span class="line">if [ "$SPARK_MASTER_HOST" = "" ]; then</span><br><span class="line">  case `uname` in</span><br><span class="line">      (SunOS)</span><br><span class="line">      SPARK_MASTER_HOST="`/usr/sbin/check-hostname | awk '&#123;print $NF&#125;'`"</span><br><span class="line">      ;;</span><br><span class="line">      (*)</span><br><span class="line">      SPARK_MASTER_HOST="`hostname -f`"</span><br><span class="line">      ;;</span><br><span class="line">  esac</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Launch the slaves</span></span><br><span class="line"><span class="meta">#</span><span class="bash">6.启动slaves，</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/slaves.sh"</span> <span class="built_in">cd</span> <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span> \; <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/start-slave.sh"</span> <span class="string">"spark://<span class="variable">$SPARK_MASTER_HOST</span>:<span class="variable">$SPARK_MASTER_PORT</span>"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">   遍历conf/slaves中主机，其中有设置SPARK_SSH_OPTS，ssh每一台机器执行<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/start-slave.sh"</span> <span class="string">"spark://<span class="variable">$SPARK_MASTER_HOST</span>:<span class="variable">$SPARK_MASTER_PORT</span>"</span></span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin/slaves.sh" cd "$&#123;SPARK_HOME&#125;" \; "$&#123;SPARK_HOME&#125;/sbin/start-slave.sh" "spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT"</span><br></pre></td></tr></table></figure><h3 id="1-9-转向start-slave-sh"><a href="#1-9-转向start-slave-sh" class="headerlink" title="1.9　转向start-slave.sh"></a>1.9　转向start-slave.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.设置CLASS=<span class="string">"org.apache.spark.deploy.worker.Worker"</span></span></span><br><span class="line">CLASS="org.apache.spark.deploy.worker.Worker"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3.如果参数结尾包含--<span class="built_in">help</span>或者-h则打印帮助信息，并退出</span></span><br><span class="line">if [[ $# -lt 1 ]] || [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then</span><br><span class="line">  echo "Usage: ./sbin/start-slave.sh [options] &lt;master&gt;"</span><br><span class="line">  pattern="Usage:"</span><br><span class="line">  pattern+="\|Using Spark's default log4j profile:"</span><br><span class="line">  pattern+="\|Registered signal handlers for"</span><br><span class="line"></span><br><span class="line">  "$&#123;SPARK_HOME&#125;"/bin/spark-class $CLASS --help 2&gt;&amp;1 | grep -v "$pattern" 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">5.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">6.MASTER=<span class="variable">$1</span>,这里MASTER=spark://hostname:7077，然后<span class="built_in">shift</span>，也就是说单独启动单个slave使用start-slave.sh spark://hostname:7077</span></span><br><span class="line">MASTER=$1</span><br><span class="line">shift</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">7.SPARK_WORKER_WEBUI_PORT为空则设置为8081</span></span><br><span class="line">if [ "$SPARK_WORKER_WEBUI_PORT" = "" ]; then</span><br><span class="line">  SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">8.函数start_instance，略过</span></span><br><span class="line">function start_instance &#123;</span><br><span class="line"><span class="meta">#</span><span class="bash">设置WORKER_NUM=<span class="variable">$1</span></span></span><br><span class="line">  WORKER_NUM=$1</span><br><span class="line">  shift</span><br><span class="line"></span><br><span class="line">  if [ "$SPARK_WORKER_PORT" = "" ]; then</span><br><span class="line">    PORT_FLAG=</span><br><span class="line">    PORT_NUM=</span><br><span class="line">  else</span><br><span class="line">    PORT_FLAG="--port"</span><br><span class="line">    PORT_NUM=$(( $SPARK_WORKER_PORT + $WORKER_NUM - 1 ))</span><br><span class="line">  fi</span><br><span class="line">  WEBUI_PORT=$(( $SPARK_WORKER_WEBUI_PORT + $WORKER_NUM - 1 ))</span><br><span class="line"></span><br><span class="line"><span class="meta">  #</span><span class="bash">直译：spark-daemon.sh start org.apache.spark.deploy.worker.Worker 1 --webui-port 7077 spark://hostname:7077</span></span><br><span class="line"><span class="meta">  #</span><span class="bash">代码再次转向spark-daemon.sh，见上诉分析</span></span><br><span class="line">  "$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start $CLASS $WORKER_NUM \</span><br><span class="line">     --webui-port "$WEBUI_PORT" $PORT_FLAG $PORT_NUM $MASTER "$@"</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">9.判断SPARK_WORKER_INSTANCES(可以认为是单节点Worker进程数)是否为空</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   为空，则start_instance 1 <span class="string">"<span class="variable">$@</span>"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">   不为空，则循环</span></span><br><span class="line"><span class="meta">#</span><span class="bash">         <span class="keyword">for</span> ((i=0; i&lt;<span class="variable">$SPARK_WORKER_INSTANCES</span>; i++)); <span class="keyword">do</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">           start_instance $(( 1 + <span class="variable">$i</span> )) <span class="string">"<span class="variable">$@</span>"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">         <span class="keyword">done</span></span></span><br><span class="line">if [ "$SPARK_WORKER_INSTANCES" = "" ]; then</span><br><span class="line">  start_instance 1 "$@"</span><br><span class="line">else</span><br><span class="line">  for ((i=0; i&lt;$SPARK_WORKER_INSTANCES; i++)); do</span><br><span class="line"><span class="meta">  #</span><span class="bash">10.转向start_instance函数</span></span><br><span class="line">    start_instance $(( 1 + $i )) "$@"</span><br><span class="line">  done</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h2 id="二、其他脚本"><a href="#二、其他脚本" class="headerlink" title="二、其他脚本"></a>二、其他脚本</h2><h3 id="2-1-start-history-server-sh"><a href="#2-1-start-history-server-sh" class="headerlink" title="2.1　start-history-server.sh"></a>2.1　start-history-server.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">3.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">4.exec <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/spark-daemon.sh start org.apache.spark.deploy.history.HistoryServer 1 <span class="variable">$@</span> ，见上诉分析</span></span><br><span class="line">exec "$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start org.apache.spark.deploy.history.HistoryServer 1 "$@"</span><br></pre></td></tr></table></figure><h3 id="2-2-start-shuffle-service-sh"><a href="#2-2-start-shuffle-service-sh" class="headerlink" title="2.2　start-shuffle-service.sh"></a>2.2　start-shuffle-service.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">3.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">4.exec <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/spark-daemon.sh start org.apache.spark.deploy.ExternalShuffleService 1 ，见上诉分析</span></span><br><span class="line">exec "$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start org.apache.spark.deploy.ExternalShuffleService 1</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器</title>
    <link href="http://zhangfuxin.cn/2019-06-14-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98JVM%E7%9A%84GC%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8.html"/>
    <id>http://zhangfuxin.cn/2019-06-14-Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器.html</id>
    <published>2019-06-14T02:30:04.000Z</published>
    <updated>2019-09-17T02:15:20.277Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        垃圾收集 Garbage Collection 通常被称为“GC”，它诞生于1960年 MIT 的 Lisp 语言，经过半个多世纪，目前已经十分成熟了。</p><p>jvm 中，程序计数器、虚拟机栈、本地方法栈都是随线程而生随线程而灭，栈帧随着方法的进入和退出做入栈和出栈操作，实现了自动的内存清理，因此，我们的<strong>内存垃圾回收主要集中于 java 堆和方法区中</strong>，在程序运行期间，这部分内存的分配和使用都是动态的。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、垃圾收集器-garbage-collector-GC-是什么？"><a href="#一、垃圾收集器-garbage-collector-GC-是什么？" class="headerlink" title="一、垃圾收集器(garbage collector (GC)) 是什么？"></a>一、<strong>垃圾收集器(garbage collector (GC)) 是什么？</strong></h2><p>GC其实是一种自动的内存管理工具，其行为主要包括2步</p><ul><li>在Java堆中，为新<strong>创建</strong>的对象分配空间</li><li>在Java堆中，<strong>回收</strong>没用的对象占用的空间</li></ul><h2 id="二、为什么需要GC？"><a href="#二、为什么需要GC？" class="headerlink" title="二、为什么需要GC？**"></a>二、为什么需要GC？**</h2><p>释放开发人员的生产力</p><h2 id="三、为什么需要多种GC？"><a href="#三、为什么需要多种GC？" class="headerlink" title="三、为什么需要多种GC？**"></a>三、为什么需要多种GC？**</h2><p>首先，Java平台被部署在各种各样的硬件资源上，其次，在Java平台上部署和运行着各种各样的应用，并且用户对不同的应用的 <em>性能指标</em> (吞吐率和延迟) 预期也不同，为了满足不同应用的对内存管理的不同需求，JVM提供了多种GC以供选择</p><p><em>性能指标</em><br>最大停顿时长：垃圾回收导致的应用停顿时间的最大值<br>吞吐率：垃圾回收停顿时长和应用运行总时长的比例</p><p>不同的GC能满足不同应用不同的性能需求，现有的GC包括：</p><ul><li><ul><li>序列化GC(serial garbage collector)：适合占用内存少的应用</li><li>并行GC 或 吞吐率GC(parallel or throughput garbage collector)：适合占用内存较多，多CPU，追求高吞吐率的应用</li><li>并发GC：适合占用内存较多，多CPU的应用，对延迟有要求的应用</li></ul></li></ul><h2 id="四、对象存活的判断"><a href="#四、对象存活的判断" class="headerlink" title="四、对象存活的判断"></a>四、对象存活的判断</h2><p>判断对象是否存活一般有两种方式：</p><p><strong>引用计数</strong>：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，缺点是无法解决对象相互循环引用的问题。</p><p><strong>可达性分析（Reachability Analysis）</strong>：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。不可达对象。</p><blockquote><p>在Java语言中，GC Roots包括：</p><p>  虚拟机栈中引用的对象。</p><p>  方法区中类静态属性实体引用的对象。</p><p>  方法区中常量引用的对象。</p><p>  本地方法栈中JNI引用的对象。</p></blockquote><p>由于循环引用的问题，一般采用跟踪（<strong>可达性分析</strong>）方法</p><h2 id="五、垃圾回收算法"><a href="#五、垃圾回收算法" class="headerlink" title="五、垃圾回收算法"></a>五、垃圾回收算法</h2><h3 id="5-1-标记-清除算法"><a href="#5-1-标记-清除算法" class="headerlink" title="5.1　标记 -清除算法"></a>5.1　标记 -清除算法</h3><p>“标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。</p><p>它的主要缺点有两个：一个是效率问题，标记和清除过程的效率都不高；另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430153744337-319263441.png" alt="img"></p><h3 id="5-2-复制算法"><a href="#5-2-复制算法" class="headerlink" title="5.2　复制算法"></a>5.2　复制算法</h3><p>“复制”（Copying）的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。</p><p>这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，持续复制长生存期的对象则导致效率降低。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430153826268-1037960889.png" alt="img"></p><h3 id="5-3-标记-整理算法"><a href="#5-3-标记-整理算法" class="headerlink" title="5.3　标记-整理算法"></a>5.3　标记-整理算法</h3><p>复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。</p><p>根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430153949072-653785951.png" alt="img"></p><h3 id="5-4-分代收集算法"><a href="#5-4-分代收集算法" class="headerlink" title="5.4　分代收集算法"></a>5.4　分代收集算法</h3><p>GC分代的基本假设：绝大部分对象的生命周期都非常短暂，存活时间短。</p><p>“分代收集”（Generational Collection）算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或“标记-整理”算法来进行回收。</p><h2 id="六、垃圾收集器"><a href="#六、垃圾收集器" class="headerlink" title="六、垃圾收集器"></a>六、垃圾收集器</h2><p>如果说收集算法是内存回收的方法论，垃圾收集器就是内存回收的具体实现，不同厂商、不同版本的虚拟机实现差别很大，HotSpot中包含的收集器如下：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430154441316-1293727491.png" alt="img"></p><h3 id="6-1-Serial收集器"><a href="#6-1-Serial收集器" class="headerlink" title="6.1　Serial收集器"></a>6.1　Serial收集器</h3><p><strong>串行收集器</strong>是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。新生代、老年代使用串行回收；新生代复制算法、老年代标记-压缩；垃圾收集的过程中会Stop The World（服务暂停）</p><p>参数控制：-XX:+UseSerialGC  串行收集器</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430154734737-1197475351.png" alt="img"></p><h3 id="6-2-ParNew收集器"><a href="#6-2-ParNew收集器" class="headerlink" title="6.2　ParNew收集器"></a>6.2　ParNew收集器</h3><p>ParNew收集器其实就是Serial收集器的多线程版本。新生代并行，老年代串行；新生代复制算法、老年代标记-压缩</p><p>参数控制：-XX:+UseParNewGC  ParNew收集器</p><p>-XX:ParallelGCThreads 限制线程数量</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430155050197-460365340.png" alt="img"></p><h3 id="6-3-Parallel收集器"><a href="#6-3-Parallel收集器" class="headerlink" title="6.3　Parallel收集器"></a>6.3　Parallel收集器</h3><p>Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。可以通过参数来打开自适应调节策略，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或最大的吞吐量；也可以通过参数控制GC的时间不大于多少毫秒或者比例；新生代复制算法、老年代标记-压缩</p><p>参数控制：-XX:+UseParallelGC  使用Parallel收集器+ 老年代串行</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430155431788-84955319.png" alt="img"></p><h3 id="6-4-CMS收集器"><a href="#6-4-CMS收集器" class="headerlink" title="6.4　CMS收集器"></a>6.4　CMS收集器</h3><p>CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用都集中在互联网站或B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。</p><p>从名字（包含“Mark Sweep”）上就可以看出CMS收集器是基于“标记-清除”算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为4个步骤，包括： </p><p>初始标记（CMS initial mark）</p><p>并发标记（CMS concurrent mark）</p><p>重新标记（CMS remark）</p><p>并发清除（CMS concurrent sweep）</p><p> 其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。<br>      由于整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，所以总体上来说，CMS收集器的内存回收过程是与用户线程一起并发地执行。老年代收集器（新生代使用ParNew）</p><p>  优点:并发收集、低停顿 </p><p>   缺点：产生大量空间碎片、并发阶段会降低吞吐量</p><p>   参数控制：-XX:+UseConcMarkSweepGC  使用CMS收集器</p><p>​             -XX:+ UseCMSCompactAtFullCollection Full GC后，进行一次碎片整理；整理过程是独占的，会引起停顿时间变长</p><p>​            -XX:+CMSFullGCsBeforeCompaction  设置进行几次Full GC后，进行一次碎片整理</p><p>​            -XX:ParallelCMSThreads  设定CMS的线程数量（一般情况约等于可用CPU数量）</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430155615448-2073405176.png" alt="img"></p><h3 id="6-5-G1收集器"><a href="#6-5-G1收集器" class="headerlink" title="6.5　G1收集器"></a>6.5　G1收集器</h3><p>G1是目前技术发展的最前沿成果之一，HotSpot开发团队赋予它的使命是未来可以替换掉JDK1.5中发布的CMS收集器。与CMS收集器相比G1收集器有以下特点：</p><ol><li><p>空间整合，G1收集器采用标记整理算法，不会产生内存空间碎片。分配大对象时不会因为无法找到连续空间而提前触发下一次GC。</p></li><li><p>可预测停顿，这是G1的另一大优势，降低停顿时间是G1和CMS的共同关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为N毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了。</p></li></ol><p>上面提到的垃圾收集器，收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔阂了，它们都是一部分（可以不连续）Region的集合。</p><h4 id="G1对Heap的划分"><a href="#G1对Heap的划分" class="headerlink" title="G1对Heap的划分"></a>G1对Heap的划分</h4><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430155951152-2106136185.png" alt="img"></p><p>G1的新生代收集跟ParNew类似，当新生代占用达到一定比例的时候，开始出发收集。和CMS类似，G1收集器收集老年代对象会有短暂停顿。</p><h4 id="收集步骤"><a href="#收集步骤" class="headerlink" title="收集步骤"></a>收集步骤</h4><p>1、标记阶段，首先初始标记(Initial-Mark),这个阶段是停顿的(Stop the World Event)，并且会触发一次普通Mintor GC。对应GC log:GC pause (young) (inital-mark)</p><p>2、Root Region Scanning，程序运行过程中会回收survivor区(存活到老年代)，这一过程必须在young GC之前完成。</p><p>3、Concurrent Marking，在整个堆中进行并发标记(和应用程序并发执行)，此过程可能被young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那个这个区域会被立即回收(图中打X)。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430160141901-1411612026.png" alt="img"></p><p>4、Remark, 再标记，会有短暂停顿(STW)。再标记阶段是用来收集 并发标记阶段 产生新的垃圾(并发阶段和应用程序一同运行)；G1中采用了比CMS更快的初始快照算法:snapshot-at-the-beginning (SATB)。</p><p>5、Copy/Clean up，多线程清除失活对象，会有STW。G1将回收区域的存活对象拷贝到新区域，清除Remember Sets，并发清空回收区域并把它返回到空闲区域链表中。</p><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430160158306-1501053707.png" alt="img"></p><p>6、复制/清除过程后。回收区域的活性对象已经被集中回收到深蓝色和深绿色区域。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430160215565-69737884.png" alt="img"></p><h2 id="八、常用的收集器组合"><a href="#八、常用的收集器组合" class="headerlink" title="八、常用的收集器组合"></a>八、常用的收集器组合</h2><table><thead><tr><th></th><th>新生代GC策略</th><th>年老代GC策略</th><th>说明</th></tr></thead><tbody><tr><td>组合1</td><td>Serial</td><td>Serial Old</td><td>Serial和Serial Old都是单线程进行GC，特点就是GC时暂停所有应用线程。</td></tr><tr><td>组合2</td><td>Serial</td><td>CMS+Serial Old</td><td>CMS（Concurrent Mark Sweep）是并发GC，实现GC线程和应用线程并发工作，不需要暂停所有应用线程。另外，当CMS进行GC失败时，会自动使用Serial Old策略进行GC。</td></tr><tr><td>组合3</td><td>ParNew</td><td>CMS</td><td>使用-XX:+UseParNewGC选项来开启。ParNew是Serial的并行版本，可以指定GC线程数，默认GC线程数为CPU的数量。可以使用-XX:ParallelGCThreads选项指定GC的线程数。如果指定了选项-XX:+UseConcMarkSweepGC选项，则新生代默认使用ParNew GC策略。</td></tr><tr><td>组合4</td><td>ParNew</td><td>Serial Old</td><td>使用-XX:+UseParNewGC选项来开启。新生代使用ParNew GC策略，年老代默认使用Serial Old GC策略。</td></tr><tr><td>组合5</td><td>Parallel Scavenge</td><td>Serial Old</td><td>Parallel Scavenge策略主要是关注一个可控的吞吐量：应用程序运行时间 / (应用程序运行时间 + GC时间)，可见这会使得CPU的利用率尽可能的高，适用于后台持久运行的应用程序，而不适用于交互较多的应用程序。</td></tr><tr><td>组合6</td><td>Parallel Scavenge</td><td>Parallel Old</td><td>Parallel Old是Serial Old的并行版本</td></tr><tr><td>组合7</td><td>G1GC</td><td>G1GC</td><td>-XX:+UnlockExperimentalVMOptions -XX:+UseG1GC        #开启 -XX:MaxGCPauseMillis =50                  #暂停时间目标 -XX:GCPauseIntervalMillis =200          #暂停间隔目标 -XX:+G1YoungGenSize=512m            #年轻代大小 -XX:SurvivorRatio=6                            #幸存区比例</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        垃圾收集 Garbage Collection 通常被称为“GC”，它诞生于1960年 MIT 的 Lisp 语言，经过半个多世纪，目前已经十分成熟了。&lt;/p&gt;
&lt;p&gt;jvm 中，程序计数器、虚拟机栈、本地方法栈都是随线程而生随线程而灭，栈帧随着方法的进入和退出做入栈和出栈操作，实现了自动的内存清理，因此，我们的&lt;strong&gt;内存垃圾回收主要集中于 java 堆和方法区中&lt;/strong&gt;，在程序运行期间，这部分内存的分配和使用都是动态的。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构</title>
    <link href="http://zhangfuxin.cn/2019-06-13-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98JVM%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84.html"/>
    <id>http://zhangfuxin.cn/2019-06-13-Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构.html</id>
    <published>2019-06-13T02:30:04.000Z</published>
    <updated>2019-09-17T01:42:22.631Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （一）Spark初识</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、JVM的结构图"><a href="#一、JVM的结构图" class="headerlink" title="一、JVM的结构图"></a>一、JVM的结构图</h2><h3 id="1-1-Java内存结构"><a href="#1-1-Java内存结构" class="headerlink" title="1.1　Java内存结构"></a>1.1　Java内存结构</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430151809602-1476931965.png" alt="img"></p><p>JVM内存结构主要有三大块：<strong>堆内存、方法区和栈</strong>。</p><p><strong>堆内存</strong>是JVM中最大的一块由年轻代和老年代组成，而年轻代内存又被分成三部分，Eden空间、From Survivor空间、To Survivor空间,默认情况下年轻代按照8:1:1的比例来分配；</p><p><strong>方法区</strong>存储类信息、常量、静态变量等数据，是线程共享的区域，为与Java堆区分，方法区还有一个别名Non-Heap(非堆)；</p><p><strong>栈</strong>又分为java虚拟机栈和本地方法栈主要用于方法的执行。</p><h3 id="1-2-如何通过参数来控制各区域的内存大小"><a href="#1-2-如何通过参数来控制各区域的内存大小" class="headerlink" title="1.2　如何通过参数来控制各区域的内存大小"></a>1.2　如何通过参数来控制各区域的内存大小</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430152017807-1294956408.png" alt="img"></p><h3 id="1-3-控制参数"><a href="#1-3-控制参数" class="headerlink" title="1.3　控制参数"></a>1.3　控制参数</h3><p>-Xms设置堆的最小空间大小。</p><p>-Xmx设置堆的最大空间大小。</p><p>-XX:NewSize设置新生代最小空间大小。</p><p>-XX:MaxNewSize设置新生代最大空间大小。</p><p>-XX:PermSize设置永久代最小空间大小。</p><p>-XX:MaxPermSize设置永久代最大空间大小。</p><p>-Xss设置每个线程的堆栈大小。</p><p>没有直接设置老年代的参数，但是可以设置堆空间大小和新生代空间大小两个参数来间接控制。</p><p>  <strong>老年代空间大小=堆空间大小-年轻代大空间大小</strong></p><h3 id="1-4-JVM和系统调用之间的关系"><a href="#1-4-JVM和系统调用之间的关系" class="headerlink" title="1.4　JVM和系统调用之间的关系"></a>1.4　JVM和系统调用之间的关系</h3><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430152138687-378200875.png" alt="img"></p><p><strong>方法区和堆是所有线程共享的内存区域；而java栈、本地方法栈和程序员计数器是运行是线程私有的内存区域。</strong></p><h2 id="二、JVM各区域的作用"><a href="#二、JVM各区域的作用" class="headerlink" title="二、JVM各区域的作用"></a>二、JVM各区域的作用</h2><h3 id="2-1-Java堆（Heap）"><a href="#2-1-Java堆（Heap）" class="headerlink" title="2.1　Java堆（Heap）"></a>2.1　Java堆（Heap）</h3><p>​    对于大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。</p><p>​     Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”。如果从内存回收的角度看，由于现在收集器基本都是采用的分代收集算法，所以Java堆中还可以细分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。</p><p>根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx和-Xms控制）。</p><p>如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。</p><h3 id="2-2-方法区（Method-Area）"><a href="#2-2-方法区（Method-Area）" class="headerlink" title="2.2　方法区（Method Area）"></a>2.2　方法区（Method Area）</h3><p>  方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。</p><p>对于习惯在HotSpot虚拟机上开发和部署程序的开发者来说，很多人愿意把方法区称为“永久代”（Permanent Generation），本质上两者并不等价，仅仅是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已。</p><p>Java虚拟机规范对这个区域的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是有必要的。</p><p>根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 </p><h3 id="2-3-程序计数器（Program-Counter-Register）"><a href="#2-3-程序计数器（Program-Counter-Register）" class="headerlink" title="2.3　程序计数器（Program Counter Register）"></a>2.3　程序计数器（Program Counter Register）</h3><p>程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。<br>由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。<br>      如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie方法，这个计数器值则为空（Undefined）。</p><p>此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。</p><h3 id="2-4-JVM栈（JVM-Stacks）"><a href="#2-4-JVM栈（JVM-Stacks）" class="headerlink" title="2.4　JVM栈（JVM Stacks）"></a>2.4　JVM栈（JVM Stacks）</h3><p>与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 </p><p>局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，根据不同的虚拟机实现，它可能是一个指向对象起始地址的引用指针，也可能指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。</p><p>其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。</p><p>在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。</p><h3 id="2-5-本地方法栈（Native-Method-Stacks）"><a href="#2-5-本地方法栈（Native-Method-Stacks）" class="headerlink" title="2.5　本地方法栈（Native Method Stacks）"></a>2.5　本地方法栈（Native Method Stacks）</h3><p>本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （十三）SparkCore的调优之资源调优JVM的基本架构：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （一）Spark初识&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （十二）SparkCore的调优之资源调优</title>
    <link href="http://zhangfuxin.cn/2019-06-12-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98.html"/>
    <id>http://zhangfuxin.cn/2019-06-12-Spark学习之路 （十二）SparkCore的调优之资源调优.html</id>
    <published>2019-06-12T02:30:04.000Z</published>
    <updated>2019-09-17T01:35:58.907Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （十二）SparkCore的调优之资源调优：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、Spark作业基本运行原理"><a href="#一、Spark作业基本运行原理" class="headerlink" title="一、Spark作业基本运行原理"></a>一、Spark作业基本运行原理</h2><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180430150441732-1974371206.png" alt="img"></p><p>详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。</p><p>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。</p><p>Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。</p><p>当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。</p><p>因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。</p><p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</p><p>以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。</p><h2 id="二、资源参数调优"><a href="#二、资源参数调优" class="headerlink" title="二、资源参数调优"></a>二、资源参数调优</h2><p>了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p><h3 id="2-1-num-executors"><a href="#2-1-num-executors" class="headerlink" title="2.1　num-executors"></a>2.1　num-executors</h3><ul><li>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。<strong>这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</strong></li><li>参数调优建议：<strong>每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适</strong>，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li></ul><h3 id="2-2-executor-memory"><a href="#2-2-executor-memory" class="headerlink" title="2.2　executor-memory"></a>2.2　executor-memory</h3><ul><li>参数说明：<strong>该参数用于设置每个Executor进程的内存</strong>。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li><li>参数调优建议：<strong>每个Executor进程的内存设置4G~8G较为合适。</strong>但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li></ul><h3 id="2-3-executor-cores"><a href="#2-3-executor-cores" class="headerlink" title="2.3　executor-cores"></a>2.3　executor-cores</h3><ul><li>参数说明：<strong>该参数用于设置每个Executor进程的CPU core数量</strong>。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li><li>参数调优建议：Executor的CPU core数量设置为2<del>4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3</del>1/2左右比较合适，也是避免影响其他同学的作业运行。<strong>最好的应该就是一个cpu core对应两到三个task</strong></li></ul><h3 id="2-4-driver-memory"><a href="#2-4-driver-memory" class="headerlink" title="2.4　driver-memory"></a>2.4　driver-memory</h3><ul><li>参数说明：该参数用于设置Driver进程的内存。</li><li>参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</li></ul><h3 id="2-5-spark-default-parallelism"><a href="#2-5-spark-default-parallelism" class="headerlink" title="2.5　spark.default.parallelism"></a>2.5　spark.default.parallelism</h3><ul><li>参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。<strong>一个分区对应一个task，也就是这个参数其实就是设置task的数量</strong></li><li>参数调优建议：<strong>Spark作业的默认task数量为500~1000个较为合适。</strong>很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</li></ul><h3 id="2-6-spark-storage-memoryFraction"><a href="#2-6-spark-storage-memoryFraction" class="headerlink" title="2.6　spark.storage.memoryFraction"></a>2.6　spark.storage.memoryFraction</h3><ul><li>参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li><li>参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><h3 id="2-7-spark-shuffle-memoryFraction"><a href="#2-7-spark-shuffle-memoryFraction" class="headerlink" title="2.7　spark.shuffle.memoryFraction"></a>2.7　spark.shuffle.memoryFraction</h3><ul><li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li><li>参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><p>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （十二）SparkCore的调优之资源调优：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （十一）SparkCore的调优之Spark内存模型</title>
    <link href="http://zhangfuxin.cn/2019-06-11-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8BSpark%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B.html"/>
    <id>http://zhangfuxin.cn/2019-06-11-Spark学习之路 （十一）SparkCore的调优之Spark内存模型.html</id>
    <published>2019-06-11T02:30:04.000Z</published>
    <updated>2019-09-17T01:21:32.711Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （十一）SparkCore的调优之Spark内存模型：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark 作为一个基于内存的分布式计算引擎，其内存管理模块在整个系统中扮演着非常重要的角色。理解 Spark 内存管理的基本原理，有助于更好地开发 Spark 应用程序和进行性能调优。本文旨在梳理出 Spark 内存管理的脉络，抛砖引玉，引出读者对这个话题的深入探讨。本文中阐述的原理基于 Spark 2.1 版本，阅读本文需要读者有一定的 Spark 和 Java 基础，了解 RDD、Shuffle、JVM 等相关概念。</p><p>在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能[1]。由于 Driver 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-堆内和堆外内存规划"><a href="#1-堆内和堆外内存规划" class="headerlink" title="1. 堆内和堆外内存规划"></a>1. 堆内和堆外内存规划</h2><p>​        作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内（On-heap）空间进行了更为详细的分配，以充分利用内存。同时，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。</p><h5 id="图-1-堆内和堆外内存示意图"><a href="#图-1-堆内和堆外内存示意图" class="headerlink" title="图 1 . 堆内和堆外内存示意图"></a>图 1 . 堆内和堆外内存示意图</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image001.png" alt="img"></p><h3 id="1-1-堆内内存"><a href="#1-1-堆内内存" class="headerlink" title="1.1 堆内内存"></a>1.1 堆内内存</h3><p>堆内内存的大小，由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置。Executor 内运行的并发任务共享 JVM 堆内内存，这些任务在缓存 RDD 数据和广播（Broadcast）数据时占用的内存被规划为存储（Storage）内存，而这些任务在执行 Shuffle 时占用的内存被规划为执行（Execution）内存，剩余的部分不做特殊规划，那些 Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间。不同的管理模式下，这三部分占用的空间大小各不相同（下面第 2 小节会进行介绍）。</p><p>Spark 对堆内内存的管理是一种逻辑上的”规划式”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前<strong>记录</strong>这些内存，我们来看其具体流程：</p><ul><li><strong>申请内存</strong>：</li></ul><ol><li>Spark 在代码中 new 一个对象实例</li><li>JVM 从堆内内存分配空间，创建对象并返回对象引用</li><li>Spark 保存该对象的引用，记录该对象占用的内存</li></ol><ul><li><strong>释放内存</strong>：</li></ul><ol><li>Spark 记录该对象释放的内存，删除该对象的引用</li><li>等待 JVM 的垃圾回收机制释放该对象占用的堆内内存</li></ol><p>我们知道，JVM 的对象可以以序列化的方式存储，序列化的过程是将对象转换为二进制字节流，本质上可以理解为将非连续空间的链式存储转化为连续空间或块存储，在访问时则需要进行序列化的逆过程——反序列化，将字节流转化为对象，序列化的方式可以节省存储空间，但增加了存储和读取时候的计算开销。</p><p>对于 Spark 中序列化的对象，由于是字节流的形式，其占用的内存大小可直接计算，而对于非序列化的对象，其占用的内存是通过周期性地采样近似估算而得，即并不是每次新增的数据项都会计算一次占用的内存大小，这种方法降低了时间开销但是有可能误差较大，导致某一时刻的实际内存有可能远远超出预期[2]。此外，在被 Spark 标记为释放的对象实例，很有可能在实际上并没有被 JVM 回收，导致实际可用的内存小于 Spark 记录的可用内存。所以 Spark 并不能准确记录实际可用的堆内内存，从而也就无法完全避免内存溢出（OOM, Out of Memory）的异常。</p><p>虽然不能精准控制堆内内存的申请和释放，但 Spark 通过对存储内存和执行内存各自独立的规划管理，可以决定是否要在存储内存里缓存新的 RDD，以及是否为新的任务分配执行内存，在一定程度上可以提升内存的利用率，减少异常的出现。</p><h3 id="1-2-堆外内存"><a href="#1-2-堆外内存" class="headerlink" title="1.2 堆外内存"></a>1.2 堆外内存</h3><p>为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。利用 JDK Unsafe API（从 Spark 2.0 开始，在管理堆外的存储内存时不再基于 Tachyon，而是与堆外的执行内存一样，基于 JDK Unsafe API 实现[3]），Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。堆外内存可以被精确地申请和释放，而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。</p><p>在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存。</p><h3 id="1-3-内存管理接口"><a href="#1-3-内存管理接口" class="headerlink" title="1.3 内存管理接口"></a>1.3 内存管理接口</h3><p>Spark 为存储内存和执行内存的管理提供了统一的接口——MemoryManager，同一个 Executor 内的任务都调用这个接口的方法来申请或释放内存:</p><h4 id="清单-1-内存管理接口的主要方法"><a href="#清单-1-内存管理接口的主要方法" class="headerlink" title="清单 1 . 内存管理接口的主要方法"></a>清单 1 . 内存管理接口的主要方法</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//申请存储内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">acquireStorageMemory</span></span>(blockId: <span class="type">BlockId</span>, numBytes: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Boolean</span></span><br><span class="line"><span class="comment">//申请展开内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">acquireUnrollMemory</span></span>(blockId: <span class="type">BlockId</span>, numBytes: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Boolean</span></span><br><span class="line"><span class="comment">//申请执行内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">acquireExecutionMemory</span></span>(numBytes: <span class="type">Long</span>, taskAttemptId: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Long</span></span><br><span class="line"><span class="comment">//释放存储内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">releaseStorageMemory</span></span>(numBytes: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Unit</span></span><br><span class="line"><span class="comment">//释放执行内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">releaseExecutionMemory</span></span>(numBytes: <span class="type">Long</span>, taskAttemptId: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Unit</span></span><br><span class="line"><span class="comment">//释放展开内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">releaseUnrollMemory</span></span>(numBytes: <span class="type">Long</span>, memoryMode: <span class="type">MemoryMode</span>): <span class="type">Unit</span></span><br></pre></td></tr></table></figure><p>我们看到，在调用这些方法时都需要指定其内存模式（MemoryMode），这个参数决定了是在堆内还是堆外完成这次操作。</p><p>MemoryManager 的具体实现上，Spark 1.6 之后默认为统一管理（<a href="https://github.com/apache/spark/blob/v2.1.0/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala" target="_blank" rel="noopener">Unified Memory Manager</a>）方式，1.6 之前采用的静态管理（<a href="https://github.com/apache/spark/blob/v2.1.0/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala" target="_blank" rel="noopener">Static Memory Manager</a>）方式仍被保留，可通过配置 spark.memory.useLegacyMode 参数启用。两种方式的区别在于对空间分配的方式，下面的第 2 小节会分别对这两种方式进行介绍。</p><h2 id="2-内存空间分配"><a href="#2-内存空间分配" class="headerlink" title="2 . 内存空间分配"></a>2 . 内存空间分配</h2><h3 id="2-1-静态内存管理"><a href="#2-1-静态内存管理" class="headerlink" title="2.1 静态内存管理"></a>2.1 静态内存管理</h3><p>在 Spark 最初采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置，堆内内存的分配如图 2 所示：</p><h5 id="图-2-静态内存管理图示——堆内"><a href="#图-2-静态内存管理图示——堆内" class="headerlink" title="图 2 . 静态内存管理图示——堆内"></a>图 2 . 静态内存管理图示——堆内</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image002.png" alt="img"></p><p>可以看到，可用的堆内内存的大小需要按照下面的方式计算：</p><h4 id="清单-2-可用堆内内存空间"><a href="#清单-2-可用堆内内存空间" class="headerlink" title="清单 2 . 可用堆内内存空间"></a>清单 2 . 可用堆内内存空间</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">可用的存储内存 = systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction</span><br><span class="line">可用的执行内存 = systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction</span><br></pre></td></tr></table></figure><p>其中 systemMaxMemory 取决于当前 JVM 堆内内存的大小，最后可用的执行内存或者存储内存要在此基础上与各自的 memoryFraction 参数和 safetyFraction 参数相乘得出。上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 1-safetyFraction 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险（上文提到，对于非序列化对象的内存采样估算会产生误差）。值得注意的是，这个预留的保险区域仅仅是一种逻辑上的规划，在具体使用时 Spark 并没有区别对待，和”其它内存”一样交给了 JVM 去管理。</p><p>堆外的空间分配较为简单，只有存储内存和执行内存，如图 3 所示。可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域。</p><h5 id="图-3-静态内存管理图示——堆外"><a href="#图-3-静态内存管理图示——堆外" class="headerlink" title="图 3 . 静态内存管理图示——堆外"></a>图 3 . 静态内存管理图示——堆外</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image003.png" alt="img"></p><p>静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成”一半海水，一半火焰”的局面，即存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。</p><h3 id="2-2-统一内存管理"><a href="#2-2-统一内存管理" class="headerlink" title="2.2 统一内存管理"></a>2.2 统一内存管理</h3><p>Spark 1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域，如图 4 和图 5 所示</p><h5 id="图-4-统一内存管理图示——堆内"><a href="#图-4-统一内存管理图示——堆内" class="headerlink" title="图 4 . 统一内存管理图示——堆内"></a>图 4 . 统一内存管理图示——堆内</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image004.png" alt="img"></p><h5 id="图-5-统一内存管理图示——堆外"><a href="#图-5-统一内存管理图示——堆外" class="headerlink" title="图 5 . 统一内存管理图示——堆外"></a>图 5 . 统一内存管理图示——堆外</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image005.png" alt="img"></p><p>其中最重要的优化在于动态占用机制，其规则如下：</p><ul><li>设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围</li><li>双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block）</li><li>执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间</li><li>存储内存的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂[4]</li></ul><h5 id="图-6-动态占用机制图示"><a href="#图-6-动态占用机制图示" class="headerlink" title="图 6 . 动态占用机制图示"></a>图 6 . 动态占用机制图示</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image006.png" alt="img"></p><p>凭借统一内存管理机制，Spark 在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护 Spark 内存的难度，但并不意味着开发者可以高枕无忧。譬如，所以如果存储内存的空间太大或者说缓存的数据过多，反而会导致频繁的全量垃圾回收，降低任务执行时的性能，因为缓存的 RDD 数据通常都是长期驻留内存的 [5] 。所以要想充分发挥 Spark 的性能，需要开发者进一步了解存储内存和执行内存各自的管理方式和实现原理。</p><h2 id="3-存储内存管理"><a href="#3-存储内存管理" class="headerlink" title="3. 存储内存管理"></a>3. 存储内存管理</h2><h3 id="3-1-RDD-的持久化机制"><a href="#3-1-RDD-的持久化机制" class="headerlink" title="3.1 RDD 的持久化机制"></a>3.1 RDD 的持久化机制</h3><p>弹性分布式数据集（RDD）作为 Spark 最根本的数据抽象，是只读的分区记录（Partition）的集合，只能基于在稳定物理存储中的数据集上创建，或者在其他已有的 RDD 上执行转换（Transformation）操作产生一个新的 RDD。转换后的 RDD 与原始的 RDD 之间产生的依赖关系，构成了血统（Lineage）。凭借血统，Spark 保证了每一个 RDD 都可以被重新恢复。但 RDD 的所有转换都是惰性的，即只有当一个返回结果给 Driver 的行动（Action）发生时，Spark 才会创建任务读取 RDD，然后真正触发转换的执行。<br>Task 在启动之初读取一个分区时，会先判断这个分区是否已经被持久化，如果没有则需要检查 Checkpoint 或按照血统重新计算。所以如果一个 RDD 上要执行多次行动，可以在第一次行动中使用 persist 或 cache 方法，在内存或磁盘中持久化或缓存这个 RDD，从而在后面的行动时提升计算速度。事实上，cache 方法是使用默认的 MEMORY_ONLY 的存储级别将 RDD 持久化到内存，故缓存是一种特殊的持久化。 <strong>堆内和堆外存储内存的设计，便可以对缓存</strong> <strong>RDD</strong> <strong>时使用的内存做统一的规划和管</strong> <strong>理</strong> （存储内存的其他应用场景，如缓存 broadcast 数据，暂时不在本文的讨论范围之内）。</p><p>RDD 的持久化由 Spark 的 Storage 模块 [7] 负责，实现了 RDD 与物理存储的解耦合。Storage 模块负责管理 Spark 在计算过程中产生的数据，将那些在内存或磁盘、在本地或远程存取数据的功能封装了起来。在具体实现时 Driver 端和 Executor 端的 Storage 模块构成了主从式的架构，即 Driver 端的 BlockManager 为 Master，Executor 端的 BlockManager 为 Slave。Storage 模块在逻辑上以 Block 为基本存储单位，RDD 的每个 Partition 经过处理后唯一对应一个 Block（BlockId 的格式为 rdd_RDD-ID_PARTITION-ID ）。Master 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 Slave 需要将 Block 的更新等状态上报到 Master，同时接收 Master 的命令，例如新增或删除一个 RDD。</p><h5 id="图-7-Storage-模块示意图"><a href="#图-7-Storage-模块示意图" class="headerlink" title="图 7 . Storage 模块示意图"></a>图 7 . Storage 模块示意图</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image007.png" alt="img"></p><p>在对 RDD 持久化时，Spark 规定了 MEMORY_ONLY、MEMORY_AND_DISK 等12 种不同的 <a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence" target="_blank" rel="noopener">存储级别 </a>，而存储级别是以下 5 个变量的组合：</p><h4 id="清单-3-存储级别"><a href="#清单-3-存储级别" class="headerlink" title="清单 3 . 存储级别"></a>清单 3 . 存储级别</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useDisk: <span class="type">Boolean</span>, //磁盘</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useMemory: <span class="type">Boolean</span>, //这里其实是指堆内内存</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useOffHeap: <span class="type">Boolean</span>, //堆外内存</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _deserialized: <span class="type">Boolean</span>, //是否为非序列化</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _replication: <span class="type">Int</span> = 1 //副本个数</span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br></pre></td></tr></table></figure><p>通过对数据结构的分析，可以看出存储级别从三个维度定义了 RDD 的 Partition（同时也就是 Block）的存储方式：</p><ul><li>存储位置：磁盘／堆内内存／堆外内存。如 MEMORY_AND_DISK 是同时在磁盘和堆内内存上存储，实现了冗余备份。OFF_HEAP 则是只在堆外内存存储，目前选择堆外内存时不能同时存储到其他位置。</li><li>存储形式：Block 缓存到存储内存后，是否为非序列化的形式。如 MEMORY_ONLY 是非序列化方式存储，OFF_HEAP 是序列化方式存储。</li><li>副本数量：大于 1 时需要远程冗余备份到其他节点。如 DISK_ONLY_2 需要远程备份 1 个副本。</li></ul><h3 id="3-2-RDD-缓存的过程"><a href="#3-2-RDD-缓存的过程" class="headerlink" title="3.2 RDD 缓存的过程"></a>3.2 RDD 缓存的过程</h3><p>RDD 在缓存到存储内存之前，Partition 中的数据一般以迭代器（<a href="http://www.scala-lang.org/docu/files/collections-api/collections_43.html" target="_blank" rel="noopener">Iterator</a>）的数据结构来访问，这是 Scala 语言中一种遍历数据集合的方法。通过 Iterator 可以获取分区中每一条序列化或者非序列化的数据项(Record)，这些 Record 的对象实例在逻辑上占用了 JVM 堆内内存的 other 部分的空间，同一 Partition 的不同 Record 的空间并不连续。</p><p>RDD 在缓存到存储内存之后，Partition 被转换成 Block，Record 在堆内或堆外存储内存中占用一块连续的空间。<strong>将Partition由不连续的存储空间转换为连续存储空间的过程，Spark称之为”展开”（Unroll）</strong>。Block 有序列化和非序列化两种存储格式，具体以哪种方式取决于该 RDD 的存储级别。非序列化的 Block 以一种 DeserializedMemoryEntry 的数据结构定义，用一个数组存储所有的对象实例，序列化的 Block 则以 SerializedMemoryEntry的数据结构定义，用字节缓冲区（ByteBuffer）来存储二进制数据。每个 Executor 的 Storage 模块用一个链式 Map 结构（LinkedHashMap）来管理堆内和堆外存储内存中所有的 Block 对象的实例[6]，对这个 LinkedHashMap 新增和删除间接记录了内存的申请和释放。</p><p>因为不能保证存储空间可以一次容纳 Iterator 中的所有数据，当前的计算任务在 Unroll 时要向 MemoryManager 申请足够的 Unroll 空间来临时占位，空间不足则 Unroll 失败，空间足够时可以继续进行。对于序列化的 Partition，其所需的 Unroll 空间可以直接累加计算，一次申请。而非序列化的 Partition 则要在遍历 Record 的过程中依次申请，即每读取一条 Record，采样估算其所需的 Unroll 空间并进行申请，空间不足时可以中断，释放已占用的 Unroll 空间。如果最终 Unroll 成功，当前 Partition 所占用的 Unroll 空间被转换为正常的缓存 RDD 的存储空间，如下图 8 所示。</p><h5 id="图-8-Spark-Unroll-示意图"><a href="#图-8-Spark-Unroll-示意图" class="headerlink" title="图 8. Spark Unroll 示意图"></a>图 8. Spark Unroll 示意图</h5><p><img src="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/image008.png" alt="img"></p><p>在图 3 和图 5 中可以看到，在静态内存管理时，Spark 在存储内存中专门划分了一块 Unroll 空间，其大小是固定的，统一内存管理时则没有对 Unroll 空间进行特别区分，当存储空间不足时会根据动态占用机制进行处理。</p><h3 id="3-3-淘汰和落盘"><a href="#3-3-淘汰和落盘" class="headerlink" title="3.3 淘汰和落盘"></a>3.3 淘汰和落盘</h3><p>由于同一个 Executor 的所有的计算任务共享有限的存储内存空间，当有新的 Block 需要缓存但是剩余空间不足且无法动态占用时，就要对 LinkedHashMap 中的旧 Block 进行淘汰（Eviction），而被淘汰的 Block 如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘（Drop），否则直接删除该 Block。</p><p>存储内存的淘汰规则为：</p><ul><li>被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存</li><li>新旧 Block 不能属于同一个 RDD，避免循环淘汰</li><li>旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题</li><li>遍历 LinkedHashMap 中 Block，按照最近最少使用（LRU）的顺序淘汰，直到满足新 Block 所需的空间。其中 LRU 是 LinkedHashMap 的特性。</li></ul><p>落盘的流程则比较简单，如果其存储级别符合_useDisk 为 true 的条件，再根据其_deserialized 判断是否是非序列化的形式，若是则对其进行序列化，最后将数据存储到磁盘，在 Storage 模块中更新其信息。</p><h2 id="4-执行内存管理"><a href="#4-执行内存管理" class="headerlink" title="4. 执行内存管理"></a>4. 执行内存管理</h2><h3 id="4-1-多任务间内存分配"><a href="#4-1-多任务间内存分配" class="headerlink" title="4.1 多任务间内存分配"></a>4.1 多任务间内存分配</h3><p>Executor 内运行的任务同样共享执行内存，Spark 用一个 HashMap 结构保存了任务到内存耗费的映射。每个任务可占用的执行内存大小的范围为 1/2N ~ 1/N，其中 N 为当前 Executor 内正在运行的任务的个数。每个任务在启动之时，要向 MemoryManager 请求申请最少为 1/2N 的执行内存，如果不能被满足要求则该任务被阻塞，直到有其他任务释放了足够的执行内存，该任务才可以被唤醒。</p><h3 id="4-2-Shuffle-的内存占用"><a href="#4-2-Shuffle-的内存占用" class="headerlink" title="4.2 Shuffle 的内存占用"></a>4.2 Shuffle 的内存占用</h3><p>执行内存主要用来存储任务在执行 Shuffle 时占用的内存，Shuffle 是按照一定规则对 RDD 数据重新分区的过程，我们来看 Shuffle 的 Write 和 Read 两阶段对执行内存的使用：</p><ul><li>Shuffle Write</li></ul><ol><li>若在 map 端选择普通的排序方式，会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间。</li><li>若在 map 端选择 Tungsten 的排序方式，则采用 ShuffleExternalSorter 直接对以序列化形式存储的数据排序，在内存中存储数据时可以占用堆外或堆内执行空间，取决于用户是否开启了堆外内存以及堆外执行内存是否足够。</li></ol><ul><li>Shuffle Read</li></ul><ol><li>在对 reduce 端的数据进行聚合时，要将数据交给 Aggregator 处理，在内存中存储数据时占用堆内执行空间。</li><li>如果需要进行最终结果排序，则要将再次将数据交给 ExternalSorter 处理，占用堆内执行空间。</li></ol><p>在 ExternalSorter 和 Aggregator 中，Spark 会使用一种叫 AppendOnlyMap 的哈希表在堆内执行内存中存储数据，但在 Shuffle 过程中所有数据并不能都保存到该哈希表中，当这个哈希表占用的内存会进行周期性地采样估算，当其大到一定程度，无法再从 MemoryManager 申请到新的执行内存时，Spark 就会将其全部内容存储到磁盘文件中，这个过程被称为溢存(Spill)，溢存到磁盘的文件最后会被归并(Merge)。</p><p>Shuffle Write 阶段中用到的 Tungsten 是 Databricks 公司提出的对 Spark 优化内存和 CPU 使用的计划[9]，解决了一些 JVM 在性能上的限制和弊端。Spark 会根据 Shuffle 的情况来自动选择是否采用 Tungsten 排序。Tungsten 采用的页式内存管理机制建立在 MemoryManager 之上，即 Tungsten 对执行内存的使用进行了一步的抽象，这样在 Shuffle 过程中无需关心数据具体存储在堆内还是堆外。每个内存页用一个 MemoryBlock 来定义，并用 Object obj 和 long offset 这两个变量统一标识一个内存页在系统内存中的地址。堆内的 MemoryBlock 是以 long 型数组的形式分配的内存，其 obj 的值为是这个数组的对象引用，offset 是 long 型数组的在 JVM 中的初始偏移地址，两者配合使用可以定位这个数组在堆内的绝对地址；堆外的 MemoryBlock 是直接申请到的内存块，其 obj 为 null，offset 是这个内存块在系统内存中的 64 位绝对地址。Spark 用 MemoryBlock 巧妙地将堆内和堆外内存页统一抽象封装，并用页表(pageTable)管理每个 Task 申请到的内存页。</p><p>Tungsten 页式管理下的所有内存用 64 位的逻辑地址表示，由页号和页内偏移量组成：</p><ul><li>页号：占 13 位，唯一标识一个内存页，Spark 在申请内存页之前要先申请空闲页号。</li><li>页内偏移量：占 51 位，是在使用内存页存储数据时，数据在页内的偏移地址。</li></ul><p>有了统一的寻址方式，Spark 可以用 64 位逻辑地址的指针定位到堆内或堆外的内存，整个 Shuffle Write 排序的过程只需要对指针进行排序，并且无需反序列化，整个过程非常高效，对于内存访问效率和 CPU 使用效率带来了明显的提升[10]。</p><p>Spark 的存储内存和执行内存有着截然不同的管理方式：对于存储内存来说，Spark 用一个 LinkedHashMap 来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成；而对于执行内存，Spark 用 AppendOnlyMap 来存储 Shuffle 过程中的数据，在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制。</p><p>转自：<a href="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （十一）SparkCore的调优之Spark内存模型：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark 作为一个基于内存的分布式计算引擎，其内存管理模块在整个系统中扮演着非常重要的角色。理解 Spark 内存管理的基本原理，有助于更好地开发 Spark 应用程序和进行性能调优。本文旨在梳理出 Spark 内存管理的脉络，抛砖引玉，引出读者对这个话题的深入探讨。本文中阐述的原理基于 Spark 2.1 版本，阅读本文需要读者有一定的 Spark 和 Java 基础，了解 RDD、Shuffle、JVM 等相关概念。&lt;/p&gt;
&lt;p&gt;在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能[1]。由于 Driver 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （十）SparkCore的调优之Shuffle调优</title>
    <link href="http://zhangfuxin.cn/2019-06-10-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%8D%81%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8BShuffle%E8%B0%83%E4%BC%98.html"/>
    <id>http://zhangfuxin.cn/2019-06-10-Spark学习之路 （十）SparkCore的调优之Shuffle调优.html</id>
    <published>2019-06-10T02:30:04.000Z</published>
    <updated>2019-09-17T00:42:16.880Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （十）SparkCore的调优之Shuffle调优：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、shuffle的定义"><a href="#一、shuffle的定义" class="headerlink" title="一、shuffle的定义"></a>一、shuffle的定义</h2><p>Spark的运行主要分为2部分：</p><p>　　一部分是驱动程序，其核心是SparkContext；</p><p>　　另一部分是Worker节点上Task,它是运行实际任务的。程序运行的时候，Driver和Executor进程相互交互：运行什么任务，即Driver会分配Task到Executor，Driver 跟 Executor 进行网络传输; 任务数据从哪儿获取，即Task要从 Driver 抓取其他上游的 Task 的数据结果，所以有这个过程中就不断的产生网络结果。其中，<strong>下一个 Stage 向上一个 Stage 要数据这个过程，我们就称之为 Shuffle</strong>。</p><h2 id="二、ShuffleManager发展概述"><a href="#二、ShuffleManager发展概述" class="headerlink" title="二、ShuffleManager发展概述"></a>二、ShuffleManager发展概述</h2><p>​        在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。</p><p>在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。</p><p>因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</p><p>下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。</p><h2 id="三、HashShuffleManager的运行原理"><a href="#三、HashShuffleManager的运行原理" class="headerlink" title="三、HashShuffleManager的运行原理"></a>三、HashShuffleManager的运行原理</h2><p>在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。</p><p>在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。</p><p>因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</p><p>下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。</p><h3 id="3-1-未经优化的HashShuffleManager"><a href="#3-1-未经优化的HashShuffleManager" class="headerlink" title="3.1　未经优化的HashShuffleManager"></a>3.1　未经优化的HashShuffleManager</h3><h4 id="图解说明"><a href="#图解说明" class="headerlink" title="图解说明"></a>图解说明</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180426185748765-1326220564.png" alt="img"></p><h4 id="文字说明"><a href="#文字说明" class="headerlink" title="文字说明"></a>文字说明</h4><p>上图说明了未经优化的HashShuffleManager的原理。这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。</p><p>我们先从shuffle write开始说起。shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。</p><p>那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢？很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。</p><p>接着我们来说说shuffle read。shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。</p><p>shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。</p><h3 id="3-2-优化后的HashShuffleManager"><a href="#3-2-优化后的HashShuffleManager" class="headerlink" title="3.2　优化后的HashShuffleManager"></a>3.2　优化后的HashShuffleManager</h3><h4 id="图解说明-1"><a href="#图解说明-1" class="headerlink" title="图解说明"></a>图解说明</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180426190133554-1433961373.png" alt="img"></p><h4 id="文字说明-1"><a href="#文字说明-1" class="headerlink" title="文字说明"></a>文字说明</h4><p>上图说明了优化后的HashShuffleManager的原理。这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p><p>开启consolidate机制之后，在shuffle write过程中，task就不是为下游stage的每个task创建一个磁盘文件了。此时会出现shuffleFileGroup的概念，每个shuffleFileGroup会对应一批磁盘文件，磁盘文件的数量与下游stage的task数量是相同的。一个Executor上有多少个CPU core，就可以并行执行多少个task。而第一批并行执行的每个task都会创建一个shuffleFileGroup，并将数据写入对应的磁盘文件内。</p><p>当Executor的CPU core执行完一批task，接着执行下一批task时，下一批task就会复用之前已有的shuffleFileGroup，包括其中的磁盘文件。也就是说，此时task会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。</p><p>假设第二个stage有100个task，第一个stage有50个task，总共还是有10个Executor，每个Executor执行5个task。那么原本使用未经优化的HashShuffleManager时，每个Executor会产生500个磁盘文件，所有Executor会产生5000个磁盘文件的。但是此时经过优化之后，每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。也就是说，每个Executor此时只会创建100个磁盘文件，所有Executor只会创建1000个磁盘文件。</p><h2 id="四、SortShuffleManager运行原理"><a href="#四、SortShuffleManager运行原理" class="headerlink" title="四、SortShuffleManager运行原理"></a>四、SortShuffleManager运行原理</h2><p>SortShuffleManager的运行机制主要分成两种，一种是普通运行机制，另一种是bypass运行机制。当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。</p><h3 id="4-1-普通运行机制"><a href="#4-1-普通运行机制" class="headerlink" title="4.1　普通运行机制"></a>4.1　普通运行机制</h3><h4 id="图解说明-2"><a href="#图解说明-2" class="headerlink" title="图解说明"></a>图解说明</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180426190605260-1242830112.png" alt="img"></p><h4 id="文字说明-2"><a href="#文字说明-2" class="headerlink" title="文字说明"></a>文字说明</h4><p>上图说明了普通的SortShuffleManager的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。</p><p>在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。<strong>默认的batch数量是10000条</strong>，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。</p><p>一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。</p><p>SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于每个task最终只有一个磁盘文件，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。</p><h3 id="4-2-bypass运行机制"><a href="#4-2-bypass运行机制" class="headerlink" title="4.2　bypass运行机制"></a>4.2　bypass运行机制</h3><h4 id="图解说明-3"><a href="#图解说明-3" class="headerlink" title="图解说明"></a>图解说明</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180426191008146-746530859.png" alt="img"></p><h4 id="文字说明-3"><a href="#文字说明-3" class="headerlink" title="文字说明"></a>文字说明</h4><p>上图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下：</p><blockquote><ul><li>shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。</li><li>不是聚合类的shuffle算子（比如reduceByKey）。</li></ul></blockquote><p>此时task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。</p><p>该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。</p><p>而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。</p><h2 id="五、shuffle相关参数调优"><a href="#五、shuffle相关参数调优" class="headerlink" title="五、shuffle相关参数调优"></a>五、shuffle相关参数调优</h2><p>以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。</p><p>Spark各个版本的参数默认值可能会有不同，具体使用请参考官方网站的说明：</p><p>（1）先选择对应的Spark版本：<a href="http://spark.apache.org/documentation.html" target="_blank" rel="noopener">http://spark.apache.org/documentation.html</a></p><p>（2）再查看对应的文档说明</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180426192151929-1075706486.png" alt="img"></p><h3 id="spark-shuffle-file-buffer"><a href="#spark-shuffle-file-buffer" class="headerlink" title="spark.shuffle.file.buffer"></a>spark.shuffle.file.buffer</h3><ul><li>默认值：32k</li><li>参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul><h3 id="spark-reducer-maxSizeInFlight"><a href="#spark-reducer-maxSizeInFlight" class="headerlink" title="spark.reducer.maxSizeInFlight"></a>spark.reducer.maxSizeInFlight</h3><ul><li>默认值：48m</li><li>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul><h3 id="spark-shuffle-io-maxRetries"><a href="#spark-shuffle-io-maxRetries" class="headerlink" title="spark.shuffle.io.maxRetries"></a>spark.shuffle.io.maxRetries</h3><ul><li>默认值：3</li><li>参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。</li><li>调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。</li></ul><h3 id="spark-shuffle-io-retryWait"><a href="#spark-shuffle-io-retryWait" class="headerlink" title="spark.shuffle.io.retryWait"></a>spark.shuffle.io.retryWait</h3><ul><li>默认值：5s</li><li>参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。</li><li>调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。</li></ul><h3 id="spark-shuffle-memoryFraction（已经弃用）"><a href="#spark-shuffle-memoryFraction（已经弃用）" class="headerlink" title="spark.shuffle.memoryFraction（已经弃用）"></a>spark.shuffle.memoryFraction（已经弃用）</h3><ul><li>默认值：0.2</li><li>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。</li><li>调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。</li></ul><h3 id="spark-shuffle-manager（已经弃用）"><a href="#spark-shuffle-manager（已经弃用）" class="headerlink" title="spark.shuffle.manager（已经弃用）"></a>spark.shuffle.manager（已经弃用）</h3><ul><li>默认值：sort</li><li>参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。</li><li>调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。</li></ul><h3 id="spark-shuffle-sort-bypassMergeThreshold"><a href="#spark-shuffle-sort-bypassMergeThreshold" class="headerlink" title="spark.shuffle.sort.bypassMergeThreshold"></a>spark.shuffle.sort.bypassMergeThreshold</h3><ul><li>默认值：200</li><li>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</li><li>调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</li></ul><h3 id="spark-shuffle-consolidateFiles（已经弃用）"><a href="#spark-shuffle-consolidateFiles（已经弃用）" class="headerlink" title="spark.shuffle.consolidateFiles（已经弃用）"></a>spark.shuffle.consolidateFiles（已经弃用）</h3><ul><li>默认值：false</li><li>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</li><li>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （十）SparkCore的调优之Shuffle调优：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （九）SparkCore的调优之数据倾斜调优</title>
    <link href="http://zhangfuxin.cn/2019-06-09-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B9%9D%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E8%B0%83%E4%BC%98.html"/>
    <id>http://zhangfuxin.cn/2019-06-09-Spark学习之路 （九）SparkCore的调优之数据倾斜调优.html</id>
    <published>2019-06-09T02:30:04.000Z</published>
    <updated>2019-09-17T00:27:40.324Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （九）SparkCore的调优之数据倾斜调优：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="数据倾斜发生时的现象"><a href="#数据倾斜发生时的现象" class="headerlink" title="数据倾斜发生时的现象"></a>数据倾斜发生时的现象</h2><ul><li>绝大多数task执行得都非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。</li><li>原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。</li></ul><h2 id="数据倾斜发生的原理"><a href="#数据倾斜发生的原理" class="headerlink" title="数据倾斜发生的原理"></a>数据倾斜发生的原理</h2><p>数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。</p><p>因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。</p><p>下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425190936210-1283450149.png" alt="img"></p><h2 id="如何定位导致数据倾斜的代码"><a href="#如何定位导致数据倾斜的代码" class="headerlink" title="如何定位导致数据倾斜的代码"></a>如何定位导致数据倾斜的代码</h2><p>数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</p><h3 id="某个task执行特别慢的情况"><a href="#某个task执行特别慢的情况" class="headerlink" title="某个task执行特别慢的情况"></a>某个task执行特别慢的情况</h3><p>首先要看的，就是数据倾斜发生在第几个stage中。</p><p>如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。</p><p>比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425190955841-989714022.png" alt="img"></p><p>知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。</p><p>这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。</p><ul><li>stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。</li><li>stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">"hdfs://..."</span>)</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> pairs = words.map((_, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">wordCounts.collect().foreach(println(_))</span><br></pre></td></tr></table></figure><p>​        通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由reduceByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。</p><h3 id="某个task莫名其妙内存溢出的情况"><a href="#某个task莫名其妙内存溢出的情况" class="headerlink" title="某个task莫名其妙内存溢出的情况"></a>某个task莫名其妙内存溢出的情况</h3><p>​        这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。</p><p>但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。</p><h2 id="查看导致数据倾斜的key的数据分布情况"><a href="#查看导致数据倾斜的key的数据分布情况" class="headerlink" title="查看导致数据倾斜的key的数据分布情况"></a>查看导致数据倾斜的key的数据分布情况</h2><p>​        知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。</p><p>此时根据你执行操作的情况不同，可以有很多种查看key分布的方式：</p><ol><li>如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。</li><li>如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。</li></ol><p>举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sampledPairs = pairs.sample(<span class="literal">false</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">val</span> sampledWordCounts = sampledPairs.countByKey()</span><br><span class="line">sampledWordCounts.foreach(println(_))</span><br></pre></td></tr></table></figure><h2 id="数据倾斜的解决方案"><a href="#数据倾斜的解决方案" class="headerlink" title="数据倾斜的解决方案"></a>数据倾斜的解决方案</h2><h3 id="解决方案一：使用Hive-ETL预处理数据"><a href="#解决方案一：使用Hive-ETL预处理数据" class="headerlink" title="解决方案一：使用Hive ETL预处理数据"></a>解决方案一：使用Hive ETL预处理数据</h3><p><strong>方案适用场景：</strong>导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。</p><p><strong>方案实现思路：</strong>此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。</p><p><strong>方案实现原理：</strong>这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。</p><p><strong>方案优点：</strong>实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p><p><strong>方案缺点：</strong>治标不治本，Hive ETL中还是会发生数据倾斜。</p><p><strong>方案实践经验：</strong>在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p><p><strong>项目实践经验：</strong>在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</p><h3 id="解决方案二：过滤少数导致倾斜的key"><a href="#解决方案二：过滤少数导致倾斜的key" class="headerlink" title="解决方案二：过滤少数导致倾斜的key"></a>解决方案二：过滤少数导致倾斜的key</h3><p><strong>方案适用场景：</strong>如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p><p><strong>方案实现思路：</strong>如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</p><p><strong>方案实现原理：</strong>将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。</p><p><strong>方案优点：</strong>实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p><p><strong>方案缺点：</strong>适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p><p><strong>方案实践经验：</strong>在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p><h3 id="解决方案三：提高shuffle操作的并行度"><a href="#解决方案三：提高shuffle操作的并行度" class="headerlink" title="解决方案三：提高shuffle操作的并行度"></a>解决方案三：提高shuffle操作的并行度</h3><p><strong>方案适用场景：</strong>如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</p><p><strong>方案实现思路：</strong>在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p><p><strong>方案实现原理：</strong>增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。</p><p><strong>方案优点：</strong>实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p><p><strong>方案缺点：</strong>只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p><p><strong>方案实践经验：</strong>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425191209688-1729367236.png" alt="img"></p><h3 id="解决方案四：两阶段聚合（局部聚合-全局聚合）"><a href="#解决方案四：两阶段聚合（局部聚合-全局聚合）" class="headerlink" title="解决方案四：两阶段聚合（局部聚合+全局聚合）"></a>解决方案四：两阶段聚合（局部聚合+全局聚合）</h3><p><strong>方案适用场景：</strong>对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</p><p><strong>方案实现思路：</strong>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p><p><strong>方案实现原理：</strong>将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p><p><strong>方案优点：</strong>对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p><p><strong>方案缺点：</strong>仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425191231729-2107528275.png" alt="img"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 第一步，给RDD中的每个key都打上一个随机前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; randomPrefixRdd = rdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Long</span>&gt;, <span class="type">String</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">                int prefix = random.nextInt(<span class="number">10</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二步，对打上随机前缀的key进行局部聚合。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; localAggrRdd = randomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Long</span> call(<span class="type">Long</span> v1, <span class="type">Long</span> v2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第三步，去除RDD中每个key的随机前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>,<span class="type">Long</span>&gt;, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                long originalKey = <span class="type">Long</span>.valueOf(tuple._1.split(<span class="string">"_"</span>)[<span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt;(originalKey, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第四步，对去除了随机前缀的RDD进行全局聚合。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Long</span> call(<span class="type">Long</span> v1, <span class="type">Long</span> v2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><h3 id="解决方案五：将reduce-join转为map-join"><a href="#解决方案五：将reduce-join转为map-join" class="headerlink" title="解决方案五：将reduce join转为map join"></a>解决方案五：将reduce join转为map join</h3><p><strong>方案适用场景：</strong>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p><p><strong>方案实现思路：</strong>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p><p><strong>方案实现原理：</strong>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p><p><strong>方案优点：</strong>对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点：</strong>适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425191304938-280219887.png" alt="img"></p><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先将数据量比较小的RDD的数据，collect到Driver中来。</span></span><br><span class="line"><span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt;&gt; rdd1Data = rdd1.collect()</span><br><span class="line"><span class="comment">// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。</span></span><br><span class="line"><span class="comment">// 可以尽可能节省内存空间，并且减少网络传输性能开销。</span></span><br><span class="line"><span class="keyword">final</span> <span class="type">Broadcast</span>&lt;<span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对另外一个RDD执行map类操作，而不再是join类操作。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRdd = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="comment">// 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。</span></span><br><span class="line">                <span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt;&gt; rdd1Data = rdd1DataBroadcast.value();</span><br><span class="line">                <span class="comment">// 可以将rdd1的数据转换为一个Map，便于后面进行join操作。</span></span><br><span class="line">                <span class="type">Map</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; rdd1DataMap = <span class="keyword">new</span> <span class="type">HashMap</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt;();</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; data : rdd1Data) &#123;</span><br><span class="line">                    rdd1DataMap.put(data._1, data._2);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 获取当前RDD数据的key以及value。</span></span><br><span class="line">                <span class="type">String</span> key = tuple._1;</span><br><span class="line">                <span class="type">String</span> value = tuple._2;</span><br><span class="line">                <span class="comment">// 从rdd1数据Map中，根据key获取到可以join到的数据。</span></span><br><span class="line">                <span class="type">Row</span> rdd1Value = rdd1DataMap.get(key);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(key, <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;(value, rdd1Value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里得提示一下。</span></span><br><span class="line"><span class="comment">// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。</span></span><br><span class="line"><span class="comment">// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。</span></span><br><span class="line"><span class="comment">// rdd2中每条数据都可能会返回多条join后的数据。</span></span><br></pre></td></tr></table></figure><h3 id="解决方案六：采样倾斜key并分拆join操作"><a href="#解决方案六：采样倾斜key并分拆join操作" class="headerlink" title="解决方案六：采样倾斜key并分拆join操作"></a>解决方案六：采样倾斜key并分拆join操作</h3><p><strong>方案适用场景：</strong>两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</p><p><strong>方案实现思路：</strong></p><ul><li>对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。</li><li>然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。</li><li>接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。</li><li>再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。</li><li>而另外两个普通的RDD就照常join即可。</li><li>最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</li></ul><p><strong>方案实现原理：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。</p><p><strong>方案优点：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</p><p><strong>方案缺点：</strong>如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425191444321-17069054.png" alt="img"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; sampledRDD = rdd1.sample(<span class="literal">false</span>, <span class="number">0.1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。</span></span><br><span class="line"><span class="comment">// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。</span></span><br><span class="line"><span class="comment">// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; mappedSampledRDD = sampledRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt;(tuple._1, <span class="number">1</span>L);</span><br><span class="line">            &#125;     </span><br><span class="line">        &#125;);</span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; countedSampledRDD = mappedSampledRDD.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Long</span> call(<span class="type">Long</span> v1, <span class="type">Long</span> v2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; reversedSampledRDD = countedSampledRDD.mapToPair( </span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Long</span>&gt;, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="keyword">final</span> <span class="type">Long</span> skewedUserid = reversedSampledRDD.sortByKey(<span class="literal">false</span>).take(<span class="number">1</span>).get(<span class="number">0</span>)._2;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; skewedRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Function</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">Boolean</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Boolean</span> call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="comment">// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; commonRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Function</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">Boolean</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Boolean</span> call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> !tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125; </span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// rdd2，就是那个所有key的分布相对较为均匀的rdd。</span></span><br><span class="line"><span class="comment">// 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。</span></span><br><span class="line"><span class="comment">// 对扩容的每条数据，都打上0～100的前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt; skewedRdd2 = rdd2.filter(</span><br><span class="line">         <span class="keyword">new</span> <span class="type">Function</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Row</span>&gt;, <span class="type">Boolean</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Boolean</span> call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).flatMapToPair(<span class="keyword">new</span> <span class="type">PairFlatMapFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Row</span>&gt;, <span class="type">String</span>, <span class="type">Row</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Iterable</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; call(</span><br><span class="line">                    <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">                <span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; list = <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(int i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;(i + <span class="string">"_"</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。</span></span><br><span class="line"><span class="comment">// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD1 = skewedRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">String</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">                int prefix = random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .join(skewedUserid2infoRDD)</span><br><span class="line">        .mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>,<span class="type">Tuple2</span>&lt;<span class="type">String</span>,<span class="type">Row</span>&gt;&gt;, <span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;() &#123;</span><br><span class="line">                        <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; call(</span><br><span class="line">                            <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; tuple)</span><br><span class="line">                            <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                            long key = <span class="type">Long</span>.valueOf(tuple._1.split(<span class="string">"_"</span>)[<span class="number">1</span>]);</span><br><span class="line">                            <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;(key, tuple._2);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将倾斜key join后的结果与普通key join后的结果，uinon起来。</span></span><br><span class="line"><span class="comment">// 就是最终的join结果。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2);</span><br></pre></td></tr></table></figure><h3 id="解决方案七：使用随机前缀和扩容RDD进行join"><a href="#解决方案七：使用随机前缀和扩容RDD进行join" class="headerlink" title="解决方案七：使用随机前缀和扩容RDD进行join"></a>解决方案七：使用随机前缀和扩容RDD进行join</h3><p><strong>方案适用场景：</strong>如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。</p><p><strong>方案实现思路：</strong></p><ul><li>该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。</li><li>然后将该RDD的每条数据都打上一个n以内的随机前缀。</li><li>同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。</li><li>最后将两个处理后的RDD进行join即可。</li></ul><p><strong>方案实现原理：</strong>将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p><p><strong>方案优点：</strong>对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p><p><strong>方案缺点：</strong>该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p><p><strong>方案实践经验：</strong>曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt; expandedRDD = rdd1.flatMapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFlatMapFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Row</span>&gt;, <span class="type">String</span>, <span class="type">Row</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Iterable</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; list = <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(int i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;(<span class="number">0</span> + <span class="string">"_"</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; mappedRDD = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">String</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple)</span><br><span class="line">                    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">                int prefix = random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将两个处理后的RDD进行join即可。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD);</span><br></pre></td></tr></table></figure><h3 id="解决方案八：多种方案组合使用"><a href="#解决方案八：多种方案组合使用" class="headerlink" title="解决方案八：多种方案组合使用"></a>解决方案八：多种方案组合使用</h3><p>​        在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，预处理一部分数据，并过滤一部分数据来缓解；其次可以对某些shuffle操作提升并行度，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （九）SparkCore的调优之数据倾斜调优：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>cache和persist的区别</title>
    <link href="http://zhangfuxin.cn/2019-06-08-cache%E5%92%8Cpersist%E7%9A%84%E5%8C%BA%E5%88%AB.html"/>
    <id>http://zhangfuxin.cn/2019-06-08-cache和persist的区别.html</id>
    <published>2019-06-08T03:30:04.000Z</published>
    <updated>2019-09-16T17:31:38.817Z</updated>
    
    <content type="html"><![CDATA[<p>** cache和persist的区别：** &lt;Excerpt in index | 首页摘要&gt;</p><p>cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h1 id="cache和persist的区别"><a href="#cache和persist的区别" class="headerlink" title="cache和persist的区别"></a>cache和persist的区别</h1><p>基于Spark 1.6.1 的源码，可以看到</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist()</span><br></pre></td></tr></table></figure><p>说明是cache()调用了persist(), 想要知道二者的不同还需要看一下persist函数：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br></pre></td></tr></table></figure><p>可以看到persist()内部调用了persist(StorageLevel.MEMORY_ONLY)，继续深入：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Set this RDD's storage level to persist its values across operations after the first time</span></span><br><span class="line"><span class="comment"> * it is computed. This can only be used to assign a new storage level if the RDD does not</span></span><br><span class="line"><span class="comment"> * have a storage level set yet..</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> Handle changes of StorageLevel</span></span><br><span class="line">  <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span> &amp;&amp; newLevel != storageLevel) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnsupportedOperationException</span>(</span><br><span class="line">      <span class="string">"Cannot change storage level of an RDD after it was already assigned a level"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  sc.persistRDD(<span class="keyword">this</span>)</span><br><span class="line">  <span class="comment">// Register the RDD with the ContextCleaner for automatic GC-based cleanup</span></span><br><span class="line">  sc.cleaner.foreach(_.registerRDDForCleanup(<span class="keyword">this</span>))</span><br><span class="line">  storageLevel = newLevel</span><br><span class="line">  <span class="keyword">this</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看出来persist有一个 StorageLevel 类型的参数，这个表示的是RDD的缓存级别。</p><p>至此便可得出cache和persist的区别了：cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。</p><h1 id="RDD的缓存级别"><a href="#RDD的缓存级别" class="headerlink" title="RDD的缓存级别"></a>RDD的缓存级别</h1><p>顺便看一下RDD都有哪些缓存级别，查看 StorageLevel 类的源码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>)</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到这里列出了12种缓存级别，但这些有什么区别呢？可以看到每个缓存级别后面都跟了一个StorageLevel的构造函数，里面包含了4个或5个参数，如下</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br></pre></td></tr></table></figure><p>查看其构造函数</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useDisk: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useMemory: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useOffHeap: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _deserialized: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _replication: <span class="type">Int</span> = 1</span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Externalizable</span> </span>&#123;</span><br><span class="line">  ......</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">useDisk</span></span>: <span class="type">Boolean</span> = _useDisk</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">useMemory</span></span>: <span class="type">Boolean</span> = _useMemory</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">useOffHeap</span></span>: <span class="type">Boolean</span> = _useOffHeap</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deserialized</span></span>: <span class="type">Boolean</span> = _deserialized</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">replication</span></span>: <span class="type">Int</span> = _replication</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到StorageLevel类的主构造器包含了5个参数：</p><ul><li>useDisk：使用硬盘（外存）</li><li>useMemory：使用内存</li><li>useOffHeap：使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。</li><li>deserialized：反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象</li><li>replication：备份数（在多个节点上备份）</li></ul><p>理解了这5个参数，StorageLevel 的12种缓存级别就不难理解了。</p><p>val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) 就表示使用这种缓存级别的RDD将存储在硬盘以及内存中，使用序列化（在硬盘中），并且在多个节点上备份2份（正常的RDD只有一份）</p><p>另外还注意到有一种特殊的缓存级别</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure><p>使用了堆外内存，StorageLevel 类的源码中有一段代码可以看出这个的特殊性，它不能和其它几个参数共存。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (useOffHeap) &#123;</span><br><span class="line">  require(!useDisk, <span class="string">"Off-heap storage level does not support using disk"</span>)</span><br><span class="line">  require(!useMemory, <span class="string">"Off-heap storage level does not support using heap memory"</span>)</span><br><span class="line">  require(!deserialized, <span class="string">"Off-heap storage level does not support deserialized storage"</span>)</span><br><span class="line">  require(replication == <span class="number">1</span>, <span class="string">"Off-heap storage level does not support multiple replication"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** cache和persist的区别：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （八）SparkCore的调优之开发调优</title>
    <link href="http://zhangfuxin.cn/2019-06-08-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%85%AB%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E5%BC%80%E5%8F%91%E8%B0%83%E4%BC%98.html"/>
    <id>http://zhangfuxin.cn/2019-06-08-Spark学习之路 （八）SparkCore的调优之开发调优.html</id>
    <published>2019-06-08T02:30:04.000Z</published>
    <updated>2019-09-16T15:05:55.685Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （八）SparkCore的调优之开发调优：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。</p><p>​        然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>​        Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。</p><p>​        笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为<strong>开发调优、资源调优、数据倾斜调优、shuffle调优</strong>几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。</p><p>本文作为Spark性能优化指南的基础篇，主要讲解开发调优以及资源调优。</p><h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>​        Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。</p><h2 id="原则一：避免创建重复的RDD"><a href="#原则一：避免创建重复的RDD" class="headerlink" title="原则一：避免创建重复的RDD"></a>原则一：避免创建重复的RDD</h2><p>​        通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。</p><p>我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。</p><p>一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。</p><h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span></span><br><span class="line"><span class="comment">// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。</span></span><br><span class="line"><span class="comment">// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd2.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span></span><br><span class="line"><span class="comment">// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。</span></span><br><span class="line"><span class="comment">// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</span></span><br><span class="line"><span class="comment">// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><h2 id="原则二：尽可能复用同一个RDD"><a href="#原则二：尽可能复用同一个RDD" class="headerlink" title="原则二：尽可能复用同一个RDD"></a>原则二：尽可能复用同一个RDD</h2><p>​        除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。</p><h3 id="一个简单的例子-1"><a href="#一个简单的例子-1" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 错误的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。</span></span><br><span class="line"><span class="comment">// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line"><span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; rdd2 = rdd1.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分别对rdd1和rdd2执行了不同的算子操作。</span></span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd2.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</span></span><br><span class="line"><span class="comment">// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 其实在这种情况下完全可以复用同一个RDD。</span></span><br><span class="line"><span class="comment">// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。</span></span><br><span class="line"><span class="comment">// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd1.map(tuple._2...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</span></span><br><span class="line"><span class="comment">// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。</span></span><br><span class="line"><span class="comment">// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。</span></span><br></pre></td></tr></table></figure><h2 id="原则三：对多次使用的RDD进行持久化"><a href="#原则三：对多次使用的RDD进行持久化" class="headerlink" title="原则三：对多次使用的RDD进行持久化"></a>原则三：对多次使用的RDD进行持久化</h2><p>​        当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。</p><p>​        Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p><p>​        因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。</p><h3 id="对多次使用的RDD进行持久化的代码示例"><a href="#对多次使用的RDD进行持久化的代码示例" class="headerlink" title="对多次使用的RDD进行持久化的代码示例"></a>对多次使用的RDD进行持久化的代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"><span class="comment">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span></span><br><span class="line"><span class="comment">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span></span><br><span class="line"><span class="comment">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).cache()</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span></span><br><span class="line"><span class="comment">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。</span></span><br><span class="line"><span class="comment">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span></span><br><span class="line"><span class="comment">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><p>对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。</p><h3 id="Spark的持久化级别"><a href="#Spark的持久化级别" class="headerlink" title="Spark的持久化级别"></a>Spark的持久化级别</h3><table><thead><tr><th>持久化级别</th><th>含义解释</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。</td></tr><tr><td>MEMORY_AND_DISK</td><td>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</td></tr><tr><td>MEMORY_ONLY_SER</td><td>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>DISK_ONLY</td><td>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</td></tr><tr><td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.</td><td>对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。</td></tr></tbody></table><h3 id="如何选择一种最合适的持久化策略"><a href="#如何选择一种最合适的持久化策略" class="headerlink" title="如何选择一种最合适的持久化策略"></a>如何选择一种最合适的持久化策略</h3><ul><li>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</li><li>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li><li>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</li><li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li></ul><h2 id="原则四：尽量避免使用shuffle类算子"><a href="#原则四：尽量避免使用shuffle类算子" class="headerlink" title="原则四：尽量避免使用shuffle类算子"></a>原则四：尽量避免使用shuffle类算子</h2><p>​    如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。</p><p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</p><p>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p><h3 id="Broadcast与map进行join代码示例"><a href="#Broadcast与map进行join代码示例" class="headerlink" title="Broadcast与map进行join代码示例"></a>Broadcast与map进行join代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统的join操作会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Broadcast+map的join操作，不会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></span><br><span class="line"><span class="keyword">val</span> rdd2Data = rdd2.collect()</span><br><span class="line"><span class="keyword">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></span><br><span class="line"><span class="comment">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span></span><br><span class="line"><span class="comment">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></span><br><span class="line"><span class="comment">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></span><br></pre></td></tr></table></figure><h2 id="原则五：使用map-side预聚合的shuffle操作"><a href="#原则五：使用map-side预聚合的shuffle操作" class="headerlink" title="原则五：使用map-side预聚合的shuffle操作"></a>原则五：使用map-side预聚合的shuffle操作</h2><p><strong>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。</strong></p><p>​        所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p><p>比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425185853777-491379087.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425185910086-1609967670.png" alt="img"></p><h2 id="原则六：使用高性能的算子"><a href="#原则六：使用高性能的算子" class="headerlink" title="原则六：使用高性能的算子"></a>原则六：使用高性能的算子</h2><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p><h3 id="使用reduceByKey-aggregateByKey替代groupByKey"><a href="#使用reduceByKey-aggregateByKey替代groupByKey" class="headerlink" title="使用reduceByKey/aggregateByKey替代groupByKey"></a>使用reduceByKey/aggregateByKey替代groupByKey</h3><p>详情见“原则五：使用map-side预聚合的shuffle操作”。</p><h3 id="使用mapPartitions替代普通map"><a href="#使用mapPartitions替代普通map" class="headerlink" title="使用mapPartitions替代普通map"></a>使用mapPartitions替代普通map</h3><p>​        mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p><h3 id="使用foreachPartitions替代foreach"><a href="#使用foreachPartitions替代foreach" class="headerlink" title="使用foreachPartitions替代foreach"></a>使用foreachPartitions替代foreach</h3><p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p><h3 id="使用filter之后进行coalesce操作"><a href="#使用filter之后进行coalesce操作" class="headerlink" title="使用filter之后进行coalesce操作"></a>使用filter之后进行coalesce操作</h3><p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p><h3 id="使用repartitionAndSortWithinPartitions替代repartition与sort类操作"><a href="#使用repartitionAndSortWithinPartitions替代repartition与sort类操作" class="headerlink" title="使用repartitionAndSortWithinPartitions替代repartition与sort类操作"></a>使用repartitionAndSortWithinPartitions替代repartition与sort类操作</h3><p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p><h2 id="原则七：广播大变量"><a href="#原则七：广播大变量" class="headerlink" title="原则七：广播大变量"></a>原则七：广播大变量</h2><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p><p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p><p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p><h3 id="广播大变量的代码示例"><a href="#广播大变量的代码示例" class="headerlink" title="广播大变量的代码示例"></a>广播大变量的代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 以下代码在算子函数中，使用了外部的变量。</span></span><br><span class="line"><span class="comment">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line">rdd1.map(list1...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下代码将list1封装成了Broadcast类型的广播变量。</span></span><br><span class="line"><span class="comment">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span></span><br><span class="line"><span class="comment">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span></span><br><span class="line"><span class="comment">// 每个Executor内存中，就只会驻留一份广播变量副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line"><span class="keyword">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class="line">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure><h2 id="原则八：使用Kryo优化序列化性能"><a href="#原则八：使用Kryo优化序列化性能" class="headerlink" title="原则八：使用Kryo优化序列化性能"></a>原则八：使用Kryo优化序列化性能</h2><p>在Spark中，主要有三个地方涉及到了序列化：</p><ul><li>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。</li><li>将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。</li><li>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</li></ul><p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p><p>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建SparkConf对象。</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line"><span class="comment">// 设置序列化器为KryoSerializer。</span></span><br><span class="line">conf.set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line"><span class="comment">// 注册要序列化的自定义类型。</span></span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br></pre></td></tr></table></figure><h2 id="原则九：优化数据结构"><a href="#原则九：优化数据结构" class="headerlink" title="原则九：优化数据结构"></a>原则九：优化数据结构</h2><p>Java中，有三种类型比较耗费内存：</p><ul><li>对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。</li><li>字符串，每个字符串内部都有一个字符数组以及长度等额外信息。</li><li>集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</li></ul><p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p><p>但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。</p><h2 id="原则十：Data-Locality本地化级别"><a href="#原则十：Data-Locality本地化级别" class="headerlink" title="原则十：Data Locality本地化级别"></a>原则十：Data Locality本地化级别</h2><p><strong>PROCESS_LOCAL</strong>：进程本地化，代码和数据在同一个进程中，也就是在同一个executor中；计算数据的task由executor执行，数据在executor的BlockManager中；性能最好</p><p><strong>NODE_LOCAL</strong>：节点本地化，代码和数据在同一个节点中；比如说，数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行；或者是，数据和task在一个节点上的不同executor中；数据需要在进程间进行传输<br><strong>NO_PREF</strong>：对于task来说，数据从哪里获取都一样，没有好坏之分<br><strong>RACK_LOCAL</strong>：机架本地化，数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输<br><strong>ANY</strong>：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差</p><p>spark.locality.wait，默认是3s</p><p>Spark在Driver上，对Application的每一个stage的task，进行分配之前，都会计算出每个task要计算的是哪个分片数据，RDD的某个partition；Spark的task分配算法，优先，会希望每个task正好分配到它要计算的数据所在的节点，这样的话，就不用在网络间传输数据；</p><p>但是可能task没有机会分配到它的数据所在的节点，因为可能那个节点的计算资源和计算能力都满了；所以呢，这种时候，通常来说，Spark会等待一段时间，默认情况下是3s钟（不是绝对的，还有很多种情况，对不同的本地化级别，都会去等待），到最后，实在是等待不了了，就会选择一个比较差的本地化级别，比如说，将task分配到靠它要计算的数据所在节点，比较近的一个节点，然后进行计算。</p><p>但是对于第二种情况，通常来说，肯定是要发生数据传输，task会通过其所在节点的BlockManager来获取数据，BlockManager发现自己本地没有数据，会通过一个getRemote()方法，通过TransferService（网络数据传输组件）从数据所在节点的BlockManager中，获取数据，通过网络传输回task所在节点。</p><p>对于我们来说，当然不希望是类似于第二种情况的了。最好的，当然是task和数据在一个节点上，直接从本地executor的BlockManager中获取数据，纯内存，或者带一点磁盘IO；如果要通过网络传输数据的话，那么实在是，性能肯定会下降的，大量网络传输，以及磁盘IO，都是性能的杀手。</p><p><strong>什么时候要调节这个参数？</strong></p><p>观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。<br>日志里面会显示，starting task。。。，PROCESS LOCAL、NODE LOCAL，观察大部分task的数据本地化级别。</p><p>如果大多都是PROCESS_LOCAL，那就不用调节了<br>如果是发现，好多的级别都是NODE_LOCAL、ANY，那么最好就去调节一下数据本地化的等待时长<br>调节完，应该是要反复调节，每次调节完以后，再来运行，观察日志<br>看看大部分的task的本地化级别有没有提升；看看，整个spark作业的运行时间有没有缩短</p><p>但是注意别本末倒置，本地化级别倒是提升了，但是因为大量的等待时长，spark作业的运行时间反而增加了，那就还是不要调节了。</p><p>spark.locality.wait，默认是3s；可以改成6s，10s</p><p>默认情况下，下面3个的等待时长，都是跟上面那个是一样的，都是3s</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.locality.wait.process//建议60s</span><br><span class="line">spark.locality.wait.node//建议30s</span><br><span class="line">spark.locality.wait.rack//建议20s</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （八）SparkCore的调优之开发调优：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。&lt;/p&gt;
&lt;p&gt;​        然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
</feed>
