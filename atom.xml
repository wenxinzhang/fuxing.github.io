<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>福星</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zhangfuxin.cn/"/>
  <updated>2019-09-06T00:12:24.698Z</updated>
  <id>http://zhangfuxin.cn/</id>
  
  <author>
    <name>福 星</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CM+CDH离线安装</title>
    <link href="http://zhangfuxin.cn/CDH-hadoop.html"/>
    <id>http://zhangfuxin.cn/CDH-hadoop.html</id>
    <published>2019-09-05T17:30:04.000Z</published>
    <updated>2019-09-06T00:12:24.698Z</updated>
    
    <content type="html"><![CDATA[<p>** CM+CDH离线安装：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Cloudera Manager可以轻松管理任何生产规模的Hadoop部署。通过直观的用户界面快速部署，配置和监控群集 - 完成滚动升级，备份和灾难恢复以及可定制警报。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1-Cloudera-简介"><a href="#1-1-Cloudera-简介" class="headerlink" title="1.1 Cloudera 简介"></a>1.1 Cloudera 简介</h2><h3 id="1-1-1Cloudera-简介"><a href="#1-1-1Cloudera-简介" class="headerlink" title="1.1.1Cloudera 简介"></a>1.1.1Cloudera 简介</h3><p>官网：<a href="https://www.cloudera.com/" target="_blank" rel="noopener">https://www.cloudera.com/</a></p><p>文档：<a href="https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_intro.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_intro.html</a></p><p>​        CDH是Apache Hadoop和相关项目中最完整，经过测试和最流行的发行版。CDH提供了Hadoop的核心元素 - 可扩展存储和分布式计算 - 以及基于Web的用户界面和重要的企业功能。CDH是Apache许可的开源软件，是唯一提供统一批处理，交互式SQL和交互式搜索以及基于角色的访问控制的Hadoop解决方案。</p><p>CDH提供：</p><ul><li><p>灵活性 - 存储任何类型的数据并使用各种不同的计算框架对其进行操作，包括批处理，交互式SQL，自由文本搜索，机器学习和统计计算。</p></li><li><p>集成 - 在完整的Hadoop平台上快速启动和运行，该平台可与各种硬件和软件解决方案配合使用。</p></li><li><p>安全 - 处理和控制敏感数据。</p></li><li><p>可扩展性 - 支持广泛的应用程序，并扩展和扩展它们以满足您的要求。</p></li><li><p>高可用性 - 充满信心地执行任务关键型业务任务。</p></li><li><p>兼容性 - 利用您现有的IT基础架构和投资。</p><p><img src="https://www.cloudera.com/documentation/enterprise/latest/images/cdh.png" alt="img"></p></li></ul><h3 id="1-1-2Hadoop起源"><a href="#1-1-2Hadoop起源" class="headerlink" title="1.1.2Hadoop起源"></a>1.1.2Hadoop起源</h3><p>​        2004年，Cutting和同为程序员出身的Mike Cafarella决定开发一款可以代替当时的主流搜索产品的开源搜索引擎，这个项目被命名为Nutch。Nutch基于Lucence实现的搜索引擎，能够从互联网上抓取网页数据。抓取来的海量数据的存储问题。但是，这些海量数据都是非结构化数据，不能存在关系型数据库里。如果连数据的存储和管理都解决不了化，就谈不上后续为用户提供搜索服务，包括通过算法去优化检索速度。那时的Cutting正处在Freelancer的生涯 中，所以他希望自己的项目能通过一种低开销的方式来构建网页中的大量算法。幸运的是，Google这时正好发布了一项研究报告，报告中介绍了两款Google为支持自家的搜索引擎而开发的软件平台。这两个平台一个是GFS（Google File System），用于存储不同设备所产生的海量数据；另一个是MapReduce，它运行在GFS之上，负责分布式大规模数据。</p><p>​        2006年cutting根据《Google File System》设计了Nutch的HDFS,hadoop distributed file system。</p><p>Hadoop最开始是nutch的子项目，目的是解决nutch的海量数据存储问题。在nutch 0.8版本之后，Hadoop独立处理，成为一个独立的项目。后来，又根据《Google MapReduce》设计了基于HDFS的MapRedce计算框架。</p><p>2006年之后，cutting带着Hadoop去了雅虎，当时有100多人的团队共同帮cutting完善hadoop。后来yahoo把Hadoop贡献了Apache。所以，现在Hadoop是Apache的顶级项目。</p><h2 id="2-1Hadoop搭建"><a href="#2-1Hadoop搭建" class="headerlink" title="2.1Hadoop搭建"></a>2.1Hadoop搭建</h2><p><strong>Hadoop的三种运行模式</strong> ：</p><ol><li><p>独立（本地）运行模式：无需任何守护进程，所有的程序都运行在同一个JVM上执行。在独立模式下调试MR程序非常高效方便。所以一般该模式主要是在学习或者开发阶段调试使用 。</p></li><li><p>伪分布式模式：  Hadoop守护进程运行在本地机器上，模拟一个小规模的集群，换句话说，可以配置一台机器的Hadoop集群,伪分布式是完全分布式的一个特例。</p></li><li><p>完全分布式模式：Hadoop守护进程运行在一个集群上。</p></li></ol><h2 id="3-1-单机伪分布模式"><a href="#3-1-单机伪分布模式" class="headerlink" title="3.1 单机伪分布模式"></a>3.1 单机伪分布模式</h2><p>​    只支持MapReduce，不支持HDFS。这种模式一般用于调试MapReduce任务用的。</p><h3 id="3-1-1-准备Linux环境，最低的工作内存1G"><a href="#3-1-1-准备Linux环境，最低的工作内存1G" class="headerlink" title="3.1.1 准备Linux环境，最低的工作内存1G"></a>3.1.1 准备Linux环境，最低的工作内存1G</h3><p>内容详见：Vmware安装Centos6.9文档</p><h3 id="3-1-2-关闭防火墙"><a href="#3-1-2-关闭防火墙" class="headerlink" title="3.1.2  关闭防火墙"></a>3.1.2  关闭防火墙</h3><p>临时关闭防火墙：service iptables stop</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br></pre></td></tr></table></figure><p>永久关闭防火墙：chkconfig iptables off </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font>永久修改防火墙需要重启，永久和临时同时执行，可以不用重启。</p><h3 id="3-1-3-配置主机名"><a href="#3-1-3-配置主机名" class="headerlink" title="3.1.3 配置主机名"></a>3.1.3 配置主机名</h3><p>查询主机名称：hostname</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname</span><br></pre></td></tr></table></figure><p>临时修改主机名：hostname  <strong><name></name></strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname &lt;name&gt;</span><br></pre></td></tr></table></figure><p>永久修改主机名：vim /etc/sysconfig/network</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><h3 id="3-1-4-配置hosts文件"><a href="#3-1-4-配置hosts文件" class="headerlink" title="3.1.4 配置hosts文件"></a>3.1.4 配置hosts文件</h3><p>执行: vim /etc/hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><ol><li>不要删除前两行内容。</li><li>IP在前，主机名在后。</li></ol><h3 id="3-1-5-配置免密码登录"><a href="#3-1-5-配置免密码登录" class="headerlink" title="3.1.5 配置免密码登录"></a>3.1.5 配置免密码登录</h3><h4 id="3-1-5-1-免密登陆原理"><a href="#3-1-5-1-免密登陆原理" class="headerlink" title="3.1.5.1 免密登陆原理"></a>3.1.5.1 免密登陆原理</h4><ol><li><p>A机器生成公钥和私钥</p></li><li><p>机器A发送自己的公钥到机器B，这个时候机器B有了机器A的公钥</p></li><li><p>机器B发送一个随机的字符串向机器A</p></li><li><p>机器A利用自己的私钥把字符串加密</p></li><li><p>机器A把加密后的字符串再次发送给机器B</p></li><li><p>机器B利用公钥解密字符串，如果和原来的一样，则OK。</p></li></ol><h4 id="3-1-5-1-免密登陆实现"><a href="#3-1-5-1-免密登陆实现" class="headerlink" title="3.1.5.1 免密登陆实现"></a>3.1.5.1 免密登陆实现</h4><ol><li><p>生成自己的公钥和私钥  ssh-keygen</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure></li><li><p>把生成的公钥copy到远程机器上 ssh-copy-id root@hadoop01</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id root@hadoop01</span><br></pre></td></tr></table></figure></li></ol><p><font color="red">注意：</font>如果是单机的伪分布式环境，自己节点也需要配置免密登录。</p><h3 id="3-1-6-安装和配置jdk"><a href="#3-1-6-安装和配置jdk" class="headerlink" title="3.1.6 安装和配置jdk"></a>3.1.6 安装和配置jdk</h3><ol><li><p>执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>在尾行添加 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASS_PATH</span><br></pre></td></tr></table></figure></li></ol><p>保存退出  :wq</p><ol start="3"><li><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>java -version 查看JDK版本信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-7-上传和安装hadoop"><a href="#3-1-7-上传和安装hadoop" class="headerlink" title="3.1.7 上传和安装hadoop"></a>3.1.7 上传和安装hadoop</h3><p>下载地址：<a href="http://hadoop.apache.org/releases.html" target="_blank" rel="noopener">http://hadoop.apache.org/releases.html</a></p><p><font color="red">注意：</font></p><p>source表示源码</p><p>binary表示二级制包（安装包）</p><h4 id="3-1-7-1-解压Hadoop文件包"><a href="#3-1-7-1-解压Hadoop文件包" class="headerlink" title="3.1.7.1 解压Hadoop文件包"></a>3.1.7.1 解压Hadoop文件包</h4><p>执行：tar -zxvf hadoop-2.7.1_64bit.tar.gz</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-2.7.1_64bit.tar.gz</span><br></pre></td></tr></table></figure><h4 id="3-1-7-2-Hadoop目录说明"><a href="#3-1-7-2-Hadoop目录说明" class="headerlink" title="3.1.7.2 Hadoop目录说明"></a>3.1.7.2 Hadoop目录说明</h4><p>bin目录：命令脚本</p><p>etc/hadoop:存放hadoop的配置文件</p><p>lib目录：hadoop运行的依赖jar包</p><p>sbin目录：启动和关闭hadoop等命令都在这里</p><p>libexec目录：存放的也是hadoop命令，但一般不常用</p><p><font color="red">注意：</font>最常用的就是bin和etc目录。</p><h3 id="3-1-8-配置hadoop配置文件"><a href="#3-1-8-配置hadoop配置文件" class="headerlink" title="3.1.8 配置hadoop配置文件"></a>3.1.8 配置hadoop配置文件</h3><p>Hadoop目录下<strong>/home/hadoop-2.7.1/etc/hadoop/</strong>目录下<strong>6个文件</strong></p><h4 id="3-1-8-1-hadoop-env-sh"><a href="#3-1-8-1-hadoop-env-sh" class="headerlink" title="3.1.8.1 hadoop-env.sh"></a>3.1.8.1 hadoop-env.sh</h4><p>执行：vim hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br></pre></td></tr></table></figure><p> 修改：修改java_home路径和hadoop_conf_dir 路径  25行  33行</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#25行</span><br><span class="line">export JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">#33行</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop-2.7.1/etc/hadoop</span><br></pre></td></tr></table></figure><p> 然后执行：source hadoop-env.sh编译文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source hadoop-env.sh</span><br></pre></td></tr></table></figure><h4 id="3-1-8-2-core-site-xml"><a href="#3-1-8-2-core-site-xml" class="headerlink" title="3.1.8.2 core-site.xml"></a>3.1.8.2 core-site.xml</h4><p>命令行执行：vim core-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim core-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hdfs的老大，namenode的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://tedu:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hadoop运行时产生文件的存放目录--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop-2.7.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-3-hdfs-site-xml"><a href="#3-1-8-3-hdfs-site-xml" class="headerlink" title="3.1.8.3 hdfs-site .xml"></a>3.1.8.3 hdfs-site .xml</h4><p>命令行执行：vim hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hdfs-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定hdfs保存数据副本的数量，包括自己，默认值是3--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--如果是伪分布模式，此值是1--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--设置hdfs的操作权限，false表示任何用户都可以在hdfs上操作文件--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-4-mapred-site-xml"><a href="#3-1-8-4-mapred-site-xml" class="headerlink" title="3.1.8.4 mapred-site.xml"></a>3.1.8.4 mapred-site.xml</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line"></span><br><span class="line">vim mapred-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定mapreduce运行在yarn上--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-5-yarn-site-xml"><a href="#3-1-8-5-yarn-site-xml" class="headerlink" title="3.1.8.5 yarn-site.xml"></a>3.1.8.5 yarn-site.xml</h4><p>命令行执行：vim yarn-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim yarn-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定yarn的老大 resoucemanager的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>tedu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--NodeManager获取数据的方式--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-  services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-6-slaves"><a href="#3-1-8-6-slaves" class="headerlink" title="3.1.8.6 slaves"></a>3.1.8.6 slaves</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves</span><br></pre></td></tr></table></figure><p><strong>修改主机名</strong></p><h3 id="3-1-9-配置hadoop的环境变量"><a href="#3-1-9-配置hadoop的环境变量" class="headerlink" title="3.1.9 配置hadoop的环境变量"></a>3.1.9 配置hadoop的环境变量</h3><ol><li><p>文件最后追加文件</p><p><strong>HADOOP_HOME=/home/hadoop-2.7.1</strong></p><p><strong>export HADOOP_HOME</strong></p></li><li><p>source /etc/profile 使更改的配置立即生效。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">HADOOP_HOME=/home/hadoop-2.7.1</span><br><span class="line">PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASSPATH HADOOP_HOME</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-10-格式化Namenode"><a href="#3-1-10-格式化Namenode" class="headerlink" title="3.1.10 格式化Namenode"></a>3.1.10 格式化Namenode</h3><p>执行：hdfs namenode -format</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>如果不好使，可以重启linux</p><p>当出现：successfully，证明格式化成功。</p><h3 id="3-1-11-启动Hadoop"><a href="#3-1-11-启动Hadoop" class="headerlink" title="3.1.11 启动Hadoop"></a>3.1.11 启动Hadoop</h3><p>在/home/hadoop-2.7.1/sbin目录下</p><p>执行:./start-all.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><h3 id="3-1-12-验证启动成功"><a href="#3-1-12-验证启动成功" class="headerlink" title="3.1.12 验证启动成功"></a>3.1.12 验证启动成功</h3><p>可以访问网址： <a href="http://192.168.220.128:50070" target="_blank" rel="noopener">http://192.168.220.128:50070</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** CM+CDH离线安装：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Cloudera Manager可以轻松管理任何生产规模的Hadoop部署。通过直观的用户界面快速部署，配置和监控群集 - 完成滚动升级，备份和灾难恢复以及可定制警报。&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://zhangfuxin.cn/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://zhangfuxin.cn/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop伪分布式搭建</title>
    <link href="http://zhangfuxin.cn/hadoop-single.html"/>
    <id>http://zhangfuxin.cn/hadoop-single.html</id>
    <published>2019-08-29T17:30:04.000Z</published>
    <updated>2019-08-29T17:28:47.951Z</updated>
    
    <content type="html"><![CDATA[<p>** Hadoop伪分布式搭建：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        大数据（Big Data），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。<br>​        大数据的定义是4Vs：数据量大、处理速度快、数据源多样、真实性。用中文简单描述就是大、快、多、真。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1Hadoop简介"><a href="#1-1Hadoop简介" class="headerlink" title="1.1Hadoop简介"></a>1.1Hadoop简介</h2><h3 id="1-1-1Hadoop创始人"><a href="#1-1-1Hadoop创始人" class="headerlink" title="1.1.1Hadoop创始人"></a>1.1.1Hadoop创始人</h3><p>​        1985年，<strong>Doug Cutting</strong>毕业于美国斯坦福大学。他并不是一开始就决心投身IT行业的，在大学时代的头两年，Cutting学习了诸如物理、地理等常规课程。因为学费的压力，Cutting开始意识到，自己必须学习一些更加实用、有趣的技能。这样，一方面可以帮助自己还清贷款，另一方面，也是为自己未来的生活做打算。因为斯坦福大学座落在IT行业的“圣地”硅谷，所以学习软件对年轻人来说是再自然不过的事情了。 1997年底，Cutting开始以每周两天的时间投入，在家里试着用Java把这个想法变成现实，不久之后，Lucene诞生了。作为第一个提供全文文本搜索的开源函数库，Lucene的伟大自不必多言。</p><p>Doug Cutting是<strong>Lucence,Nutch,Hadoop</strong>的创始人。</p><h3 id="1-1-2Hadoop起源"><a href="#1-1-2Hadoop起源" class="headerlink" title="1.1.2Hadoop起源"></a>1.1.2Hadoop起源</h3><p>​        2004年，Cutting和同为程序员出身的Mike Cafarella决定开发一款可以代替当时的主流搜索产品的开源搜索引擎，这个项目被命名为Nutch。Nutch基于Lucence实现的搜索引擎，能够从互联网上抓取网页数据。抓取来的海量数据的存储问题。但是，这些海量数据都是非结构化数据，不能存在关系型数据库里。如果连数据的存储和管理都解决不了化，就谈不上后续为用户提供搜索服务，包括通过算法去优化检索速度。那时的Cutting正处在Freelancer的生涯 中，所以他希望自己的项目能通过一种低开销的方式来构建网页中的大量算法。幸运的是，Google这时正好发布了一项研究报告，报告中介绍了两款Google为支持自家的搜索引擎而开发的软件平台。这两个平台一个是GFS（Google File System），用于存储不同设备所产生的海量数据；另一个是MapReduce，它运行在GFS之上，负责分布式大规模数据。</p><p>​        2006年cutting根据《Google File System》设计了Nutch的HDFS,hadoop distributed file system。</p><p>Hadoop最开始是nutch的子项目，目的是解决nutch的海量数据存储问题。在nutch 0.8版本之后，Hadoop独立处理，成为一个独立的项目。后来，又根据《Google MapReduce》设计了基于HDFS的MapRedce计算框架。</p><p>2006年之后，cutting带着Hadoop去了雅虎，当时有100多人的团队共同帮cutting完善hadoop。后来yahoo把Hadoop贡献了Apache。所以，现在Hadoop是Apache的顶级项目。</p><h2 id="2-1Hadoop搭建"><a href="#2-1Hadoop搭建" class="headerlink" title="2.1Hadoop搭建"></a>2.1Hadoop搭建</h2><p><strong>Hadoop的三种运行模式</strong> ：</p><ol><li><p>独立（本地）运行模式：无需任何守护进程，所有的程序都运行在同一个JVM上执行。在独立模式下调试MR程序非常高效方便。所以一般该模式主要是在学习或者开发阶段调试使用 。</p></li><li><p>伪分布式模式：  Hadoop守护进程运行在本地机器上，模拟一个小规模的集群，换句话说，可以配置一台机器的Hadoop集群,伪分布式是完全分布式的一个特例。</p></li><li><p>完全分布式模式：Hadoop守护进程运行在一个集群上。</p></li></ol><h2 id="3-1-单机伪分布模式"><a href="#3-1-单机伪分布模式" class="headerlink" title="3.1 单机伪分布模式"></a>3.1 单机伪分布模式</h2><p>​    只支持MapReduce，不支持HDFS。这种模式一般用于调试MapReduce任务用的。</p><h3 id="3-1-1-准备Linux环境，最低的工作内存1G"><a href="#3-1-1-准备Linux环境，最低的工作内存1G" class="headerlink" title="3.1.1 准备Linux环境，最低的工作内存1G"></a>3.1.1 准备Linux环境，最低的工作内存1G</h3><p>内容详见：Vmware安装Centos6.9文档</p><h3 id="3-1-2-关闭防火墙"><a href="#3-1-2-关闭防火墙" class="headerlink" title="3.1.2  关闭防火墙"></a>3.1.2  关闭防火墙</h3><p>临时关闭防火墙：service iptables stop</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br></pre></td></tr></table></figure><p>永久关闭防火墙：chkconfig iptables off </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font>永久修改防火墙需要重启，永久和临时同时执行，可以不用重启。</p><h3 id="3-1-3-配置主机名"><a href="#3-1-3-配置主机名" class="headerlink" title="3.1.3 配置主机名"></a>3.1.3 配置主机名</h3><p>查询主机名称：hostname</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname</span><br></pre></td></tr></table></figure><p>临时修改主机名：hostname  <strong><name></name></strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname &lt;name&gt;</span><br></pre></td></tr></table></figure><p>永久修改主机名：vim /etc/sysconfig/network</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><h3 id="3-1-4-配置hosts文件"><a href="#3-1-4-配置hosts文件" class="headerlink" title="3.1.4 配置hosts文件"></a>3.1.4 配置hosts文件</h3><p>执行: vim /etc/hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><ol><li>不要删除前两行内容。</li><li>IP在前，主机名在后。</li></ol><h3 id="3-1-5-配置免密码登录"><a href="#3-1-5-配置免密码登录" class="headerlink" title="3.1.5 配置免密码登录"></a>3.1.5 配置免密码登录</h3><h4 id="3-1-5-1-免密登陆原理"><a href="#3-1-5-1-免密登陆原理" class="headerlink" title="3.1.5.1 免密登陆原理"></a>3.1.5.1 免密登陆原理</h4><ol><li><p>A机器生成公钥和私钥</p></li><li><p>机器A发送自己的公钥到机器B，这个时候机器B有了机器A的公钥</p></li><li><p>机器B发送一个随机的字符串向机器A</p></li><li><p>机器A利用自己的私钥把字符串加密</p></li><li><p>机器A把加密后的字符串再次发送给机器B</p></li><li><p>机器B利用公钥解密字符串，如果和原来的一样，则OK。</p></li></ol><h4 id="3-1-5-1-免密登陆实现"><a href="#3-1-5-1-免密登陆实现" class="headerlink" title="3.1.5.1 免密登陆实现"></a>3.1.5.1 免密登陆实现</h4><ol><li><p>生成自己的公钥和私钥  ssh-keygen</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure></li><li><p>把生成的公钥copy到远程机器上 ssh-copy-id root@hadoop01</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id root@hadoop01</span><br></pre></td></tr></table></figure></li></ol><p><font color="red">注意：</font>如果是单机的伪分布式环境，自己节点也需要配置免密登录。</p><h3 id="3-1-6-安装和配置jdk"><a href="#3-1-6-安装和配置jdk" class="headerlink" title="3.1.6 安装和配置jdk"></a>3.1.6 安装和配置jdk</h3><ol><li><p>执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>在尾行添加 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASS_PATH</span><br></pre></td></tr></table></figure></li></ol><p>保存退出  :wq</p><ol start="3"><li><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>java -version 查看JDK版本信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-7-上传和安装hadoop"><a href="#3-1-7-上传和安装hadoop" class="headerlink" title="3.1.7 上传和安装hadoop"></a>3.1.7 上传和安装hadoop</h3><p>下载地址：<a href="http://hadoop.apache.org/releases.html" target="_blank" rel="noopener">http://hadoop.apache.org/releases.html</a></p><p><font color="red">注意：</font></p><p>source表示源码</p><p>binary表示二级制包（安装包）</p><h4 id="3-1-7-1-解压Hadoop文件包"><a href="#3-1-7-1-解压Hadoop文件包" class="headerlink" title="3.1.7.1 解压Hadoop文件包"></a>3.1.7.1 解压Hadoop文件包</h4><p>执行：tar -zxvf hadoop-2.7.1_64bit.tar.gz</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-2.7.1_64bit.tar.gz</span><br></pre></td></tr></table></figure><h4 id="3-1-7-2-Hadoop目录说明"><a href="#3-1-7-2-Hadoop目录说明" class="headerlink" title="3.1.7.2 Hadoop目录说明"></a>3.1.7.2 Hadoop目录说明</h4><p>bin目录：命令脚本</p><p>etc/hadoop:存放hadoop的配置文件</p><p>lib目录：hadoop运行的依赖jar包</p><p>sbin目录：启动和关闭hadoop等命令都在这里</p><p>libexec目录：存放的也是hadoop命令，但一般不常用</p><p><font color="red">注意：</font>最常用的就是bin和etc目录。</p><h3 id="3-1-8-配置hadoop配置文件"><a href="#3-1-8-配置hadoop配置文件" class="headerlink" title="3.1.8 配置hadoop配置文件"></a>3.1.8 配置hadoop配置文件</h3><p>Hadoop目录下<strong>/home/hadoop-2.7.1/etc/hadoop/</strong>目录下<strong>6个文件</strong></p><h4 id="3-1-8-1-hadoop-env-sh"><a href="#3-1-8-1-hadoop-env-sh" class="headerlink" title="3.1.8.1 hadoop-env.sh"></a>3.1.8.1 hadoop-env.sh</h4><p>执行：vim hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br></pre></td></tr></table></figure><p> 修改：修改java_home路径和hadoop_conf_dir 路径  25行  33行</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#25行</span><br><span class="line">export JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">#33行</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop-2.7.1/etc/hadoop</span><br></pre></td></tr></table></figure><p> 然后执行：source hadoop-env.sh编译文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source hadoop-env.sh</span><br></pre></td></tr></table></figure><h4 id="3-1-8-2-core-site-xml"><a href="#3-1-8-2-core-site-xml" class="headerlink" title="3.1.8.2 core-site.xml"></a>3.1.8.2 core-site.xml</h4><p>命令行执行：vim core-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim core-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hdfs的老大，namenode的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://tedu:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hadoop运行时产生文件的存放目录--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop-2.7.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-3-hdfs-site-xml"><a href="#3-1-8-3-hdfs-site-xml" class="headerlink" title="3.1.8.3 hdfs-site .xml"></a>3.1.8.3 hdfs-site .xml</h4><p>命令行执行：vim hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hdfs-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定hdfs保存数据副本的数量，包括自己，默认值是3--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--如果是伪分布模式，此值是1--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--设置hdfs的操作权限，false表示任何用户都可以在hdfs上操作文件--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-4-mapred-site-xml"><a href="#3-1-8-4-mapred-site-xml" class="headerlink" title="3.1.8.4 mapred-site.xml"></a>3.1.8.4 mapred-site.xml</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line"></span><br><span class="line">vim mapred-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定mapreduce运行在yarn上--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-5-yarn-site-xml"><a href="#3-1-8-5-yarn-site-xml" class="headerlink" title="3.1.8.5 yarn-site.xml"></a>3.1.8.5 yarn-site.xml</h4><p>命令行执行：vim yarn-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim yarn-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定yarn的老大 resoucemanager的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>tedu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--NodeManager获取数据的方式--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-  services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-6-slaves"><a href="#3-1-8-6-slaves" class="headerlink" title="3.1.8.6 slaves"></a>3.1.8.6 slaves</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves</span><br></pre></td></tr></table></figure><p><strong>修改主机名</strong></p><h3 id="3-1-9-配置hadoop的环境变量"><a href="#3-1-9-配置hadoop的环境变量" class="headerlink" title="3.1.9 配置hadoop的环境变量"></a>3.1.9 配置hadoop的环境变量</h3><ol><li><p>文件最后追加文件</p><p><strong>HADOOP_HOME=/home/hadoop-2.7.1</strong></p><p><strong>export HADOOP_HOME</strong></p></li><li><p>source /etc/profile 使更改的配置立即生效。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">HADOOP_HOME=/home/hadoop-2.7.1</span><br><span class="line">PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASSPATH HADOOP_HOME</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-10-格式化Namenode"><a href="#3-1-10-格式化Namenode" class="headerlink" title="3.1.10 格式化Namenode"></a>3.1.10 格式化Namenode</h3><p>执行：hdfs namenode -format</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>如果不好使，可以重启linux</p><p>当出现：successfully，证明格式化成功。</p><h3 id="3-1-11-启动Hadoop"><a href="#3-1-11-启动Hadoop" class="headerlink" title="3.1.11 启动Hadoop"></a>3.1.11 启动Hadoop</h3><p>在/home/hadoop-2.7.1/sbin目录下</p><p>执行:./start-all.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><h3 id="3-1-12-验证启动成功"><a href="#3-1-12-验证启动成功" class="headerlink" title="3.1.12 验证启动成功"></a>3.1.12 验证启动成功</h3><p>可以访问网址： <a href="http://192.168.220.128:50070" target="_blank" rel="noopener">http://192.168.220.128:50070</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Hadoop伪分布式搭建：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        大数据（Big Data），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。&lt;br&gt;​        大数据的定义是4Vs：数据量大、处理速度快、数据源多样、真实性。用中文简单描述就是大、快、多、真。&lt;/p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://zhangfuxin.cn/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://zhangfuxin.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>如何选购合适的电脑</title>
    <link href="http://zhangfuxin.cn/buy-computer.html"/>
    <id>http://zhangfuxin.cn/buy-computer.html</id>
    <published>2019-08-28T16:30:04.000Z</published>
    <updated>2019-08-28T17:10:49.787Z</updated>
    
    <content type="html"><![CDATA[<p>** 购买合适的电脑：** &lt;Excerpt in index | 首页摘要&gt;<br>随着时代的发展，人们生活水平的提高，计算机也成为了我们工作必不可少的生产力，办公，打游戏，看电影，购物等等。市面上电脑种类根据用途又可以分为很多类，台式机、笔记本、IPAD（平板）。购买一台适合自己的电脑工作时可以如虎添翼，电脑用着舒服，心情也会舒畅许多。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="买电脑主要需求"><a href="#买电脑主要需求" class="headerlink" title="买电脑主要需求"></a>买电脑主要需求</h2><ol><li>看电影，上网（购物）   </li><li>打游戏</li><li>办公（移动办公）</li><li>平面设计（CAD）</li><li>UI(影视剪辑)</li><li>编程</li><li>其他</li></ol><h2 id="电脑配置说明"><a href="#电脑配置说明" class="headerlink" title="电脑配置说明"></a>电脑配置说明</h2><p>目前电脑配置的CPU（绝对过剩），内存Win10最低要8个G，显卡要根据自己需求一般显卡基本够用，电脑最大的瓶颈都是在硬盘上，所以现在买电脑带不带固态硬盘是我首选的配置（我对固态硬盘定义最低要128G,512G固态才是标配，毕竟固态大小会影响到一定的读写速率，还有为了保证固态寿命做系统时会留出10%的空间不划分到分区中），至于买笔记本还是台式机需要根据不同应用场景来定。台式机性能肯定远超同价位笔记本，这个是毋庸置疑的。</p><h2 id="看电影，上网（购物）"><a href="#看电影，上网（购物）" class="headerlink" title="看电影，上网（购物）"></a>看电影，上网（购物）</h2><p>对于这方面需求的一般一女生居多，看电影上网，对电脑配置要求比较低的，一般普通双核CPU，AMD、酷睿i3都可以（最好是i5），内存8G就够了（win7的话4G就够，但是Win7现在不支持更新了）。要是女生最重要的是漂亮，这里推荐DELL或者HP相对性价比会比较合适。毕竟要是要以轻薄、美观为主。要是资金充足可以考虑各家品牌的超级本。要是父母的需求的话其实买笔记本或者台式机都可以。这里不推荐苹果笔记本，因为用苹果看电影会容易热，要是妹子是苹果控或者周边产品都是苹果产品，苹果笔记本也可在考虑之列。</p><h2 id="打游戏"><a href="#打游戏" class="headerlink" title="打游戏"></a>打游戏</h2><p>游戏主机两个最主要的要求配置和扩展性，主要是CPU和显卡，我们又称之为“双烧”，建议买台式机。要是需要便携的话，外星人品牌是一个不错的考虑，笔记本显卡最好不要超过GTX2070以上，也许你会问为什么不买笔记本GTX2080的本子，一方面是贵，价格会差很多。还有就是散热问题。为了更好体验还是台式机加水冷。</p><ul><li>一般的主流网游：i5或i7处理器，内存16G，中端显卡就可以了，硬盘128G固态+1T机械起</li><li>大型单机：i7或i9处理器（水冷），内存16-32G，，显卡中高端GTX1060起，要是玩刺客信条奥德赛GTX2080Ti不用犹豫，硬盘512三星固态+1T机械（最好在配置1T的固态，毕竟游戏不小）起</li><li>发烧友：i9处理器（水冷），内存32G-64G，显卡高端GTX2080或者是多显交火，硬盘512G（三星固态PRO系列）+1T固态</li></ul><h2 id="办公"><a href="#办公" class="headerlink" title="办公"></a>办公</h2><p>用于办公的大多是商务人士，对笔记本的性能要求一般，最主要的是便携性，各大品牌的超极本都很合适，还能衬托气质，最推荐的还是联想的thinkpad系列，没钱买个E系类（基本三年就会坏），要是有资金充裕T系列或者X系列是首选配置（尤其是X系列）。</p><h2 id="平面设计（CAD）"><a href="#平面设计（CAD）" class="headerlink" title="平面设计（CAD）"></a>平面设计（CAD）</h2><p>这个是专业领域的需求，对CPU、显卡和内存、显示器都较高，能好一点就好一点。    </p><h2 id="UI-影视剪辑"><a href="#UI-影视剪辑" class="headerlink" title="UI(影视剪辑)"></a>UI(影视剪辑)</h2><p>苹果的Macbookpro 16G，512SSD（固态太小用久了会后悔的），i7处理器 最为合适。没有比苹果更适合做平面设计的电脑。Windows系统和苹果系统没得比。</p><h2 id="编程"><a href="#编程" class="headerlink" title="编程"></a>编程</h2><p>苹果的Macbookpro 16G、512SSD、i7处理器。个人推荐MAC的笔记本做编程，一用就停不下来，会上瘾。Windows系统用来打游戏就好了。<br>推荐配置：Macbookpro 16G、i7处理器（i9也是阉割版没意义）、512SSD（固态真的不能太小，512G就不大，考虑到价格没办法）、最好是能带键盘灯、Air pods耳机还是要有一个的，用了就知道不亏。经济允许最好是配置一个IPAD PRO做分屏开发可以调高效率。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>IPAD我对它的定义就是一台游戏机，不建议用IPAD看电影（用久了手会麻）。因为我不做UI我也没有体会到那只笔的好处。</p><p>还有一个设备一点光要说一下就是亚马逊的Kindle，要是你经常看小说，或者是看英文，建议有一个（前期是你不是必须要纸质书）还是很方便的，尤其是书多了的时候。IPAD优势在于pdf文档做笔记。用了就会知道两个不一样。Kindle看电子书是生活品质提升的表现。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 购买合适的电脑：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;br&gt;随着时代的发展，人们生活水平的提高，计算机也成为了我们工作必不可少的生产力，办公，打游戏，看电影，购物等等。市面上电脑种类根据用途又可以分为很多类，台式机、笔记本、IPAD（平板）。购买一台适合自己的电脑工作时可以如虎添翼，电脑用着舒服，心情也会舒畅许多。&lt;/p&gt;
    
    </summary>
    
      <category term="others" scheme="http://zhangfuxin.cn/categories/others/"/>
    
    
      <category term="数码产品" scheme="http://zhangfuxin.cn/tags/%E6%95%B0%E7%A0%81%E4%BA%A7%E5%93%81/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （三）Spark之RDD</title>
    <link href="http://zhangfuxin.cn/2019-06-03-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%89%EF%BC%89Spark%E4%B9%8BRDD.html"/>
    <id>http://zhangfuxin.cn/2019-06-03-Spark学习之路 （三）Spark之RDD.html</id>
    <published>2019-06-03T02:30:04.000Z</published>
    <updated>2019-09-16T04:12:59.674Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （三）Spark之RDD：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （三）Spark之RDD</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、RDD的概述"><a href="#一、RDD的概述" class="headerlink" title="一、RDD的概述"></a>一、RDD的概述</h2><h3 id="1-1-什么是RDD"><a href="#1-1-什么是RDD" class="headerlink" title="1.1　什么是RDD"></a>1.1　什么是RDD</h3><p>​        <strong>RDD</strong>（Resilient Distributed Dataset）叫做<strong>弹性分布式数据集</strong>，<strong>是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。</strong>RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。</p><h3 id="1-2-RDD的属性"><a href="#1-2-RDD的属性" class="headerlink" title="1.2　RDD的属性"></a>1.2　RDD的属性</h3><p><a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala</a></p><blockquote><p> A list of partitions<br> A function for computing each split<br> A list of dependencies on other RDDs<br> Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<br> Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</p></blockquote><p>（1）一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</p><p>（2）一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</p><p>（3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p><p>（4）一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p><p>（5）一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421133911520-1150689001.png" alt="img"></p><p>其中hello.txt</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421134031551-1670646166.png" alt="img"></p><h2 id="二、RDD的创建方式"><a href="#二、RDD的创建方式" class="headerlink" title="二、RDD的创建方式"></a>二、RDD的创建方式</h2><h3 id="2-1-通过读取文件生成的"><a href="#2-1-通过读取文件生成的" class="headerlink" title="2.1　通过读取文件生成的"></a>2.1　通过读取文件生成的</h3><p>由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val file = sc.textFile(&quot;/spark/hello.txt&quot;)</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421134515478-653027491.png" alt="img"></p><h3 id="2-2-通过并行化的方式创建RDD"><a href="#2-2-通过并行化的方式创建RDD" class="headerlink" title="2.2　通过并行化的方式创建RDD"></a>2.2　通过并行化的方式创建RDD</h3><p>由一个已经存在的Scala集合创建。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> array = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">array: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(array)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">27</span>] at parallelize at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421134820158-111255712.png" alt="img"></p><h3 id="2-3-其他方式"><a href="#2-3-其他方式" class="headerlink" title="2.3　其他方式"></a>2.3　其他方式</h3><p>读取数据库等等其他的操作。也可以生成RDD。RDD转换为ParallelCollectionRDD。</p><h2 id="三、RDD编程API"><a href="#三、RDD编程API" class="headerlink" title="三、RDD编程API"></a>三、RDD编程API</h2><p><strong>Spark支持两个类型（算子）操作：Transformation和Action</strong></p><h3 id="3-1-Transformation"><a href="#3-1-Transformation" class="headerlink" title="3.1　Transformation"></a>3.1　Transformation</h3><p>​    主要做的是就是将一个已有的RDD生成另外一个RDD。Transformation具有<strong>lazy**</strong>特性(延迟加载)**。Transformation算子的代码不会真正被执行。只有当我们的程序里面遇到一个action算子的时候，代码才会真正的被执行。这种设计让Spark更加有效率地运行。</p><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds</a></p><p><strong>常用的Transformation</strong>：</p><table><thead><tr><th><strong>转换</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td><strong>map</strong>(func)</td><td>返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成</td></tr><tr><td><strong>filter</strong>(func)</td><td>返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成</td></tr><tr><td><strong>flatMap</strong>(func)</td><td>类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）</td></tr><tr><td><strong>mapPartitions</strong>(func)</td><td>类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]</td></tr><tr><td><strong>mapPartitionsWithIndex</strong>(func)</td><td>类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]</td></tr><tr><td><strong>sample</strong>(withReplacement, fraction, seed)</td><td>根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子</td></tr><tr><td><strong>union</strong>(otherDataset)</td><td>对源RDD和参数RDD求并集后返回一个新的RDD</td></tr><tr><td><strong>intersection</strong>(otherDataset)</td><td>对源RDD和参数RDD求交集后返回一个新的RDD</td></tr><tr><td><strong>distinct</strong>([numTasks]))</td><td>对源RDD进行去重后返回一个新的RDD</td></tr><tr><td><strong>groupByKey</strong>([numTasks])</td><td>在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD</td></tr><tr><td><strong>reduceByKey</strong>(func, [numTasks])</td><td>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置</td></tr><tr><td><strong>aggregateByKey</strong>(zeroValue)(seqOp, combOp, [numTasks])</td><td>先按分区聚合 再总的聚合   每次要跟初始值交流 例如：aggregateByKey(0)(<em>+</em>,<em>+</em>) 对k/y的RDD进行操作</td></tr><tr><td><strong>sortByKey</strong>([ascending], [numTasks])</td><td>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</td></tr><tr><td><strong>sortBy</strong>(func,[ascending], [numTasks])</td><td>与sortByKey类似，但是更灵活 第一个参数是根据什么排序  第二个是怎么排序 false倒序   第三个排序后分区数  默认与原RDD一样</td></tr><tr><td><strong>join</strong>(otherDataset, [numTasks])</td><td>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD  相当于内连接（求交集）</td></tr><tr><td><strong>cogroup</strong>(otherDataset, [numTasks])</td><td>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<v>,Iterable<w>))类型的RDD</w></v></td></tr><tr><td><strong>cartesian</strong>(otherDataset)</td><td>两个RDD的笛卡尔积  的成很多个K/V</td></tr><tr><td><strong>pipe</strong>(command, [envVars])</td><td>调用外部程序</td></tr><tr><td><strong>coalesce</strong>(numPartitions<strong>)</strong></td><td>重新分区 第一个参数是要分多少区，第二个参数是否shuffle 默认false  少分区变多分区 true   多分区变少分区 false</td></tr><tr><td><strong>repartition</strong>(numPartitions)</td><td>重新分区 必须shuffle  参数是要分多少区  少变多</td></tr><tr><td><strong>repartitionAndSortWithinPartitions</strong>(partitioner)</td><td>重新分区+排序  比先分区再排序效率高  对K/V的RDD进行操作</td></tr><tr><td><strong>foldByKey</strong>(zeroValue)(seqOp)</td><td>该函数用于K/V做折叠，合并处理 ，与aggregate类似   第一个括号的参数应用于每个V值  第二括号函数是聚合例如：<em>+</em></td></tr><tr><td><strong>combineByKey</strong></td><td>合并相同的key的值 rdd1.combineByKey(x =&gt; x, (a: Int, b: Int) =&gt; a + b, (m: Int, n: Int) =&gt; m + n)</td></tr><tr><td><strong>partitionBy**</strong>（partitioner）**</td><td>对RDD进行分区  partitioner是分区器 例如new HashPartition(2</td></tr><tr><td><strong>cache</strong></td><td>RDD缓存，可以避免重复计算从而减少时间，区别：cache内部调用了persist算子，cache默认就一个缓存级别MEMORY-ONLY ，而persist则可以选择缓存级别</td></tr><tr><td><strong>persist</strong></td><td></td></tr><tr><td></td><td></td></tr><tr><td><strong>Subtract**</strong>（rdd）**</td><td>返回前rdd元素不在后rdd的rdd</td></tr><tr><td><strong>leftOuterJoin</strong></td><td>leftOuterJoin类似于SQL中的左外关联left outer join，返回结果以前面的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可。</td></tr><tr><td><strong>rightOuterJoin</strong></td><td>rightOuterJoin类似于SQL中的有外关联right outer join，返回结果以参数中的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可</td></tr><tr><td>subtractByKey</td><td>substractByKey和基本转换操作中的subtract类似只不过这里是针对K的，返回在主RDD中出现，并且不在otherRDD中出现的元素</td></tr></tbody></table><h3 id="3-2-Action"><a href="#3-2-Action" class="headerlink" title="3.2　Action"></a>3.2　Action</h3><p>触发代码的运行，我们一段spark代码里面至少需要有一个action操作。</p><p><strong>常用的Action</strong>:</p><table><thead><tr><th><strong>动作</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td><strong>reduce</strong>(<em>func</em>)</td><td>通过func函数聚集RDD中的所有元素，这个功能必须是课交换且可并联的</td></tr><tr><td><strong>collect</strong>()</td><td>在驱动程序中，以数组的形式返回数据集的所有元素</td></tr><tr><td><strong>count</strong>()</td><td>返回RDD的元素个数</td></tr><tr><td><strong>first</strong>()</td><td>返回RDD的第一个元素（类似于take(1)）</td></tr><tr><td><strong>take</strong>(<em>n</em>)</td><td>返回一个由数据集的前n个元素组成的数组</td></tr><tr><td><strong>takeSample</strong>(<em>withReplacement</em>,<em>num</em>, [<em>seed</em>])</td><td>返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子</td></tr><tr><td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td><td></td></tr><tr><td><strong>saveAsTextFile</strong>(<em>path</em>)</td><td>将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本</td></tr><tr><td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td><td>将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。</td></tr><tr><td><strong>saveAsObjectFile</strong>(<em>path</em>)</td><td></td></tr><tr><td><strong>countByKey</strong>()</td><td>针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</td></tr><tr><td><strong>foreach</strong>(<em>func</em>)</td><td>在数据集的每一个元素上，运行函数func进行更新。</td></tr><tr><td><strong>aggregate</strong></td><td>先对分区进行操作，在总体操作</td></tr><tr><td><strong>reduceByKeyLocally</strong></td><td></td></tr><tr><td><strong>lookup</strong></td><td></td></tr><tr><td><strong>top</strong></td><td></td></tr><tr><td><strong>fold</strong></td><td></td></tr><tr><td><strong>foreachPartition</strong></td><td></td></tr></tbody></table><h3 id="3-3-Spark-WordCount代码编写"><a href="#3-3-Spark-WordCount代码编写" class="headerlink" title="3.3　Spark WordCount代码编写"></a>3.3　Spark WordCount代码编写</h3><p>使用maven进行项目构建</p><h4 id="（1）使用scala进行编写"><a href="#（1）使用scala进行编写" class="headerlink" title="（1）使用scala进行编写"></a>（1）使用scala进行编写</h4><p>查看官方网站，需要导入2个依赖包</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421144526496-1152731884.png" alt="img"></p><p>详细代码</p><p>SparkWordCountWithScala.scala</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkWordCountWithScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 如果这个参数不设置，默认认为你运行的是集群模式</span></span><br><span class="line"><span class="comment">      * 如果设置成local代表运行的是local模式</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    conf.setMaster(<span class="string">"local"</span>)</span><br><span class="line">    <span class="comment">//设置任务名</span></span><br><span class="line">    conf.setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">    <span class="comment">//创建SparkCore的程序入口</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//读取文件 生成RDD</span></span><br><span class="line">    <span class="keyword">val</span> file: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"E:\\hello.txt"</span>)</span><br><span class="line">    <span class="comment">//把每一行数据按照，分割</span></span><br><span class="line">    <span class="keyword">val</span> word: <span class="type">RDD</span>[<span class="type">String</span>] = file.flatMap(_.split(<span class="string">","</span>))</span><br><span class="line">    <span class="comment">//让每一个单词都出现一次</span></span><br><span class="line">    <span class="keyword">val</span> wordOne: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = word.map((_,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">//单词计数</span></span><br><span class="line">    <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.reduceByKey(_+_)</span><br><span class="line">    <span class="comment">//按照单词出现的次数 降序排序</span></span><br><span class="line">    <span class="keyword">val</span> sortRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordCount.sortBy(tuple =&gt; tuple._2,<span class="literal">false</span>)</span><br><span class="line">    <span class="comment">//将最终的结果进行保存</span></span><br><span class="line">    sortRdd.saveAsTextFile(<span class="string">"E:\\result"</span>)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421151823241-1753369845.png" alt="img"></p><h4 id="（2）使用java-jdk7进行编写"><a href="#（2）使用java-jdk7进行编写" class="headerlink" title="（2）使用java jdk7进行编写"></a>（2）使用java jdk7进行编写</h4><p>SparkWordCountWithJava7.java</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaPairRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaSparkContext</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.<span class="type">FlatMapFunction</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.<span class="type">Function2</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.<span class="type">PairFunction</span>;</span><br><span class="line"><span class="keyword">import</span> scala.<span class="type">Tuple2</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Arrays</span>;</span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Iterator</span>;</span><br><span class="line"></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SparkWordCountWithJava7</span> </span>&#123;</span><br><span class="line">    public static void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line">        <span class="type">SparkConf</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>();</span><br><span class="line">        conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">        conf.setAppName(<span class="string">"WordCount"</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> sc = <span class="keyword">new</span> <span class="type">JavaSparkContext</span>(conf);</span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; fileRdd = sc.textFile(<span class="string">"E:\\hello.txt"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; wordRDD = fileRdd.flatMap(<span class="keyword">new</span> <span class="type">FlatMapFunction</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Iterator</span>&lt;<span class="type">String</span>&gt; call(<span class="type">String</span> line) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="type">Arrays</span>.asList(line.split(<span class="string">","</span>)).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordOneRDD = wordRDD.mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">String</span>, <span class="type">String</span>, <span class="type">Integer</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; call(<span class="type">String</span> word) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(word, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordCountRDD = wordOneRDD.reduceByKey(<span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Integer</span>, <span class="type">Integer</span>, <span class="type">Integer</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Integer</span> call(<span class="type">Integer</span> i1, <span class="type">Integer</span> i2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> i1 + i2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; count2WordRDD = wordCountRDD.mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt;, <span class="type">Integer</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; sortRDD = count2WordRDD.sortByKey(<span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; resultRDD = sortRDD.mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt;, <span class="type">String</span>, <span class="type">Integer</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        resultRDD.saveAsTextFile(<span class="string">"E:\\result7"</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（3）使用java-jdk8进行编写"><a href="#（3）使用java-jdk8进行编写" class="headerlink" title="（3）使用java jdk8进行编写"></a>（3）使用java jdk8进行编写</h4><p>lambda表达式</p><p>SparkWordCountWithJava8.java</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaPairRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaSparkContext</span>;</span><br><span class="line"><span class="keyword">import</span> scala.<span class="type">Tuple2</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Arrays</span>;</span><br><span class="line"></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SparkWordCountWithJava8</span> </span>&#123;</span><br><span class="line">    public static void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line">        <span class="type">SparkConf</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>();</span><br><span class="line">        conf.setAppName(<span class="string">"WortCount"</span>);</span><br><span class="line">        conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> sc = <span class="keyword">new</span> <span class="type">JavaSparkContext</span>(conf);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; fileRDD = sc.textFile(<span class="string">"E:\\hello.txt"</span>);</span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; wordRdd = fileRDD.flatMap(line -&gt; <span class="type">Arrays</span>.asList(line.split(<span class="string">","</span>)).iterator());</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordOneRDD = wordRdd.mapToPair(word -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordCountRDD = wordOneRDD.reduceByKey((x, y) -&gt; x + y);</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; count2WordRDD = wordCountRDD.mapToPair(tuple -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1));</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; sortRDD = count2WordRDD.sortByKey(<span class="literal">false</span>);</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; resultRDD = sortRDD.mapToPair(tuple -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1));</span><br><span class="line">        resultRDD.saveAsTextFile(<span class="string">"E:\\result8"</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425153140543-8294264.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425153515149-1269337605.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425153556469-238789142.png" alt="img"></p><h2 id="四、RDD的宽依赖和窄依赖"><a href="#四、RDD的宽依赖和窄依赖" class="headerlink" title="四、RDD的宽依赖和窄依赖"></a>四、RDD的宽依赖和窄依赖</h2><h3 id="4-1-RDD依赖关系的本质内幕"><a href="#4-1-RDD依赖关系的本质内幕" class="headerlink" title="4.1　RDD依赖关系的本质内幕"></a>4.1　<strong>RDD依赖关系的本质内幕</strong></h3><p>由于RDD是粗粒度的操作数据集，每个Transformation操作都会生成一个新的RDD，所以RDD之间就会形成类似流水线的前后依赖关系；RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。如图所示显示了RDD之间的依赖关系。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425151022105-1121308065.png" alt="img"></p><p>从图中可知：</p><p><strong>窄依赖：</strong>是指每个父RDD的一个Partition最多被子RDD的一个Partition所使用，例如map、filter、union等操作都会产生窄依赖；（独生子女）</p><p><strong>宽依赖：</strong>是指一个父RDD的Partition会被多个子RDD的Partition所使用，例如groupByKey、reduceByKey、sortByKey等操作都会产生宽依赖；（超生）</p><p>需要特别说明的是对join操作有两种情况：</p><p>（1）图中左半部分join：如果两个RDD在进行join操作时，一个RDD的partition仅仅和另一个RDD中已知个数的Partition进行join，那么这种类型的join操作就是窄依赖，例如图1中左半部分的join操作(join with inputs co-partitioned)；</p><p>（2）图中右半部分join：其它情况的join操作就是宽依赖,例如图1中右半部分的join操作(join with inputs not co-partitioned)，由于是需要父RDD的所有partition进行join的转换，这就涉及到了shuffle，因此这种类型的join操作也是宽依赖。</p><p>总结：</p><blockquote><p>在这里我们是从父RDD的partition被使用的个数来定义窄依赖和宽依赖，因此可以用一句话概括下：如果父RDD的一个Partition被子RDD的一个Partition所使用就是窄依赖，否则的话就是宽依赖。因为是确定的partition数量的依赖关系，所以RDD之间的依赖关系就是窄依赖；由此我们可以得出一个推论：即窄依赖不仅包含一对一的窄依赖，还包含一对固定个数的窄依赖。</p><p>一对固定个数的窄依赖的理解：即子RDD的partition对父RDD依赖的Partition的数量不会随着RDD数据规模的改变而改变；换句话说，无论是有100T的数据量还是1P的数据量，在窄依赖中，子RDD所依赖的父RDD的partition的个数是确定的，而宽依赖是shuffle级别的，数据量越大，那么子RDD所依赖的父RDD的个数就越多，从而子RDD所依赖的父RDD的partition的个数也会变得越来越多。</p></blockquote><h3 id="4-2-依赖关系下的数据流视图"><a href="#4-2-依赖关系下的数据流视图" class="headerlink" title="4.2　依赖关系下的数据流视图"></a>4.2　<strong>依赖关系下的数据流视图</strong></h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425151747136-185909749.png" alt="img"></p><p>在spark中，会根据RDD之间的依赖关系将DAG图（有向无环图）划分为不同的阶段，对于窄依赖，由于partition依赖关系的确定性，partition的转换处理就可以在同一个线程里完成，窄依赖就被spark划分到同一个stage中，而对于宽依赖，只能等父RDD shuffle处理完成后，下一个stage才能开始接下来的计算。</p><p><strong>因此spark划分stage的整体思路是</strong>：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。因此在图2中RDD C,RDD D,RDD E,RDDF被构建在一个stage中,RDD A被构建在一个单独的Stage中,而RDD B和RDD G又被构建在同一个stage中。</p><p>在spark中，Task的类型分为2种：<strong>ShuffleMapTask</strong>和<strong>ResultTask</strong>；</p><p>简单来说，DAG的最后一个阶段会为每个结果的partition生成一个ResultTask，即每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！而其余所有阶段都会生成ShuffleMapTask；之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中；也就是说上图中的stage1和stage2相当于mapreduce中的Mapper,而ResultTask所代表的stage3就相当于mapreduce中的reducer。</p><p>在之前动手操作了一个wordcount程序，因此可知，Hadoop中MapReduce操作中的Mapper和Reducer在spark中的基本等量算子是map和reduceByKey;不过区别在于：Hadoop中的MapReduce天生就是排序的；而reduceByKey只是根据Key进行reduce，但spark除了这两个算子还有其他的算子；因此从这个意义上来说，Spark比Hadoop的计算算子更为丰富。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （三）Spark之RDD：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （三）Spark之RDD&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler</title>
    <link href="http://zhangfuxin.cn/2019-06-02-Hadoop%20%E7%9A%84%E4%B8%89%E7%A7%8D%E8%B0%83%E5%BA%A6%E5%99%A8FIFO%E3%80%81Capacity%20Scheduler%E3%80%81Fair%20Scheduler.html"/>
    <id>http://zhangfuxin.cn/2019-06-02-Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler.html</id>
    <published>2019-06-02T05:30:04.000Z</published>
    <updated>2019-09-16T02:59:42.540Z</updated>
    
    <content type="html"><![CDATA[<p>** Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p><strong>目前Hadoop有三种比较流行的资源调度器：FIFO 、Capacity Scheduler、Fair Scheduler。目前hadoop2.7默认使用的是Capacity Scheduler容量调度器。</strong></p><h3 id="一、FIFO（先入先出调度器）"><a href="#一、FIFO（先入先出调度器）" class="headerlink" title="一、FIFO（先入先出调度器）"></a>一、FIFO（先入先出调度器）</h3><p>hadoop1.x使用的默认调度器就是FIFO。FIFO采用队列方式将一个一个job任务按照时间先后顺序进行服务。比如排在最前面的job需要若干maptask和若干reducetask，当发现有空闲的服务器节点就分配给这个job，直到job执行完毕。</p><p><img src="https://img-blog.csdn.net/20180907181312127?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hmd2VhdGhlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h3 id="二、Capacity-Scheduler（容量调度器）"><a href="#二、Capacity-Scheduler（容量调度器）" class="headerlink" title="二、Capacity Scheduler（容量调度器）"></a>二、Capacity Scheduler（容量调度器）</h3><p>hadoop2.x使用的默认调度器是Capacity Scheduler。</p><p>1、支持多个队列，每个队列可配置一定量的资源，每个采用FIFO的方式调度。</p><p>2、为了防止同一个用户的job任务独占队列中的资源，调度器会对同一用户提交的job任务所占资源进行限制。</p><p>3、分配新的job任务时，首先计算每个队列中正在运行task个数与其队列应该分配的资源量做比值，然后选择比值最小的队列。比如如图队列A15个task，20%资源量，那么就是15%0.2=70，队列B是25%0.5=50 ，队列C是25%0.3=80.33 。所以选择最小值队列B。</p><p>4、其次，按照job任务的优先级和时间顺序，同时要考虑到用户的资源量和内存的限制，对队列中的job任务进行排序执行。</p><p>5、多个队列同时按照任务队列内的先后顺序一次执行。例如下图中job11、job21、job31分别在各自队列中顺序比较靠前，三个任务就同时执行。</p><p><img src="https://img-blog.csdn.net/20180907183145655?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hmd2VhdGhlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h3 id="三、Fair-Scheduler（公平调度器）"><a href="#三、Fair-Scheduler（公平调度器）" class="headerlink" title="三、Fair Scheduler（公平调度器）"></a>三、Fair Scheduler（公平调度器）</h3><p>1、支持多个队列，每个队列可以配置一定的资源，每个队列中的job任务公平共享其所在队列的所有资源。</p><p>2、队列中的job任务都是按照优先级分配资源，优先级越高分配的资源越多，但是为了确保公平每个job任务都会分配到资源。优先级是根据每个job任务的理想获取资源量减去实际获取资源量的差值决定的，差值越大优先级越高。</p><p><img src="https://img-blog.csdn.net/20180909212500326?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hmd2VhdGhlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>原文链接：<a href="https://blog.csdn.net/xiaomage510/article/details/82500067" target="_blank" rel="noopener">https://blog.csdn.net/xiaomage510/article/details/82500067</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler&lt;/p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://zhangfuxin.cn/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://zhangfuxin.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Spark调度模式-FIFO和FAIR</title>
    <link href="http://zhangfuxin.cn/2019-06-02-Spark%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%BC%8F-FIFO%E5%92%8CFAIR.html"/>
    <id>http://zhangfuxin.cn/2019-06-02-Spark调度模式-FIFO和FAIR.html</id>
    <published>2019-06-02T05:20:04.000Z</published>
    <updated>2019-09-16T03:25:31.638Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark调度模式-FIFO和FAIR：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark调度模式-FIFO和FAIR</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>　　<strong>Spark中的调度模式主要有两种：FIFO和FAIR。</strong>默认情况下Spark的调度模式是FIFO（先进先出），谁先提交谁先执行，后面的任务需要等待前面的任务执行。而FAIR（公平调度）模式支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。对这两种调度模式的具体实现，接下来会根据spark-1.6.0的源码来进行详细的分析。使用哪种调度器由参数spark.scheduler.mode来设置，可选的参数有FAIR和FIFO，默认是FIFO。</p><h2 id="一、源码入口"><a href="#一、源码入口" class="headerlink" title="一、源码入口"></a>一、源码入口</h2><p>　　在Scheduler模块中，当Stage划分好，然后提交Task的过程中，会进入TaskSchedulerImpl#submitTasks方法。</p><blockquote><p>schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)   </p><p>//目前支持FIFO和FAIR两种调度策略。</p></blockquote><p>在上面代码中有一个schedulableBuilder对象，这个对象在TaskSchedulerImpl类中的定义及实现可以参考下面这段源代码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> schedulableBuilder: <span class="type">SchedulableBuilder</span> = <span class="literal">null</span></span><br><span class="line">...</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(backend: <span class="type">SchedulerBackend</span>) &#123;</span><br><span class="line">    <span class="keyword">this</span>.backend = backend</span><br><span class="line">    <span class="comment">// temporarily set rootPool name to empty</span></span><br><span class="line">    rootPool = <span class="keyword">new</span> <span class="type">Pool</span>(<span class="string">""</span>, schedulingMode, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    schedulableBuilder = &#123;</span><br><span class="line">      schedulingMode <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">SchedulingMode</span>.<span class="type">FIFO</span> =&gt;</span><br><span class="line">          <span class="keyword">new</span> <span class="type">FIFOSchedulableBuilder</span>(rootPool)  <span class="comment">//rootPool包含了一组TaskSetManager</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">SchedulingMode</span>.<span class="type">FAIR</span> =&gt;</span><br><span class="line">          <span class="keyword">new</span> <span class="type">FairSchedulableBuilder</span>(rootPool, conf)  <span class="comment">//rootPool包含了一组Pool树，这棵树的叶子节点都是TaskSetManager</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    schedulableBuilder.buildPools() <span class="comment">//在FIFO中的实现是空</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>　　根据用户配置的SchedulingMode决定是生成FIFOSchedulableBuilder还是生成FairSchedulableBuilder类型的schedulableBuilder对象。<br>　 <img src="https://img-blog.csdn.net/20160528143551855" alt="SchedulableBuilder继承关系" style="zoom:150%;"></p><p>　　在生成schedulableBuilder后，调用其buildPools方法生成调度池。 调度模式由配置参数spark.scheduler.mode（默认值为FIFO）来确定。 两种模式的调度逻辑图如下：<br>　<img src="https://img-blog.csdn.net/20160528181016610" alt="调度模式逻辑图" style="zoom:150%;"></p><h2 id="二、FIFOSchedulableBuilder"><a href="#二、FIFOSchedulableBuilder" class="headerlink" title="二、FIFOSchedulableBuilder"></a>二、FIFOSchedulableBuilder</h2><p>　　FIFO的rootPool包含一组TaskSetManager。从上面的类继承图中看出在FIFOSchedulableBuilder中有两个方法：</p><h3 id="1、buildPools"><a href="#1、buildPools" class="headerlink" title="1、buildPools"></a>1、buildPools</h3><p>实现为空:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildPools</span></span>() &#123;</span><br><span class="line">    <span class="comment">// nothing</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>所以，对于FIFO模式，获取到schedulableBuilder对象后，在调用buildPools方法后，不做任何操作。</p><h3 id="2、addTaskSetManager"><a href="#2、addTaskSetManager" class="headerlink" title="2、addTaskSetManager"></a>2、addTaskSetManager</h3><p>　　该方法将TaskSetManager装载到rootPool中。直接调用的方法是Pool#addSchedulable()。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">addTaskSetManager</span></span>(manager: <span class="type">Schedulable</span>, properties: <span class="type">Properties</span>) &#123;</span><br><span class="line">  rootPool.addSchedulable(manager)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Pool#addSchedulable()方法：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> schedulableQueue = <span class="keyword">new</span> <span class="type">ConcurrentLinkedQueue</span>[<span class="type">Schedulable</span>]</span><br><span class="line">...</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">addSchedulable</span></span>(schedulable: <span class="type">Schedulable</span>) &#123;</span><br><span class="line">    require(schedulable != <span class="literal">null</span>)</span><br><span class="line">    schedulableQueue.add(schedulable)</span><br><span class="line">    schedulableNameToSchedulable.put(schedulable.name, schedulable)</span><br><span class="line">    schedulable.parent = <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>将该TaskSetManager加入到调度队列schedulableQueue中。</p><h2 id="三、FairSchedulableBuilder"><a href="#三、FairSchedulableBuilder" class="headerlink" title="三、FairSchedulableBuilder"></a>三、FairSchedulableBuilder</h2><p>　　FAIR的rootPool中包含一组Pool，在Pool中包含了TaskSetManager。</p><h3 id="1、buildPools-1"><a href="#1、buildPools-1" class="headerlink" title="1、buildPools"></a>1、buildPools</h3><p>　　在该方法中，会读取配置文件，按照配置文件中的配置参数调用buildFairSchedulerPool生成配置的调度池，以及调用buildDefaultPool生成默认调度池。<br>　　默认情况下FAIR模式的配置文件是位于SPARK_HOME/conf/fairscheduler.xml文件，也可以通过参数spark.scheduler.allocation.file设置用户自定义配置文件。<br>spark中提供的fairscheduler.xml模板如下所示：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">allocations</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pool</span> <span class="attr">name</span>=<span class="string">"production"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">schedulingMode</span>&gt;</span>FAIR<span class="tag">&lt;/<span class="name">schedulingMode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">weight</span>&gt;</span>1<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minShare</span>&gt;</span>2<span class="tag">&lt;/<span class="name">minShare</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">pool</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pool</span> <span class="attr">name</span>=<span class="string">"test"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">schedulingMode</span>&gt;</span>FIFO<span class="tag">&lt;/<span class="name">schedulingMode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">weight</span>&gt;</span>2<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minShare</span>&gt;</span>3<span class="tag">&lt;/<span class="name">minShare</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">pool</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">allocations</span>&gt;</span></span><br></pre></td></tr></table></figure><p>参数含义：<br>（1）name: 该调度池的名称，可根据该参数使用指定pool，入sc.setLocalProperty(“spark.scheduler.pool”, “test”)<br>（2）weight: 该调度池的权重，各调度池根据该参数分配系统资源。每个调度池得到的资源数为weight / sum(weight)，weight为2的分配到的资源为weight为1的两倍。<br>（3）minShare: 该调度池需要的最小资源数（CPU核数）。fair调度器首先会尝试为每个调度池分配最少minShare资源，然后剩余资源才会按照weight大小继续分配。<br>（4）schedulingMode: 该调度池内的调度模式。</p><h3 id="2、buildFairSchedulerPool"><a href="#2、buildFairSchedulerPool" class="headerlink" title="2、buildFairSchedulerPool"></a>2、buildFairSchedulerPool</h3><p>　　从上面的配置文件可以看到，每一个调度池有一个name属性指定名字，然后在该pool中可以设置其schedulingMode(可为空，默认为FIFO), weight(可为空，默认值是1), 以及minShare(可为空，默认值是0)参数。然后使用这些参数生成一个Pool对象，把该pool对象放入rootPool中。入下所示：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pool = <span class="keyword">new</span> <span class="type">Pool</span>(poolName, schedulingMode, minShare, weight)</span><br><span class="line">rootPool.addSchedulable(pool)</span><br></pre></td></tr></table></figure><h3 id="3、buildDefaultPool"><a href="#3、buildDefaultPool" class="headerlink" title="3、buildDefaultPool"></a>3、buildDefaultPool</h3><p>　　如果如果配置文件中没有设置一个name为default的pool，系统才会自动生成一个使用默认参数生成的pool对象。各项参数的默认值在buildFairSchedulerPool中有提到。</p><h3 id="4、addTaskSetManager"><a href="#4、addTaskSetManager" class="headerlink" title="4、addTaskSetManager"></a>4、addTaskSetManager</h3><p>　　这一段逻辑中是把配置文件中的pool，或者default pool放入rootPool中，然后把TaskSetManager存入rootPool对应的子pool。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">addTaskSetManager</span></span>(manager: <span class="type">Schedulable</span>, properties: <span class="type">Properties</span>) &#123;</span><br><span class="line">   <span class="keyword">var</span> poolName = <span class="type">DEFAULT_POOL_NAME</span></span><br><span class="line">   <span class="keyword">var</span> parentPool = rootPool.getSchedulableByName(poolName)</span><br><span class="line">   <span class="keyword">if</span> (properties != <span class="literal">null</span>) &#123;</span><br><span class="line">     poolName = properties.getProperty(<span class="type">FAIR_SCHEDULER_PROPERTIES</span>, <span class="type">DEFAULT_POOL_NAME</span>)</span><br><span class="line">     parentPool = rootPool.getSchedulableByName(poolName)</span><br><span class="line">     <span class="keyword">if</span> (parentPool == <span class="literal">null</span>) &#123;</span><br><span class="line">       <span class="comment">// we will create a new pool that user has configured in app</span></span><br><span class="line">       <span class="comment">// instead of being defined in xml file</span></span><br><span class="line">       parentPool = <span class="keyword">new</span> <span class="type">Pool</span>(poolName, <span class="type">DEFAULT_SCHEDULING_MODE</span>,</span><br><span class="line">         <span class="type">DEFAULT_MINIMUM_SHARE</span>, <span class="type">DEFAULT_WEIGHT</span>)</span><br><span class="line">       rootPool.addSchedulable(parentPool)</span><br><span class="line">       logInfo(<span class="string">"Created pool %s, schedulingMode: %s, minShare: %d, weight: %d"</span>.format(</span><br><span class="line">         poolName, <span class="type">DEFAULT_SCHEDULING_MODE</span>, <span class="type">DEFAULT_MINIMUM_SHARE</span>, <span class="type">DEFAULT_WEIGHT</span>))</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   parentPool.addSchedulable(manager)</span><br><span class="line">   logInfo(<span class="string">"Added task set "</span> + manager.name + <span class="string">" tasks to pool "</span> + poolName)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="5、FAIR调度池使用方法"><a href="#5、FAIR调度池使用方法" class="headerlink" title="5、FAIR调度池使用方法"></a>5、FAIR调度池使用方法</h3><p>　　在Spark-1.6.1官方文档中写道：</p><blockquote><p>如果不加设置，jobs会提交到default调度池中。由于调度池的使用是Thread级别的，只能通过具体的SparkContext来设置local属性（即无法在配置文件中通过参数spark.scheduler.pool来设置，因为配置文件中的参数会被加载到SparkConf对象中）。所以需要使用指定调度池的话，需要在具体代码中通过SparkContext对象sc来按照如下方法进行设置：<br>sc.setLocalProperty(“spark.scheduler.pool”, “test”)<br>设置该参数后，在该thread中提交的所有job都会提交到test Pool中。<br>如果接下来不再需要使用到该test调度池，<br>sc.setLocalProperty(“spark.scheduler.pool”, null)</p></blockquote><h2 id="四、FIFO和FAIR的调度顺序"><a href="#四、FIFO和FAIR的调度顺序" class="headerlink" title="四、FIFO和FAIR的调度顺序"></a>四、FIFO和FAIR的调度顺序</h2><p>这里必须提到的一个类是上面提到的Pool，在这个类中实现了不同调度模式的调度算法。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> taskSetSchedulingAlgorithm: <span class="type">SchedulingAlgorithm</span> = &#123;</span><br><span class="line">  schedulingMode <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">SchedulingMode</span>.<span class="type">FAIR</span> =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">FairSchedulingAlgorithm</span>()</span><br><span class="line">    <span class="keyword">case</span> <span class="type">SchedulingMode</span>.<span class="type">FIFO</span> =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">FIFOSchedulingAlgorithm</span>()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>FIFO模式的算法类是FIFOSchedulingAlgorithm，FAIR模式的算法实现类是FairSchedulingAlgorithm。</p><p>　　接下来的两节中comparator方法传入参数Schedulable类型是一个trait，具体实现主要有两个：1，Pool；2，TaskSetManager。与最前面那个调度模式的逻辑图相对应。</p><h3 id="1、FIFO模式的调度算法FIFOSchedulingAlgorithm"><a href="#1、FIFO模式的调度算法FIFOSchedulingAlgorithm" class="headerlink" title="1、FIFO模式的调度算法FIFOSchedulingAlgorithm"></a>1、FIFO模式的调度算法FIFOSchedulingAlgorithm</h3><p>在这个类里面，主要逻辑是一个comparator方法。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">comparator</span></span>(s1: <span class="type">Schedulable</span>, s2: <span class="type">Schedulable</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> priority1 = s1.priority   <span class="comment">//实际上是Job ID</span></span><br><span class="line">  <span class="keyword">val</span> priority2 = s2.priority</span><br><span class="line">  <span class="keyword">var</span> res = math.signum(priority1 - priority2)</span><br><span class="line">  <span class="keyword">if</span> (res == <span class="number">0</span>) &#123; <span class="comment">//如果Job ID相同，就比较Stage ID</span></span><br><span class="line">    <span class="keyword">val</span> stageId1 = s1.stageId</span><br><span class="line">    <span class="keyword">val</span> stageId2 = s2.stageId</span><br><span class="line">    res = math.signum(stageId1 - stageId2)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (res &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果有两个调度任务s1和s2，首先获得两个任务的priority，在FIFO中该优先级实际上是Job ID。首先比较两个任务的Job ID，如果priority1比priority2小，那么返回true，表示s1的优先级比s2的高。我们知道Job ID是顺序生成的，先生成的Job ID比较小，所以先提交的job肯定比后提交的job先执行。但是如果是同一个job的不同任务，接下来就比较各自的Stage ID，类似于比较Job ID，Stage ID小的优先级高。</p><h3 id="2、FAIR模式的调度算法FairSchedulingAlgorithm"><a href="#2、FAIR模式的调度算法FairSchedulingAlgorithm" class="headerlink" title="2、FAIR模式的调度算法FairSchedulingAlgorithm"></a>2、FAIR模式的调度算法FairSchedulingAlgorithm</h3><p>　　这个类中的comparator方法源代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">comparator</span></span>(s1: <span class="type">Schedulable</span>, s2: <span class="type">Schedulable</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">   <span class="keyword">val</span> minShare1 = s1.minShare <span class="comment">//在这里share理解成份额，即每个调度池要求的最少cpu核数</span></span><br><span class="line">   <span class="keyword">val</span> minShare2 = s2.minShare</span><br><span class="line">   <span class="keyword">val</span> runningTasks1 = s1.runningTasks <span class="comment">// 该Pool或者TaskSetManager中正在运行的任务数</span></span><br><span class="line">   <span class="keyword">val</span> runningTasks2 = s2.runningTasks</span><br><span class="line">   <span class="keyword">val</span> s1Needy = runningTasks1 &lt; minShare1 <span class="comment">// 如果正在运行任务数比该调度池最小cpu核数要小</span></span><br><span class="line">   <span class="keyword">val</span> s2Needy = runningTasks2 &lt; minShare2</span><br><span class="line">   <span class="keyword">val</span> minShareRatio1 = runningTasks1.toDouble / math.max(minShare1, <span class="number">1.0</span>).toDouble</span><br><span class="line">   <span class="keyword">val</span> minShareRatio2 = runningTasks2.toDouble / math.max(minShare2, <span class="number">1.0</span>).toDouble</span><br><span class="line">   <span class="keyword">val</span> taskToWeightRatio1 = runningTasks1.toDouble / s1.weight.toDouble</span><br><span class="line">   <span class="keyword">val</span> taskToWeightRatio2 = runningTasks2.toDouble / s2.weight.toDouble</span><br><span class="line">   <span class="keyword">var</span> compare: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">   <span class="keyword">if</span> (s1Needy &amp;&amp; !s2Needy) &#123;</span><br><span class="line">     <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!s1Needy &amp;&amp; s2Needy) &#123;</span><br><span class="line">     <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (s1Needy &amp;&amp; s2Needy) &#123;</span><br><span class="line">     compare = minShareRatio1.compareTo(minShareRatio2)</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     compare = taskToWeightRatio1.compareTo(taskToWeightRatio2)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (compare &lt; <span class="number">0</span>) &#123;</span><br><span class="line">     <span class="literal">true</span></span><br><span class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (compare &gt; <span class="number">0</span>) &#123;</span><br><span class="line">     <span class="literal">false</span></span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     s1.name &lt; s2.name</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>　　minShare对应fairscheduler.xml配置文件中的minShare属性。<br>（1）如果s1所在Pool或者TaskSetManager中运行状态的task数量比minShare小，s2所在Pool或者TaskSetManager中运行状态的task数量比minShare大，那么s1会优先调度。反之，s2优先调度。<br>（2）如果s1和s2所在Pool或者TaskSetManager中运行状态的task数量都比各自minShare小，那么minShareRatio小的优先被调度。<br>minShareRatio是运行状态task数与minShare的比值，即相对来说minShare使用较少的先被调度。<br>（3）如果minShareRatio相同，那么最后比较各自Pool的名字。</p><p>原文链接：<a href="https://blog.csdn.net/dabokele/article/details/51526048" target="_blank" rel="noopener">https://blog.csdn.net/dabokele/article/details/51526048</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark调度模式-FIFO和FAIR：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark调度模式-FIFO和FAIR&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中parallelize函数和makeRDD函数的区别</title>
    <link href="http://zhangfuxin.cn/2019-06-02-Spark%E4%B8%ADparallelize%E5%87%BD%E6%95%B0%E5%92%8CmakeRDD%E5%87%BD%E6%95%B0%E7%9A%84%E5%8C%BA%E5%88%AB.html"/>
    <id>http://zhangfuxin.cn/2019-06-02-Spark中parallelize函数和makeRDD函数的区别.html</id>
    <published>2019-06-02T03:30:04.000Z</published>
    <updated>2019-09-16T01:39:52.469Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark中parallelize函数和makeRDD函数的区别：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark中parallelize函数和makeRDD函数的区别</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>我们知道，在<a href="https://www.iteblog.com/archives/tag/spark/" target="_blank" rel="noopener">Spark</a>中创建RDD的创建方式大概可以分为三种：</p><p>（1）、从集合中创建RDD；</p><p>（2）、从外部存储创建RDD；</p><p>（3）、从其他RDD创建。</p><p>　　而从集合中创建RDD，<a href="https://www.iteblog.com/archives/tag/spark/" target="_blank" rel="noopener">Spark</a>主要提供了两中函数：parallelize和makeRDD。我们可以先看看这两个函数的声明：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p>　　我们可以从上面看出makeRDD有两种实现，而且第一个makeRDD函数接收的参数和parallelize完全一致。其实第一种makeRDD函数实现是依赖了parallelize函数的实现，来看看Spark中是怎么实现这个makeRDD函数的：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  parallelize(seq, numSlices)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​        我们可以看出，这个makeRDD函数完全和parallelize函数一致。但是我们得看看第二种makeRDD函数函数实现了，它接收的参数类型是Seq[(T, Seq[String])]，Spark文档的说明是</p><blockquote><p>Distribute a local Scala collection to form an RDD, with one or more location preferences (hostnames of Spark nodes) for each object. Create a new partition for each collection item.</p><p>分发本地scala集合以形成RDD，每个对象具有一个或多个位置首选项（spark节点的主机名）。为每个集合项创建一个新分区。</p></blockquote><p>原来，这个函数还为数据提供了位置信息，来看看我们怎么使用：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> iteblog1 = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">iteblog1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> iteblog2 = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">iteblog2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">11</span>] at makeRDD at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> seq = <span class="type">List</span>((<span class="number">1</span>, <span class="type">List</span>(<span class="string">"iteblog.com"</span>, <span class="string">"sparkhost1.com"</span>, <span class="string">"sparkhost2.com"</span>)),</span><br><span class="line">     | (<span class="number">2</span>, <span class="type">List</span>(<span class="string">"iteblog.com"</span>, <span class="string">"sparkhost2.com"</span>)))</span><br><span class="line">seq: <span class="type">List</span>[(<span class="type">Int</span>, <span class="type">List</span>[<span class="type">String</span>])] = <span class="type">List</span>((<span class="number">1</span>,<span class="type">List</span>(iteblog.com, sparkhost1.com, sparkhost2.com)),</span><br><span class="line"> (<span class="number">2</span>,<span class="type">List</span>(iteblog.com, sparkhost2.com)))</span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> iteblog3 = sc.makeRDD(seq)</span><br><span class="line">iteblog3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at makeRDD at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"> </span><br><span class="line">scala&gt; iteblog3.preferredLocations(iteblog3.partitions(<span class="number">1</span>))</span><br><span class="line">res26: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>(iteblog.com, sparkhost2.com)</span><br><span class="line"> </span><br><span class="line">scala&gt; iteblog3.preferredLocations(iteblog3.partitions(<span class="number">0</span>))</span><br><span class="line">res27: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>(iteblog.com, sparkhost1.com, sparkhost2.com)</span><br><span class="line"> </span><br><span class="line">scala&gt; iteblog1.preferredLocations(iteblog1.partitions(<span class="number">0</span>))</span><br><span class="line">res28: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>()</span><br></pre></td></tr></table></figure><p>我们可以看到，makeRDD函数有两种实现，第一种实现其实完全和parallelize一致；而第二种实现可以为数据提供位置信息，而除此之外的实现和parallelize函数也是一致的，如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq, numSlices, <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">String</span>]]())</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">val</span> indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq.map(_._1), seq.size, indexToPrefs)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>都是返回ParallelCollectionRDD，而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。</p><p><strong>转载自过往记忆（<a href="https://www.iteblog.com/）" target="_blank" rel="noopener">https://www.iteblog.com/）</a></strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark中parallelize函数和makeRDD函数的区别：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark中parallelize函数和makeRDD函数的区别&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （二）Spark2.3 HA集群的分布式安装</title>
    <link href="http://zhangfuxin.cn/2019-06-02-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%EF%BC%89Spark2.3%20HA%E9%9B%86%E7%BE%A4%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85.html"/>
    <id>http://zhangfuxin.cn/2019-06-02-Spark学习之路 （二）Spark2.3 HA集群的分布式安装.html</id>
    <published>2019-06-02T02:31:04.000Z</published>
    <updated>2019-09-16T02:02:45.545Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （二）Spark2.3 HA集群的分布式安装：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （二）Spark2.3 HA集群的分布式安装</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、下载Spark安装包"><a href="#一、下载Spark安装包" class="headerlink" title="一、下载Spark安装包"></a>一、下载Spark安装包</h2><h3 id="1、从官网下载"><a href="#1、从官网下载" class="headerlink" title="1、从官网下载"></a>1、从官网下载</h3><p><a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">http://spark.apache.org/downloads.html</a></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420084850299-679933183.png" alt="img"></p><h2 id="二、安装基础"><a href="#二、安装基础" class="headerlink" title="二、安装基础"></a>二、安装基础</h2><p>1、Java8安装成功</p><p>2、Zookeeper安装成功</p><p>3、hadoop2.7.5 HA安装成功</p><p>4、Scala安装成功（不安装进程也可以启动）</p><h2 id="三、Spark安装过程"><a href="#三、Spark安装过程" class="headerlink" title="三、Spark安装过程"></a>三、Spark安装过程</h2><h3 id="1、上传并解压缩"><a href="#1、上传并解压缩" class="headerlink" title="1、上传并解压缩"></a>1、上传并解压缩</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ ls</span><br><span class="line">apps     data      exam        inithive.conf  movie     spark-2.3.0-bin-hadoop2.7.tgz  udf.jar</span><br><span class="line">cookies  data.txt  executions  json.txt       projects  student                        zookeeper.out</span><br><span class="line">course   emp       hive.sql    log            sougou    temp</span><br><span class="line">[hadoop@hadoop1 ~]$ tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz -C apps/</span><br></pre></td></tr></table></figure><h3 id="2、为安装包创建一个软连接"><a href="#2、为安装包创建一个软连接" class="headerlink" title="2、为安装包创建一个软连接"></a>2、为安装包创建一个软连接</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop1 apps]$ ls</span><br><span class="line">hadoop-2.7.5  hbase-1.2.6  spark-2.3.0-bin-hadoop2.7  zookeeper-3.4.10  zookeeper.out</span><br><span class="line">[hadoop@hadoop1 apps]$ ln -s spark-2.3.0-bin-hadoop2.7/ spark</span><br><span class="line">[hadoop@hadoop1 apps]$ ll</span><br><span class="line">总用量 36</span><br><span class="line">drwxr-xr-x. 10 hadoop hadoop  4096 3月  23 20:29 hadoop-2.7.5</span><br><span class="line">drwxrwxr-x.  7 hadoop hadoop  4096 3月  29 13:15 hbase-1.2.6</span><br><span class="line">lrwxrwxrwx.  1 hadoop hadoop    26 4月  20 13:48 spark -&gt; spark-2.3.0-bin-hadoop2.7/</span><br><span class="line">drwxr-xr-x. 13 hadoop hadoop  4096 2月  23 03:42 spark-2.3.0-bin-hadoop2.7</span><br><span class="line">drwxr-xr-x. 10 hadoop hadoop  4096 3月  23 2017 zookeeper-3.4.10</span><br><span class="line">-rw-rw-r--.  1 hadoop hadoop 17559 3月  29 13:37 zookeeper.out</span><br><span class="line">[hadoop@hadoop1 apps]$</span><br></pre></td></tr></table></figure><h3 id="3、进入spark-conf修改配置文件"><a href="#3、进入spark-conf修改配置文件" class="headerlink" title="3、进入spark/conf修改配置文件"></a>3、进入spark/conf修改配置文件</h3><p>（1）进入配置文件所在目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/spark/conf/</span><br><span class="line">[hadoop@hadoop1 conf]$ ll</span><br><span class="line">总用量 36</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  996 2月  23 03:42 docker.properties.template</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 1105 2月  23 03:42 fairscheduler.xml.template</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 2025 2月  23 03:42 log4j.properties.template</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 7801 2月  23 03:42 metrics.properties.template</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  865 2月  23 03:42 slaves.template</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 1292 2月  23 03:42 spark-defaults.conf.template</span><br><span class="line">-rwxr-xr-x. 1 hadoop hadoop 4221 2月  23 03:42 spark-env.sh.template</span><br><span class="line">[hadoop@hadoop1 conf]$</span><br></pre></td></tr></table></figure><p>（2）复制spark-env.sh.template并重命名为spark-env.sh，并在文件最后添加配置内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ cp spark-env.sh.template spark-env.sh</span><br><span class="line">[hadoop@hadoop1 conf]$ vi spark-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_73</span><br><span class="line">#export SCALA_HOME=/usr/share/scala</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop/apps/hadoop-2.7.5/etc/hadoop</span><br><span class="line">export SPARK_WORKER_MEMORY=500m</span><br><span class="line">export SPARK_WORKER_CORES=1</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop1:2181,hadoop2:2181,hadoop3:2181,hadoop4:2181 -Dspark.deploy.zookeeper.dir=/spark"</span><br></pre></td></tr></table></figure><blockquote><p>注：</p><p>#export SPARK_MASTER_IP=hadoop1  这个配置要注释掉。<br>集群搭建时配置的spark参数可能和现在的不一样，主要是考虑个人电脑配置问题，如果memory配置太大，机器运行很慢。<br>说明：<br>-Dspark.deploy.recoveryMode=ZOOKEEPER    #说明整个集群状态是通过zookeeper来维护的，整个集群状态的恢复也是通过zookeeper来维护的。就是说用zookeeper做了spark的HA配置，Master(Active)挂掉的话，Master(standby)要想变成Master（Active）的话，Master(Standby)就要像zookeeper读取整个集群状态信息，然后进行恢复所有Worker和Driver的状态信息，和所有的Application状态信息；<br>-Dspark.deploy.zookeeper.url=hadoop1:2181,hadoop2:2181,hadoop3:2181,hadoop4:2181#将所有配置了zookeeper，并且在这台机器上有可能做master(Active)的机器都配置进来；（我用了4台，就配置了4台） </p><p>-Dspark.deploy.zookeeper.dir=/spark<br>这里的dir和zookeeper配置文件zoo.cfg中的dataDir的区别？？？<br>-Dspark.deploy.zookeeper.dir是保存spark的元数据，保存了spark的作业运行状态；<br>zookeeper会保存spark集群的所有的状态信息，包括所有的Workers信息，所有的Applactions信息，所有的Driver信息,如果集群 </p></blockquote><p>（3）复制slaves.template成slaves</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ cp slaves.template slaves</span><br><span class="line">[hadoop@hadoop1 conf]$ vi slaves</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop1</span><br><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br><span class="line">hadoop4</span><br></pre></td></tr></table></figure><p>（4）将安装包分发给其他节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop1 apps]$ scp -r spark-2.3.0-bin-hadoop2.7/ hadoop2:$PWD</span><br><span class="line">[hadoop@hadoop1 apps]$ scp -r spark-2.3.0-bin-hadoop2.7/ hadoop3:$PWD</span><br><span class="line">[hadoop@hadoop1 apps]$ scp -r spark-2.3.0-bin-hadoop2.7/ hadoop4:$PWD</span><br></pre></td></tr></table></figure><p>创建软连接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop2 apps]$ ls</span><br><span class="line">hadoop-2.7.5  hbase-1.2.6  spark-2.3.0-bin-hadoop2.7  zookeeper-3.4.10</span><br><span class="line">[hadoop@hadoop2 apps]$ ln -s spark-2.3.0-bin-hadoop2.7/ spark</span><br><span class="line">[hadoop@hadoop2 apps]$ ll</span><br><span class="line">总用量 16</span><br><span class="line">drwxr-xr-x 10 hadoop hadoop 4096 3月  23 20:29 hadoop-2.7.5</span><br><span class="line">drwxrwxr-x  7 hadoop hadoop 4096 3月  29 13:15 hbase-1.2.6</span><br><span class="line">lrwxrwxrwx  1 hadoop hadoop   26 4月  20 19:26 spark -&gt; spark-2.3.0-bin-hadoop2.7/</span><br><span class="line">drwxr-xr-x 13 hadoop hadoop 4096 4月  20 19:24 spark-2.3.0-bin-hadoop2.7</span><br><span class="line">drwxr-xr-x 10 hadoop hadoop 4096 3月  21 19:31 zookeeper-3.4.10</span><br><span class="line">[hadoop@hadoop2 apps]$</span><br></pre></td></tr></table></figure><h3 id="4、配置环境变量"><a href="#4、配置环境变量" class="headerlink" title="4、配置环境变量"></a>4、配置环境变量</h3><p>所有节点均要配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 spark]$ vi ~/.bashrc </span><br><span class="line"><span class="meta">#</span><span class="bash">Spark</span></span><br><span class="line">export SPARK_HOME=/home/hadoop/apps/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><p>保存并使其立即生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 spark]$ source ~/.bashrc</span><br></pre></td></tr></table></figure><h2 id="四、启动"><a href="#四、启动" class="headerlink" title="四、启动"></a>四、启动</h2><h3 id="1、先启动zookeeper集群"><a href="#1、先启动zookeeper集群" class="headerlink" title="1、先启动zookeeper集群"></a>1、先启动zookeeper集群</h3><p>所有节点均要执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /home/hadoop/apps/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">[hadoop@hadoop1 ~]$ zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /home/hadoop/apps/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><h3 id="2、在启动HDFS集群"><a href="#2、在启动HDFS集群" class="headerlink" title="2、在启动HDFS集群"></a>2、在启动HDFS集群</h3><p>任意一个节点执行即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ start-dfs.sh</span><br></pre></td></tr></table></figure><h3 id="3、在启动Spark集群"><a href="#3、在启动Spark集群" class="headerlink" title="3、在启动Spark集群"></a>3、在启动Spark集群</h3><p>在一个节点上执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/spark/sbin/</span><br><span class="line">[hadoop@hadoop1 sbin]$ start-all.sh</span><br></pre></td></tr></table></figure><h3 id="4、查看进程"><a href="#4、查看进程" class="headerlink" title="4、查看进程"></a>4、查看进程</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420201419188-161981735.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420201445760-578558622.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420201503589-1845421183.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420201519992-501293445.png" alt="img"></p><h3 id="5、问题"><a href="#5、问题" class="headerlink" title="5、问题"></a>5、问题</h3><p>查看进程发现spark集群只有hadoop1成功启动了Master进程，其他3个节点均没有启动成功，需要手动启动，进入到/home/hadoop/apps/spark/sbin目录下执行以下命令，3个节点都要执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~]$ cd ~/apps/spark/sbin/</span><br><span class="line">[hadoop@hadoop2 sbin]$ start-master.sh</span><br></pre></td></tr></table></figure><h3 id="6、执行之后再次查看进程"><a href="#6、执行之后再次查看进程" class="headerlink" title="6、执行之后再次查看进程"></a>6、执行之后再次查看进程</h3><p>Master进程和Worker进程都以启动成功</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420201942655-1416446127.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202004959-38463551.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202024084-1987751130.png" alt="img"></p><h2 id="五、验证"><a href="#五、验证" class="headerlink" title="五、验证"></a>五、验证</h2><h3 id="1、查看Web界面Master状态"><a href="#1、查看Web界面Master状态" class="headerlink" title="1、查看Web界面Master状态"></a>1、查看Web界面Master状态</h3><p>hadoop1是ALIVE状态，hadoop2、hadoop3和hadoop4均是STANDBY状态</p><p><strong>hadoop1节点</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202404904-1216450970.png" alt="img"></p><p><strong>hadoop2节点</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202505998-2125653978.png" alt="img"></p><p><strong>hadoop3</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202714859-448500440.png" alt="img"></p><p><strong>hadoop4</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202843592-757375998.png" alt="img"></p><h3 id="2、验证HA的高可用"><a href="#2、验证HA的高可用" class="headerlink" title="2、验证HA的高可用"></a>2、验证HA的高可用</h3><p>手动干掉hadoop1上面的Master进程，观察是否会自动进行切换</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420203250642-1623659016.png" alt="img"></p><p>干掉hadoop1上的Master进程之后，再次查看web界面</p><p><strong>hadoo1节点</strong>，由于Master进程被干掉，所以界面无法访问</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420203421981-757224448.png" alt="img"></p><p><strong>hadoop2节点</strong>，Master被干掉之后，hadoop2节点上的Master成功篡位成功，成为ALIVE状态</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420203548607-1978973082.png" alt="img"></p><p><strong>hadoop3节点</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420203751677-1108153453.png" alt="img"></p><p><strong>hadoop4节点</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420203824105-812085082.png" alt="img"></p><h3 id="1、执行第一个Spark程序"><a href="#1、执行第一个Spark程序" class="headerlink" title="1、执行第一个Spark程序"></a>1、执行第一个Spark程序</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ /home/hadoop/apps/spark/bin/spark-submit \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --class org.apache.spark.examples.SparkPi \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --master spark://hadoop1:7077 \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --total-executor-cores 1 \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> /home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 100</span></span><br></pre></td></tr></table></figure><p>其中的spark://hadoop1:7077是下图中的地址</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421115203993-483927862.png" alt="img"></p><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421115300933-1021926480.png" alt="img"></p><h3 id="2、启动spark-shell"><a href="#2、启动spark-shell" class="headerlink" title="2、启动spark shell"></a>2、启动spark shell</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ /home/hadoop/apps/spark/bin/spark-shell \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --master spark://hadoop1:7077 \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --total-executor-cores 1</span></span><br></pre></td></tr></table></figure><p>参数说明：</p><blockquote><p><strong>–master spark://hadoop1:</strong>7077 指定Master的地址</p><p><strong>–executor-memory 500m:</strong>指定每个worker可用内存为500m</p><p><strong>–total-executor-cores 1:</strong> 指定整个集群使用的cup核数为1个</p></blockquote><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421115600861-294197793.png" alt="img"></p><p>注意：</p><p>如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。</p><p>Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可</p><p>Spark Shell中已经默认将SparkSQl类初始化为对象spark。用户代码如果需要用到，则直接应用spark即可</p><h3 id="3、-在spark-shell中编写WordCount程序"><a href="#3、-在spark-shell中编写WordCount程序" class="headerlink" title="3、 在spark shell中编写WordCount程序"></a>3、 在spark shell中编写WordCount程序</h3><p>（1）编写一个hello.txt文件并上传到HDFS上的spark目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ vi hello.txt</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -mkdir -p /spark</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -put hello.txt /spark</span><br></pre></td></tr></table></figure><p>hello.txt的内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">you,jump</span><br><span class="line">i,jump</span><br><span class="line">you,jump</span><br><span class="line">i,jump</span><br><span class="line">jump</span><br></pre></td></tr></table></figure><p>（2）在spark shell中用scala语言编写spark程序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.textFile(&quot;/spark/hello.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;/spark/out&quot;)</span><br></pre></td></tr></table></figure><p>说明：</p><blockquote><p>sc是SparkContext对象，该对象是提交spark程序的入口</p><p>textFile(“/spark/hello.txt”)是hdfs中读取数据</p><p>flatMap(_.split(“ “))先map再压平</p><p>map((_,1))将单词和1构成元组</p><p>reduceByKey(<em>+</em>)按照key进行reduce，并将value累加</p><p>saveAsTextFile(“/spark/out”)将结果写入到hdfs中</p></blockquote><p>（3）使用hdfs命令查看结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~]$ hadoop fs -cat /spark/out/p*</span><br><span class="line">(jump,5)</span><br><span class="line">(you,2)</span><br><span class="line">(i,2)</span><br><span class="line">[hadoop@hadoop2 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421131948389-32143897.png" alt="img"></p><h2 id="七、-执行Spark程序on-YARN"><a href="#七、-执行Spark程序on-YARN" class="headerlink" title="七、 执行Spark程序on YARN"></a>七、 执行Spark程序on YARN</h2><h3 id="1、前提"><a href="#1、前提" class="headerlink" title="1、前提"></a>1、前提</h3><p>成功启动zookeeper集群、HDFS集群、YARN集群</p><h3 id="2、启动Spark-on-YARN"><a href="#2、启动Spark-on-YARN" class="headerlink" title="2、启动Spark on YARN"></a>2、启动Spark on YARN</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 bin]$ spark-shell --master yarn --deploy-mode client</span><br></pre></td></tr></table></figure><p>报错如下：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421172825817-1328970589.png" alt="img"></p><p><strong>报错原因：内存资源给的过小，yarn直接kill掉进程，则报rpc连接失败、ClosedChannelException等错误。</strong></p><p><strong>解决方法：</strong></p><p><strong>先停止YARN服务，然后修改yarn-site.xml，增加如下内容</strong></p><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Whether virtual memory limits will be enforced for containers&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Ratio between virtual memory to physical memory when setting memory limits for containers&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><p>将新的yarn-site.xml文件分发到其他Hadoop节点对应的目录下，最后在重新启动YARN。 </p><p>重新执行以下命令启动spark on yarn</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ spark-shell --master yarn --deploy-mode client</span><br></pre></td></tr></table></figure><p>启动成功</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421173924763-194192730.png" alt="img"></p><h3 id="3、打开YARN的web界面"><a href="#3、打开YARN的web界面" class="headerlink" title="3、打开YARN的web界面"></a>3、打开YARN的web界面</h3><p>打开YARN WEB页面：<a href="http://hadoop4:8088" target="_blank" rel="noopener">http://hadoop4:8088</a><br>可以看到Spark shell应用程序正在运行</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421174140672-405904964.png" alt="img"></p><p> 单击ID号链接，可以看到该应用程序的详细信息</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421174305638-985452065.png" alt="img"></p><p>单击“ApplicationMaster”链接</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421174408279-1529306592.png" alt="img"></p><h3 id="4、运行程序"><a href="#4、运行程序" class="headerlink" title="4、运行程序"></a>4、运行程序</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> array = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">array: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(array)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">res0: <span class="type">Long</span> = <span class="number">5</span>                                                                  </span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421174609129-1153563069.png" alt="img"></p><p>再次查看YARN的web界面</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421174716496-1159210602.png" alt="img"></p><p> 查看executors</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421175002608-557719051.png" alt="img"></p><h3 id="5、执行Spark自带的示例程序PI"><a href="#5、执行Spark自带的示例程序PI" class="headerlink" title="5、执行Spark自带的示例程序PI"></a>5、执行Spark自带的示例程序PI</h3><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --master yarn \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --deploy-mode cluster \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --driver-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-cores 1 \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> /home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 10</span></span><br></pre></td></tr></table></figure><p>执行过程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --master yarn \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --deploy-mode cluster \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --driver-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-cores 1 \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> /home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 10</span></span><br><span class="line">2018-04-21 17:57:32 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2018-04-21 17:57:34 INFO  ConfiguredRMFailoverProxyProvider:100 - Failing over to rm2</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Requesting a new application from cluster with 4 NodeManagers</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Will allocate AM container, with 884 MB memory including 384 MB overhead</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Setting up container launch context for our AM</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Setting up the launch environment for our AM container</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Preparing resources for our AM container</span><br><span class="line">2018-04-21 17:57:36 WARN  Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line">2018-04-21 17:57:39 INFO  Client:54 - Uploading resource file:/tmp/spark-93bd68c9-85de-482e-bbd7-cd2cee60e720/__spark_libs__8262081479435245591.zip -&gt; hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005/__spark_libs__8262081479435245591.zip</span><br><span class="line">2018-04-21 17:57:44 INFO  Client:54 - Uploading resource file:/home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar -&gt; hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005/spark-examples_2.11-2.3.0.jar</span><br><span class="line">2018-04-21 17:57:44 INFO  Client:54 - Uploading resource file:/tmp/spark-93bd68c9-85de-482e-bbd7-cd2cee60e720/__spark_conf__2498510663663992254.zip -&gt; hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005/__spark_conf__.zip</span><br><span class="line">2018-04-21 17:57:44 INFO  SecurityManager:54 - Changing view acls to: hadoop</span><br><span class="line">2018-04-21 17:57:44 INFO  SecurityManager:54 - Changing modify acls to: hadoop</span><br><span class="line">2018-04-21 17:57:44 INFO  SecurityManager:54 - Changing view acls groups to: </span><br><span class="line">2018-04-21 17:57:44 INFO  SecurityManager:54 - Changing modify acls groups to: </span><br><span class="line">2018-04-21 17:57:44 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()</span><br><span class="line">2018-04-21 17:57:44 INFO  Client:54 - Submitting application application_1524303370510_0005 to ResourceManager</span><br><span class="line">2018-04-21 17:57:44 INFO  YarnClientImpl:273 - Submitted application application_1524303370510_0005</span><br><span class="line">2018-04-21 17:57:45 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:45 INFO  Client:54 - </span><br><span class="line">     client token: N/A</span><br><span class="line">     diagnostics: N/A</span><br><span class="line">     ApplicationMaster host: N/A</span><br><span class="line">     ApplicationMaster RPC port: -1</span><br><span class="line">     queue: default</span><br><span class="line">     start time: 1524304664749</span><br><span class="line">     final status: UNDEFINED</span><br><span class="line">     tracking URL: http://hadoop4:8088/proxy/application_1524303370510_0005/</span><br><span class="line">     user: hadoop</span><br><span class="line">2018-04-21 17:57:46 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:47 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:48 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:49 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:50 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:51 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:52 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:53 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:54 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:57:54 INFO  Client:54 - </span><br><span class="line">     client token: N/A</span><br><span class="line">     diagnostics: N/A</span><br><span class="line">     ApplicationMaster host: 192.168.123.104</span><br><span class="line">     ApplicationMaster RPC port: 0</span><br><span class="line">     queue: default</span><br><span class="line">     start time: 1524304664749</span><br><span class="line">     final status: UNDEFINED</span><br><span class="line">     tracking URL: http://hadoop4:8088/proxy/application_1524303370510_0005/</span><br><span class="line">     user: hadoop</span><br><span class="line">2018-04-21 17:57:55 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:57:56 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:57:57 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:57:58 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:57:59 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:00 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:01 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:02 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:03 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:04 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:05 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:06 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:07 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:08 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:09 INFO  Client:54 - Application report for application_1524303370510_0005 (state: FINISHED)</span><br><span class="line">2018-04-21 17:58:09 INFO  Client:54 - </span><br><span class="line">     client token: N/A</span><br><span class="line">     diagnostics: N/A</span><br><span class="line">     ApplicationMaster host: 192.168.123.104</span><br><span class="line">     ApplicationMaster RPC port: 0</span><br><span class="line">     queue: default</span><br><span class="line">     start time: 1524304664749</span><br><span class="line">     final status: SUCCEEDED</span><br><span class="line">     tracking URL: http://hadoop4:8088/proxy/application_1524303370510_0005/</span><br><span class="line">     user: hadoop</span><br><span class="line">2018-04-21 17:58:09 INFO  Client:54 - Deleted staging directory hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005</span><br><span class="line">2018-04-21 17:58:09 INFO  ShutdownHookManager:54 - Shutdown hook called</span><br><span class="line">2018-04-21 17:58:09 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-93bd68c9-85de-482e-bbd7-cd2cee60e720</span><br><span class="line">2018-04-21 17:58:09 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-06de6905-8067-4f1e-a0a0-bc8a51daf535</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （二）Spark2.3 HA集群的分布式安装：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （二）Spark2.3 HA集群的分布式安装&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （一）Spark初识</title>
    <link href="http://zhangfuxin.cn/2019-06-01-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%80%EF%BC%89Spark%E5%88%9D%E8%AF%86.html"/>
    <id>http://zhangfuxin.cn/2019-06-01-Spark学习之路 （一）Spark初识.html</id>
    <published>2019-06-01T02:30:04.000Z</published>
    <updated>2019-09-16T02:07:51.317Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （一）Spark初识：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （一）Spark初识</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1官网"><a href="#1-1官网" class="headerlink" title="1.1官网"></a>1.1官网</h2><p>官网地址：<a href="http://spark.apache.org" target="_blank" rel="noopener">http://spark.apache.org</a></p><h3 id="1、什么是Spark"><a href="#1、什么是Spark" class="headerlink" title="1、什么是Spark"></a>1、什么是Spark</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419210341840-1023204495.png" alt="img"></p><p><strong>Apache Spark™</strong>是用于大规模数据处理的统一分析引擎。</p><p>spark是一个实现快速通用的集群计算平台。它是由加州大学伯克利分校AMP实验室 开发的通用内存并行计算框架，用来构建大型的、低延迟的数据分析应用程序。它扩展了广泛使用的MapReduce计算</p><p>模型。高效的支撑更多计算模式，包括交互式查询和流处理。spark的一个主要特点是能够在内存中进行计算，及时依赖磁盘进行复杂的运算，Spark依然比MapReduce更加高效。</p><h3 id="2、为什么要学Spark"><a href="#2、为什么要学Spark" class="headerlink" title="2、为什么要学Spark"></a>2、为什么要学Spark</h3><p><strong>中间结果输出</strong>：基于MapReduce的计算引擎通常会将中间结果输出到磁盘上，进行存储和容错。出于任务管道承接的，考虑，当一些查询翻译到MapReduce任务时，往往会产生多个Stage，而这些串联的Stage又依赖于底层文件系统（如HDFS）来存储每一个Stage的输出结果。</p><p><strong>Spark是MapReduce的替代方案，而且兼容HDFS、Hive，可融入Hadoop的生态系统，以弥补MapReduce的不足。</strong></p><h2 id="二、Spark的四大特性"><a href="#二、Spark的四大特性" class="headerlink" title="二、Spark的四大特性"></a>二、Spark的四大特性</h2><h3 id="1、高效性"><a href="#1、高效性" class="headerlink" title="1、高效性"></a>1、高效性</h3><p>运行速度提高100倍。</p><p>​        Apache Spark使用最先进的DAG调度程序，查询优化程序和物理执行引擎，实现批量和流式数据的高性能。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419210923327-1692975321.png" alt="img"></p><h3 id="2、易用性"><a href="#2、易用性" class="headerlink" title="2、易用性"></a>2、易用性</h3><p>​        Spark提供80多个高级算法，可以轻松构建并行应用程序。您可以 从Scala，Python，R和SQL shell中以<em>交互</em>方式使用它。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419211144845-2020753937.png" alt="img"></p><h3 id="3、通用性"><a href="#3、通用性" class="headerlink" title="3、通用性"></a>3、通用性</h3><p>​        Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419211451611-367631364.png" alt="img"></p><p>Spark组成(BDAS)：全称伯克利数据分析栈，通过大规模集成算法、机器、人之间展现大数据应用的一个平台。也是处理大数据、云计算、通信的技术解决方案。</p><p>它的主要组件有：</p><p><strong>SparkCore</strong>：将分布式数据抽象为弹性分布式数据集（RDD），实现了应用任务调度、RPC、序列化和压缩，并为运行在其上的上层组件提供API。</p><p><strong>SparkSQL</strong>：Spark Sql 是Spark来操作结构化数据的程序包，可以让我使用SQL语句的方式来查询数据，Spark支持 多种数据源，包含Hive表，parquest以及JSON等内容。</p><p><strong>SparkStreaming</strong>： 是Spark提供的实时数据进行流式计算的组件。</p><p><strong>MLlib</strong>：提供常用机器学习算法的实现库。</p><p><strong>GraphX</strong>：提供一个分布式图计算框架，能高效进行图计算。</p><h3 id="4、兼容性"><a href="#4、兼容性" class="headerlink" title="4、兼容性"></a>4、兼容性</h3><p>​        Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180419211657058-248260284.png" alt="img"></p><p><a href="https://mesos.apache.org/" target="_blank" rel="noopener">Mesos</a>：Spark可以运行在Mesos里面（Mesos 类似于yarn的一个资源调度框架）</p><p><a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener">standalone</a>：Spark自己可以给自己分配资源（master，worker）</p><p><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">YARN</a>：Spark可以运行在yarn上面</p><p> <a href="https://kubernetes.io/" target="_blank" rel="noopener">Kubernetes</a>：Spark接收 <a href="https://kubernetes.io/" target="_blank" rel="noopener">Kubernetes</a>的资源调度</p><h2 id="三、应用场景"><a href="#三、应用场景" class="headerlink" title="三、应用场景"></a>三、应用场景</h2><p>Yahoo将Spark用在Audience Expansion中的应用，进行点击预测和即席查询等</p><p>淘宝技术团队使用了Spark来解决多次迭代的机器学习算法、高计算复杂度的算法等。应用于内容推荐、社区发现等<br>腾讯大数据精准推荐借助Spark快速迭代的优势，实现了在“数据实时采集、算法实时训练、系统实时预测”的全流程实时并行高维算法，最终成功应用于广点通pCTR投放系统上。<br>优酷土豆将Spark应用于视频推荐(图计算)、广告业务，主要实现机器学习、图计算等迭代计算。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （一）Spark初识：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （一）Spark初识&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>模板</title>
    <link href="http://zhangfuxin.cn/2019-06-02-%E6%A8%A1%E6%9D%BF.html"/>
    <id>http://zhangfuxin.cn/2019-06-02-模板.html</id>
    <published>2019-06-01T02:30:04.000Z</published>
    <updated>2019-09-16T01:53:35.632Z</updated>
    
    <content type="html"><![CDATA[<p>** 模板：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （一）Spark初识</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1官网"><a href="#1-1官网" class="headerlink" title="1.1官网"></a>1.1官网</h2><p>官网地址：<a href="http://spark.apache.org" target="_blank" rel="noopener">http://spark.apache.org</a></p><h3 id="1、什么是Spark"><a href="#1、什么是Spark" class="headerlink" title="1、什么是Spark"></a>1、什么是Spark</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 模板：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （一）Spark初识&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>一文读懂 Spark</title>
    <link href="http://zhangfuxin.cn/2018-10-30-spark-%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82spark.html"/>
    <id>http://zhangfuxin.cn/2018-10-30-spark-一文读懂spark.html</id>
    <published>2018-10-30T04:30:04.000Z</published>
    <updated>2019-09-15T23:44:33.871Z</updated>
    
    <content type="html"><![CDATA[<p>** 一文读懂 Spark：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1-前言"><a href="#1-1-前言" class="headerlink" title="1.1 前言"></a>1.1 前言</h2><p>​        Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。</p><p>​        Apache Spark 诞生于大名鼎鼎的 AMPLab（这里还诞生过 Mesos 和 Alluxio），从创立之初就带有浓厚的学术气质，其设计目标是为各种大数据处理需求提供一个统一的技术栈。如今 Spark 背后的商业公司 Databricks 创始人也是来自 AMPLab 的博士毕业生。</p><p>​        Spark 本身使用 Scala 语言编写，Scala 是一门融合了面向对象与函数式的“双范式”语言，运行在 JVM 之上。Spark 大量使用了它的函数式、即时代码生成等特性。Spark 目前提供了 Java、Scala、Python、R 四种语言的 API，前两者因为同样运行在 JVM 上可以达到更原生的支持。</p><p><strong>MapReduce 的问题所在</strong></p><p>​        Hadoop 是大数据处理领域的开创者。严格来说，Hadoop 不只是一个软件，而是一整套生态系统，例如 MapReduce 负责进行分布式计算，而 HDFS 负责存储大量文件。</p><p>​        MapReduce 模型的诞生是大数据处理从无到有的飞跃。但随着技术的进步，对大数据处理的需求也变得越来越复杂，MapReduce 的问题也日渐凸显。通常，我们将 MapReduce 的输入和输出数据保留在 HDFS 上，很多时候，<strong>复杂的 ETL、数据清洗等工作无法用一次 MapReduce 完成，所以需要将多个 MapReduce 过程连接起来</strong>：</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/d3f3c77e124e420bb147bc19389c6db1.jpeg" alt="img"></p><p>▲ 上图中只有两个 MapReduce 串联，实际上可能有几十个甚至更多，依赖关系也更复杂。</p><p>这种方式下，<strong>每次中间结果都要写入 HDFS 落盘保存，代价很大</strong>（别忘了，HDFS 的每份数据都需要冗余若干份拷贝）。另外，由于本质上是多次 MapReduce 任务，调度也比较麻烦，实时性无从谈起。</p><h2 id="Spark-与-RDD-模型"><a href="#Spark-与-RDD-模型" class="headerlink" title="Spark 与 RDD 模型"></a>Spark 与 RDD 模型</h2><p>针对上面的问题，如果能把中间结果保存在内存里，岂不是快的多？之所以不能这么做，最大的障碍是：分布式系统必须能容忍一定的故障，所谓 fault-tolerance。如果只是放在内存中，一旦某个计算节点宕机，其他节点无法恢复出丢失的数据，只能重启整个计算任务，这对于动辄成百上千节点的集群来说是不可接受的。</p><p>一般来说，想做到 fault-tolerance 只有两个方案：要么存储到外部（例如 HDFS），要么拷贝到多个副本。<strong>Spark 大胆地提出了第三种——重算一遍。但是之所以能做到这一点，是依赖于一个额外的假设：所有计算过程都是确定性的（deterministic）。</strong>Spark 借鉴了函数式编程思想，提出了 RDD（Resilient Distributed Datasets），译作“弹性分布式数据集”。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/afc23758a78b4d25958338fd7649b3e0.jpeg" alt="img"></p><p><strong>RDD 是一个只读的、分区的（partitioned）数据集合</strong>。RDD 要么来源于不可变的外部文件（例如 HDFS 上的文件），要么由确定的算子由其他 RDD 计算得到。<strong>RDD 通过算子连接构成有向无环图（DAG）</strong>，上图演示了一个简单的例子，其中节点对应 RDD，边对应算子。</p><p>回到刚刚的问题，RDD 如何做到 fault-tolerance？很简单，RDD 中的每个分区都能被确定性的计算出来，所以<strong>一旦某个分区丢失了，另一个计算节点可以从它的前继节点出发、用同样的计算过程重算一次，即可得到完全一样的 RDD 分区</strong>。这个过程可以递归的进行下去。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/ac3c7bd069244036817c416494610da2.jpeg" alt="img"></p><p>▲ 上图演示了 RDD 分区的恢复。为了简洁并没有画出分区，实际上恢复是以分区为单位的。</p><p>Spark 的编程接口和 Java 8 的 Stream 很相似：RDD 作为数据，在多种算子间变换，构成对执行计划 DAG 的描述。最后，一旦遇到类似 collect()这样的输出命令，执行计划会被发往 Spark 集群、开始计算。不难发现，算子分成两类：</p><ul><li>map()、filter()、join() 等算子称为 Transformation，它们输入一个或多个 RDD，输出一个 RDD。</li><li>collect()、count()、save() 等算子称为 Action，它们通常是将数据收集起来返回；</li></ul><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/87e40f722c334e2aba35c0e76c98c7ae.jpeg" alt="img"></p><p>▲ 上图的例子用来收集包含“HDFS”关键字的错误日志时间戳。当执行到 collect() 时，右边的执行计划开始运行。</p><p>像之前提到的，RDD 的数据由多个分区（partition）构成，这些分区可以分布在集群的各个机器上，这也就是 RDD 中 “distributed” 的含义。熟悉 DBMS 的同学可以把 RDD 理解为逻辑执行计划，partition 理解为物理执行计划。</p><p>此外，RDD 还包含它的每个分区的依赖分区（dependency），以及一个函数指出如何计算出本分区的数据。Spark 的设计者发现，依赖关系依据执行方式的不同可以很自然地分成两种：<strong>窄依赖（Narrow Dependency）和宽依赖（Wide Dependency）</strong>，举例来说：</p><ul><li>map()、filter() 等算子构成窄依赖：生产的每个分区只依赖父 RDD 中的一个分区。</li><li>groupByKey() 等算子构成宽依赖：生成的每个分区依赖父 RDD 中的多个分区（往往是全部分区）。</li></ul><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/b5373ed5294948b5a2f652ad489fcca3.jpeg" alt="img"></p><p>▲ 左图展示了宽依赖和窄依赖，其中 Join 算子因为 Join key 分区情况不同二者皆有；右图展示了执行过程，由于宽依赖的存在，执行计划被分成 3 个阶段。</p><p>在执行时，窄依赖可以很容易的按流水线（pipeline）的方式计算：对于每个分区从前到后依次代入各个算子即可。<strong>然而，宽依赖需要等待前继 RDD 中所有分区计算完成；换句话说，宽依赖就像一个栅栏（barrier）会阻塞到之前的所有计算完成。</strong>整个计算过程被宽依赖分割成多个阶段（stage），如上右图所示。</p><blockquote><p>了解 MapReduce 的同学可能已经发现，宽依赖本质上就是一个 MapReduce 过程。但是相比 MapReduce 自己写 Map 和 Reduce 函数的编程接口，Spark 的接口要容易的多；并且在 Spark 中，多个阶段的 MapReduce 只需要构造一个 DAG 即可。</p></blockquote><h2 id="声明式接口：Spark-SQL"><a href="#声明式接口：Spark-SQL" class="headerlink" title="声明式接口：Spark SQL"></a>声明式接口：Spark SQL</h2><p>命令式编程中，你需要编写一个程序。下面给出了一种伪代码实现：</p><p>employees = db.getAllEmployees() countByDept = dict() // 统计各部门女生人数 (dept_id -&gt; count) for employee in employees: if (employee.gender == ‘female’) countByDept[employee.dept_id] += 1 results = list() // 加上 dept.name 列 depts = db.getAllDepartments() for dept in depts: if (countByDept containsKey dept.id) results.add(row(dept.id, dept.name, countByDept[dept.id])) return results;</p><p>声明式编程中，你只要用关系代数的运算表达出结果：</p><p>employees.join(dept, employees.deptId == dept.id) .where(employees.gender == ‘female’) .groupBy(dept.id, dept.name) .agg()</p><blockquote><p>等价地，如果你更熟悉 SQL，也可以写成这样：</p><p>SELECTdept.id,dept.name,COUNT(*)FROMemployees JOINdept ONemployees.dept_id ==dept.idWHEREemployees.gender =’female’GROUPBYdept.id,dept.name</p></blockquote><p>显然，声明式的要简洁的多！但声明式编程依赖于执行者产生真正的程序代码，所以除了上面这段程序，还需要把数据模型（即 schema）一并告知执行者。声明式编程最广为人知的形式就是 SQL。</p><p>Spark SQL 就是这样一个基于 SQL 的声明式编程接口。<strong>你可以将它看作在 Spark 之上的一层封装，在 RDD 计算模型的基础上，提供了 DataFrame API 以及一个内置的 SQL 执行计划优化器 Catalyst。</strong></p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/a4d07f1924c44c968fd3a19e64183042.jpeg" alt="img"></p><p>▲ 上图黄色部分是 Spark SQL 中新增的部分。</p><p><strong>DataFrame 就像数据库中的表，除了数据之外它还保存了数据的 schema 信息。</strong>计算中，schema 信息也会经过算子进行相应的变换。DataFrame 的数据是行（row）对象组成的 RDD，对 DataFrame 的操作最终会变成对底层 RDD 的操作。</p><p><strong>Catalyst 是一个内置的 SQL 优化器，负责把用户输入的 SQL 转化成执行计划。</strong>Catelyst 强大之处是它利用了 Scala 提供的代码生成（codegen）机制，物理执行计划经过编译，产出的执行代码效率很高，和直接操作 RDD 的命令式代码几乎没有分别。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/5558627b9ae347ac9dd7e17a8cfa0eff.jpeg" alt="img"></p><p>▲ 上图是 Catalyst 的工作流程，与大多数 SQL 优化器一样是一个 Cost-Based Optimizer (CBO)，但最后使用代码生成（codegen）转化成直接对 RDD 的操作。</p><h2 id="流计算框架：Spark-Streaming"><a href="#流计算框架：Spark-Streaming" class="headerlink" title="流计算框架：Spark Streaming"></a>流计算框架：Spark Streaming</h2><p>以往，批处理和流计算被看作大数据系统的两个方面。我们常常能看到这样的架构——以 Kafka、Storm 为代表的流计算框架用于实时计算，而 Spark 或 MapReduce 则负责每天、每小时的数据批处理。在 ETL 等场合，这样的设计常常导致同样的计算逻辑被实现两次，耗费人力不说，保证一致性也是个问题。</p><p>Spark Streaming 正是诞生于此类需求。传统的流计算框架大多注重于低延迟，采用了持续的（continuous）算子模型；而 Spark Streaming 基于 Spark，另辟蹊径提出了 <strong>D-Stream（Discretized Streams）方案：将流数据切成很小的批（micro-batch），用一系列的短暂、无状态、确定性的批处理实现流处理。</strong></p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/091d8858c96c4b09895b70c24c138932.jpeg" alt="img"></p><p>Spark Streaming 的做法在流计算框架中很有创新性，它虽然牺牲了低延迟（一般流计算能做到 100ms 级别，Spark Streaming 延迟一般为 1s 左右），但是带来了三个诱人的优势：</p><ul><li>更高的吞吐量（大约是 Storm 的 2-5 倍）</li><li>更快速的失败恢复（通常只要 1-2s），因此对于 straggler（性能拖后腿的节点）直接杀掉即可</li><li>开发者只需要维护一套 ETL 逻辑即可同时用于批处理和流计算</li></ul><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/49926e28d6014358ae51e234771c089e.jpeg" alt="img"></p><p>▲ 上左图中，为了在持续算子模型的流计算系统中保证一致性，不得不在主备机之间使用同步机制，导致性能损失，Spark Streaming 完全没有这个问题；右图是 D-Stream 的原理示意图。</p><p>你可能会困惑，流计算中的状态一直是个难题。但我们刚刚提到 D-Stream 方案是无状态的，那诸如 word count 之类的问题，怎么做到保持 count 算子的状态呢？</p><p>答案是通过 RDD：<strong>将前一个时间步的 RDD 作为当前时间步的 RDD 的前继节点，就能造成状态不断更替的效果</strong>。实际上，新的状态 RDD 总是不断生成，而旧的 RDD 并不会被“替代”，而是作为新 RDD 的前继依赖。对于底层的 Spark 框架来说，并没有时间步的概念，有的只是不断扩张的 DAG 图和新的 RDD 节点。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/f0696a56517c463f8be35e35dba5739d.jpeg" alt="img"></p><p>▲ 上图是流式计算 word count 的例子，count 结果在不同时间步中不断累积。</p><p>那么另一个问题也随之而来：随着时间的推进，上图中的状态 RDD counts会越来越多，他的祖先（lineage）变得越来越长，极端情况下，恢复过程可能溯源到很久之前。这是不可接受的！<strong>因此，Spark Streming 会定期地对状态 RDD 做 checkpoint，将其持久化到 HDFS 等存储中，这被称为 lineage cut</strong>，在它之前更早的 RDD 就可以没有顾虑地清理掉了。</p><blockquote><p>关于流行的几个开源流计算框架的对比，可以参考文章 Comparison of Apache Stream Processing Frameworks。</p></blockquote><h2 id="流计算与-SQL：Spark-Structured-Streaming"><a href="#流计算与-SQL：Spark-Structured-Streaming" class="headerlink" title="流计算与 SQL：Spark Structured Streaming"></a>流计算与 SQL：Spark Structured Streaming</h2><blockquote><p>出人意料的是，Spark Structured Streaming 的流式计算引擎并没有复用 Spark Streaming，而是在 Spark SQL 上设计了新的一套引擎。因此，从 Spark SQL 迁移到 Spark Structured Streaming 十分容易，但从 Spark Streaming 迁移过来就要困难得多。</p></blockquote><p>很自然的，基于这样的模型，Spark SQL 中的大部分接口、实现都得以在 Spark Structured Streaming 中直接复用。将用户的 SQL 执行计划转化成流计算执行计划的过程被称为<strong>增量化</strong>（incrementalize），这一步是由 Spark 框架自动完成的。对于用户来说只要知道：每次计算的输入是某一小段时间的流数据，而输出是对应数据产生的计算结果。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/e492c7df930e461ab18abe0be8f960c0.jpeg" alt="img"></p><p>▲ 左图是 Spark Structured Streaming 模型示意图；右图展示了同一个任务的批处理、流计算版本，可以看到，除了输入输出不同，内部计算过程完全相同。</p><p>与 Spark SQL 相比，流式 SQL 计算还有两个额外的特性，分别是窗口（window）和水位（watermark）。</p><p><strong>窗口（window）是对过去某段时间的定义。</strong>批处理中，查询通常是全量的（例如：总用户量是多少）；而流计算中，我们通常关心近期一段时间的数据（例如：最近24小时新增的用户量是多少）。用户通过选用合适的窗口来获得自己所需的计算结果，常见的窗口有滑动窗口（Sliding Window）、滚动窗口（Tumbling Window）等。</p><p><strong>水位（watermark）用来丢弃过早的数据。</strong>在流计算中，上游的输入事件可能存在不确定的延迟，而流计算系统的内存是有限的、只能保存有限的状态，一定时间之后必须丢弃历史数据。以双流 A JOIN B 为例，假设窗口为 1 小时，那么 A 中比当前时间减 1 小时更早的数据（行）会被丢弃；如果 B 中出现 1 小时前的事件，因为无法处理只能忽略。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/5f85be06a9664db1885b48902d3d27cf.jpeg" alt="img"></p><p>▲ 上图为水位的示意图，“迟到”太久的数据（行）由于已经低于当前水位无法处理，将被忽略。</p><p>水位和窗口的概念都是因时间而来。在其他流计算系统中，也存在相同或类似的概念。</p><blockquote><p>关于 SQL 的流计算模型，常常被拿来对比的还有另一个流计算框架 Apache Flink。与 Spark 相比，它们的实现思路有很大不同，但在模型上是很相似的。</p></blockquote><h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20181022/3128a0aecac04e2db7d18fc9b05af266.jpeg" alt="img"></p><p><strong>驱动程序（Driver）</strong>即用户编写的程序，对应一个 SparkContext，负责任务的构造、调度、故障恢复等。驱动程序可以直接运行在客户端，例如用户的应用程序中；也可以托管在 Master 上，这被称为集群模式（cluster mode），通常用于流计算等长期任务。</p><p><strong>Cluster Manager</strong>顾名思义负责集群的资源分配，Spark 自带的 Spark Master 支持任务的资源分配，并包含一个 Web UI 用来监控任务运行状况。多个 Master 可以构成一主多备，通过 ZooKeeper 进行协调和故障恢复。通常 Spark 集群使用 Spark Master 即可，但如果用户的集群中不仅有 Spark 框架、还要承担其他任务，官方推荐使用 Mesos 作为集群调度器。</p><p><strong>Worker</strong>节点负责执行计算任务，上面保存了 RDD 等数据。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Spark 是一个同时支持批处理和流计算的分布式计算系统。Spark 的所有计算均构建于 RDD 之上，RDD 通过算子连接形成 DAG 的执行计划，RDD 的确定性及不可变性是 Spark 实现故障恢复的基础。Spark Streaming 的 D-Stream 本质上也是将输入数据分成一个个 micro-batch 的 RDD。</p><p>Spark SQL 是在 RDD 之上的一层封装，相比原始 RDD，DataFrame API 支持数据表的 schema 信息，从而可以执行 SQL 关系型查询，大幅降低了开发成本。Spark Structured Streaming 是 Spark SQL 的流计算版本，它将输入的数据流看作不断追加的数据行。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 一文读懂 Spark：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>分布式锁的几种实现方式</title>
    <link href="http://zhangfuxin.cn/dslock.html"/>
    <id>http://zhangfuxin.cn/dslock.html</id>
    <published>2018-03-02T14:18:29.000Z</published>
    <updated>2019-08-29T03:01:15.917Z</updated>
    
    <content type="html"><![CDATA[<p>** 分布式锁的几种实现方式：** &lt;Excerpt in index | 首页摘要&gt;<br>在分布式架构中，由于多线程和多台服务器，何难保证顺序性。如果需要对某一个资源进行限制，比如票务，比如请求幂等性控制等，这个时候分布式锁就排上用处。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="什么是分布式锁"><a href="#什么是分布式锁" class="headerlink" title="什么是分布式锁"></a>什么是分布式锁</h2><p>分布式锁是控制分布式系统或不同系统之间共同访问共享资源的一种锁实现，如果不同的系统或同一个系统的不同主机之间共享了某个资源时，往往需要互斥来防止彼此干扰来保证一致性。</p><h2 id="分布式锁需要解决的问题"><a href="#分布式锁需要解决的问题" class="headerlink" title="分布式锁需要解决的问题"></a>分布式锁需要解决的问题</h2><ol><li>互斥性：任意时刻，只能有一个客户端获取锁，不能同时有两个客户端获取到锁。</li><li>安全性：锁只能被持有该锁的客户端删除，不能由其它客户端删除。</li><li>死锁：获取锁的客户端因为某些原因（如down机等）而未能释放锁，其它客户端再也无法获取到该锁。</li><li>容错：当部分节点（redis节点等）down机时，客户端仍然能够获取锁和释放锁。</li></ol><h2 id="分布式锁的实现方式"><a href="#分布式锁的实现方式" class="headerlink" title="分布式锁的实现方式"></a>分布式锁的实现方式</h2><ol><li><p>数据库实现</p></li><li><p>缓存实现，比如redis</p></li><li><p>zookeeper实现</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 分布式锁的几种实现方式：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;br&gt;在分布式架构中，由于多线程和多台服务器，何难保证顺序性。如果需要对某一个资源进行限制，比如票务，比如请求幂等性控制等，这个时候分布式锁就排上用处。&lt;/p&gt;
    
    </summary>
    
      <category term="分布式架构" scheme="http://zhangfuxin.cn/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84/"/>
    
    
      <category term="java" scheme="http://zhangfuxin.cn/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>分布式系统理论基础</title>
    <link href="http://zhangfuxin.cn/dsbasic.html"/>
    <id>http://zhangfuxin.cn/dsbasic.html</id>
    <published>2018-02-26T14:31:40.000Z</published>
    <updated>2019-08-29T03:00:59.583Z</updated>
    
    <content type="html"><![CDATA[<p>** 分布式系统理论基础：** &lt;Excerpt in index | 首页摘要&gt;<br>分布式系统不是万能，不能解决所有痛点。在高可用，一致性，分区容错性必须有所权衡。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="CAP理论"><a href="#CAP理论" class="headerlink" title="CAP理论"></a>CAP理论</h2><p>定理：任何分布式架构都只能同时满足两点，无法三者兼顾。</p><ul><li>Consistency（一致性），数据一致更新，所有的数据变动都是同步的。</li><li>Availability（可用性），好的响应性能。</li><li>Partition tolerance（分区容忍性）可靠性，机器宕机是否影响使用。</li></ul><p>关系数据库的ACID模型拥有 高一致性 + 可用性 很难进行分区：</p><ol><li>Atomicity原子性：一个事务中所有操作都必须全部完成，要么全部不完成。</li><li>Consistency一致性. 在事务开始或结束时，数据库应该在一致状态。</li><li>Isolation隔离性. 事务将假定只有它自己在操作数据库，彼此不知晓。</li><li>Durability持久性 一旦事务完成，就不能返回。<br>跨数据库两段提交事务：2PC (two-phase commit)， 2PC is the anti-scalability pattern (Pat Helland)<br>是反可伸缩模式的，JavaEE中的JTA事务可以支持2PC。因为2PC是反模式，尽量不要使用2PC，使用BASE来回避。</li></ol><h2 id="BASE理论"><a href="#BASE理论" class="headerlink" title="BASE理论"></a>BASE理论</h2><ul><li>Basically Available 基本可用，支持分区失败</li><li>Soft state 软状态，允许状态某个时间短不同步，或者异步</li><li>Eventually consistent 最终一致性，要求数据最终结果一致，而不是时刻高度一致。</li></ul><h2 id="paxos协议"><a href="#paxos协议" class="headerlink" title="paxos协议"></a>paxos协议</h2><p>Paxos算法的目的是为了解决分布式环境下一致性的问题。多个节点并发操纵数据，如何保证在读写过程中数据的一致性，并且解决方案要能适应分布式环境下的不可靠性（系统如何就一个值达到统一）。</p><h3 id="Paxos的两个组件"><a href="#Paxos的两个组件" class="headerlink" title="Paxos的两个组件:"></a>Paxos的两个组件:</h3><ul><li>Proposer：提议发起者，处理客户端请求，将客户端的请求发送到集群中，以便决定这个值是否可以被批准。</li><li>Acceptor:提议批准者，负责处理接收到的提议，他们的回复就是一次投票。会存储一些状态来决定是否接收一个值</li></ul><h3 id="Paxos有两个原则"><a href="#Paxos有两个原则" class="headerlink" title="Paxos有两个原则"></a>Paxos有两个原则</h3><ol><li>安全原则—保证不能做错的事<ul><li>a） 针对某个实例的表决只能有一个值被批准，不能出现一个被批准的值被另一个值覆盖的情况；(假设有一个值被多数Acceptor批准了，那么这个值就只能被学习)</li><li>b） 每个节点只能学习到已经被批准的值，不能学习没有被批准的值。</li></ul></li><li>存活原则—只要有多数服务器存活并且彼此间可以通信，最终都要做到的下列事情：<ul><li>a）最终会批准某个被提议的值；</li><li>b）一个值被批准了，其他服务器最终会学习到这个值。</li></ul></li></ol><h2 id="zab协议-ZooKeeper-Atomic-broadcast-protocol"><a href="#zab协议-ZooKeeper-Atomic-broadcast-protocol" class="headerlink" title="zab协议(ZooKeeper Atomic broadcast protocol)"></a>zab协议(ZooKeeper Atomic broadcast protocol)</h2><p>ZAB协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。</p><h3 id="Phase-0-Leader-election（选举阶段）"><a href="#Phase-0-Leader-election（选举阶段）" class="headerlink" title="Phase 0: Leader election（选举阶段）"></a>Phase 0: Leader election（选举阶段）</h3><p>节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。只有到达 Phase 3 准 leader 才会成为真正的 leader。这一阶段的目的是就是为了选出一个准 leader，然后进入下一个阶段。</p><h3 id="Phase-1-Discovery（发现阶段）"><a href="#Phase-1-Discovery（发现阶段）" class="headerlink" title="Phase 1: Discovery（发现阶段）"></a>Phase 1: Discovery（发现阶段）</h3><p>在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。这个一阶段的主要目的是发现当前大多数节点接收的最新提议，并且准 leader 生成新的 epoch，让 followers 接受，更新它们的 acceptedEpoch。<br>一个 follower 只会连接一个 leader，如果有一个节点 f 认为另一个 follower p 是 leader，f 在尝试连接 p 时会被拒绝，f 被拒绝之后，就会进入 Phase 0。</p><h3 id="Phase-2-Synchronization（同步阶段）"><a href="#Phase-2-Synchronization（同步阶段）" class="headerlink" title="Phase 2: Synchronization（同步阶段）"></a>Phase 2: Synchronization（同步阶段）</h3><p>同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。只有当 quorum 都同步完成，准 leader 才会成为真正的 leader。follower 只会接收 zxid 比自己的 lastZxid 大的提议。</p><h3 id="Phase-3-Broadcast（广播阶段）"><a href="#Phase-3-Broadcast（广播阶段）" class="headerlink" title="Phase 3: Broadcast（广播阶段）"></a>Phase 3: Broadcast（广播阶段）</h3><p>到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。</p><h2 id="raft协议"><a href="#raft协议" class="headerlink" title="raft协议"></a>raft协议</h2><p>在Raft中，每个结点会处于下面三种状态中的一种：</p><h3 id="follower"><a href="#follower" class="headerlink" title="follower"></a>follower</h3><p>所有结点都以follower的状态开始。如果没收到leader消息则会变成candidate状态。</p><h3 id="candidate"><a href="#candidate" class="headerlink" title="candidate"></a>candidate</h3><p>会向其他结点“拉选票”，如果得到大部分的票则成为leader。这个过程就叫做Leader选举(Leader Election)</p><h3 id="leader"><a href="#leader" class="headerlink" title="leader"></a>leader</h3><p>所有对系统的修改都会先经过leader。每个修改都会写一条日志(log entry)。leader收到修改请求后的过程如下，这个过程叫做日志复制(Log Replication)：</p><pre><code>1. 复制日志到所有follower结点(replicate entry)2. 大部分结点响应时才提交日志3. 通知所有follower结点日志已提交4. 所有follower也提交日志5. 现在整个系统处于一致的状态</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 分布式系统理论基础：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;br&gt;分布式系统不是万能，不能解决所有痛点。在高可用，一致性，分区容错性必须有所权衡。&lt;/p&gt;
    
    </summary>
    
      <category term="分布式架构" scheme="http://zhangfuxin.cn/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84/"/>
    
    
      <category term="protocol" scheme="http://zhangfuxin.cn/tags/protocol/"/>
    
  </entry>
  
  <entry>
    <title>为什么使用zookeeper？</title>
    <link href="http://zhangfuxin.cn/zookeeper.html"/>
    <id>http://zhangfuxin.cn/zookeeper.html</id>
    <published>2018-02-18T14:15:44.000Z</published>
    <updated>2019-08-29T02:22:11.829Z</updated>
    
    <content type="html"><![CDATA[<p>** 为什么使用zookeeper？：** &lt;Excerpt in index | 首页摘要&gt;<br>随着大型互联网的发展，分布式系统应用越来越来越广泛，zookeeper成了分布式系统的标配。集群容错，动态负载均衡，动态扩容，异地多活等架构都依赖于zookeeper而搭建。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="zookeeper是什么？"><a href="#zookeeper是什么？" class="headerlink" title="zookeeper是什么？"></a>zookeeper是什么？</h2><p>zookeeper是由雅虎创建的，基于google chubby,一个开源的分布式协调服务，是分布式数据一致性的解决方案。</p><h2 id="zookeeper的特性"><a href="#zookeeper的特性" class="headerlink" title="zookeeper的特性"></a>zookeeper的特性</h2><ul><li>顺序一致性，从同一个客户端发起的事务请求，最终会严格按照顺序被应用到zookeeper中。</li><li>原子性，事务请求在所有集群是一致的，要么都成功，要么都失败。</li><li>可靠性，一旦服务器成功应用某个事务，那么所有集群中一定同步并保留。</li><li>实时性，一个事务被应用，客户端能立即从服务端读取到状态变化。</li></ul><h2 id="zookeeper的原理？"><a href="#zookeeper的原理？" class="headerlink" title="zookeeper的原理？"></a>zookeeper的原理？</h2><p>基于分布式协议pasxo，而实现了自己的zab协议。保证数据的一致性。</p><h2 id="zookeeper的数据模型"><a href="#zookeeper的数据模型" class="headerlink" title="zookeeper的数据模型"></a>zookeeper的数据模型</h2><ul><li>持久化节点，节点创建后一直存在，直到主动删除。</li><li>持久化有序节点，每个节点都会为它的一级子节点维护一个顺序。</li><li>临时节点，临时节点的生命周期和客户端会话保持一直。客户端会话失效，节点自动清理。</li><li>临时有序节点，临时节点基础上多一个顺序性特性。</li></ul><h2 id="zookeeper使用场景有哪些？"><a href="#zookeeper使用场景有哪些？" class="headerlink" title="zookeeper使用场景有哪些？"></a>zookeeper使用场景有哪些？</h2><ul><li>订阅发布<ul><li>watcher机制</li><li>统一配置管理(disconf)</li></ul></li><li>分布式锁（redis也可以）</li><li>分布式队列</li><li>负载均衡(dubbo)</li><li>ID生成器</li><li>master选举(kafka,hadoop,hbase)</li></ul><h2 id="集群角色有哪些？"><a href="#集群角色有哪些？" class="headerlink" title="集群角色有哪些？"></a>集群角色有哪些？</h2><h3 id="leader"><a href="#leader" class="headerlink" title="leader"></a>leader</h3><ol><li>事务请求的唯一调度者和处理者，保证集群事务的处理顺序</li><li>集群内部服务的调度者</li></ol><h3 id="follower"><a href="#follower" class="headerlink" title="follower"></a>follower</h3><ol><li>处理非事务请求，以及转发事务请求到leader</li><li>参与事务请求提议的投票</li><li>参与leader选举的投票</li></ol><h3 id="observer"><a href="#observer" class="headerlink" title="observer"></a>observer</h3><ol><li>观察集群中最新状态的变化并同步到observer服务器上</li><li>增加observer不影响集群事务处理能力，还能提升非事务请求的处理能力</li></ol><h2 id="zookeeper集群为什么是奇数？"><a href="#zookeeper集群为什么是奇数？" class="headerlink" title="zookeeper集群为什么是奇数？"></a>zookeeper集群为什么是奇数？</h2><p>zookeeper事务请求提议需要超过半数的机器，假如是2(n+1)台,需要n+2台机器同意，<br>由于在增删改操作中需要半数以上服务器通过，来分析以下情况。<br>2台服务器，至少2台正常运行才行（2的半数为1，半数以上最少为2），正常运行1台服务器都不允许挂掉<br>3台服务器，至少2台正常运行才行（3的半数为1.5，半数以上最少为2），正常运行可以允许1台服务器挂掉<br>4台服务器，至少3台正常运行才行（4的半数为2，半数以上最少为3），正常运行可以允许1台服务器挂掉<br>5台服务器，至少3台正常运行才行（5的半数为2.5，半数以上最少为3），正常运行可以允许2台服务器挂掉<br>6台服务器，至少3台正常运行才行（6的半数为3，半数以上最少为4），正常运行可以允许2台服务器挂掉</p><h2 id="zookeeper日志管理？"><a href="#zookeeper日志管理？" class="headerlink" title="zookeeper日志管理？"></a>zookeeper日志管理？</h2><h2 id="leader选举的原理"><a href="#leader选举的原理" class="headerlink" title="leader选举的原理"></a>leader选举的原理</h2><h2 id="未完待续"><a href="#未完待续" class="headerlink" title="未完待续"></a>未完待续</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 为什么使用zookeeper？：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;br&gt;随着大型互联网的发展，分布式系统应用越来越来越广泛，zookeeper成了分布式系统的标配。集群容错，动态负载均衡，动态扩容，异地多活等架构都依赖于zookeeper而搭建。&lt;/p&gt;
    
    </summary>
    
      <category term="分布式架构" scheme="http://zhangfuxin.cn/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84/"/>
    
    
      <category term="zookeeper" scheme="http://zhangfuxin.cn/tags/zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>查找链表倒数第n个元素</title>
    <link href="http://zhangfuxin.cn/descNode.html"/>
    <id>http://zhangfuxin.cn/descNode.html</id>
    <published>2018-02-16T13:45:45.000Z</published>
    <updated>2019-08-29T03:00:24.260Z</updated>
    
    <content type="html"><![CDATA[<p>** 查找链表倒数第n个元素：** &lt;Excerpt in index | 首页摘要&gt;<br>链表应用很广泛，有单向链表，双向链表。单向链表如何查找倒数第n个元素呢？本文以java代码实现链表反向查找。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="单向链表的定义"><a href="#单向链表的定义" class="headerlink" title="单向链表的定义"></a>单向链表的定义</h2><p>单向链表，主要有数据存储，下一个节点的引用这两个元素组成。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public class Node &#123;</span><br><span class="line">    int value;</span><br><span class="line">    Node next;</span><br><span class="line"></span><br><span class="line">    Node(int value) &#123;</span><br><span class="line">        this.value = value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="遍历倒数第n个元素"><a href="#遍历倒数第n个元素" class="headerlink" title="遍历倒数第n个元素"></a>遍历倒数第n个元素</h2><p>在查找过程中，设置两个指针，让其中一个指针比另一个指针先前移k-1步，<br>然后两个指针同时往前移动。循环直到先行的指针指为NULL时，另一个指针所指的位置就是所要找的位置<br>算法复杂度为o（n）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">public Node findDescEle(Node head, int k) &#123;</span><br><span class="line">    if (k &lt; 1 || head == null) &#123;</span><br><span class="line">        return null;</span><br><span class="line">    &#125;</span><br><span class="line">    Node p1 = head;</span><br><span class="line">    Node p2 = head;</span><br><span class="line">    //前移k-1步</span><br><span class="line">    int step = 0;</span><br><span class="line">    for (int i = 0; i &lt; k; i++) &#123;</span><br><span class="line">        step++;</span><br><span class="line">        if (p1.next != null) &#123;</span><br><span class="line">            p1 = p1.next;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    while (p1 != null) &#123;</span><br><span class="line">        step++;</span><br><span class="line">        p1 = p1.next;</span><br><span class="line">        p2 = p2.next;</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(&quot;o(n)==&quot; + step);</span><br><span class="line">    return p2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>查找链表倒数第n个元素，复杂度为o(n),使用两个指针即可简单实现。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 查找链表倒数第n个元素：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;br&gt;链表应用很广泛，有单向链表，双向链表。单向链表如何查找倒数第n个元素呢？本文以java代码实现链表反向查找。&lt;/p&gt;
    
    </summary>
    
      <category term="algorithm" scheme="http://zhangfuxin.cn/categories/algorithm/"/>
    
    
      <category term="算法" scheme="http://zhangfuxin.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>sprigmvc项目转为springboot</title>
    <link href="http://zhangfuxin.cn/sprigmvc2boot.html"/>
    <id>http://zhangfuxin.cn/sprigmvc2boot.html</id>
    <published>2018-02-06T14:12:55.000Z</published>
    <updated>2019-08-29T02:30:40.437Z</updated>
    
    <content type="html"><![CDATA[<p>** sprigmvc项目转为springboot：** &lt;Excerpt in index | 首页摘要&gt;<br>是否有老掉牙的springmvc项目，想转成springboot项目，看这个文章就对了。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><ul><li>如果你的项目连maven项目都不是，请自行转为maven项目，在按照本教程进行。</li><li>本教程适用于spring+springmvc+mybatis+shiro的maven项目。</li></ul><h2 id="1-修改pom文件依赖"><a href="#1-修改pom文件依赖" class="headerlink" title="1.修改pom文件依赖"></a>1.修改pom文件依赖</h2><ol><li><p>删除之前的spring依赖，添加springboot依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-parent<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.5.9.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">&lt;!-- 这个是剔除掉自带的 tomcat部署的--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-tomcat<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- tomcat容器部署 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-tomcat<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;scope&gt;compile&lt;/scope&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis.spring.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis-spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-devtools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">optional</span>&gt;</span>true<span class="tag">&lt;/<span class="name">optional</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 支持 @ConfigurationProperties 注解 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-configuration-processor<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">optional</span>&gt;</span>true<span class="tag">&lt;/<span class="name">optional</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.tomcat.embed<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>tomcat-embed-jasper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>添加springboot构建插件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.7<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.7<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.5.9.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goal</span>&gt;</span>repackage<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="2-添加application启动文件"><a href="#2-添加application启动文件" class="headerlink" title="2.添加application启动文件"></a>2.添加application启动文件</h2><p>注意，如果Application在controller，service，dao的上一层包里，无需配置<code>@ComponentScan</code>,<br>否则，需要指明要扫描的包。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="comment">//@ComponentScan(&#123;"com.cms.controller","com.cms.service","com.cms.dao"&#125;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> <span class="keyword">extends</span> <span class="title">SpringBootServletInitializer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> SpringApplicationBuilder <span class="title">configure</span><span class="params">(SpringApplicationBuilder application)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> application.sources(Application.class);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        SpringApplication.run(Application.class, args);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-添加springboot配置文件"><a href="#3-添加springboot配置文件" class="headerlink" title="3.添加springboot配置文件"></a>3.添加springboot配置文件</h2><ol><li>在resources下面添加application.properties文件</li><li>添加基本配置<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#默认前缀</span><br><span class="line">server.contextPath=/</span><br><span class="line"># 指定环境</span><br><span class="line">spring.profiles.active=local</span><br><span class="line"># jsp配置</span><br><span class="line">spring.mvc.view.prefix=/WEB-INF/jsp/</span><br><span class="line">spring.mvc.view.suffix=.jsp</span><br><span class="line">#log配置文件</span><br><span class="line">logging.config=classpath:logback-cms.xml</span><br><span class="line">#log路径</span><br><span class="line">logging.path=/Users/mac/work-tommy/cms-springboot/logs/</span><br><span class="line">#数据源</span><br><span class="line">spring.datasource.name=adminDataSource</span><br><span class="line">spring.datasource.driverClassName = com.mysql.jdbc.Driver</span><br><span class="line">spring.datasource.url = jdbc:mysql://localhost:3306/mycms?useUnicode=true&amp;autoReconnect=true&amp;characterEncoding=utf-8</span><br><span class="line">spring.datasource.username = root</span><br><span class="line">spring.datasource.password = 123456</span><br></pre></td></tr></table></figure></li></ol><h2 id="4-使用-Configuration注入配置"><a href="#4-使用-Configuration注入配置" class="headerlink" title="4.使用@Configuration注入配置"></a>4.使用@Configuration注入配置</h2><ol><li><p>注入mybatis配置,分页插件请自主选择</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@MapperScan</span>(basePackages = <span class="string">"com.kuwo.dao"</span>,sqlSessionTemplateRef  = <span class="string">"adminSqlSessionTemplate"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AdminDataSourceConfig</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span>(name = <span class="string">"adminDataSource"</span>)</span><br><span class="line">    <span class="meta">@ConfigurationProperties</span>(prefix = <span class="string">"spring.datasource"</span>)</span><br><span class="line">    <span class="meta">@Primary</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataSource <span class="title">adminDataSource</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> DataSourceBuilder.create().build();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span>(name = <span class="string">"adminSqlSessionFactory"</span>)</span><br><span class="line">    <span class="meta">@Primary</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SqlSessionFactory <span class="title">adminSqlSessionFactory</span><span class="params">(@Qualifier(<span class="string">"adminDataSource"</span>)</span> DataSource dataSource) <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        SqlSessionFactoryBean bean = <span class="keyword">new</span> SqlSessionFactoryBean();</span><br><span class="line">        bean.setDataSource(dataSource);</span><br><span class="line">        <span class="comment">//分页插件</span></span><br><span class="line"><span class="comment">//        PageHelper pageHelper = new PageHelper();</span></span><br><span class="line">        PagePlugin pagePlugin = <span class="keyword">new</span> PagePlugin();</span><br><span class="line"><span class="comment">//        Properties props = new Properties();</span></span><br><span class="line"><span class="comment">//        props.setProperty("reasonable", "true");</span></span><br><span class="line"><span class="comment">//        props.setProperty("supportMethodsArguments", "true");</span></span><br><span class="line"><span class="comment">//        props.setProperty("returnPageInfo", "check");</span></span><br><span class="line"><span class="comment">//        props.setProperty("params", "count=countSql");</span></span><br><span class="line"><span class="comment">//        pageHelper.setProperties(props);</span></span><br><span class="line">        <span class="comment">//添加插件</span></span><br><span class="line">        bean.setPlugins(<span class="keyword">new</span> Interceptor[]&#123;pagePlugin&#125;);</span><br><span class="line">        <span class="comment">// 添加mybatis配置文件</span></span><br><span class="line">        bean.setConfigLocation(<span class="keyword">new</span> DefaultResourceLoader().getResource(<span class="string">"classpath:mybatis/mybatis-config.xml"</span>));</span><br><span class="line">        <span class="comment">// 添加mybatis映射文件</span></span><br><span class="line">        bean.setMapperLocations(<span class="keyword">new</span> PathMatchingResourcePatternResolver().getResources(<span class="string">"classpath:mybatis/system/*.xml"</span>));</span><br><span class="line">        <span class="keyword">return</span> bean.getObject();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span>(name = <span class="string">"adminTransactionManager"</span>)</span><br><span class="line">    <span class="meta">@Primary</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataSourceTransactionManager <span class="title">adminTransactionManager</span><span class="params">(@Qualifier(<span class="string">"adminDataSource"</span>)</span> DataSource dataSource) </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> DataSourceTransactionManager(dataSource);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span>(name = <span class="string">"adminSqlSessionTemplate"</span>)</span><br><span class="line">    <span class="meta">@Primary</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SqlSessionTemplate <span class="title">adminSqlSessionTemplate</span><span class="params">(@Qualifier(<span class="string">"adminSqlSessionFactory"</span>)</span> SqlSessionFactory sqlSessionFactory) <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> SqlSessionTemplate(sqlSessionFactory);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>添加Interceptor配置,注意addInterceptor的顺序，不要搞乱了</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InterceptorConfiguration</span> <span class="keyword">extends</span> <span class="title">WebMvcConfigurerAdapter</span></span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addInterceptors</span><span class="params">(InterceptorRegistry registry)</span> </span>&#123;</span><br><span class="line">        registry.addInterceptor(<span class="keyword">new</span> LoginHandlerInterceptor());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>添加shiro配置文件</p><ul><li>注意：本来使用redis做session缓存，但是和shiro集成发现一个问题，user对象存储以后，从shiro中获取后，无法进行类型转换，所以暂时放弃了redis做session缓存。</li></ul></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ShiroConfiguration</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;spring.redis.host&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> String host;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;spring.redis.port&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> port;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;spring.redis.timeout&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> timeout;</span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> LifecycleBeanPostProcessor <span class="title">getLifecycleBeanPostProcessor</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> LifecycleBeanPostProcessor();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * ShiroFilterFactoryBean 处理拦截资源文件问题。</span></span><br><span class="line"><span class="comment">     * 注意：单独一个ShiroFilterFactoryBean配置是或报错的，因为在</span></span><br><span class="line"><span class="comment">     * 初始化ShiroFilterFactoryBean的时候需要注入：SecurityManager</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     Filter Chain定义说明</span></span><br><span class="line"><span class="comment">     1、一个URL可以配置多个Filter，使用逗号分隔</span></span><br><span class="line"><span class="comment">     2、当设置多个过滤器时，全部验证通过，才视为通过</span></span><br><span class="line"><span class="comment">     3、部分过滤器可指定参数，如perms，roles</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ShiroFilterFactoryBean <span class="title">shiroFilter</span><span class="params">(SecurityManager securityManager)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"ShiroConfiguration.shirFilter()"</span>);</span><br><span class="line">        ShiroFilterFactoryBean shiroFilterFactoryBean  = <span class="keyword">new</span> ShiroFilterFactoryBean();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 必须设置 SecurityManager</span></span><br><span class="line">        shiroFilterFactoryBean.setSecurityManager(securityManager);</span><br><span class="line">        <span class="comment">// 如果不设置默认会自动寻找Web工程根目录下的"/login.jsp"页面</span></span><br><span class="line">        shiroFilterFactoryBean.setLoginUrl(<span class="string">"/login_toLogin"</span>);</span><br><span class="line">        <span class="comment">// 登录成功后要跳转的链接</span></span><br><span class="line">        shiroFilterFactoryBean.setSuccessUrl(<span class="string">"/usersPage"</span>);</span><br><span class="line">        <span class="comment">//未授权界面;</span></span><br><span class="line">        shiroFilterFactoryBean.setUnauthorizedUrl(<span class="string">"/403"</span>);</span><br><span class="line">        <span class="comment">//拦截器.</span></span><br><span class="line">        Map&lt;String,String&gt; filterChainDefinitionMap = <span class="keyword">new</span> LinkedHashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//配置退出 过滤器,其中的具体的退出代码Shiro已经替我们实现了</span></span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/logout"</span>, <span class="string">"logout"</span>);</span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/login_toLogin"</span>, <span class="string">"anon"</span>);</span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/login_login"</span>, <span class="string">"anon"</span>);</span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/static/login/**"</span>,<span class="string">"anon"</span>);</span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/static/js/**"</span>,<span class="string">"anon"</span>);</span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/uploadFiles/uploadImgs/**"</span>,<span class="string">"anon"</span>);</span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/code.do"</span>,<span class="string">"anon"</span>);</span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/font-awesome/**"</span>,<span class="string">"anon"</span>);</span><br><span class="line">        <span class="comment">//&lt;!-- 过滤链定义，从上向下顺序执行，一般将 /**放在最为下边 --&gt;:这是一个坑呢，一不小心代码就不好使了;</span></span><br><span class="line">        <span class="comment">//&lt;!-- authc:所有url都必须认证通过才可以访问; anon:所有url都都可以匿名访问--&gt;</span></span><br><span class="line"></span><br><span class="line">        filterChainDefinitionMap.put(<span class="string">"/**"</span>, <span class="string">"authc"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        shiroFilterFactoryBean.setFilterChainDefinitionMap(filterChainDefinitionMap);</span><br><span class="line">        <span class="keyword">return</span> shiroFilterFactoryBean;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SecurityManager <span class="title">securityManager</span><span class="params">()</span></span>&#123;</span><br><span class="line">        DefaultWebSecurityManager securityManager =  <span class="keyword">new</span> DefaultWebSecurityManager();</span><br><span class="line">        <span class="comment">//设置realm.</span></span><br><span class="line">        securityManager.setRealm(myShiroRealm());</span><br><span class="line">        <span class="comment">// 自定义缓存实现 使用redis</span></span><br><span class="line">        <span class="comment">//securityManager.setCacheManager(cacheManager());</span></span><br><span class="line">        <span class="comment">// 自定义session管理 使用redis</span></span><br><span class="line">        securityManager.setSessionManager(sessionManager());</span><br><span class="line">        <span class="keyword">return</span> securityManager;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ShiroRealm <span class="title">myShiroRealm</span><span class="params">()</span></span>&#123;</span><br><span class="line">        ShiroRealm myShiroRealm = <span class="keyword">new</span> ShiroRealm();</span><br><span class="line"><span class="comment">//        myShiroRealm.setCredentialsMatcher(hashedCredentialsMatcher());</span></span><br><span class="line">        <span class="keyword">return</span> myShiroRealm;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *  开启shiro aop注解支持.</span></span><br><span class="line"><span class="comment">     *  使用代理方式;所以需要开启代码支持;</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> securityManager</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> AuthorizationAttributeSourceAdvisor <span class="title">authorizationAttributeSourceAdvisor</span><span class="params">(SecurityManager securityManager)</span></span>&#123;</span><br><span class="line">        AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor = <span class="keyword">new</span> AuthorizationAttributeSourceAdvisor();</span><br><span class="line">        authorizationAttributeSourceAdvisor.setSecurityManager(securityManager);</span><br><span class="line">        <span class="keyword">return</span> authorizationAttributeSourceAdvisor;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 配置shiro redisManager</span></span><br><span class="line"><span class="comment">     * 使用的是shiro-redis开源插件</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RedisManager <span class="title">redisManager</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        RedisManager redisManager = <span class="keyword">new</span> RedisManager();</span><br><span class="line">        redisManager.setHost(host);</span><br><span class="line">        redisManager.setPort(port);</span><br><span class="line">        redisManager.setExpire(<span class="number">1800</span>);</span><br><span class="line">        redisManager.setTimeout(timeout);</span><br><span class="line">        <span class="comment">// redisManager.setPassword(password);</span></span><br><span class="line">        <span class="keyword">return</span> redisManager;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * cacheManager 缓存 redis实现</span></span><br><span class="line"><span class="comment">     * 使用的是shiro-redis开源插件</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RedisCacheManager <span class="title">cacheManager</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        RedisCacheManager redisCacheManager = <span class="keyword">new</span> RedisCacheManager();</span><br><span class="line">        redisCacheManager.setRedisManager(redisManager());</span><br><span class="line">        <span class="keyword">return</span> redisCacheManager;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * RedisSessionDAO shiro sessionDao层的实现 通过redis</span></span><br><span class="line"><span class="comment">     * 使用的是shiro-redis开源插件</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RedisSessionDAO <span class="title">redisSessionDAO</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        RedisSessionDAO redisSessionDAO = <span class="keyword">new</span> RedisSessionDAO();</span><br><span class="line">        redisSessionDAO.setRedisManager(redisManager());</span><br><span class="line">        <span class="keyword">return</span> redisSessionDAO;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DefaultWebSessionManager <span class="title">sessionManager</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        DefaultWebSessionManager sessionManager = <span class="keyword">new</span> DefaultWebSessionManager();</span><br><span class="line"><span class="comment">//        sessionManager.setSessionDAO(redisSessionDAO());</span></span><br><span class="line">        <span class="keyword">return</span> sessionManager;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>搞了一天时间把项目转成springboot，查阅各种资料，希望这篇文章能够为你带来帮助。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** sprigmvc项目转为springboot：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;br&gt;是否有老掉牙的springmvc项目，想转成springboot项目，看这个文章就对了。&lt;/p&gt;
    
    </summary>
    
      <category term="项目实战" scheme="http://zhangfuxin.cn/categories/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="java" scheme="http://zhangfuxin.cn/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>mybatis-generator</title>
    <link href="http://zhangfuxin.cn/mybatis-generator.html"/>
    <id>http://zhangfuxin.cn/mybatis-generator.html</id>
    <published>2018-01-28T09:30:19.000Z</published>
    <updated>2019-08-29T02:33:08.554Z</updated>
    
    <content type="html"><![CDATA[<p>** mybatis-generator：** &lt;Excerpt in index | 首页摘要&gt;<br>mybatis反向生成器，根据数据库表，自动创建pojo，mapper以及mybatis配置文件，能极大的提高开发效率。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="插件介绍"><a href="#插件介绍" class="headerlink" title="插件介绍"></a>插件介绍</h2><p>本插件fork自<a href="http://link" target="_blank" rel="noopener">mybatis-generator-gui</a>,在此基础上加了批量生成表。</p><h2 id="插件特性"><a href="#插件特性" class="headerlink" title="插件特性"></a>插件特性</h2><ol><li>保存数据库配置</li><li>根据表生成pojo，mapper以及mybatis配置文件</li><li>批量生成</li><li>其它功能（待开发）</li></ol><h2 id="插件使用"><a href="#插件使用" class="headerlink" title="插件使用"></a>插件使用</h2><h3 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h3><p>本工具由于使用了Java 8的众多特性，所以要求JDK <strong>1.8.0.60</strong>以上版本，对于JDK版本还没有升级的童鞋表示歉意。</p><h3 id="启动本软件"><a href="#启动本软件" class="headerlink" title="启动本软件"></a>启动本软件</h3><ul><li>方法一: 自助构建</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/maochunguang/mybatis-generator-gui</span><br><span class="line"><span class="built_in">cd</span> mybatis-generator-gui</span><br><span class="line">mvn jfx:jar</span><br><span class="line"><span class="built_in">cd</span> target/jfx/app/</span><br><span class="line">java -jar mybatis-generator-gui.jar</span><br></pre></td></tr></table></figure><ul><li>方法二: IDE中运行Eclipse or IntelliJ IDEA中启动, 找到<code>com.zzg.mybatis.generator.MainUI</code>类并运行就可以了</li></ul><h3 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h3><p>更多详细文档请参考本库的Wiki</p><ul><li><a href="https://github.com/maochunguang/mybatis-generator-gui/wiki" target="_blank" rel="noopener">Usage</a></li></ul><h2 id="截图参考"><a href="#截图参考" class="headerlink" title="截图参考"></a>截图参考</h2><p><img src="http://o7kalf5h3.bkt.clouddn.com/mybatis.png" alt="MainUI"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** mybatis-generator：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;br&gt;mybatis反向生成器，根据数据库表，自动创建pojo，mapper以及mybatis配置文件，能极大的提高开发效率。&lt;/p&gt;
    
    </summary>
    
      <category term="开发工具" scheme="http://zhangfuxin.cn/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="mysql" scheme="http://zhangfuxin.cn/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>突破算法第11天-红黑树</title>
    <link href="http://zhangfuxin.cn/suanfa-11.html"/>
    <id>http://zhangfuxin.cn/suanfa-11.html</id>
    <published>2017-10-30T14:35:37.000Z</published>
    <updated>2019-08-29T02:27:53.213Z</updated>
    
    <content type="html"><![CDATA[<p>** 突破算法第11天-红黑树：** &lt;Excerpt in index | 首页摘要&gt;<br>红黑树</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="红黑树"><a href="#红黑树" class="headerlink" title="红黑树"></a>红黑树</h2><p>本文的主要内容：</p><ol><li>红黑树的基本概念以及最重要的 5 点规则。</li><li>红黑树的左旋转、右旋转、重新着色的原理与 Java 实现；</li><li>红黑树的增加结点、删除结点过程解析；</li></ol><h2 id="红黑树的基本概念与数据结构表示"><a href="#红黑树的基本概念与数据结构表示" class="headerlink" title="红黑树的基本概念与数据结构表示"></a>红黑树的基本概念与数据结构表示</h2><p>首先红黑树来个定义：</p><blockquote><p>红黑树定义：红黑树又称红 - 黑二叉树，它首先是一颗二叉树，它具体二叉树所有的特性。同时红黑树更是一颗自平衡的排序二叉树 (平衡二叉树的一种实现方式)。</p></blockquote><p>我们知道一颗基本的二叉排序树他们都需要满足一个基本性质：即树中的任何节点的值大于它的左子节点，且小于它的右子节点。</p><p>按照这个基本性质使得树的检索效率大大提高。我们知道在生成二叉排序树的过程是非常容易失衡的，最坏的情况就是一边倒（只有右 / 左子树），这样势必会导致二叉树的检索效率大大降低（O(n)），所以为了维持二叉排序树的平衡，大牛们提出了各种平衡二叉树的实现算法，如：AVL，SBT，伸展树，TREAP ，红黑树等等。</p><blockquote><p>平衡二叉树必须具备如下特性：它是一棵空树或它的左右两个子树的高度差的绝对值不超过 1，并且左右两个子树都是一棵平衡二叉树。也就是说该二叉树的任何一个子节点，其左右子树的高度都相近。下面给出平衡二叉树的几个示意图：</p></blockquote><p><img src="http://img.blog.csdn.net/20170110134212154?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="红黑树"></p><p>红黑树顾名思义就是结点是红色或者是黑色的平衡二叉树，它通过颜色的约束来维持着二叉树的平衡。对于一棵有效的红黑树而言我们必须增加如下规则，这也是红黑树最重要的 5 点规则：</p><ol><li>每个结点都只能是红色或者黑色中的一种。</li><li>根结点是黑色的。</li><li>每个叶结点（NIL 节点，空节点）是黑色的。</li><li>如果一个结点是红的，则它两个子节点都是黑的。也就是说在一条路径上不能出现相邻的两个红色结点。</li><li>从任一结点到其每个叶子的所有路径都包含相同数目的黑色结点。</li></ol><p>这些约束强制了红黑树的关键性质: 从根到叶子最长的可能路径不多于最短的可能路径的两倍长。结果是这棵树大致上是平衡的。因为操作比如插入、删除和查找某个值的最坏情况时间都要求与树的高度成比例，这个在高度上的理论上限允许红黑树在最坏情况下都是高效的，而不同于普通的二叉查找树。所以红黑树它是复杂而高效的，其检索效率 O(lg n)。下图为一颗典型的红黑二叉树：<br><img src="http://img.blog.csdn.net/20170110134903553?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><p>上面关于红黑树的概念基本已经说得很清楚了，下面给出红黑树的结点用 Java 表示数据结构：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">boolean</span> RED = <span class="keyword">true</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">boolean</span> BLACK = <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">private</span> Node root;<span class="comment">//二叉查找树的根节点</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//结点数据结构</span></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Key key;<span class="comment">//键</span></span><br><span class="line">    <span class="keyword">private</span> Value value;<span class="comment">//值</span></span><br><span class="line">    <span class="keyword">private</span> Node left, right;<span class="comment">//指向子树的链接:左子树和右子树.</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> N;<span class="comment">//以该节点为根的子树中的结点总数</span></span><br><span class="line">    <span class="keyword">boolean</span> color;<span class="comment">//由其父结点指向它的链接的颜色也就是结点颜色.</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">(Key key, Value value, <span class="keyword">int</span> N, <span class="keyword">boolean</span> color)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.key = key;</span><br><span class="line">        <span class="keyword">this</span>.value = value;</span><br><span class="line">        <span class="keyword">this</span>.N = N;</span><br><span class="line">        <span class="keyword">this</span>.color = color;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取整个二叉查找树的大小</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> size(root);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取某一个结点为根结点的二叉查找树的大小</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> x</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">(Node x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(x == <span class="keyword">null</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> x.N;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isRed</span><span class="params">(Node x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(x == <span class="keyword">null</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> x.color == RED;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="红黑树的三个基本操作"><a href="#红黑树的三个基本操作" class="headerlink" title="红黑树的三个基本操作"></a>红黑树的三个基本操作</h2><p>红黑树在插入，删除过程中可能会破坏原本的平衡条件导致不满足红黑树的性质，这时候一般情况下要通过左旋、右旋和重新着色这个三个操作来使红黑树重新满足平衡化条件。</p><h2 id="旋转"><a href="#旋转" class="headerlink" title="旋转"></a>旋转</h2><p>旋转分为左旋和右旋。在我们实现某些操作中可能会出现红色右链接或则两个连续的红链接，这时候就要通过旋转修复。</p><p>通常左旋操作用于将一个向右倾斜的红色链接 (这个红色链接链连接的两个结点均是红色结点) 旋转为向左链接。对比操作前后，可以看出，该操作实际上是将红线链接的两个结点中的一个较大的结点移动到根结点上。</p><p>左旋的示意图如下：<br><img src="http://img.blog.csdn.net/20170110141248765?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><p><img src="http://img.blog.csdn.net/20170110141309245?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><p>左旋的 Java 实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 左旋转</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> h</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Node <span class="title">rotateLeft</span><span class="params">(Node h)</span></span>&#123;</span><br><span class="line">    Node x = h.right;</span><br><span class="line">    <span class="comment">//把x的左结点赋值给h的右结点</span></span><br><span class="line">    h.right = x.left;</span><br><span class="line">    <span class="comment">//把h赋值给x的左结点</span></span><br><span class="line">    x.left = h;</span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    x.color = h.color;</span><br><span class="line">    h.color = RED;</span><br><span class="line">    x.N = h.N;</span><br><span class="line">    h.N = <span class="number">1</span>+ size(h.left) + size(h.right);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>左旋的动画效果如下：<br><img src="http://img.blog.csdn.net/20170110142027660?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><p>右旋其实就是左旋的逆操作：<br><img src="http://img.blog.csdn.net/20170110142230957?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br><img src="http://img.blog.csdn.net/20170110142252648?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>右旋的代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 右旋转</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> h</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Node <span class="title">rotateRight</span><span class="params">(Node h)</span></span>&#123;</span><br><span class="line">    Node x = h.left;</span><br><span class="line">    h.left = x.right;</span><br><span class="line">    x.right = h;</span><br><span class="line"></span><br><span class="line">    x.color = h.color;</span><br><span class="line">    h.color = RED;</span><br><span class="line">    x.N = h.N;</span><br><span class="line">    h.N = <span class="number">1</span>+ size(h.left) + size(h.right);</span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>右旋的动态示意图：<br><img src="http://img.blog.csdn.net/20170110142410322?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><h2 id="颜色反转"><a href="#颜色反转" class="headerlink" title="颜色反转"></a><a></a>颜色反转</h2><p>当出现一个临时的 4-node 的时候，即一个节点的两个子节点均为红色，如下图：<br><img src="http://img.blog.csdn.net/20170110143015321?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>我们需要将 E 提升至父节点，操作方法很简单，就是把 E 对子节点的连线设置为黑色，自己的颜色设置为红色。颜色反转之后颜色如下：<br><img src="http://img.blog.csdn.net/20170110143225712?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDg1MzI2MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt><br>实现代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 颜色转换</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> h</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">flipColors</span><span class="params">(Node h)</span></span>&#123;</span><br><span class="line">    h.color = RED;<span class="comment">//父结点颜色变红</span></span><br><span class="line">    h.left.color = BLACK;<span class="comment">//子结点颜色变黑</span></span><br><span class="line">    h.right.color = BLACK;<span class="comment">//子结点颜色变黑</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>注意：以上的旋转和颜色反转操作都是针对单一结点的，反转或则颜色反转操作之后可能引起其父结点又不满足平衡性质。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 突破算法第11天-红黑树：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;br&gt;红黑树&lt;/p&gt;
    
    </summary>
    
      <category term="algorithm" scheme="http://zhangfuxin.cn/categories/algorithm/"/>
    
    
      <category term="算法" scheme="http://zhangfuxin.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>突破算法第10天-二叉树</title>
    <link href="http://zhangfuxin.cn/suanfa-10.html"/>
    <id>http://zhangfuxin.cn/suanfa-10.html</id>
    <published>2017-10-29T13:17:09.000Z</published>
    <updated>2019-08-29T02:28:07.188Z</updated>
    
    <content type="html"><![CDATA[<p>** 突破算法第10天-二叉树：** &lt;Excerpt in index | 首页摘要&gt;<br>用java实现算法求出二叉树的高度</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="树"><a href="#树" class="headerlink" title="树"></a>树</h2><ul><li>先序遍历：先访问根结点，然后左节点，最后右节点</li><li>中序遍历：先访问左结点，然后根节点，最后右节点</li><li>后续遍历：先访问左结点，然后右节点，最后根节点</li></ul><h2 id="java实现"><a href="#java实现" class="headerlink" title="java实现"></a>java实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span> </span>&#123;</span><br><span class="line">    TreeNode left;</span><br><span class="line">    TreeNode right;</span><br><span class="line">    <span class="keyword">int</span> val;</span><br><span class="line"></span><br><span class="line">    TreeNode(<span class="keyword">int</span> val) &#123;</span><br><span class="line">        <span class="keyword">this</span>.val = val;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        TreeNode root = <span class="keyword">new</span> TreeNode(<span class="number">1</span>);</span><br><span class="line">        TreeNode left1 = <span class="keyword">new</span> TreeNode(<span class="number">2</span>);</span><br><span class="line">        TreeNode left2 = <span class="keyword">new</span> TreeNode(<span class="number">3</span>);</span><br><span class="line">        TreeNode right1 = <span class="keyword">new</span> TreeNode(<span class="number">4</span>);</span><br><span class="line">        <span class="comment">//创建一棵树</span></span><br><span class="line">        root.left = left1;</span><br><span class="line">        left1.right = left2;</span><br><span class="line">        root.right = right1;</span><br><span class="line">        scanNodes(root);</span><br><span class="line">        System.out.println(<span class="string">"树的深度是："</span> + getDepth(root));</span><br><span class="line">        System.out.println(<span class="string">"非递归深度："</span> + findDeep2(root));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 递归返回二叉树的深度</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getDepth</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> left = getDepth(root.left);</span><br><span class="line">        <span class="keyword">int</span> right = getDepth(root.right);</span><br><span class="line">        <span class="keyword">return</span> left &gt; right ? left + <span class="number">1</span> : right + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">scanNodes</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"><span class="comment">//        System.out.println(root.val); //先序遍历</span></span><br><span class="line">        scanNodes(root.left);</span><br><span class="line"><span class="comment">//        System.out.println(root.val); //中序遍历</span></span><br><span class="line">        scanNodes(root.right);</span><br><span class="line">        System.out.println(root.val); <span class="comment">// 后序遍历</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 非递归求深度</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">findDeep2</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        TreeNode current = <span class="keyword">null</span>;</span><br><span class="line">        LinkedList&lt;TreeNode&gt; queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">        queue.offer(root);</span><br><span class="line">        <span class="keyword">int</span> cur, next;</span><br><span class="line">        <span class="keyword">int</span> level = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (!queue.isEmpty()) &#123;</span><br><span class="line">            cur = <span class="number">0</span>;</span><br><span class="line">            <span class="comment">//当遍历完当前层以后，队列里元素全是下一层的元素，队列的长度是这一层的节点的个数</span></span><br><span class="line">            next = queue.size();</span><br><span class="line">            <span class="keyword">while</span> (cur &lt; next) &#123;</span><br><span class="line">                current = queue.poll();</span><br><span class="line">                cur++;</span><br><span class="line">                <span class="comment">//把当前节点的左右节点入队（如果存在的话）  </span></span><br><span class="line">                <span class="keyword">if</span> (current.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    queue.offer(current.left);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (current.right != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    queue.offer(current.right);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            level++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> level;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="树的变种"><a href="#树的变种" class="headerlink" title="树的变种"></a>树的变种</h2><p>二叉查找树，平衡二叉查找树，红黑树，b树<br>红黑树和平衡二叉树（AVL树）类似，都是在进行插入和删除操作时通过特定操作保持二叉查找树的平衡，从而获得较高的查找性能。红黑树和AVL树的区别在于它使用颜色来标识结点的高度，它所追求的是局部平衡而不是AVL树中的非常严格的平衡。<br>由于二叉树的效率和深度息息相关，于是出现了多路的B树，B+树等等。b树是叶子为n的平衡树。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 突破算法第10天-二叉树：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;br&gt;用java实现算法求出二叉树的高度&lt;/p&gt;
    
    </summary>
    
      <category term="algorithm" scheme="http://zhangfuxin.cn/categories/algorithm/"/>
    
    
      <category term="算法" scheme="http://zhangfuxin.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>突破算法第九天-排序算法比较</title>
    <link href="http://zhangfuxin.cn/suanfa-9.html"/>
    <id>http://zhangfuxin.cn/suanfa-9.html</id>
    <published>2017-10-28T15:10:58.000Z</published>
    <updated>2019-08-29T02:28:19.550Z</updated>
    
    <content type="html"><![CDATA[<p>** 突破算法第九天-排序算法比较：** &lt;Excerpt in index | 首页摘要&gt;<br>排序算法个有千秋，有的性能高，有的性能很低。这就要求我们对常用的排序算法要全面了解，不要用错了算法，导致性能问题。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="排序算法性能比较·"><a href="#排序算法性能比较·" class="headerlink" title="排序算法性能比较·"></a>排序算法性能比较·</h2><p>借一张网路上的比较图。特别直观。<br><img src="http://o7kalf5h3.bkt.clouddn.com/sortcom.jpg" alt="算法比较"></p><h2 id="排序算法总结"><a href="#排序算法总结" class="headerlink" title="排序算法总结"></a>排序算法总结</h2><p>个人看法：</p><ul><li>一般的情况还是以快速排序为主，</li><li>对于多个有序的数组合并的情况使用归并排序</li><li>性能要求快，空间足够，待排序的元素都要在一定的范围内使用桶排序</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 突破算法第九天-排序算法比较：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;br&gt;排序算法个有千秋，有的性能高，有的性能很低。这就要求我们对常用的排序算法要全面了解，不要用错了算法，导致性能问题。&lt;/p&gt;
    
    </summary>
    
      <category term="algorithm" scheme="http://zhangfuxin.cn/categories/algorithm/"/>
    
    
      <category term="算法" scheme="http://zhangfuxin.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
