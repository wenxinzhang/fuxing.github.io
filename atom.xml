<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>福星</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zhangfuxin.cn/"/>
  <updated>2019-09-06T00:12:24.698Z</updated>
  <id>http://zhangfuxin.cn/</id>
  
  <author>
    <name>福 星</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CM+CDH离线安装</title>
    <link href="http://zhangfuxin.cn/CDH-hadoop.html"/>
    <id>http://zhangfuxin.cn/CDH-hadoop.html</id>
    <published>2019-09-05T17:30:04.000Z</published>
    <updated>2019-09-06T00:12:24.698Z</updated>
    
    <content type="html"><![CDATA[<p>** CM+CDH离线安装：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Cloudera Manager可以轻松管理任何生产规模的Hadoop部署。通过直观的用户界面快速部署，配置和监控群集 - 完成滚动升级，备份和灾难恢复以及可定制警报。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1-Cloudera-简介"><a href="#1-1-Cloudera-简介" class="headerlink" title="1.1 Cloudera 简介"></a>1.1 Cloudera 简介</h2><h3 id="1-1-1Cloudera-简介"><a href="#1-1-1Cloudera-简介" class="headerlink" title="1.1.1Cloudera 简介"></a>1.1.1Cloudera 简介</h3><p>官网：<a href="https://www.cloudera.com/" target="_blank" rel="noopener">https://www.cloudera.com/</a></p><p>文档：<a href="https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_intro.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_intro.html</a></p><p>​        CDH是Apache Hadoop和相关项目中最完整，经过测试和最流行的发行版。CDH提供了Hadoop的核心元素 - 可扩展存储和分布式计算 - 以及基于Web的用户界面和重要的企业功能。CDH是Apache许可的开源软件，是唯一提供统一批处理，交互式SQL和交互式搜索以及基于角色的访问控制的Hadoop解决方案。</p><p>CDH提供：</p><ul><li><p>灵活性 - 存储任何类型的数据并使用各种不同的计算框架对其进行操作，包括批处理，交互式SQL，自由文本搜索，机器学习和统计计算。</p></li><li><p>集成 - 在完整的Hadoop平台上快速启动和运行，该平台可与各种硬件和软件解决方案配合使用。</p></li><li><p>安全 - 处理和控制敏感数据。</p></li><li><p>可扩展性 - 支持广泛的应用程序，并扩展和扩展它们以满足您的要求。</p></li><li><p>高可用性 - 充满信心地执行任务关键型业务任务。</p></li><li><p>兼容性 - 利用您现有的IT基础架构和投资。</p><p><img src="https://www.cloudera.com/documentation/enterprise/latest/images/cdh.png" alt="img"></p></li></ul><h3 id="1-1-2Hadoop起源"><a href="#1-1-2Hadoop起源" class="headerlink" title="1.1.2Hadoop起源"></a>1.1.2Hadoop起源</h3><p>​        2004年，Cutting和同为程序员出身的Mike Cafarella决定开发一款可以代替当时的主流搜索产品的开源搜索引擎，这个项目被命名为Nutch。Nutch基于Lucence实现的搜索引擎，能够从互联网上抓取网页数据。抓取来的海量数据的存储问题。但是，这些海量数据都是非结构化数据，不能存在关系型数据库里。如果连数据的存储和管理都解决不了化，就谈不上后续为用户提供搜索服务，包括通过算法去优化检索速度。那时的Cutting正处在Freelancer的生涯 中，所以他希望自己的项目能通过一种低开销的方式来构建网页中的大量算法。幸运的是，Google这时正好发布了一项研究报告，报告中介绍了两款Google为支持自家的搜索引擎而开发的软件平台。这两个平台一个是GFS（Google File System），用于存储不同设备所产生的海量数据；另一个是MapReduce，它运行在GFS之上，负责分布式大规模数据。</p><p>​        2006年cutting根据《Google File System》设计了Nutch的HDFS,hadoop distributed file system。</p><p>Hadoop最开始是nutch的子项目，目的是解决nutch的海量数据存储问题。在nutch 0.8版本之后，Hadoop独立处理，成为一个独立的项目。后来，又根据《Google MapReduce》设计了基于HDFS的MapRedce计算框架。</p><p>2006年之后，cutting带着Hadoop去了雅虎，当时有100多人的团队共同帮cutting完善hadoop。后来yahoo把Hadoop贡献了Apache。所以，现在Hadoop是Apache的顶级项目。</p><h2 id="2-1Hadoop搭建"><a href="#2-1Hadoop搭建" class="headerlink" title="2.1Hadoop搭建"></a>2.1Hadoop搭建</h2><p><strong>Hadoop的三种运行模式</strong> ：</p><ol><li><p>独立（本地）运行模式：无需任何守护进程，所有的程序都运行在同一个JVM上执行。在独立模式下调试MR程序非常高效方便。所以一般该模式主要是在学习或者开发阶段调试使用 。</p></li><li><p>伪分布式模式：  Hadoop守护进程运行在本地机器上，模拟一个小规模的集群，换句话说，可以配置一台机器的Hadoop集群,伪分布式是完全分布式的一个特例。</p></li><li><p>完全分布式模式：Hadoop守护进程运行在一个集群上。</p></li></ol><h2 id="3-1-单机伪分布模式"><a href="#3-1-单机伪分布模式" class="headerlink" title="3.1 单机伪分布模式"></a>3.1 单机伪分布模式</h2><p>​    只支持MapReduce，不支持HDFS。这种模式一般用于调试MapReduce任务用的。</p><h3 id="3-1-1-准备Linux环境，最低的工作内存1G"><a href="#3-1-1-准备Linux环境，最低的工作内存1G" class="headerlink" title="3.1.1 准备Linux环境，最低的工作内存1G"></a>3.1.1 准备Linux环境，最低的工作内存1G</h3><p>内容详见：Vmware安装Centos6.9文档</p><h3 id="3-1-2-关闭防火墙"><a href="#3-1-2-关闭防火墙" class="headerlink" title="3.1.2  关闭防火墙"></a>3.1.2  关闭防火墙</h3><p>临时关闭防火墙：service iptables stop</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br></pre></td></tr></table></figure><p>永久关闭防火墙：chkconfig iptables off </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font>永久修改防火墙需要重启，永久和临时同时执行，可以不用重启。</p><h3 id="3-1-3-配置主机名"><a href="#3-1-3-配置主机名" class="headerlink" title="3.1.3 配置主机名"></a>3.1.3 配置主机名</h3><p>查询主机名称：hostname</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname</span><br></pre></td></tr></table></figure><p>临时修改主机名：hostname  <strong><name></name></strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname &lt;name&gt;</span><br></pre></td></tr></table></figure><p>永久修改主机名：vim /etc/sysconfig/network</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><h3 id="3-1-4-配置hosts文件"><a href="#3-1-4-配置hosts文件" class="headerlink" title="3.1.4 配置hosts文件"></a>3.1.4 配置hosts文件</h3><p>执行: vim /etc/hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><ol><li>不要删除前两行内容。</li><li>IP在前，主机名在后。</li></ol><h3 id="3-1-5-配置免密码登录"><a href="#3-1-5-配置免密码登录" class="headerlink" title="3.1.5 配置免密码登录"></a>3.1.5 配置免密码登录</h3><h4 id="3-1-5-1-免密登陆原理"><a href="#3-1-5-1-免密登陆原理" class="headerlink" title="3.1.5.1 免密登陆原理"></a>3.1.5.1 免密登陆原理</h4><ol><li><p>A机器生成公钥和私钥</p></li><li><p>机器A发送自己的公钥到机器B，这个时候机器B有了机器A的公钥</p></li><li><p>机器B发送一个随机的字符串向机器A</p></li><li><p>机器A利用自己的私钥把字符串加密</p></li><li><p>机器A把加密后的字符串再次发送给机器B</p></li><li><p>机器B利用公钥解密字符串，如果和原来的一样，则OK。</p></li></ol><h4 id="3-1-5-1-免密登陆实现"><a href="#3-1-5-1-免密登陆实现" class="headerlink" title="3.1.5.1 免密登陆实现"></a>3.1.5.1 免密登陆实现</h4><ol><li><p>生成自己的公钥和私钥  ssh-keygen</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure></li><li><p>把生成的公钥copy到远程机器上 ssh-copy-id root@hadoop01</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id root@hadoop01</span><br></pre></td></tr></table></figure></li></ol><p><font color="red">注意：</font>如果是单机的伪分布式环境，自己节点也需要配置免密登录。</p><h3 id="3-1-6-安装和配置jdk"><a href="#3-1-6-安装和配置jdk" class="headerlink" title="3.1.6 安装和配置jdk"></a>3.1.6 安装和配置jdk</h3><ol><li><p>执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>在尾行添加 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASS_PATH</span><br></pre></td></tr></table></figure></li></ol><p>保存退出  :wq</p><ol start="3"><li><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>java -version 查看JDK版本信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-7-上传和安装hadoop"><a href="#3-1-7-上传和安装hadoop" class="headerlink" title="3.1.7 上传和安装hadoop"></a>3.1.7 上传和安装hadoop</h3><p>下载地址：<a href="http://hadoop.apache.org/releases.html" target="_blank" rel="noopener">http://hadoop.apache.org/releases.html</a></p><p><font color="red">注意：</font></p><p>source表示源码</p><p>binary表示二级制包（安装包）</p><h4 id="3-1-7-1-解压Hadoop文件包"><a href="#3-1-7-1-解压Hadoop文件包" class="headerlink" title="3.1.7.1 解压Hadoop文件包"></a>3.1.7.1 解压Hadoop文件包</h4><p>执行：tar -zxvf hadoop-2.7.1_64bit.tar.gz</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-2.7.1_64bit.tar.gz</span><br></pre></td></tr></table></figure><h4 id="3-1-7-2-Hadoop目录说明"><a href="#3-1-7-2-Hadoop目录说明" class="headerlink" title="3.1.7.2 Hadoop目录说明"></a>3.1.7.2 Hadoop目录说明</h4><p>bin目录：命令脚本</p><p>etc/hadoop:存放hadoop的配置文件</p><p>lib目录：hadoop运行的依赖jar包</p><p>sbin目录：启动和关闭hadoop等命令都在这里</p><p>libexec目录：存放的也是hadoop命令，但一般不常用</p><p><font color="red">注意：</font>最常用的就是bin和etc目录。</p><h3 id="3-1-8-配置hadoop配置文件"><a href="#3-1-8-配置hadoop配置文件" class="headerlink" title="3.1.8 配置hadoop配置文件"></a>3.1.8 配置hadoop配置文件</h3><p>Hadoop目录下<strong>/home/hadoop-2.7.1/etc/hadoop/</strong>目录下<strong>6个文件</strong></p><h4 id="3-1-8-1-hadoop-env-sh"><a href="#3-1-8-1-hadoop-env-sh" class="headerlink" title="3.1.8.1 hadoop-env.sh"></a>3.1.8.1 hadoop-env.sh</h4><p>执行：vim hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br></pre></td></tr></table></figure><p> 修改：修改java_home路径和hadoop_conf_dir 路径  25行  33行</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#25行</span><br><span class="line">export JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">#33行</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop-2.7.1/etc/hadoop</span><br></pre></td></tr></table></figure><p> 然后执行：source hadoop-env.sh编译文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source hadoop-env.sh</span><br></pre></td></tr></table></figure><h4 id="3-1-8-2-core-site-xml"><a href="#3-1-8-2-core-site-xml" class="headerlink" title="3.1.8.2 core-site.xml"></a>3.1.8.2 core-site.xml</h4><p>命令行执行：vim core-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim core-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hdfs的老大，namenode的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://tedu:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hadoop运行时产生文件的存放目录--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop-2.7.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-3-hdfs-site-xml"><a href="#3-1-8-3-hdfs-site-xml" class="headerlink" title="3.1.8.3 hdfs-site .xml"></a>3.1.8.3 hdfs-site .xml</h4><p>命令行执行：vim hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hdfs-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定hdfs保存数据副本的数量，包括自己，默认值是3--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--如果是伪分布模式，此值是1--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--设置hdfs的操作权限，false表示任何用户都可以在hdfs上操作文件--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-4-mapred-site-xml"><a href="#3-1-8-4-mapred-site-xml" class="headerlink" title="3.1.8.4 mapred-site.xml"></a>3.1.8.4 mapred-site.xml</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line"></span><br><span class="line">vim mapred-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定mapreduce运行在yarn上--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-5-yarn-site-xml"><a href="#3-1-8-5-yarn-site-xml" class="headerlink" title="3.1.8.5 yarn-site.xml"></a>3.1.8.5 yarn-site.xml</h4><p>命令行执行：vim yarn-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim yarn-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定yarn的老大 resoucemanager的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>tedu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--NodeManager获取数据的方式--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-  services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-6-slaves"><a href="#3-1-8-6-slaves" class="headerlink" title="3.1.8.6 slaves"></a>3.1.8.6 slaves</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves</span><br></pre></td></tr></table></figure><p><strong>修改主机名</strong></p><h3 id="3-1-9-配置hadoop的环境变量"><a href="#3-1-9-配置hadoop的环境变量" class="headerlink" title="3.1.9 配置hadoop的环境变量"></a>3.1.9 配置hadoop的环境变量</h3><ol><li><p>文件最后追加文件</p><p><strong>HADOOP_HOME=/home/hadoop-2.7.1</strong></p><p><strong>export HADOOP_HOME</strong></p></li><li><p>source /etc/profile 使更改的配置立即生效。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">HADOOP_HOME=/home/hadoop-2.7.1</span><br><span class="line">PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASSPATH HADOOP_HOME</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-10-格式化Namenode"><a href="#3-1-10-格式化Namenode" class="headerlink" title="3.1.10 格式化Namenode"></a>3.1.10 格式化Namenode</h3><p>执行：hdfs namenode -format</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>如果不好使，可以重启linux</p><p>当出现：successfully，证明格式化成功。</p><h3 id="3-1-11-启动Hadoop"><a href="#3-1-11-启动Hadoop" class="headerlink" title="3.1.11 启动Hadoop"></a>3.1.11 启动Hadoop</h3><p>在/home/hadoop-2.7.1/sbin目录下</p><p>执行:./start-all.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><h3 id="3-1-12-验证启动成功"><a href="#3-1-12-验证启动成功" class="headerlink" title="3.1.12 验证启动成功"></a>3.1.12 验证启动成功</h3><p>可以访问网址： <a href="http://192.168.220.128:50070" target="_blank" rel="noopener">http://192.168.220.128:50070</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** CM+CDH离线安装：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Cloudera Manager可以轻松管理任何生产规模的Hadoop部署。通过直观的用户界面快速部署，配置和监控群集 - 完成滚动升级，备份和灾难恢复以及可定制警报。&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://zhangfuxin.cn/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://zhangfuxin.cn/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop伪分布式搭建</title>
    <link href="http://zhangfuxin.cn/hadoop-single.html"/>
    <id>http://zhangfuxin.cn/hadoop-single.html</id>
    <published>2019-08-29T17:30:04.000Z</published>
    <updated>2019-08-29T17:28:47.951Z</updated>
    
    <content type="html"><![CDATA[<p>** Hadoop伪分布式搭建：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        大数据（Big Data），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。<br>​        大数据的定义是4Vs：数据量大、处理速度快、数据源多样、真实性。用中文简单描述就是大、快、多、真。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1Hadoop简介"><a href="#1-1Hadoop简介" class="headerlink" title="1.1Hadoop简介"></a>1.1Hadoop简介</h2><h3 id="1-1-1Hadoop创始人"><a href="#1-1-1Hadoop创始人" class="headerlink" title="1.1.1Hadoop创始人"></a>1.1.1Hadoop创始人</h3><p>​        1985年，<strong>Doug Cutting</strong>毕业于美国斯坦福大学。他并不是一开始就决心投身IT行业的，在大学时代的头两年，Cutting学习了诸如物理、地理等常规课程。因为学费的压力，Cutting开始意识到，自己必须学习一些更加实用、有趣的技能。这样，一方面可以帮助自己还清贷款，另一方面，也是为自己未来的生活做打算。因为斯坦福大学座落在IT行业的“圣地”硅谷，所以学习软件对年轻人来说是再自然不过的事情了。 1997年底，Cutting开始以每周两天的时间投入，在家里试着用Java把这个想法变成现实，不久之后，Lucene诞生了。作为第一个提供全文文本搜索的开源函数库，Lucene的伟大自不必多言。</p><p>Doug Cutting是<strong>Lucence,Nutch,Hadoop</strong>的创始人。</p><h3 id="1-1-2Hadoop起源"><a href="#1-1-2Hadoop起源" class="headerlink" title="1.1.2Hadoop起源"></a>1.1.2Hadoop起源</h3><p>​        2004年，Cutting和同为程序员出身的Mike Cafarella决定开发一款可以代替当时的主流搜索产品的开源搜索引擎，这个项目被命名为Nutch。Nutch基于Lucence实现的搜索引擎，能够从互联网上抓取网页数据。抓取来的海量数据的存储问题。但是，这些海量数据都是非结构化数据，不能存在关系型数据库里。如果连数据的存储和管理都解决不了化，就谈不上后续为用户提供搜索服务，包括通过算法去优化检索速度。那时的Cutting正处在Freelancer的生涯 中，所以他希望自己的项目能通过一种低开销的方式来构建网页中的大量算法。幸运的是，Google这时正好发布了一项研究报告，报告中介绍了两款Google为支持自家的搜索引擎而开发的软件平台。这两个平台一个是GFS（Google File System），用于存储不同设备所产生的海量数据；另一个是MapReduce，它运行在GFS之上，负责分布式大规模数据。</p><p>​        2006年cutting根据《Google File System》设计了Nutch的HDFS,hadoop distributed file system。</p><p>Hadoop最开始是nutch的子项目，目的是解决nutch的海量数据存储问题。在nutch 0.8版本之后，Hadoop独立处理，成为一个独立的项目。后来，又根据《Google MapReduce》设计了基于HDFS的MapRedce计算框架。</p><p>2006年之后，cutting带着Hadoop去了雅虎，当时有100多人的团队共同帮cutting完善hadoop。后来yahoo把Hadoop贡献了Apache。所以，现在Hadoop是Apache的顶级项目。</p><h2 id="2-1Hadoop搭建"><a href="#2-1Hadoop搭建" class="headerlink" title="2.1Hadoop搭建"></a>2.1Hadoop搭建</h2><p><strong>Hadoop的三种运行模式</strong> ：</p><ol><li><p>独立（本地）运行模式：无需任何守护进程，所有的程序都运行在同一个JVM上执行。在独立模式下调试MR程序非常高效方便。所以一般该模式主要是在学习或者开发阶段调试使用 。</p></li><li><p>伪分布式模式：  Hadoop守护进程运行在本地机器上，模拟一个小规模的集群，换句话说，可以配置一台机器的Hadoop集群,伪分布式是完全分布式的一个特例。</p></li><li><p>完全分布式模式：Hadoop守护进程运行在一个集群上。</p></li></ol><h2 id="3-1-单机伪分布模式"><a href="#3-1-单机伪分布模式" class="headerlink" title="3.1 单机伪分布模式"></a>3.1 单机伪分布模式</h2><p>​    只支持MapReduce，不支持HDFS。这种模式一般用于调试MapReduce任务用的。</p><h3 id="3-1-1-准备Linux环境，最低的工作内存1G"><a href="#3-1-1-准备Linux环境，最低的工作内存1G" class="headerlink" title="3.1.1 准备Linux环境，最低的工作内存1G"></a>3.1.1 准备Linux环境，最低的工作内存1G</h3><p>内容详见：Vmware安装Centos6.9文档</p><h3 id="3-1-2-关闭防火墙"><a href="#3-1-2-关闭防火墙" class="headerlink" title="3.1.2  关闭防火墙"></a>3.1.2  关闭防火墙</h3><p>临时关闭防火墙：service iptables stop</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br></pre></td></tr></table></figure><p>永久关闭防火墙：chkconfig iptables off </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font>永久修改防火墙需要重启，永久和临时同时执行，可以不用重启。</p><h3 id="3-1-3-配置主机名"><a href="#3-1-3-配置主机名" class="headerlink" title="3.1.3 配置主机名"></a>3.1.3 配置主机名</h3><p>查询主机名称：hostname</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname</span><br></pre></td></tr></table></figure><p>临时修改主机名：hostname  <strong><name></name></strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname &lt;name&gt;</span><br></pre></td></tr></table></figure><p>永久修改主机名：vim /etc/sysconfig/network</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><p><font color="red">注意：</font></p><p>1.永久修改主机名需要重启，永久和临时同时执行，可以不用重启。</p><p>2.主机名里不能有下滑线，或者特殊字符 #$，不然会找不到主机导致无法启动。</p><h3 id="3-1-4-配置hosts文件"><a href="#3-1-4-配置hosts文件" class="headerlink" title="3.1.4 配置hosts文件"></a>3.1.4 配置hosts文件</h3><p>执行: vim /etc/hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font></p><ol><li>不要删除前两行内容。</li><li>IP在前，主机名在后。</li></ol><h3 id="3-1-5-配置免密码登录"><a href="#3-1-5-配置免密码登录" class="headerlink" title="3.1.5 配置免密码登录"></a>3.1.5 配置免密码登录</h3><h4 id="3-1-5-1-免密登陆原理"><a href="#3-1-5-1-免密登陆原理" class="headerlink" title="3.1.5.1 免密登陆原理"></a>3.1.5.1 免密登陆原理</h4><ol><li><p>A机器生成公钥和私钥</p></li><li><p>机器A发送自己的公钥到机器B，这个时候机器B有了机器A的公钥</p></li><li><p>机器B发送一个随机的字符串向机器A</p></li><li><p>机器A利用自己的私钥把字符串加密</p></li><li><p>机器A把加密后的字符串再次发送给机器B</p></li><li><p>机器B利用公钥解密字符串，如果和原来的一样，则OK。</p></li></ol><h4 id="3-1-5-1-免密登陆实现"><a href="#3-1-5-1-免密登陆实现" class="headerlink" title="3.1.5.1 免密登陆实现"></a>3.1.5.1 免密登陆实现</h4><ol><li><p>生成自己的公钥和私钥  ssh-keygen</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure></li><li><p>把生成的公钥copy到远程机器上 ssh-copy-id root@hadoop01</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id root@hadoop01</span><br></pre></td></tr></table></figure></li></ol><p><font color="red">注意：</font>如果是单机的伪分布式环境，自己节点也需要配置免密登录。</p><h3 id="3-1-6-安装和配置jdk"><a href="#3-1-6-安装和配置jdk" class="headerlink" title="3.1.6 安装和配置jdk"></a>3.1.6 安装和配置jdk</h3><ol><li><p>执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>在尾行添加 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASS_PATH</span><br></pre></td></tr></table></figure></li></ol><p>保存退出  :wq</p><ol start="3"><li><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></li><li><p>java -version 查看JDK版本信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-7-上传和安装hadoop"><a href="#3-1-7-上传和安装hadoop" class="headerlink" title="3.1.7 上传和安装hadoop"></a>3.1.7 上传和安装hadoop</h3><p>下载地址：<a href="http://hadoop.apache.org/releases.html" target="_blank" rel="noopener">http://hadoop.apache.org/releases.html</a></p><p><font color="red">注意：</font></p><p>source表示源码</p><p>binary表示二级制包（安装包）</p><h4 id="3-1-7-1-解压Hadoop文件包"><a href="#3-1-7-1-解压Hadoop文件包" class="headerlink" title="3.1.7.1 解压Hadoop文件包"></a>3.1.7.1 解压Hadoop文件包</h4><p>执行：tar -zxvf hadoop-2.7.1_64bit.tar.gz</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-2.7.1_64bit.tar.gz</span><br></pre></td></tr></table></figure><h4 id="3-1-7-2-Hadoop目录说明"><a href="#3-1-7-2-Hadoop目录说明" class="headerlink" title="3.1.7.2 Hadoop目录说明"></a>3.1.7.2 Hadoop目录说明</h4><p>bin目录：命令脚本</p><p>etc/hadoop:存放hadoop的配置文件</p><p>lib目录：hadoop运行的依赖jar包</p><p>sbin目录：启动和关闭hadoop等命令都在这里</p><p>libexec目录：存放的也是hadoop命令，但一般不常用</p><p><font color="red">注意：</font>最常用的就是bin和etc目录。</p><h3 id="3-1-8-配置hadoop配置文件"><a href="#3-1-8-配置hadoop配置文件" class="headerlink" title="3.1.8 配置hadoop配置文件"></a>3.1.8 配置hadoop配置文件</h3><p>Hadoop目录下<strong>/home/hadoop-2.7.1/etc/hadoop/</strong>目录下<strong>6个文件</strong></p><h4 id="3-1-8-1-hadoop-env-sh"><a href="#3-1-8-1-hadoop-env-sh" class="headerlink" title="3.1.8.1 hadoop-env.sh"></a>3.1.8.1 hadoop-env.sh</h4><p>执行：vim hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br></pre></td></tr></table></figure><p> 修改：修改java_home路径和hadoop_conf_dir 路径  25行  33行</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#25行</span><br><span class="line">export JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">#33行</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop-2.7.1/etc/hadoop</span><br></pre></td></tr></table></figure><p> 然后执行：source hadoop-env.sh编译文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source hadoop-env.sh</span><br></pre></td></tr></table></figure><h4 id="3-1-8-2-core-site-xml"><a href="#3-1-8-2-core-site-xml" class="headerlink" title="3.1.8.2 core-site.xml"></a>3.1.8.2 core-site.xml</h4><p>命令行执行：vim core-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim core-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hdfs的老大，namenode的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://tedu:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--用来指定hadoop运行时产生文件的存放目录--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop-2.7.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-3-hdfs-site-xml"><a href="#3-1-8-3-hdfs-site-xml" class="headerlink" title="3.1.8.3 hdfs-site .xml"></a>3.1.8.3 hdfs-site .xml</h4><p>命令行执行：vim hdfs-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hdfs-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定hdfs保存数据副本的数量，包括自己，默认值是3--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--如果是伪分布模式，此值是1--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--设置hdfs的操作权限，false表示任何用户都可以在hdfs上操作文件--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-4-mapred-site-xml"><a href="#3-1-8-4-mapred-site-xml" class="headerlink" title="3.1.8.4 mapred-site.xml"></a>3.1.8.4 mapred-site.xml</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line"></span><br><span class="line">vim mapred-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定mapreduce运行在yarn上--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-5-yarn-site-xml"><a href="#3-1-8-5-yarn-site-xml" class="headerlink" title="3.1.8.5 yarn-site.xml"></a>3.1.8.5 yarn-site.xml</h4><p>命令行执行：vim yarn-site.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim yarn-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定yarn的老大 resoucemanager的地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>tedu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--NodeManager获取数据的方式--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-  services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-1-8-6-slaves"><a href="#3-1-8-6-slaves" class="headerlink" title="3.1.8.6 slaves"></a>3.1.8.6 slaves</h4><p>命令行执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves</span><br></pre></td></tr></table></figure><p><strong>修改主机名</strong></p><h3 id="3-1-9-配置hadoop的环境变量"><a href="#3-1-9-配置hadoop的环境变量" class="headerlink" title="3.1.9 配置hadoop的环境变量"></a>3.1.9 配置hadoop的环境变量</h3><ol><li><p>文件最后追加文件</p><p><strong>HADOOP_HOME=/home/hadoop-2.7.1</strong></p><p><strong>export HADOOP_HOME</strong></p></li><li><p>source /etc/profile 使更改的配置立即生效。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Set Java ENV</span></span><br><span class="line">JAVA_HOME=/home/jdk1.8.0_65</span><br><span class="line">HADOOP_HOME=/home/hadoop-2.7.1</span><br><span class="line">PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH</span><br><span class="line">CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">export JAVA_HOME PATH CLASSPATH HADOOP_HOME</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-1-10-格式化Namenode"><a href="#3-1-10-格式化Namenode" class="headerlink" title="3.1.10 格式化Namenode"></a>3.1.10 格式化Namenode</h3><p>执行：hdfs namenode -format</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>如果不好使，可以重启linux</p><p>当出现：successfully，证明格式化成功。</p><h3 id="3-1-11-启动Hadoop"><a href="#3-1-11-启动Hadoop" class="headerlink" title="3.1.11 启动Hadoop"></a>3.1.11 启动Hadoop</h3><p>在/home/hadoop-2.7.1/sbin目录下</p><p>执行:./start-all.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><h3 id="3-1-12-验证启动成功"><a href="#3-1-12-验证启动成功" class="headerlink" title="3.1.12 验证启动成功"></a>3.1.12 验证启动成功</h3><p>可以访问网址： <a href="http://192.168.220.128:50070" target="_blank" rel="noopener">http://192.168.220.128:50070</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Hadoop伪分布式搭建：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        大数据（Big Data），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。&lt;br&gt;​        大数据的定义是4Vs：数据量大、处理速度快、数据源多样、真实性。用中文简单描述就是大、快、多、真。&lt;/p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://zhangfuxin.cn/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://zhangfuxin.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>如何选购合适的电脑</title>
    <link href="http://zhangfuxin.cn/buy-computer.html"/>
    <id>http://zhangfuxin.cn/buy-computer.html</id>
    <published>2019-08-28T16:30:04.000Z</published>
    <updated>2019-08-28T17:10:49.787Z</updated>
    
    <content type="html"><![CDATA[<p>** 购买合适的电脑：** &lt;Excerpt in index | 首页摘要&gt;<br>随着时代的发展，人们生活水平的提高，计算机也成为了我们工作必不可少的生产力，办公，打游戏，看电影，购物等等。市面上电脑种类根据用途又可以分为很多类，台式机、笔记本、IPAD（平板）。购买一台适合自己的电脑工作时可以如虎添翼，电脑用着舒服，心情也会舒畅许多。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="买电脑主要需求"><a href="#买电脑主要需求" class="headerlink" title="买电脑主要需求"></a>买电脑主要需求</h2><ol><li>看电影，上网（购物）   </li><li>打游戏</li><li>办公（移动办公）</li><li>平面设计（CAD）</li><li>UI(影视剪辑)</li><li>编程</li><li>其他</li></ol><h2 id="电脑配置说明"><a href="#电脑配置说明" class="headerlink" title="电脑配置说明"></a>电脑配置说明</h2><p>目前电脑配置的CPU（绝对过剩），内存Win10最低要8个G，显卡要根据自己需求一般显卡基本够用，电脑最大的瓶颈都是在硬盘上，所以现在买电脑带不带固态硬盘是我首选的配置（我对固态硬盘定义最低要128G,512G固态才是标配，毕竟固态大小会影响到一定的读写速率，还有为了保证固态寿命做系统时会留出10%的空间不划分到分区中），至于买笔记本还是台式机需要根据不同应用场景来定。台式机性能肯定远超同价位笔记本，这个是毋庸置疑的。</p><h2 id="看电影，上网（购物）"><a href="#看电影，上网（购物）" class="headerlink" title="看电影，上网（购物）"></a>看电影，上网（购物）</h2><p>对于这方面需求的一般一女生居多，看电影上网，对电脑配置要求比较低的，一般普通双核CPU，AMD、酷睿i3都可以（最好是i5），内存8G就够了（win7的话4G就够，但是Win7现在不支持更新了）。要是女生最重要的是漂亮，这里推荐DELL或者HP相对性价比会比较合适。毕竟要是要以轻薄、美观为主。要是资金充足可以考虑各家品牌的超级本。要是父母的需求的话其实买笔记本或者台式机都可以。这里不推荐苹果笔记本，因为用苹果看电影会容易热，要是妹子是苹果控或者周边产品都是苹果产品，苹果笔记本也可在考虑之列。</p><h2 id="打游戏"><a href="#打游戏" class="headerlink" title="打游戏"></a>打游戏</h2><p>游戏主机两个最主要的要求配置和扩展性，主要是CPU和显卡，我们又称之为“双烧”，建议买台式机。要是需要便携的话，外星人品牌是一个不错的考虑，笔记本显卡最好不要超过GTX2070以上，也许你会问为什么不买笔记本GTX2080的本子，一方面是贵，价格会差很多。还有就是散热问题。为了更好体验还是台式机加水冷。</p><ul><li>一般的主流网游：i5或i7处理器，内存16G，中端显卡就可以了，硬盘128G固态+1T机械起</li><li>大型单机：i7或i9处理器（水冷），内存16-32G，，显卡中高端GTX1060起，要是玩刺客信条奥德赛GTX2080Ti不用犹豫，硬盘512三星固态+1T机械（最好在配置1T的固态，毕竟游戏不小）起</li><li>发烧友：i9处理器（水冷），内存32G-64G，显卡高端GTX2080或者是多显交火，硬盘512G（三星固态PRO系列）+1T固态</li></ul><h2 id="办公"><a href="#办公" class="headerlink" title="办公"></a>办公</h2><p>用于办公的大多是商务人士，对笔记本的性能要求一般，最主要的是便携性，各大品牌的超极本都很合适，还能衬托气质，最推荐的还是联想的thinkpad系列，没钱买个E系类（基本三年就会坏），要是有资金充裕T系列或者X系列是首选配置（尤其是X系列）。</p><h2 id="平面设计（CAD）"><a href="#平面设计（CAD）" class="headerlink" title="平面设计（CAD）"></a>平面设计（CAD）</h2><p>这个是专业领域的需求，对CPU、显卡和内存、显示器都较高，能好一点就好一点。    </p><h2 id="UI-影视剪辑"><a href="#UI-影视剪辑" class="headerlink" title="UI(影视剪辑)"></a>UI(影视剪辑)</h2><p>苹果的Macbookpro 16G，512SSD（固态太小用久了会后悔的），i7处理器 最为合适。没有比苹果更适合做平面设计的电脑。Windows系统和苹果系统没得比。</p><h2 id="编程"><a href="#编程" class="headerlink" title="编程"></a>编程</h2><p>苹果的Macbookpro 16G、512SSD、i7处理器。个人推荐MAC的笔记本做编程，一用就停不下来，会上瘾。Windows系统用来打游戏就好了。<br>推荐配置：Macbookpro 16G、i7处理器（i9也是阉割版没意义）、512SSD（固态真的不能太小，512G就不大，考虑到价格没办法）、最好是能带键盘灯、Air pods耳机还是要有一个的，用了就知道不亏。经济允许最好是配置一个IPAD PRO做分屏开发可以调高效率。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>IPAD我对它的定义就是一台游戏机，不建议用IPAD看电影（用久了手会麻）。因为我不做UI我也没有体会到那只笔的好处。</p><p>还有一个设备一点光要说一下就是亚马逊的Kindle，要是你经常看小说，或者是看英文，建议有一个（前期是你不是必须要纸质书）还是很方便的，尤其是书多了的时候。IPAD优势在于pdf文档做笔记。用了就会知道两个不一样。Kindle看电子书是生活品质提升的表现。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 购买合适的电脑：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;br&gt;随着时代的发展，人们生活水平的提高，计算机也成为了我们工作必不可少的生产力，办公，打游戏，看电影，购物等等。市面上电脑种类根据用途又可以分为很多类，台式机、笔记本、IPAD（平板）。购买一台适合自己的电脑工作时可以如虎添翼，电脑用着舒服，心情也会舒畅许多。&lt;/p&gt;
    
    </summary>
    
      <category term="others" scheme="http://zhangfuxin.cn/categories/others/"/>
    
    
      <category term="数码产品" scheme="http://zhangfuxin.cn/tags/%E6%95%B0%E7%A0%81%E4%BA%A7%E5%93%81/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （九）SparkCore的调优之数据倾斜调优</title>
    <link href="http://zhangfuxin.cn/2019-06-09-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B9%9D%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E8%B0%83%E4%BC%98.html"/>
    <id>http://zhangfuxin.cn/2019-06-09-Spark学习之路 （九）SparkCore的调优之数据倾斜调优.html</id>
    <published>2019-06-09T02:30:04.000Z</published>
    <updated>2019-09-16T17:38:39.489Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （九）SparkCore的调优之数据倾斜调优：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （九）SparkCore的调优之数据倾斜调优</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1官网"><a href="#1-1官网" class="headerlink" title="1.1官网"></a>1.1官网</h2><p>官网地址：<a href="http://spark.apache.org" target="_blank" rel="noopener">http://spark.apache.org</a></p><h3 id="1、什么是Spark"><a href="#1、什么是Spark" class="headerlink" title="1、什么是Spark"></a>1、什么是Spark</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （九）SparkCore的调优之数据倾斜调优：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （九）SparkCore的调优之数据倾斜调优&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>cache和persist的区别</title>
    <link href="http://zhangfuxin.cn/2019-06-08-cache%E5%92%8Cpersist%E7%9A%84%E5%8C%BA%E5%88%AB.html"/>
    <id>http://zhangfuxin.cn/2019-06-08-cache和persist的区别.html</id>
    <published>2019-06-08T03:30:04.000Z</published>
    <updated>2019-09-16T17:31:38.817Z</updated>
    
    <content type="html"><![CDATA[<p>** cache和persist的区别：** &lt;Excerpt in index | 首页摘要&gt;</p><p>cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h1 id="cache和persist的区别"><a href="#cache和persist的区别" class="headerlink" title="cache和persist的区别"></a>cache和persist的区别</h1><p>基于Spark 1.6.1 的源码，可以看到</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist()</span><br></pre></td></tr></table></figure><p>说明是cache()调用了persist(), 想要知道二者的不同还需要看一下persist函数：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br></pre></td></tr></table></figure><p>可以看到persist()内部调用了persist(StorageLevel.MEMORY_ONLY)，继续深入：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Set this RDD's storage level to persist its values across operations after the first time</span></span><br><span class="line"><span class="comment"> * it is computed. This can only be used to assign a new storage level if the RDD does not</span></span><br><span class="line"><span class="comment"> * have a storage level set yet..</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> Handle changes of StorageLevel</span></span><br><span class="line">  <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span> &amp;&amp; newLevel != storageLevel) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnsupportedOperationException</span>(</span><br><span class="line">      <span class="string">"Cannot change storage level of an RDD after it was already assigned a level"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  sc.persistRDD(<span class="keyword">this</span>)</span><br><span class="line">  <span class="comment">// Register the RDD with the ContextCleaner for automatic GC-based cleanup</span></span><br><span class="line">  sc.cleaner.foreach(_.registerRDDForCleanup(<span class="keyword">this</span>))</span><br><span class="line">  storageLevel = newLevel</span><br><span class="line">  <span class="keyword">this</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看出来persist有一个 StorageLevel 类型的参数，这个表示的是RDD的缓存级别。</p><p>至此便可得出cache和persist的区别了：cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。</p><h1 id="RDD的缓存级别"><a href="#RDD的缓存级别" class="headerlink" title="RDD的缓存级别"></a>RDD的缓存级别</h1><p>顺便看一下RDD都有哪些缓存级别，查看 StorageLevel 类的源码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>)</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到这里列出了12种缓存级别，但这些有什么区别呢？可以看到每个缓存级别后面都跟了一个StorageLevel的构造函数，里面包含了4个或5个参数，如下</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br></pre></td></tr></table></figure><p>查看其构造函数</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useDisk: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useMemory: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useOffHeap: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _deserialized: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _replication: <span class="type">Int</span> = 1</span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Externalizable</span> </span>&#123;</span><br><span class="line">  ......</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">useDisk</span></span>: <span class="type">Boolean</span> = _useDisk</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">useMemory</span></span>: <span class="type">Boolean</span> = _useMemory</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">useOffHeap</span></span>: <span class="type">Boolean</span> = _useOffHeap</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deserialized</span></span>: <span class="type">Boolean</span> = _deserialized</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">replication</span></span>: <span class="type">Int</span> = _replication</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到StorageLevel类的主构造器包含了5个参数：</p><ul><li>useDisk：使用硬盘（外存）</li><li>useMemory：使用内存</li><li>useOffHeap：使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。</li><li>deserialized：反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象</li><li>replication：备份数（在多个节点上备份）</li></ul><p>理解了这5个参数，StorageLevel 的12种缓存级别就不难理解了。</p><p>val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) 就表示使用这种缓存级别的RDD将存储在硬盘以及内存中，使用序列化（在硬盘中），并且在多个节点上备份2份（正常的RDD只有一份）</p><p>另外还注意到有一种特殊的缓存级别</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure><p>使用了堆外内存，StorageLevel 类的源码中有一段代码可以看出这个的特殊性，它不能和其它几个参数共存。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (useOffHeap) &#123;</span><br><span class="line">  require(!useDisk, <span class="string">"Off-heap storage level does not support using disk"</span>)</span><br><span class="line">  require(!useMemory, <span class="string">"Off-heap storage level does not support using heap memory"</span>)</span><br><span class="line">  require(!deserialized, <span class="string">"Off-heap storage level does not support deserialized storage"</span>)</span><br><span class="line">  require(replication == <span class="number">1</span>, <span class="string">"Off-heap storage level does not support multiple replication"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** cache和persist的区别：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （八）SparkCore的调优之开发调优</title>
    <link href="http://zhangfuxin.cn/2019-06-08-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%85%AB%EF%BC%89SparkCore%E7%9A%84%E8%B0%83%E4%BC%98%E4%B9%8B%E5%BC%80%E5%8F%91%E8%B0%83%E4%BC%98.html"/>
    <id>http://zhangfuxin.cn/2019-06-08-Spark学习之路 （八）SparkCore的调优之开发调优.html</id>
    <published>2019-06-08T02:30:04.000Z</published>
    <updated>2019-09-16T15:05:55.685Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （八）SparkCore的调优之开发调优：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。</p><p>​        然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>​        Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。</p><p>​        笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为<strong>开发调优、资源调优、数据倾斜调优、shuffle调优</strong>几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。</p><p>本文作为Spark性能优化指南的基础篇，主要讲解开发调优以及资源调优。</p><h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>​        Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。</p><h2 id="原则一：避免创建重复的RDD"><a href="#原则一：避免创建重复的RDD" class="headerlink" title="原则一：避免创建重复的RDD"></a>原则一：避免创建重复的RDD</h2><p>​        通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。</p><p>我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。</p><p>一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。</p><h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span></span><br><span class="line"><span class="comment">// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。</span></span><br><span class="line"><span class="comment">// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd2.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span></span><br><span class="line"><span class="comment">// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。</span></span><br><span class="line"><span class="comment">// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</span></span><br><span class="line"><span class="comment">// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><h2 id="原则二：尽可能复用同一个RDD"><a href="#原则二：尽可能复用同一个RDD" class="headerlink" title="原则二：尽可能复用同一个RDD"></a>原则二：尽可能复用同一个RDD</h2><p>​        除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。</p><h3 id="一个简单的例子-1"><a href="#一个简单的例子-1" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 错误的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。</span></span><br><span class="line"><span class="comment">// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line"><span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; rdd2 = rdd1.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分别对rdd1和rdd2执行了不同的算子操作。</span></span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd2.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</span></span><br><span class="line"><span class="comment">// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 其实在这种情况下完全可以复用同一个RDD。</span></span><br><span class="line"><span class="comment">// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。</span></span><br><span class="line"><span class="comment">// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd1.map(tuple._2...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</span></span><br><span class="line"><span class="comment">// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。</span></span><br><span class="line"><span class="comment">// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。</span></span><br></pre></td></tr></table></figure><h2 id="原则三：对多次使用的RDD进行持久化"><a href="#原则三：对多次使用的RDD进行持久化" class="headerlink" title="原则三：对多次使用的RDD进行持久化"></a>原则三：对多次使用的RDD进行持久化</h2><p>​        当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。</p><p>​        Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p><p>​        因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。</p><h3 id="对多次使用的RDD进行持久化的代码示例"><a href="#对多次使用的RDD进行持久化的代码示例" class="headerlink" title="对多次使用的RDD进行持久化的代码示例"></a>对多次使用的RDD进行持久化的代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"><span class="comment">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span></span><br><span class="line"><span class="comment">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span></span><br><span class="line"><span class="comment">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).cache()</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span></span><br><span class="line"><span class="comment">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。</span></span><br><span class="line"><span class="comment">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span></span><br><span class="line"><span class="comment">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><p>对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。</p><h3 id="Spark的持久化级别"><a href="#Spark的持久化级别" class="headerlink" title="Spark的持久化级别"></a>Spark的持久化级别</h3><table><thead><tr><th>持久化级别</th><th>含义解释</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。</td></tr><tr><td>MEMORY_AND_DISK</td><td>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</td></tr><tr><td>MEMORY_ONLY_SER</td><td>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>DISK_ONLY</td><td>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</td></tr><tr><td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.</td><td>对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。</td></tr></tbody></table><h3 id="如何选择一种最合适的持久化策略"><a href="#如何选择一种最合适的持久化策略" class="headerlink" title="如何选择一种最合适的持久化策略"></a>如何选择一种最合适的持久化策略</h3><ul><li>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</li><li>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li><li>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</li><li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li></ul><h2 id="原则四：尽量避免使用shuffle类算子"><a href="#原则四：尽量避免使用shuffle类算子" class="headerlink" title="原则四：尽量避免使用shuffle类算子"></a>原则四：尽量避免使用shuffle类算子</h2><p>​    如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。</p><p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</p><p>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p><h3 id="Broadcast与map进行join代码示例"><a href="#Broadcast与map进行join代码示例" class="headerlink" title="Broadcast与map进行join代码示例"></a>Broadcast与map进行join代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统的join操作会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Broadcast+map的join操作，不会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></span><br><span class="line"><span class="keyword">val</span> rdd2Data = rdd2.collect()</span><br><span class="line"><span class="keyword">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></span><br><span class="line"><span class="comment">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span></span><br><span class="line"><span class="comment">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></span><br><span class="line"><span class="comment">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></span><br></pre></td></tr></table></figure><h2 id="原则五：使用map-side预聚合的shuffle操作"><a href="#原则五：使用map-side预聚合的shuffle操作" class="headerlink" title="原则五：使用map-side预聚合的shuffle操作"></a>原则五：使用map-side预聚合的shuffle操作</h2><p><strong>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。</strong></p><p>​        所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p><p>比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425185853777-491379087.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425185910086-1609967670.png" alt="img"></p><h2 id="原则六：使用高性能的算子"><a href="#原则六：使用高性能的算子" class="headerlink" title="原则六：使用高性能的算子"></a>原则六：使用高性能的算子</h2><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p><h3 id="使用reduceByKey-aggregateByKey替代groupByKey"><a href="#使用reduceByKey-aggregateByKey替代groupByKey" class="headerlink" title="使用reduceByKey/aggregateByKey替代groupByKey"></a>使用reduceByKey/aggregateByKey替代groupByKey</h3><p>详情见“原则五：使用map-side预聚合的shuffle操作”。</p><h3 id="使用mapPartitions替代普通map"><a href="#使用mapPartitions替代普通map" class="headerlink" title="使用mapPartitions替代普通map"></a>使用mapPartitions替代普通map</h3><p>​        mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p><h3 id="使用foreachPartitions替代foreach"><a href="#使用foreachPartitions替代foreach" class="headerlink" title="使用foreachPartitions替代foreach"></a>使用foreachPartitions替代foreach</h3><p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p><h3 id="使用filter之后进行coalesce操作"><a href="#使用filter之后进行coalesce操作" class="headerlink" title="使用filter之后进行coalesce操作"></a>使用filter之后进行coalesce操作</h3><p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p><h3 id="使用repartitionAndSortWithinPartitions替代repartition与sort类操作"><a href="#使用repartitionAndSortWithinPartitions替代repartition与sort类操作" class="headerlink" title="使用repartitionAndSortWithinPartitions替代repartition与sort类操作"></a>使用repartitionAndSortWithinPartitions替代repartition与sort类操作</h3><p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p><h2 id="原则七：广播大变量"><a href="#原则七：广播大变量" class="headerlink" title="原则七：广播大变量"></a>原则七：广播大变量</h2><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p><p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p><p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p><h3 id="广播大变量的代码示例"><a href="#广播大变量的代码示例" class="headerlink" title="广播大变量的代码示例"></a>广播大变量的代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 以下代码在算子函数中，使用了外部的变量。</span></span><br><span class="line"><span class="comment">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line">rdd1.map(list1...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下代码将list1封装成了Broadcast类型的广播变量。</span></span><br><span class="line"><span class="comment">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span></span><br><span class="line"><span class="comment">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span></span><br><span class="line"><span class="comment">// 每个Executor内存中，就只会驻留一份广播变量副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line"><span class="keyword">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class="line">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure><h2 id="原则八：使用Kryo优化序列化性能"><a href="#原则八：使用Kryo优化序列化性能" class="headerlink" title="原则八：使用Kryo优化序列化性能"></a>原则八：使用Kryo优化序列化性能</h2><p>在Spark中，主要有三个地方涉及到了序列化：</p><ul><li>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。</li><li>将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。</li><li>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</li></ul><p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p><p>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建SparkConf对象。</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line"><span class="comment">// 设置序列化器为KryoSerializer。</span></span><br><span class="line">conf.set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line"><span class="comment">// 注册要序列化的自定义类型。</span></span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br></pre></td></tr></table></figure><h2 id="原则九：优化数据结构"><a href="#原则九：优化数据结构" class="headerlink" title="原则九：优化数据结构"></a>原则九：优化数据结构</h2><p>Java中，有三种类型比较耗费内存：</p><ul><li>对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。</li><li>字符串，每个字符串内部都有一个字符数组以及长度等额外信息。</li><li>集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</li></ul><p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p><p>但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。</p><h2 id="原则十：Data-Locality本地化级别"><a href="#原则十：Data-Locality本地化级别" class="headerlink" title="原则十：Data Locality本地化级别"></a>原则十：Data Locality本地化级别</h2><p><strong>PROCESS_LOCAL</strong>：进程本地化，代码和数据在同一个进程中，也就是在同一个executor中；计算数据的task由executor执行，数据在executor的BlockManager中；性能最好</p><p><strong>NODE_LOCAL</strong>：节点本地化，代码和数据在同一个节点中；比如说，数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行；或者是，数据和task在一个节点上的不同executor中；数据需要在进程间进行传输<br><strong>NO_PREF</strong>：对于task来说，数据从哪里获取都一样，没有好坏之分<br><strong>RACK_LOCAL</strong>：机架本地化，数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输<br><strong>ANY</strong>：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差</p><p>spark.locality.wait，默认是3s</p><p>Spark在Driver上，对Application的每一个stage的task，进行分配之前，都会计算出每个task要计算的是哪个分片数据，RDD的某个partition；Spark的task分配算法，优先，会希望每个task正好分配到它要计算的数据所在的节点，这样的话，就不用在网络间传输数据；</p><p>但是可能task没有机会分配到它的数据所在的节点，因为可能那个节点的计算资源和计算能力都满了；所以呢，这种时候，通常来说，Spark会等待一段时间，默认情况下是3s钟（不是绝对的，还有很多种情况，对不同的本地化级别，都会去等待），到最后，实在是等待不了了，就会选择一个比较差的本地化级别，比如说，将task分配到靠它要计算的数据所在节点，比较近的一个节点，然后进行计算。</p><p>但是对于第二种情况，通常来说，肯定是要发生数据传输，task会通过其所在节点的BlockManager来获取数据，BlockManager发现自己本地没有数据，会通过一个getRemote()方法，通过TransferService（网络数据传输组件）从数据所在节点的BlockManager中，获取数据，通过网络传输回task所在节点。</p><p>对于我们来说，当然不希望是类似于第二种情况的了。最好的，当然是task和数据在一个节点上，直接从本地executor的BlockManager中获取数据，纯内存，或者带一点磁盘IO；如果要通过网络传输数据的话，那么实在是，性能肯定会下降的，大量网络传输，以及磁盘IO，都是性能的杀手。</p><p><strong>什么时候要调节这个参数？</strong></p><p>观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。<br>日志里面会显示，starting task。。。，PROCESS LOCAL、NODE LOCAL，观察大部分task的数据本地化级别。</p><p>如果大多都是PROCESS_LOCAL，那就不用调节了<br>如果是发现，好多的级别都是NODE_LOCAL、ANY，那么最好就去调节一下数据本地化的等待时长<br>调节完，应该是要反复调节，每次调节完以后，再来运行，观察日志<br>看看大部分的task的本地化级别有没有提升；看看，整个spark作业的运行时间有没有缩短</p><p>但是注意别本末倒置，本地化级别倒是提升了，但是因为大量的等待时长，spark作业的运行时间反而增加了，那就还是不要调节了。</p><p>spark.locality.wait，默认是3s；可以改成6s，10s</p><p>默认情况下，下面3个的等待时长，都是跟上面那个是一样的，都是3s</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.locality.wait.process//建议60s</span><br><span class="line">spark.locality.wait.node//建议30s</span><br><span class="line">spark.locality.wait.rack//建议20s</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （八）SparkCore的调优之开发调优：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。&lt;/p&gt;
&lt;p&gt;​        然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （七）Spark 运行流程</title>
    <link href="http://zhangfuxin.cn/2019-06-07-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%83%EF%BC%89Spark%20%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B.html"/>
    <id>http://zhangfuxin.cn/2019-06-07-Spark学习之路 （七）Spark 运行流程.html</id>
    <published>2019-06-07T02:30:04.000Z</published>
    <updated>2019-09-16T15:05:27.815Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （七）Spark 运行流程：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （七）Spark 运行流程</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、Spark中的基本概念"><a href="#一、Spark中的基本概念" class="headerlink" title="一、Spark中的基本概念"></a>一、Spark中的基本概念</h2><p>（1）Application：表示你的应用程序</p><p>（2）Driver：表示main()函数，创建SparkContext。由SparkContext负责与ClusterManager通信，进行资源的申请，任务的分配和监控等。程序执行完毕后关闭SparkContext</p><p>（3）Executor：某个Application运行在Worker节点上的一个进程，该进程负责运行某些task，并且负责将数据存在内存或者磁盘上。在Spark on Yarn模式下，其进程名称为 CoarseGrainedExecutor Backend，一个CoarseGrainedExecutor Backend进程有且仅有一个executor对象，它负责将Task包装成taskRunner，并从线程池中抽取出一个空闲线程运行Task，这样，每个CoarseGrainedExecutorBackend能并行运行Task的数据就取决于分配给它的CPU的个数。</p><p>（4）Worker：集群中可以运行Application代码的节点。在Standalone模式中指的是通过slave文件配置的worker节点，在Spark on Yarn模式中指的就是NodeManager节点。</p><p>（5）Task：在Executor进程中执行任务的工作单元，多个Task组成一个Stage</p><p>（6）Job：包含多个Task组成的并行计算，是由Action行为触发的</p><p>（7）Stage：每个Job会被拆分很多组Task，作为一个TaskSet，其名称为Stage</p><p>（8）DAGScheduler：根据Job构建基于Stage的DAG，并提交Stage给TaskScheduler，其划分Stage的依据是RDD之间的依赖关系</p><p>（9）TaskScheduler：将TaskSet提交给Worker（集群）运行，每个Executor运行什么Task就是在此处分配的。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425154512801-941033013.png" alt="img"></p><h2 id="二、Spark的运行流程"><a href="#二、Spark的运行流程" class="headerlink" title="二、Spark的运行流程"></a>二、Spark的运行流程</h2><h3 id="2-1-Spark的基本运行流程"><a href="#2-1-Spark的基本运行流程" class="headerlink" title="2.1　Spark的基本运行流程"></a>2.1　Spark的基本运行流程</h3><h4 id="1、说明"><a href="#1、说明" class="headerlink" title="1、说明"></a>1、说明</h4><blockquote><p>(1)构建Spark Application的运行环境（启动SparkContext），SparkContext向资源管理器（可以是Standalone、Mesos或YARN）注册并申请运行Executor资源；</p><p>(2)资源管理器分配Executor资源并启动StandaloneExecutorBackend，Executor运行情况将随着心跳发送到资源管理器上；</p><p>(3)SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler。Executor向SparkContext申请Task</p><p>(4)Task Scheduler将Task发放给Executor运行同时SparkContext将应用程序代码发放给Executor。</p><p>(5)Task在Executor上运行，运行完毕释放所有资源。</p></blockquote><h4 id="2、图解"><a href="#2、图解" class="headerlink" title="2、图解"></a>2、图解</h4><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425172026316-1086206534.png" alt="img"></p><h4 id="3、Spark运行架构特点"><a href="#3、Spark运行架构特点" class="headerlink" title="3、Spark运行架构特点"></a>3、Spark运行架构特点</h4><blockquote><p>（1）每个Application获取专属的executor进程，该进程在Application期间一直驻留，并以多线程方式运行tasks。这种Application隔离机制有其优势的，无论是从调度角度看（每个Driver调度它自己的任务），还是从运行角度看（来自不同Application的Task运行在不同的JVM中）。当然，这也意味着Spark Application不能跨应用程序共享数据，除非将数据写入到外部存储系统。</p><p>（2）Spark与资源管理器无关，只要能够获取executor进程，并能保持相互通信就可以了。</p><p>（3）提交SparkContext的Client应该靠近Worker节点（运行Executor的节点)，最好是在同一个Rack里，因为Spark Application运行过程中SparkContext和Executor之间有大量的信息交换；如果想在远程集群中运行，最好使用RPC将SparkContext提交给集群，不要远离Worker运行SparkContext。</p><p>（4）Task采用了数据本地性和推测执行的优化机制。</p></blockquote><h4 id="4、DAGScheduler"><a href="#4、DAGScheduler" class="headerlink" title="4、DAGScheduler"></a>4、DAGScheduler</h4><p>Job=多个stage，Stage=多个同种task, Task分为ShuffleMapTask和ResultTask，Dependency分为ShuffleDependency和NarrowDependency</p><p>面向stage的切分，切分依据为宽依赖</p><p>维护waiting jobs和active jobs，维护waiting stages、active stages和failed stages，以及与jobs的映射关系</p><p><strong>主要职能：</strong></p><blockquote><p>1、接收提交Job的主入口，<code>submitJob(rdd, ...)</code>或<code>runJob(rdd, ...)</code>。在<code>SparkContext</code>里会调用这两个方法。 </p><ul><li><ul><li>生成一个Stage并提交，接着判断Stage是否有父Stage未完成，若有，提交并等待父Stage，以此类推。结果是：DAGScheduler里增加了一些waiting stage和一个running stage。</li><li>running stage提交后，分析stage里Task的类型，生成一个Task描述，即TaskSet。</li><li>调用<code>TaskScheduler.submitTask(taskSet, ...)</code>方法，把Task描述提交给TaskScheduler。TaskScheduler依据资源量和触发分配条件，会为这个TaskSet分配资源并触发执行。</li><li><code>DAGScheduler</code>提交job后，异步返回<code>JobWaiter</code>对象，能够返回job运行状态，能够cancel job，执行成功后会处理并返回结果</li></ul></li></ul><p>2、处理<code>TaskCompletionEvent</code> </p><ul><li><ul><li>如果task执行成功，对应的stage里减去这个task，做一些计数工作： <ul><li>如果task是ResultTask，计数器<code>Accumulator</code>加一，在job里为该task置true，job finish总数加一。加完后如果finish数目与partition数目相等，说明这个stage完成了，标记stage完成，从running stages里减去这个stage，做一些stage移除的清理工作</li><li>如果task是ShuffleMapTask，计数器<code>Accumulator</code>加一，在stage里加上一个output location，里面是一个<code>MapStatus</code>类。<code>MapStatus</code>是<code>ShuffleMapTask</code>执行完成的返回，包含location信息和block size(可以选择压缩或未压缩)。同时检查该stage完成，向<code>MapOutputTracker</code>注册本stage里的shuffleId和location信息。然后检查stage的output location里是否存在空，若存在空，说明一些task失败了，整个stage重新提交；否则，继续从waiting stages里提交下一个需要做的stage</li></ul></li><li>如果task是重提交，对应的stage里增加这个task</li><li>如果task是fetch失败，马上标记对应的stage完成，从running stages里减去。如果不允许retry，abort整个stage；否则，重新提交整个stage。另外，把这个fetch相关的location和map任务信息，从stage里剔除，从<code>MapOutputTracker</code>注销掉。最后，如果这次fetch的blockManagerId对象不为空，做一次<code>ExecutorLost</code>处理，下次shuffle会换在另一个executor上去执行。</li><li>其他task状态会由<code>TaskScheduler</code>处理，如Exception, TaskResultLost, commitDenied等。</li></ul></li></ul><p>3、其他与job相关的操作还包括：cancel job， cancel stage, resubmit failed stage等</p></blockquote><p>其他职能：</p><p> cacheLocations 和 preferLocation</p><h4 id="5、TaskScheduler"><a href="#5、TaskScheduler" class="headerlink" title="5、TaskScheduler"></a>5、TaskScheduler</h4><p>维护task和executor对应关系，executor和物理资源对应关系，在排队的task和正在跑的task。</p><p>内部维护一个任务队列，根据FIFO或Fair策略，调度任务。</p><p><code>TaskScheduler</code>本身是个接口，spark里只实现了一个<code>TaskSchedulerImpl</code>，理论上任务调度可以定制。</p><p>主要功能：</p><blockquote><p><code>1、submitTasks(taskSet)</code>，接收<code>DAGScheduler</code>提交来的tasks </p><ul><li><ul><li>为tasks创建一个<code>TaskSetManager</code>，添加到任务队列里。<code>TaskSetManager</code>跟踪每个task的执行状况，维护了task的许多具体信息。</li><li>触发一次资源的索要。 <ul><li>首先，<code>TaskScheduler</code>对照手头的可用资源和Task队列，进行executor分配(考虑优先级、本地化等策略)，符合条件的executor会被分配给<code>TaskSetManager</code>。</li><li>然后，得到的Task描述交给<code>SchedulerBackend</code>，调用<code>launchTask(tasks)</code>，触发executor上task的执行。task描述被序列化后发给executor，executor提取task信息，调用task的<code>run()</code>方法执行计算。</li></ul></li></ul></li></ul><p><code>2、cancelTasks(stageId)</code>，取消一个stage的tasks </p><ul><li><ul><li>调用<code>SchedulerBackend</code>的<code>killTask(taskId, executorId, ...)</code>方法。taskId和executorId在<code>TaskScheduler</code>里一直维护着。</li></ul></li></ul><p>3、resourceOffer(offers: Seq[Workers])，这是非常重要的一个方法，调用者是SchedulerBacnend，用途是底层资源SchedulerBackend把空余的workers资源交给TaskScheduler，让其根据调度策略为排队的任务分配合理的cpu和内存资源，然后把任务描述列表传回给SchedulerBackend</p><ul><li><ul><li>从worker offers里，搜集executor和host的对应关系、active executors、机架信息等等</li><li>worker offers资源列表进行随机洗牌，任务队列里的任务列表依据调度策略进行一次排序</li><li>遍历每个taskSet，按照进程本地化、worker本地化、机器本地化、机架本地化的优先级顺序，为每个taskSet提供可用的cpu核数，看是否满足 <ul><li>默认一个task需要一个cpu，设置参数为<code>&quot;spark.task.cpus=1&quot;</code></li><li>为taskSet分配资源，校验是否满足的逻辑，最终在<code>TaskSetManager</code>的<code>resourceOffer(execId, host, maxLocality)</code>方法里</li><li>满足的话，会生成最终的任务描述，并且调用<code>DAGScheduler</code>的<code>taskStarted(task, info)</code>方法，通知<code>DAGScheduler</code>，这时候每次会触发<code>DAGScheduler</code>做一次<code>submitMissingStage</code>的尝试，即stage的tasks都分配到了资源的话，马上会被提交执行</li></ul></li></ul></li></ul><p><code>4、statusUpdate(taskId, taskState, data)</code>,另一个非常重要的方法，调用者是<code>SchedulerBacnend</code>，用途是<code>SchedulerBacnend</code>会将task执行的状态汇报给<code>TaskScheduler</code>做一些决定 </p><ul><li><ul><li>若<code>TaskLost</code>，找到该task对应的executor，从active executor里移除，避免这个executor被分配到其他task继续失败下去。</li><li>task finish包括四种状态：finished, killed, failed, lost。只有finished是成功执行完成了。其他三种是失败。</li><li>task成功执行完，调用<code>TaskResultGetter.enqueueSuccessfulTask(taskSet, tid, data)</code>，否则调用<code>TaskResultGetter.enqueueFailedTask(taskSet, tid, state, data)</code>。<code>TaskResultGetter</code>内部维护了一个线程池，负责异步fetch task执行结果并反序列化。默认开四个线程做这件事，可配参数<code>&quot;spark.resultGetter.threads&quot;=4</code>。</li></ul></li></ul></blockquote><p> <strong>TaskResultGetter取task result的逻辑</strong></p><blockquote><p>1、对于success task，如果taskResult里的数据是直接结果数据，直接把data反序列出来得到结果；如果不是，会调用<code>blockManager.getRemoteBytes(blockId)</code>从远程获取。如果远程取回的数据是空的，那么会调用<code>TaskScheduler.handleFailedTask</code>，告诉它这个任务是完成了的但是数据是丢失的。否则，取到数据之后会通知<code>BlockManagerMaster</code>移除这个block信息，调用<code>TaskScheduler.handleSuccessfulTask</code>，告诉它这个任务是执行成功的，并且把result data传回去。</p><p>2、对于failed task，从data里解析出fail的理由，调用<code>TaskScheduler.handleFailedTask</code>，告诉它这个任务失败了，理由是什么。</p></blockquote><h4 id="6、SchedulerBackend"><a href="#6、SchedulerBackend" class="headerlink" title="6、SchedulerBackend"></a>6、SchedulerBackend</h4><p>在<code>TaskScheduler</code>下层，用于对接不同的资源管理系统，<code>SchedulerBackend</code>是个接口，需要实现的主要方法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def start(): Unit</span><br><span class="line">def stop(): Unit</span><br><span class="line">def reviveOffers(): Unit // 重要方法：SchedulerBackend把自己手头上的可用资源交给TaskScheduler，TaskScheduler根据调度策略分配给排队的任务吗，返回一批可执行的任务描述，SchedulerBackend负责launchTask，即最终把task塞到了executor模型上，executor里的线程池会执行task的run()</span><br><span class="line">def killTask(taskId: Long, executorId: String, interruptThread: Boolean): Unit =</span><br><span class="line">    throw new UnsupportedOperationException</span><br></pre></td></tr></table></figure><p>粗粒度：进程常驻的模式，典型代表是standalone模式，mesos粗粒度模式，yarn</p><p>细粒度：mesos细粒度模式</p><p>这里讨论粗粒度模式，更好理解：<code>CoarseGrainedSchedulerBackend</code>。</p><p>维护executor相关信息(包括executor的地址、通信端口、host、总核数，剩余核数)，手头上executor有多少被注册使用了，有多少剩余，总共还有多少核是空的等等。</p><p>主要职能</p><blockquote><p>1、Driver端主要通过actor监听和处理下面这些事件： </p><ul><li><ul><li><code>RegisterExecutor(executorId, hostPort, cores, logUrls)</code>。这是executor添加的来源，通常worker拉起、重启会触发executor的注册。<code>CoarseGrainedSchedulerBackend</code>把这些executor维护起来，更新内部的资源信息，比如总核数增加。最后调用一次<code>makeOffer()</code>，即把手头资源丢给<code>TaskScheduler</code>去分配一次，返回任务描述回来，把任务launch起来。这个<code>makeOffer()</code>的调用会出现在<em>任何与资源变化相关的事件</em>中，下面会看到。</li><li><code>StatusUpdate(executorId, taskId, state, data)</code>。task的状态回调。首先，调用<code>TaskScheduler.statusUpdate</code>上报上去。然后，判断这个task是否执行结束了，结束了的话把executor上的freeCore加回去，调用一次<code>makeOffer()</code>。</li><li><code>ReviveOffers</code>。这个事件就是别人直接向<code>SchedulerBackend</code>请求资源，直接调用<code>makeOffer()</code>。</li><li><code>KillTask(taskId, executorId, interruptThread)</code>。这个killTask的事件，会被发送给executor的actor，executor会处理<code>KillTask</code>这个事件。</li><li><code>StopExecutors</code>。通知每一个executor，处理<code>StopExecutor</code>事件。</li><li><code>RemoveExecutor(executorId, reason)</code>。从维护信息中，那这堆executor涉及的资源数减掉，然后调用<code>TaskScheduler.executorLost()</code>方法，通知上层我这边有一批资源不能用了，你处理下吧。<code>TaskScheduler</code>会继续把<code>executorLost</code>的事件上报给<code>DAGScheduler</code>，原因是<code>DAGScheduler</code>关心shuffle任务的output location。<code>DAGScheduler</code>会告诉<code>BlockManager</code>这个executor不可用了，移走它，然后把所有的stage的shuffleOutput信息都遍历一遍，移走这个executor，并且把更新后的shuffleOutput信息注册到<code>MapOutputTracker</code>上，最后清理下本地的<code>CachedLocations</code>Map。</li></ul></li></ul><p><code>2、reviveOffers()</code>方法的实现。直接调用了<code>makeOffers()</code>方法，得到一批可执行的任务描述，调用<code>launchTasks</code>。</p><p><code>3、launchTasks(tasks: Seq[Seq[TaskDescription]])</code>方法。 </p><ul><li><ul><li>遍历每个task描述，序列化成二进制，然后发送给每个对应的executor这个任务信息 <ul><li>如果这个二进制信息太大，超过了9.2M(默认的akkaFrameSize 10M 减去 默认 为akka留空的200K)，会出错，abort整个taskSet，并打印提醒增大akka frame size</li><li>如果二进制数据大小可接受，发送给executor的actor，处理<code>LaunchTask(serializedTask)</code>事件。</li></ul></li></ul></li></ul></blockquote><h4 id="7、Executor"><a href="#7、Executor" class="headerlink" title="7、Executor"></a>7、Executor</h4><p>Executor是spark里的进程模型，可以套用到不同的资源管理系统上，与<code>SchedulerBackend</code>配合使用。</p><p>内部有个线程池，有个running tasks map，有个actor，接收上面提到的由<code>SchedulerBackend</code>发来的事件。</p><p><strong>事件处理</strong></p><ol><li><code>launchTask</code>。根据task描述，生成一个<code>TaskRunner</code>线程，丢尽running tasks map里，用线程池执行这个<code>TaskRunner</code></li><li><code>killTask</code>。从running tasks map里拿出线程对象，调它的kill方法。</li></ol><h2 id="三、Spark在不同集群中的运行架构"><a href="#三、Spark在不同集群中的运行架构" class="headerlink" title="三、Spark在不同集群中的运行架构"></a>三、Spark在不同集群中的运行架构</h2><p>Spark注重建立良好的生态系统，它不仅支持多种外部文件存储系统，提供了多种多样的集群运行模式。部署在单台机器上时，既可以用本地（Local）模式运行，也可以使用伪分布式模式来运行；当以分布式集群部署的时候，可以根据自己集群的实际情况选择Standalone模式（Spark自带的模式）、YARN-Client模式或者YARN-Cluster模式。Spark的各种运行模式虽然在启动方式、运行位置、调度策略上各有不同，但它们的目的基本都是一致的，就是在合适的位置安全可靠的根据用户的配置和Job的需要运行和管理Task。</p><h3 id="3-1-Spark-on-Standalone运行过程"><a href="#3-1-Spark-on-Standalone运行过程" class="headerlink" title="3.1　Spark on Standalone运行过程"></a>3.1　Spark on Standalone运行过程</h3><p>Standalone模式是Spark实现的资源调度框架，其主要的节点有Client节点、Master节点和Worker节点。其中Driver既可以运行在Master节点上中，也可以运行在本地Client端。当用spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行；当使用spark-submit工具提交Job或者在Eclips、IDEA等开发平台上使用”new SparkConf().setMaster(“spark://master:7077”)”方式运行Spark任务时，Driver是运行在本地Client端上的。</p><p>运行过程文字说明</p><blockquote><p>1、我们提交一个任务，任务就叫Application<br>2、初始化程序的入口SparkContext，<br>　　2.1 初始化DAG Scheduler<br>　　2.2 初始化Task Scheduler<br>3、Task Scheduler向master去进行注册并申请资源（CPU Core和Memory）<br>4、Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上获取资源，然后启动StandaloneExecutorBackend；顺便初<br>      始化好了一个线程池<br>5、StandaloneExecutorBackend向Driver(SparkContext)注册,这样Driver就知道哪些Executor为他进行服务了。<br>　  到这个时候其实我们的初始化过程基本完成了，我们开始执行transformation的代码，但是代码并不会真正的运行，直到我们遇到一个action操作。生产一个job任务，进行stage的划分<br>6、SparkContext将Applicaiton代码发送给StandaloneExecutorBackend；并且SparkContext解析Applicaiton代码，构建DAG图，并提交给DAG Scheduler分解成Stage（当碰到Action操作        时，就会催生Job；每个Job中含有1个或多个Stage，Stage一般在获取外部数据和shuffle之前产生）。<br>7、将Stage（或者称为TaskSet）提交给Task Scheduler。Task Scheduler负责将Task分配到相应的Worker，最后提交给StandaloneExecutorBackend执行；<br>8、对task进行序列化，并根据task的分配算法，分配task<br>9、对接收过来的task进行反序列化，把task封装成一个线程<br>10、开始执行Task，并向SparkContext报告，直至Task完成。<br>11、资源注销</p></blockquote><p>运行过程图形说明</p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425170820868-121220770.png" alt="img" style="zoom:200%;"><h3 id="3-2-Spark-on-YARN运行过程"><a href="#3-2-Spark-on-YARN运行过程" class="headerlink" title="3.2　Spark on YARN运行过程"></a>3.2　Spark on YARN运行过程</h3><p>YARN是一种统一资源管理机制，在其上面可以运行多套计算框架。目前的大数据技术世界，大多数公司除了使用Spark来进行数据计算，由于历史原因或者单方面业务处理的性能考虑而使用着其他的计算框架，比如MapReduce、Storm等计算框架。Spark基于此种情况开发了Spark on YARN的运行模式，由于借助了YARN良好的弹性资源管理机制，不仅部署Application更加方便，而且用户在YARN集群中运行的服务和Application的资源也完全隔离，更具实践应用价值的是YARN可以通过队列的方式，管理同时运行在集群中的多个服务。</p><p>Spark on YARN模式根据Driver在集群中的位置分为两种模式：一种是YARN-Client模式，另一种是YARN-Cluster（或称为YARN-Standalone模式）。</p><h4 id="3-2-1-YARN框架流程"><a href="#3-2-1-YARN框架流程" class="headerlink" title="3.2.1　YARN框架流程"></a>3.2.1　YARN框架流程</h4><p>任何框架与YARN的结合，都必须遵循YARN的开发模式。在分析Spark on YARN的实现细节之前，有必要先分析一下YARN框架的一些基本原理。</p><p>参考：<a href="http://www.cnblogs.com/qingyunzong/p/8615096.html" target="_blank" rel="noopener">http://www.cnblogs.com/qingyunzong/p/8615096.html</a></p><h4 id="3-2-2-YARN-Client"><a href="#3-2-2-YARN-Client" class="headerlink" title="3.2.2　YARN-Client"></a>3.2.2　YARN-Client</h4><p>Yarn-Client模式中，Driver在客户端本地运行，这种模式可以使得Spark Application和客户端进行交互，因为Driver在客户端，所以可以通过webUI访问Driver的状态，默认是<a href="http://hadoop1:4040访问，而YARN通过http://" target="_blank" rel="noopener">http://hadoop1:4040访问，而YARN通过http://</a> hadoop1:8088访问。</p><p>YARN-client的工作流程分为以下几个步骤：</p><p>文字说明</p><blockquote><p>1.Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend；</p><p>2.ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派；</p><p>3.Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）；</p><p>4.一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task；</p><p>5.Client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；</p><p>6.应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。</p></blockquote><p>图片说明</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425184438832-531815073.png" alt="img"></p><h4 id="3-2-3-YARN-Cluster"><a href="#3-2-3-YARN-Cluster" class="headerlink" title="3.2.3　YARN-Cluster"></a>3.2.3　YARN-Cluster</h4><p>在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动；第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成。</p><p>YARN-cluster的工作流程分为以下几个步骤：</p><p>文字说明</p><blockquote><ol><li><p>Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等；</p></li><li><p>ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化；</p></li><li><p>ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束；</p></li><li><p>一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等；</p></li><li><p>ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；</p></li><li><p>应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己。</p></li></ol></blockquote><p> 图片说明</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425184600831-1537558526.png" alt="img"></p><h4 id="3-2-4-YARN-Client-与-YARN-Cluster-区别"><a href="#3-2-4-YARN-Client-与-YARN-Cluster-区别" class="headerlink" title="3.2.4　YARN-Client 与 YARN-Cluster 区别"></a>3.2.4　YARN-Client 与 YARN-Cluster 区别</h4><p>理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别。</p><blockquote><p>1、YARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业；</p><p>2、YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开。</p></blockquote><p> <img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425184834257-26846302.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425184850521-1989517165.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （七）Spark 运行流程：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （七）Spark 运行流程&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>map与flatMap的区别</title>
    <link href="http://zhangfuxin.cn/2019-06-06-map%E4%B8%8EflatMap%E7%9A%84%E5%8C%BA%E5%88%AB.html"/>
    <id>http://zhangfuxin.cn/2019-06-06-map与flatMap的区别.html</id>
    <published>2019-06-06T04:30:04.000Z</published>
    <updated>2019-09-16T09:39:14.905Z</updated>
    
    <content type="html"><![CDATA[<p>** map与flatMap的区别：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        map与flatMap的区别</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>spark的转换算子中map和flatMap都十分常见，要了解清楚它们的区别，我们必须弄懂每执行一次的数据结构是什么。</p><blockquote><p>we are superman</p><p>torrow is good</p><p>color green red</p></blockquote><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p>map操作结果：Array[Array[String]] = Array(Array(we, are, superman), Array(torrow, is, good), Array(color, green, red))</p><p>flatmap操作结果：Array[String] = Array(we, are, superman, torrow, is, good, color, green, red)</p><p>spark中map函数会对每一条输入进行指定操作，然后为每一条输入返回一个对象；</p><p>而flatmap函数则是两个操作的集合，最后将所有对象合并为一个对象。需要特别说明一下，flatmap适用于统计文件单词类的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** map与flatMap的区别：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        map与flatMap的区别&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （五）Spark伪分布式安装</title>
    <link href="http://zhangfuxin.cn/2019-06-05-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%94%EF%BC%89Spark%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85.html"/>
    <id>http://zhangfuxin.cn/2019-06-05-Spark学习之路 （五）Spark伪分布式安装.html</id>
    <published>2019-06-05T02:30:04.000Z</published>
    <updated>2019-09-16T08:27:02.534Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （五）Spark伪分布式安装：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （五）Spark伪分布式安装</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p><strong>Hadoop部分建议参考hadoop伪分布部署</strong></p><h2 id="一、JDK的安装"><a href="#一、JDK的安装" class="headerlink" title="一、JDK的安装"></a>一、JDK的安装</h2><p>​    LINUX系统安装jdk（最好是1.8版本）</p><h3 id="1-1-上传安装包并解压"><a href="#1-1-上传安装包并解压" class="headerlink" title="1.1　上传安装包并解压"></a>1.1　上传安装包并解压</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 soft]# tar -zxvf jdk-8u73-linux-x64.tar.gz -C /usr/local/</span><br></pre></td></tr></table></figure><h3 id="1-2-配置环境变量"><a href="#1-2-配置环境变量" class="headerlink" title="1.2　配置环境变量"></a>1.2　配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 soft]# vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">JAVA</span></span><br><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_73</span><br><span class="line">export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib </span><br><span class="line">export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH:$HOME/bin</span><br></pre></td></tr></table></figure><h3 id="1-3-验证Java版本"><a href="#1-3-验证Java版本" class="headerlink" title="1.3　验证Java版本"></a>1.3　验证Java版本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 soft]# java -version</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421201645784-1699980464.png" alt="img"></p><h2 id="二、配置免密登陆"><a href="#二、配置免密登陆" class="headerlink" title="二、配置免密登陆"></a>二、配置免密登陆</h2><h3 id="2-1-检测"><a href="#2-1-检测" class="headerlink" title="2.1　检测"></a>2.1　检测</h3><p>正常情况下，本机通过ssh连接自己也是需要输入密码的</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421202047798-1815346135.png" alt="img"></p><h3 id="2-2-生成私钥和公钥秘钥对"><a href="#2-2-生成私钥和公钥秘钥对" class="headerlink" title="2.2　生成私钥和公钥秘钥对"></a>2.2　生成私钥和公钥秘钥对</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421202332319-461480990.png" alt="img"></p><h3 id="2-3-将公钥添加到authorized-keys"><a href="#2-3-将公钥添加到authorized-keys" class="headerlink" title="2.3　将公钥添加到authorized_keys"></a>2.3　将公钥添加到authorized_keys</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><h3 id="2-4-赋予authorized-keys文件600的权限"><a href="#2-4-赋予authorized-keys文件600的权限" class="headerlink" title="2.4　赋予authorized_keys文件600的权限"></a>2.4　赋予authorized_keys文件600的权限</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><h3 id="2-5-修改Linux映射文件-root用户"><a href="#2-5-修改Linux映射文件-root用户" class="headerlink" title="2.5　修改Linux映射文件(root用户)"></a>2.5　修改Linux映射文件(root用户)</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 ~]$ vi /etc/hosts</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421202658345-1846477390.png" alt="img"></p><h3 id="2-6-验证"><a href="#2-6-验证" class="headerlink" title="2.6　验证"></a>2.6　验证</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ ssh hadoop1</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421203427884-1155695129.png" alt="img"></p><p>此时不需要输入密码，免密登录设置成功。</p><h2 id="三、安装Hadoop-2-7-5"><a href="#三、安装Hadoop-2-7-5" class="headerlink" title="三、安装Hadoop-2.7.5"></a>三、安装Hadoop-2.7.5</h2><h3 id="3-1-上传解压缩"><a href="#3-1-上传解压缩" class="headerlink" title="3.1　上传解压缩"></a>3.1　上传解压缩</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ tar -zxvf hadoop-2.7.5-centos-6.7.tar.gz -C apps/</span><br></pre></td></tr></table></figure><h3 id="3-2-创建安装包对应的软连接"><a href="#3-2-创建安装包对应的软连接" class="headerlink" title="3.2　创建安装包对应的软连接"></a>3.2　创建安装包对应的软连接</h3><p>为解压的hadoop包创建软连接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop1 apps]$ ll</span><br><span class="line">总用量 4</span><br><span class="line">drwxr-xr-x. 9 hadoop hadoop 4096 12月 24 13:43 hadoop-2.7.5</span><br><span class="line">[hadoop@hadoop1 apps]$ ln -s hadoop-2.7.5/ hadoop</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421203914403-1198562712.png" alt="img"></p><h3 id="3-3-修改配置文件"><a href="#3-3-修改配置文件" class="headerlink" title="3.3　修改配置文件"></a>3.3　修改配置文件</h3><p>进入/home/hadoop/apps/hadoop/etc/hadoop/目录下修改配置文件</p><h4 id="（1）修改hadoop-env-sh"><a href="#（1）修改hadoop-env-sh" class="headerlink" title="（1）修改hadoop-env.sh"></a>（1）修改hadoop-env.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ vi hadoop-env.sh </span><br><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_73</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421204243611-778828987.png" alt="img"></p><h4 id="（2）修改core-site-xml"><a href="#（2）修改core-site-xml" class="headerlink" title="（2）修改core-site.xml"></a>（2）修改core-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ vi core-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/data/hadoopdata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421204517804-907096171.png" alt="img"></p><h4 id="（3）修改hdfs-site-xml"><a href="#（3）修改hdfs-site-xml" class="headerlink" title="（3）修改hdfs-site.xml"></a>（3）修改hdfs-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ vi hdfs-site.xml</span><br></pre></td></tr></table></figure><p>dfs的备份数目，单机用1份就行</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/data/hadoopdata/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>为了保证元数据的安全一般配置多个不同目录<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/data/hadoopdata/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>datanode 的数据存储目录<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>HDFS 的数据块的副本存储个数, 默认是3<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421204809515-38083445.png" alt="img"></p><h4 id="（4）修改mapred-site-xml"><a href="#（4）修改mapred-site-xml" class="headerlink" title="（4）修改mapred-site.xml"></a>（4）修改mapred-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ cp mapred-site.xml.template mapred-site.xml</span><br><span class="line">[hadoop@hadoop1 hadoop]$ vi mapred-site.xml</span><br></pre></td></tr></table></figure><p>mapreduce.framework.name：指定mr框架为yarn方式,Hadoop二代MP也基于资源管理系统Yarn来运行 。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421205034511-1378796109.png" alt="img"></p><h4 id="（5）修改yarn-site-xml"><a href="#（5）修改yarn-site-xml" class="headerlink" title="（5）修改yarn-site.xml"></a>（5）修改yarn-site.xml</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ vi yarn-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">description</span>&gt;</span>YARN 集群为 MapReduce 程序提供的 shuffle 服务<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421205232938-938870356.png" alt="img"></p><h3 id="3-4-配置环境变量"><a href="#3-4-配置环境变量" class="headerlink" title="3.4　配置环境变量"></a>3.4　配置环境变量</h3><p>千万注意：</p><p>1、如果你使用root用户进行安装。 vi /etc/profile 即可 系统变量</p><p>2、如果你使用普通用户进行安装。 vi ~/.bashrc 用户变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ vi .bashrc</span><br><span class="line"><span class="meta">#</span><span class="bash">HADOOP_HOMEexport HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5</span></span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421205503165-1776868707.png" alt="img"></p><p>使环境变量生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 bin]$ source ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="3-5-查看hadoop版本"><a href="#3-5-查看hadoop版本" class="headerlink" title="3.5　查看hadoop版本"></a>3.5　查看hadoop版本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop version</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421205704500-162738228.png" alt="img"></p><h3 id="3-6-创建文件夹"><a href="#3-6-创建文件夹" class="headerlink" title="3.6　创建文件夹"></a>3.6　创建文件夹</h3><p>文件夹的路径参考配置文件hdfs-site.xml里面的路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ mkdir -p /home/hadoop/data/hadoopdata/name</span><br><span class="line">[hadoop@hadoop1 ~]$ mkdir -p /home/hadoop/data/hadoopdata/data</span><br></pre></td></tr></table></figure><h3 id="3-7-Hadoop的初始化"><a href="#3-7-Hadoop的初始化" class="headerlink" title="3.7　Hadoop的初始化"></a>3.7　Hadoop的初始化</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ hadoop namenode -format</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421210111098-1871019705.png" alt="img"></p><h3 id="3-8-启动HDFS和YARN"><a href="#3-8-启动HDFS和YARN" class="headerlink" title="3.8　启动HDFS和YARN"></a>3.8　启动HDFS和YARN</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ start-dfs.sh[hadoop@hadoop1 ~]$ start-yarn.sh</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180422113818864-1072637752.png" alt="img"></p><h3 id="3-9-检查WebUI"><a href="#3-9-检查WebUI" class="headerlink" title="3.9　检查WebUI"></a>3.9　检查WebUI</h3><p>浏览器打开端口50070：<a href="http://hadoop1:50070/" target="_blank" rel="noopener">http://hadoop1:50070</a></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180422113937564-1552905493.png" alt="img"></p><p>其他端口说明：<br>port 8088: cluster and all applications<br>port 50070: Hadoop NameNode<br>port 50090: Secondary NameNode<br>port 50075: DataNode </p><h2 id="四、Scala的安装（可选）"><a href="#四、Scala的安装（可选）" class="headerlink" title="四、Scala的安装（可选）"></a>四、Scala的安装（可选）</h2><p>使用root安装</p><h3 id="4-1-下载"><a href="#4-1-下载" class="headerlink" title="4.1　下载"></a>4.1　下载</h3><p>Scala下载地址<a href="http://www.scala-lang.org/download/all.html" target="_blank" rel="noopener">http://www.scala-lang.org/download/all.html</a></p><p>选择对应的版本，此处在Linux上安装，选择的版本是scala-2.11.8.tgz</p><h3 id="4-2-上传解压缩"><a href="#4-2-上传解压缩" class="headerlink" title="4.2　上传解压缩"></a>4.2　上传解压缩</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 hadoop]# tar -zxvf scala-2.11.8.tgz -C /usr/local/</span><br></pre></td></tr></table></figure><h3 id="4-3-配置环境变量"><a href="#4-3-配置环境变量" class="headerlink" title="4.3　配置环境变量"></a>4.3　配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 hadoop]# vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">Scala</span></span><br><span class="line">export SCALA_HOME=/usr/local/scala-2.11.8</span><br><span class="line">export PATH=$SCALA_HOME/bin:$PATH</span><br></pre></td></tr></table></figure><p>保存并使其立即生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 scala-2.11.8]# source /etc/profile</span><br></pre></td></tr></table></figure><h3 id="4-4-验证是否安装成功"><a href="#4-4-验证是否安装成功" class="headerlink" title="4.4　验证是否安装成功"></a>4.4　验证是否安装成功</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop1 ~]# scala -version</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180422115201465-370446833.png" alt="img"></p><h2 id="五、Spark的安装"><a href="#五、Spark的安装" class="headerlink" title="五、Spark的安装"></a>五、Spark的安装</h2><h3 id="5-1-下载安装包"><a href="#5-1-下载安装包" class="headerlink" title="5.1　下载安装包"></a>5.1　下载安装包</h3><p>下载地址：</p><p><a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">http://spark.apache.org/downloads.html</a></p><h3 id="5-2-上传解压缩"><a href="#5-2-上传解压缩" class="headerlink" title="5.2　上传解压缩"></a>5.2　上传解压缩</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz -C apps/</span><br></pre></td></tr></table></figure><h3 id="5-3-为解压包创建一个软连接"><a href="#5-3-为解压包创建一个软连接" class="headerlink" title="5.3　为解压包创建一个软连接"></a>5.3　为解压包创建一个软连接</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop1 apps]$ ls</span><br><span class="line">hadoop  hadoop-2.7.5  spark-2.3.0-bin-hadoop2.7</span><br><span class="line">[hadoop@hadoop1 apps]$ ln -s spark-2.3.0-bin-hadoop2.7/ spark</span><br></pre></td></tr></table></figure><h3 id="5-4-进入spark-conf修改配置文件"><a href="#5-4-进入spark-conf修改配置文件" class="headerlink" title="5.4　进入spark/conf修改配置文件"></a>5.4　进入spark/conf修改配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 apps]$ cd spark/conf/</span><br></pre></td></tr></table></figure><p> 复制spark-env.sh.template并重命名为spark-env.sh，并在文件最后添加配置内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ cp spark-env.sh.template spark-env.sh</span><br><span class="line">[hadoop@hadoop1 conf]$ vi spark-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_73</span><br><span class="line">export SCALA_HOME=/usr/share/scala-2.11.8</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop/apps/hadoop-2.7.5/etc/hadoop</span><br><span class="line">export SPARK_MASTER_IP=hadoop1</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure><h3 id="5-5-配置环境变量"><a href="#5-5-配置环境变量" class="headerlink" title="5.5　配置环境变量"></a>5.5　配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ vi ~/.bashrc </span><br><span class="line"><span class="meta">#</span><span class="bash">SPARK_HOME</span></span><br><span class="line">export SPARK_HOME=/home/hadoop/apps/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><p>保存使其立即生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ source ~/.bashrc</span><br></pre></td></tr></table></figure><h3 id="5-6-启动Spark"><a href="#5-6-启动Spark" class="headerlink" title="5.6　启动Spark"></a>5.6　启动Spark</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$  ~/apps/spark/sbin/start-all.sh</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180422120649667-993379882.png" alt="img"></p><h3 id="5-7-查看进程"><a href="#5-7-查看进程" class="headerlink" title="5.7　查看进程"></a>5.7　查看进程</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180422120759529-1681016835.png" alt="img"></p><h3 id="5-8-查看web界面"><a href="#5-8-查看web界面" class="headerlink" title="5.8　查看web界面"></a>5.8　查看web界面</h3><p><a href="http://hadoop1:8080/" target="_blank" rel="noopener">http://hadoop1:8080/</a></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180422120839429-839164601.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （五）Spark伪分布式安装：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （五）Spark伪分布式安装&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （四）Spark的广播变量和累加器</title>
    <link href="http://zhangfuxin.cn/2019-06-04-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E5%9B%9B%EF%BC%89Spark%E7%9A%84%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F%E5%92%8C%E7%B4%AF%E5%8A%A0%E5%99%A8.html"/>
    <id>http://zhangfuxin.cn/2019-06-04-Spark学习之路 （四）Spark的广播变量和累加器.html</id>
    <published>2019-06-04T03:30:04.000Z</published>
    <updated>2019-09-16T08:06:46.996Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （四）Spark的广播变量和累加器：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        在spark程序中，当一个传递给Spark操作(例如map和reduce)的函数在远程节点上面运行时，Spark操作实际上操作的是这个函数所用变量的一个独立副本。这些变量会被复制到每台机器上，并且这些变量在远程机器上的所有更新都不会传递回驱动程序。通常跨任务的读写变量是低效的，但是，Spark还是为两种常见的使用模式提供了两种有限的共享变量：广播变（broadcast variable）和累加器（accumulator）</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、广播变量broadcast-variable"><a href="#一、广播变量broadcast-variable" class="headerlink" title="一、广播变量broadcast variable"></a>一、广播变量broadcast variable</h2><p>​        广播变量允许程序员在每台机器上保留一个只读变量，而不是随副本一起发送它的副本。例如，它们可用于以有效的方式为每个节点提供大输入数据集的副本。Spark还尝试使用有效的广播算法来分发广播变量，以降低通信成本。</p><p>​        Spark动作通过一组阶段执行，由分布式“shuffle”操作分隔。Spark自动广播每个阶段中任务所需的公共数据。以这种方式广播的数据以序列化形式缓存并在运行每个任务之前反序列化。这意味着显式创建广播变量仅在跨多个阶段的任务需要相同数据或以反序列化形式缓存数据很重要时才有用。</p><p>​        广播变量是<code>v</code>通过调用从变量创建的<code>SparkContext.broadcast(v)</code>。广播变量是一个包装器<code>v</code>，可以通过调用该<code>value</code> 方法来访问它的值。下面的代码显示了这个：</p><h3 id="1-1-为什么要将变量定义成广播变量？"><a href="#1-1-为什么要将变量定义成广播变量？" class="headerlink" title="1.1　为什么要将变量定义成广播变量？"></a>1.1　为什么要将变量定义成广播变量？</h3><p>如果我们要在分布式计算里面分发大对象，例如：字典，集合，黑白名单等，这个都会由Driver端进行分发，一般来讲，如果这个变量不是广播变量，那么每个task就会分发一份，这在<strong>task数目十分多的情况下Driver的带宽会成为系统的瓶颈，而且会大量消耗task服务器上的资源</strong>，如果将这个变量声明为广播变量，那么知识每个executor拥有一份，这个executor启动的task会共享这个变量，节省了通信的成本和服务器的资源。</p><h3 id="1-2-广播变量图解"><a href="#1-2-广播变量图解" class="headerlink" title="1.2　广播变量图解"></a>1.2　广播变量图解</h3><p>错误的，不使用广播变量</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421162057226-1988253385.png" alt="img"></p><p>正确的，使用广播变量的情况</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421162148572-1992224700.png" alt="img"></p><h3 id="2-3-如何定义一个广播变量？"><a href="#2-3-如何定义一个广播变量？" class="headerlink" title="2.3　如何定义一个广播变量？"></a>2.3　如何定义一个广播变量？</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> a = <span class="number">3</span></span><br><span class="line"><span class="keyword">val</span> broadcast = sc.broadcast(a)</span><br></pre></td></tr></table></figure><h3 id="2-4-如何还原一个广播变量？"><a href="#2-4-如何还原一个广播变量？" class="headerlink" title="2.4　如何还原一个广播变量？"></a>2.4　如何还原一个广播变量？</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> c = broadcast.value</span><br></pre></td></tr></table></figure><h3 id="2-5-定义广播变量需要的注意点？"><a href="#2-5-定义广播变量需要的注意点？" class="headerlink" title="2.5　定义广播变量需要的注意点？"></a>2.5　定义广播变量需要的注意点？</h3><p>变量一旦被定义为一个广播变量，那么这个变量只能读，不能修改</p><h3 id="2-6-注意事项"><a href="#2-6-注意事项" class="headerlink" title="2.6　注意事项"></a>2.6　<strong>注意事项</strong></h3><p>1、能不能将一个RDD使用广播变量广播出去？</p><p>​       不能，因为RDD是不存储数据的。<strong>可以将RDD的结果广播出去。</strong></p><p>2、 广播变量只能在Driver端定义，<strong>不能在Executor端定义。</strong></p><p>3、 在Driver端可以修改广播变量的值，<strong>在Executor端无法修改广播变量的值。</strong></p><p>4、如果executor端用到了Driver的变量，如果<strong>不使用广播变量在Executor有多少task就有多少Driver端的变量副本。</strong></p><p>5、如果Executor端用到了Driver的变量，如果<strong>使用广播变量在每个Executor中只有一份Driver端的变量副本。</strong></p><h2 id="二、累加器"><a href="#二、累加器" class="headerlink" title="二、累加器"></a>二、累加器</h2><p>​    累加器是仅通过关联和交换操作“添加”的变量，因此可以并行有效地支持。它们可用于实现计数器（如MapReduce）或总和。Spark本身支持数值类型的累加器，程序员可以添加对新类型的支持。</p><p>作为用户，您可以创建命名或未命名的累加器。如下图所示，命名累加器（在此实例中<code>counter</code>）将显示在Web UI中，用于修改该累加器的阶段。Spark显示“任务”表中任务修改的每个累加器的值。</p><p><img src="http://spark.apache.org/docs/latest/img/spark-webui-accumulators.png" alt="Spark UI中的累加器"></p><p>跟踪UI中的累加器对于理解运行阶段的进度非常有用（注意：Python中尚不支持）。</p><h3 id="2-1-为什么要将一个变量定义为一个累加器？"><a href="#2-1-为什么要将一个变量定义为一个累加器？" class="headerlink" title="2.1　为什么要将一个变量定义为一个累加器？"></a>2.1　为什么要将一个变量定义为一个累加器？</h3><p>​        在spark应用程序中，我们经常会有这样的需求，如异常监控，调试，记录符合某特性的数据的数目，这种需求都需要用到计数器，如果一个变量不被声明为一个累加器，那么它将在被改变时不会再driver端进行全局汇总，即在分布式运行时每个task运行的只是原始变量的一个副本，并不能改变原始变量的值，但是当这个变量被声明为累加器后，该变量就会有分布式计数的功能。</p><h3 id="2-2-图解累加器"><a href="#2-2-图解累加器" class="headerlink" title="2.2　图解累加器"></a>2.2　图解累加器</h3><p>错误的图解</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421164701390-9845184.png" alt="img"></p><p>正确的图解</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421165419534-240211041.png" alt="img"></p><h3 id="2-3-如何定义一个累加器？"><a href="#2-3-如何定义一个累加器？" class="headerlink" title="2.3　如何定义一个累加器？"></a>2.3　如何定义一个累加器？</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> a = sc.accumulator(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="2-4-如何还原一个累加器？"><a href="#2-4-如何还原一个累加器？" class="headerlink" title="2.4　如何还原一个累加器？"></a>2.4　如何还原一个累加器？</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> b = a.value</span><br></pre></td></tr></table></figure><h3 id="2-5-注意事项"><a href="#2-5-注意事项" class="headerlink" title="2.5　注意事项"></a>2.5　<strong>注意事项</strong></h3><p>1、 <strong>累加器在Driver端定义赋初始值，累加器只能在Driver端读取最后的值，在Excutor端更新。</strong></p><p>2、累加器不是一个调优的操作，因为如果不这样做，结果是错的</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （四）Spark的广播变量和累加器：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        在spark程序中，当一个传递给Spark操作(例如map和reduce)的函数在远程节点上面运行时，Spark操作实际上操作的是这个函数所用变量的一个独立副本。这些变量会被复制到每台机器上，并且这些变量在远程机器上的所有更新都不会传递回驱动程序。通常跨任务的读写变量是低效的，但是，Spark还是为两种常见的使用模式提供了两种有限的共享变量：广播变（broadcast variable）和累加器（accumulator）&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （三）Spark之RDD</title>
    <link href="http://zhangfuxin.cn/2019-06-03-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%B8%89%EF%BC%89Spark%E4%B9%8BRDD.html"/>
    <id>http://zhangfuxin.cn/2019-06-03-Spark学习之路 （三）Spark之RDD.html</id>
    <published>2019-06-03T02:30:04.000Z</published>
    <updated>2019-09-16T04:12:59.674Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （三）Spark之RDD：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （三）Spark之RDD</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、RDD的概述"><a href="#一、RDD的概述" class="headerlink" title="一、RDD的概述"></a>一、RDD的概述</h2><h3 id="1-1-什么是RDD"><a href="#1-1-什么是RDD" class="headerlink" title="1.1　什么是RDD"></a>1.1　什么是RDD</h3><p>​        <strong>RDD</strong>（Resilient Distributed Dataset）叫做<strong>弹性分布式数据集</strong>，<strong>是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。</strong>RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。</p><h3 id="1-2-RDD的属性"><a href="#1-2-RDD的属性" class="headerlink" title="1.2　RDD的属性"></a>1.2　RDD的属性</h3><p><a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala</a></p><blockquote><p> A list of partitions<br> A function for computing each split<br> A list of dependencies on other RDDs<br> Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<br> Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</p></blockquote><p>（1）一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</p><p>（2）一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</p><p>（3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p><p>（4）一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p><p>（5）一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421133911520-1150689001.png" alt="img"></p><p>其中hello.txt</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421134031551-1670646166.png" alt="img"></p><h2 id="二、RDD的创建方式"><a href="#二、RDD的创建方式" class="headerlink" title="二、RDD的创建方式"></a>二、RDD的创建方式</h2><h3 id="2-1-通过读取文件生成的"><a href="#2-1-通过读取文件生成的" class="headerlink" title="2.1　通过读取文件生成的"></a>2.1　通过读取文件生成的</h3><p>由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val file = sc.textFile(&quot;/spark/hello.txt&quot;)</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421134515478-653027491.png" alt="img"></p><h3 id="2-2-通过并行化的方式创建RDD"><a href="#2-2-通过并行化的方式创建RDD" class="headerlink" title="2.2　通过并行化的方式创建RDD"></a>2.2　通过并行化的方式创建RDD</h3><p>由一个已经存在的Scala集合创建。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> array = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">array: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(array)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">27</span>] at parallelize at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421134820158-111255712.png" alt="img"></p><h3 id="2-3-其他方式"><a href="#2-3-其他方式" class="headerlink" title="2.3　其他方式"></a>2.3　其他方式</h3><p>读取数据库等等其他的操作。也可以生成RDD。RDD转换为ParallelCollectionRDD。</p><h2 id="三、RDD编程API"><a href="#三、RDD编程API" class="headerlink" title="三、RDD编程API"></a>三、RDD编程API</h2><p><strong>Spark支持两个类型（算子）操作：Transformation和Action</strong></p><h3 id="3-1-Transformation"><a href="#3-1-Transformation" class="headerlink" title="3.1　Transformation"></a>3.1　Transformation</h3><p>​    主要做的是就是将一个已有的RDD生成另外一个RDD。Transformation具有<strong>lazy**</strong>特性(延迟加载)**。Transformation算子的代码不会真正被执行。只有当我们的程序里面遇到一个action算子的时候，代码才会真正的被执行。这种设计让Spark更加有效率地运行。</p><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds</a></p><p><strong>常用的Transformation</strong>：</p><table><thead><tr><th><strong>转换</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td><strong>map</strong>(func)</td><td>返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成</td></tr><tr><td><strong>filter</strong>(func)</td><td>返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成</td></tr><tr><td><strong>flatMap</strong>(func)</td><td>类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）</td></tr><tr><td><strong>mapPartitions</strong>(func)</td><td>类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]</td></tr><tr><td><strong>mapPartitionsWithIndex</strong>(func)</td><td>类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]</td></tr><tr><td><strong>sample</strong>(withReplacement, fraction, seed)</td><td>根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子</td></tr><tr><td><strong>union</strong>(otherDataset)</td><td>对源RDD和参数RDD求并集后返回一个新的RDD</td></tr><tr><td><strong>intersection</strong>(otherDataset)</td><td>对源RDD和参数RDD求交集后返回一个新的RDD</td></tr><tr><td><strong>distinct</strong>([numTasks]))</td><td>对源RDD进行去重后返回一个新的RDD</td></tr><tr><td><strong>groupByKey</strong>([numTasks])</td><td>在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD</td></tr><tr><td><strong>reduceByKey</strong>(func, [numTasks])</td><td>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置</td></tr><tr><td><strong>aggregateByKey</strong>(zeroValue)(seqOp, combOp, [numTasks])</td><td>先按分区聚合 再总的聚合   每次要跟初始值交流 例如：aggregateByKey(0)(<em>+</em>,<em>+</em>) 对k/y的RDD进行操作</td></tr><tr><td><strong>sortByKey</strong>([ascending], [numTasks])</td><td>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</td></tr><tr><td><strong>sortBy</strong>(func,[ascending], [numTasks])</td><td>与sortByKey类似，但是更灵活 第一个参数是根据什么排序  第二个是怎么排序 false倒序   第三个排序后分区数  默认与原RDD一样</td></tr><tr><td><strong>join</strong>(otherDataset, [numTasks])</td><td>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD  相当于内连接（求交集）</td></tr><tr><td><strong>cogroup</strong>(otherDataset, [numTasks])</td><td>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<v>,Iterable<w>))类型的RDD</w></v></td></tr><tr><td><strong>cartesian</strong>(otherDataset)</td><td>两个RDD的笛卡尔积  的成很多个K/V</td></tr><tr><td><strong>pipe</strong>(command, [envVars])</td><td>调用外部程序</td></tr><tr><td><strong>coalesce</strong>(numPartitions<strong>)</strong></td><td>重新分区 第一个参数是要分多少区，第二个参数是否shuffle 默认false  少分区变多分区 true   多分区变少分区 false</td></tr><tr><td><strong>repartition</strong>(numPartitions)</td><td>重新分区 必须shuffle  参数是要分多少区  少变多</td></tr><tr><td><strong>repartitionAndSortWithinPartitions</strong>(partitioner)</td><td>重新分区+排序  比先分区再排序效率高  对K/V的RDD进行操作</td></tr><tr><td><strong>foldByKey</strong>(zeroValue)(seqOp)</td><td>该函数用于K/V做折叠，合并处理 ，与aggregate类似   第一个括号的参数应用于每个V值  第二括号函数是聚合例如：<em>+</em></td></tr><tr><td><strong>combineByKey</strong></td><td>合并相同的key的值 rdd1.combineByKey(x =&gt; x, (a: Int, b: Int) =&gt; a + b, (m: Int, n: Int) =&gt; m + n)</td></tr><tr><td><strong>partitionBy**</strong>（partitioner）**</td><td>对RDD进行分区  partitioner是分区器 例如new HashPartition(2</td></tr><tr><td><strong>cache</strong></td><td>RDD缓存，可以避免重复计算从而减少时间，区别：cache内部调用了persist算子，cache默认就一个缓存级别MEMORY-ONLY ，而persist则可以选择缓存级别</td></tr><tr><td><strong>persist</strong></td><td></td></tr><tr><td></td><td></td></tr><tr><td><strong>Subtract**</strong>（rdd）**</td><td>返回前rdd元素不在后rdd的rdd</td></tr><tr><td><strong>leftOuterJoin</strong></td><td>leftOuterJoin类似于SQL中的左外关联left outer join，返回结果以前面的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可。</td></tr><tr><td><strong>rightOuterJoin</strong></td><td>rightOuterJoin类似于SQL中的有外关联right outer join，返回结果以参数中的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可</td></tr><tr><td>subtractByKey</td><td>substractByKey和基本转换操作中的subtract类似只不过这里是针对K的，返回在主RDD中出现，并且不在otherRDD中出现的元素</td></tr></tbody></table><h3 id="3-2-Action"><a href="#3-2-Action" class="headerlink" title="3.2　Action"></a>3.2　Action</h3><p>触发代码的运行，我们一段spark代码里面至少需要有一个action操作。</p><p><strong>常用的Action</strong>:</p><table><thead><tr><th><strong>动作</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td><strong>reduce</strong>(<em>func</em>)</td><td>通过func函数聚集RDD中的所有元素，这个功能必须是课交换且可并联的</td></tr><tr><td><strong>collect</strong>()</td><td>在驱动程序中，以数组的形式返回数据集的所有元素</td></tr><tr><td><strong>count</strong>()</td><td>返回RDD的元素个数</td></tr><tr><td><strong>first</strong>()</td><td>返回RDD的第一个元素（类似于take(1)）</td></tr><tr><td><strong>take</strong>(<em>n</em>)</td><td>返回一个由数据集的前n个元素组成的数组</td></tr><tr><td><strong>takeSample</strong>(<em>withReplacement</em>,<em>num</em>, [<em>seed</em>])</td><td>返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子</td></tr><tr><td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td><td></td></tr><tr><td><strong>saveAsTextFile</strong>(<em>path</em>)</td><td>将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本</td></tr><tr><td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td><td>将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。</td></tr><tr><td><strong>saveAsObjectFile</strong>(<em>path</em>)</td><td></td></tr><tr><td><strong>countByKey</strong>()</td><td>针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</td></tr><tr><td><strong>foreach</strong>(<em>func</em>)</td><td>在数据集的每一个元素上，运行函数func进行更新。</td></tr><tr><td><strong>aggregate</strong></td><td>先对分区进行操作，在总体操作</td></tr><tr><td><strong>reduceByKeyLocally</strong></td><td></td></tr><tr><td><strong>lookup</strong></td><td></td></tr><tr><td><strong>top</strong></td><td></td></tr><tr><td><strong>fold</strong></td><td></td></tr><tr><td><strong>foreachPartition</strong></td><td></td></tr></tbody></table><h3 id="3-3-Spark-WordCount代码编写"><a href="#3-3-Spark-WordCount代码编写" class="headerlink" title="3.3　Spark WordCount代码编写"></a>3.3　Spark WordCount代码编写</h3><p>使用maven进行项目构建</p><h4 id="（1）使用scala进行编写"><a href="#（1）使用scala进行编写" class="headerlink" title="（1）使用scala进行编写"></a>（1）使用scala进行编写</h4><p>查看官方网站，需要导入2个依赖包</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421144526496-1152731884.png" alt="img"></p><p>详细代码</p><p>SparkWordCountWithScala.scala</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkWordCountWithScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 如果这个参数不设置，默认认为你运行的是集群模式</span></span><br><span class="line"><span class="comment">      * 如果设置成local代表运行的是local模式</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    conf.setMaster(<span class="string">"local"</span>)</span><br><span class="line">    <span class="comment">//设置任务名</span></span><br><span class="line">    conf.setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">    <span class="comment">//创建SparkCore的程序入口</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//读取文件 生成RDD</span></span><br><span class="line">    <span class="keyword">val</span> file: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"E:\\hello.txt"</span>)</span><br><span class="line">    <span class="comment">//把每一行数据按照，分割</span></span><br><span class="line">    <span class="keyword">val</span> word: <span class="type">RDD</span>[<span class="type">String</span>] = file.flatMap(_.split(<span class="string">","</span>))</span><br><span class="line">    <span class="comment">//让每一个单词都出现一次</span></span><br><span class="line">    <span class="keyword">val</span> wordOne: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = word.map((_,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">//单词计数</span></span><br><span class="line">    <span class="keyword">val</span> wordCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.reduceByKey(_+_)</span><br><span class="line">    <span class="comment">//按照单词出现的次数 降序排序</span></span><br><span class="line">    <span class="keyword">val</span> sortRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordCount.sortBy(tuple =&gt; tuple._2,<span class="literal">false</span>)</span><br><span class="line">    <span class="comment">//将最终的结果进行保存</span></span><br><span class="line">    sortRdd.saveAsTextFile(<span class="string">"E:\\result"</span>)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421151823241-1753369845.png" alt="img"></p><h4 id="（2）使用java-jdk7进行编写"><a href="#（2）使用java-jdk7进行编写" class="headerlink" title="（2）使用java jdk7进行编写"></a>（2）使用java jdk7进行编写</h4><p>SparkWordCountWithJava7.java</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaPairRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaSparkContext</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.<span class="type">FlatMapFunction</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.<span class="type">Function2</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.<span class="type">PairFunction</span>;</span><br><span class="line"><span class="keyword">import</span> scala.<span class="type">Tuple2</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Arrays</span>;</span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Iterator</span>;</span><br><span class="line"></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SparkWordCountWithJava7</span> </span>&#123;</span><br><span class="line">    public static void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line">        <span class="type">SparkConf</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>();</span><br><span class="line">        conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">        conf.setAppName(<span class="string">"WordCount"</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> sc = <span class="keyword">new</span> <span class="type">JavaSparkContext</span>(conf);</span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; fileRdd = sc.textFile(<span class="string">"E:\\hello.txt"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; wordRDD = fileRdd.flatMap(<span class="keyword">new</span> <span class="type">FlatMapFunction</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Iterator</span>&lt;<span class="type">String</span>&gt; call(<span class="type">String</span> line) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="type">Arrays</span>.asList(line.split(<span class="string">","</span>)).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordOneRDD = wordRDD.mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">String</span>, <span class="type">String</span>, <span class="type">Integer</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; call(<span class="type">String</span> word) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(word, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordCountRDD = wordOneRDD.reduceByKey(<span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Integer</span>, <span class="type">Integer</span>, <span class="type">Integer</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Integer</span> call(<span class="type">Integer</span> i1, <span class="type">Integer</span> i2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> i1 + i2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; count2WordRDD = wordCountRDD.mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt;, <span class="type">Integer</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; sortRDD = count2WordRDD.sortByKey(<span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; resultRDD = sortRDD.mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt;, <span class="type">String</span>, <span class="type">Integer</span>&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        resultRDD.saveAsTextFile(<span class="string">"E:\\result7"</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（3）使用java-jdk8进行编写"><a href="#（3）使用java-jdk8进行编写" class="headerlink" title="（3）使用java jdk8进行编写"></a>（3）使用java jdk8进行编写</h4><p>lambda表达式</p><p>SparkWordCountWithJava8.java</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaPairRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaRDD</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.<span class="type">JavaSparkContext</span>;</span><br><span class="line"><span class="keyword">import</span> scala.<span class="type">Tuple2</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Arrays</span>;</span><br><span class="line"></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SparkWordCountWithJava8</span> </span>&#123;</span><br><span class="line">    public static void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line">        <span class="type">SparkConf</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>();</span><br><span class="line">        conf.setAppName(<span class="string">"WortCount"</span>);</span><br><span class="line">        conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> sc = <span class="keyword">new</span> <span class="type">JavaSparkContext</span>(conf);</span><br><span class="line"></span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; fileRDD = sc.textFile(<span class="string">"E:\\hello.txt"</span>);</span><br><span class="line">        <span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; wordRdd = fileRDD.flatMap(line -&gt; <span class="type">Arrays</span>.asList(line.split(<span class="string">","</span>)).iterator());</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordOneRDD = wordRdd.mapToPair(word -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; wordCountRDD = wordOneRDD.reduceByKey((x, y) -&gt; x + y);</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; count2WordRDD = wordCountRDD.mapToPair(tuple -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1));</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">Integer</span>, <span class="type">String</span>&gt; sortRDD = count2WordRDD.sortByKey(<span class="literal">false</span>);</span><br><span class="line">        <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Integer</span>&gt; resultRDD = sortRDD.mapToPair(tuple -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;&gt;(tuple._2, tuple._1));</span><br><span class="line">        resultRDD.saveAsTextFile(<span class="string">"E:\\result8"</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425153140543-8294264.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425153515149-1269337605.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425153556469-238789142.png" alt="img"></p><h2 id="四、RDD的宽依赖和窄依赖"><a href="#四、RDD的宽依赖和窄依赖" class="headerlink" title="四、RDD的宽依赖和窄依赖"></a>四、RDD的宽依赖和窄依赖</h2><h3 id="4-1-RDD依赖关系的本质内幕"><a href="#4-1-RDD依赖关系的本质内幕" class="headerlink" title="4.1　RDD依赖关系的本质内幕"></a>4.1　<strong>RDD依赖关系的本质内幕</strong></h3><p>由于RDD是粗粒度的操作数据集，每个Transformation操作都会生成一个新的RDD，所以RDD之间就会形成类似流水线的前后依赖关系；RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。如图所示显示了RDD之间的依赖关系。</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425151022105-1121308065.png" alt="img"></p><p>从图中可知：</p><p><strong>窄依赖：</strong>是指每个父RDD的一个Partition最多被子RDD的一个Partition所使用，例如map、filter、union等操作都会产生窄依赖；（独生子女）</p><p><strong>宽依赖：</strong>是指一个父RDD的Partition会被多个子RDD的Partition所使用，例如groupByKey、reduceByKey、sortByKey等操作都会产生宽依赖；（超生）</p><p>需要特别说明的是对join操作有两种情况：</p><p>（1）图中左半部分join：如果两个RDD在进行join操作时，一个RDD的partition仅仅和另一个RDD中已知个数的Partition进行join，那么这种类型的join操作就是窄依赖，例如图1中左半部分的join操作(join with inputs co-partitioned)；</p><p>（2）图中右半部分join：其它情况的join操作就是宽依赖,例如图1中右半部分的join操作(join with inputs not co-partitioned)，由于是需要父RDD的所有partition进行join的转换，这就涉及到了shuffle，因此这种类型的join操作也是宽依赖。</p><p>总结：</p><blockquote><p>在这里我们是从父RDD的partition被使用的个数来定义窄依赖和宽依赖，因此可以用一句话概括下：如果父RDD的一个Partition被子RDD的一个Partition所使用就是窄依赖，否则的话就是宽依赖。因为是确定的partition数量的依赖关系，所以RDD之间的依赖关系就是窄依赖；由此我们可以得出一个推论：即窄依赖不仅包含一对一的窄依赖，还包含一对固定个数的窄依赖。</p><p>一对固定个数的窄依赖的理解：即子RDD的partition对父RDD依赖的Partition的数量不会随着RDD数据规模的改变而改变；换句话说，无论是有100T的数据量还是1P的数据量，在窄依赖中，子RDD所依赖的父RDD的partition的个数是确定的，而宽依赖是shuffle级别的，数据量越大，那么子RDD所依赖的父RDD的个数就越多，从而子RDD所依赖的父RDD的partition的个数也会变得越来越多。</p></blockquote><h3 id="4-2-依赖关系下的数据流视图"><a href="#4-2-依赖关系下的数据流视图" class="headerlink" title="4.2　依赖关系下的数据流视图"></a>4.2　<strong>依赖关系下的数据流视图</strong></h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180425151747136-185909749.png" alt="img"></p><p>在spark中，会根据RDD之间的依赖关系将DAG图（有向无环图）划分为不同的阶段，对于窄依赖，由于partition依赖关系的确定性，partition的转换处理就可以在同一个线程里完成，窄依赖就被spark划分到同一个stage中，而对于宽依赖，只能等父RDD shuffle处理完成后，下一个stage才能开始接下来的计算。</p><p><strong>因此spark划分stage的整体思路是</strong>：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。因此在图2中RDD C,RDD D,RDD E,RDDF被构建在一个stage中,RDD A被构建在一个单独的Stage中,而RDD B和RDD G又被构建在同一个stage中。</p><p>在spark中，Task的类型分为2种：<strong>ShuffleMapTask</strong>和<strong>ResultTask</strong>；</p><p>简单来说，DAG的最后一个阶段会为每个结果的partition生成一个ResultTask，即每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！而其余所有阶段都会生成ShuffleMapTask；之所以称之为ShuffleMapTask是因为它需要将自己的计算结果通过shuffle到下一个stage中；也就是说上图中的stage1和stage2相当于mapreduce中的Mapper,而ResultTask所代表的stage3就相当于mapreduce中的reducer。</p><p>在之前动手操作了一个wordcount程序，因此可知，Hadoop中MapReduce操作中的Mapper和Reducer在spark中的基本等量算子是map和reduceByKey;不过区别在于：Hadoop中的MapReduce天生就是排序的；而reduceByKey只是根据Key进行reduce，但spark除了这两个算子还有其他的算子；因此从这个意义上来说，Spark比Hadoop的计算算子更为丰富。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （三）Spark之RDD：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （三）Spark之RDD&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler</title>
    <link href="http://zhangfuxin.cn/2019-06-02-Hadoop%20%E7%9A%84%E4%B8%89%E7%A7%8D%E8%B0%83%E5%BA%A6%E5%99%A8FIFO%E3%80%81Capacity%20Scheduler%E3%80%81Fair%20Scheduler.html"/>
    <id>http://zhangfuxin.cn/2019-06-02-Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler.html</id>
    <published>2019-06-02T05:30:04.000Z</published>
    <updated>2019-09-16T02:59:42.540Z</updated>
    
    <content type="html"><![CDATA[<p>** Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p><strong>目前Hadoop有三种比较流行的资源调度器：FIFO 、Capacity Scheduler、Fair Scheduler。目前hadoop2.7默认使用的是Capacity Scheduler容量调度器。</strong></p><h3 id="一、FIFO（先入先出调度器）"><a href="#一、FIFO（先入先出调度器）" class="headerlink" title="一、FIFO（先入先出调度器）"></a>一、FIFO（先入先出调度器）</h3><p>hadoop1.x使用的默认调度器就是FIFO。FIFO采用队列方式将一个一个job任务按照时间先后顺序进行服务。比如排在最前面的job需要若干maptask和若干reducetask，当发现有空闲的服务器节点就分配给这个job，直到job执行完毕。</p><p><img src="https://img-blog.csdn.net/20180907181312127?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hmd2VhdGhlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h3 id="二、Capacity-Scheduler（容量调度器）"><a href="#二、Capacity-Scheduler（容量调度器）" class="headerlink" title="二、Capacity Scheduler（容量调度器）"></a>二、Capacity Scheduler（容量调度器）</h3><p>hadoop2.x使用的默认调度器是Capacity Scheduler。</p><p>1、支持多个队列，每个队列可配置一定量的资源，每个采用FIFO的方式调度。</p><p>2、为了防止同一个用户的job任务独占队列中的资源，调度器会对同一用户提交的job任务所占资源进行限制。</p><p>3、分配新的job任务时，首先计算每个队列中正在运行task个数与其队列应该分配的资源量做比值，然后选择比值最小的队列。比如如图队列A15个task，20%资源量，那么就是15%0.2=70，队列B是25%0.5=50 ，队列C是25%0.3=80.33 。所以选择最小值队列B。</p><p>4、其次，按照job任务的优先级和时间顺序，同时要考虑到用户的资源量和内存的限制，对队列中的job任务进行排序执行。</p><p>5、多个队列同时按照任务队列内的先后顺序一次执行。例如下图中job11、job21、job31分别在各自队列中顺序比较靠前，三个任务就同时执行。</p><p><img src="https://img-blog.csdn.net/20180907183145655?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hmd2VhdGhlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><h3 id="三、Fair-Scheduler（公平调度器）"><a href="#三、Fair-Scheduler（公平调度器）" class="headerlink" title="三、Fair Scheduler（公平调度器）"></a>三、Fair Scheduler（公平调度器）</h3><p>1、支持多个队列，每个队列可以配置一定的资源，每个队列中的job任务公平共享其所在队列的所有资源。</p><p>2、队列中的job任务都是按照优先级分配资源，优先级越高分配的资源越多，但是为了确保公平每个job任务都会分配到资源。优先级是根据每个job任务的理想获取资源量减去实际获取资源量的差值决定的，差值越大优先级越高。</p><p><img src="https://img-blog.csdn.net/20180909212500326?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hmd2VhdGhlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>原文链接：<a href="https://blog.csdn.net/xiaomage510/article/details/82500067" target="_blank" rel="noopener">https://blog.csdn.net/xiaomage510/article/details/82500067</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Hadoop 的三种调度器FIFO、Capacity Scheduler、Fair Scheduler&lt;/p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://zhangfuxin.cn/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://zhangfuxin.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Spark调度模式-FIFO和FAIR</title>
    <link href="http://zhangfuxin.cn/2019-06-02-Spark%E8%B0%83%E5%BA%A6%E6%A8%A1%E5%BC%8F-FIFO%E5%92%8CFAIR.html"/>
    <id>http://zhangfuxin.cn/2019-06-02-Spark调度模式-FIFO和FAIR.html</id>
    <published>2019-06-02T05:20:04.000Z</published>
    <updated>2019-09-16T03:25:31.638Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark调度模式-FIFO和FAIR：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark调度模式-FIFO和FAIR</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>　　<strong>Spark中的调度模式主要有两种：FIFO和FAIR。</strong>默认情况下Spark的调度模式是FIFO（先进先出），谁先提交谁先执行，后面的任务需要等待前面的任务执行。而FAIR（公平调度）模式支持在调度池中为任务进行分组，不同的调度池权重不同，任务可以按照权重来决定执行顺序。对这两种调度模式的具体实现，接下来会根据spark-1.6.0的源码来进行详细的分析。使用哪种调度器由参数spark.scheduler.mode来设置，可选的参数有FAIR和FIFO，默认是FIFO。</p><h2 id="一、源码入口"><a href="#一、源码入口" class="headerlink" title="一、源码入口"></a>一、源码入口</h2><p>　　在Scheduler模块中，当Stage划分好，然后提交Task的过程中，会进入TaskSchedulerImpl#submitTasks方法。</p><blockquote><p>schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)   </p><p>//目前支持FIFO和FAIR两种调度策略。</p></blockquote><p>在上面代码中有一个schedulableBuilder对象，这个对象在TaskSchedulerImpl类中的定义及实现可以参考下面这段源代码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> schedulableBuilder: <span class="type">SchedulableBuilder</span> = <span class="literal">null</span></span><br><span class="line">...</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(backend: <span class="type">SchedulerBackend</span>) &#123;</span><br><span class="line">    <span class="keyword">this</span>.backend = backend</span><br><span class="line">    <span class="comment">// temporarily set rootPool name to empty</span></span><br><span class="line">    rootPool = <span class="keyword">new</span> <span class="type">Pool</span>(<span class="string">""</span>, schedulingMode, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    schedulableBuilder = &#123;</span><br><span class="line">      schedulingMode <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">SchedulingMode</span>.<span class="type">FIFO</span> =&gt;</span><br><span class="line">          <span class="keyword">new</span> <span class="type">FIFOSchedulableBuilder</span>(rootPool)  <span class="comment">//rootPool包含了一组TaskSetManager</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">SchedulingMode</span>.<span class="type">FAIR</span> =&gt;</span><br><span class="line">          <span class="keyword">new</span> <span class="type">FairSchedulableBuilder</span>(rootPool, conf)  <span class="comment">//rootPool包含了一组Pool树，这棵树的叶子节点都是TaskSetManager</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    schedulableBuilder.buildPools() <span class="comment">//在FIFO中的实现是空</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>　　根据用户配置的SchedulingMode决定是生成FIFOSchedulableBuilder还是生成FairSchedulableBuilder类型的schedulableBuilder对象。<br>　 <img src="https://img-blog.csdn.net/20160528143551855" alt="SchedulableBuilder继承关系" style="zoom:150%;"></p><p>　　在生成schedulableBuilder后，调用其buildPools方法生成调度池。 调度模式由配置参数spark.scheduler.mode（默认值为FIFO）来确定。 两种模式的调度逻辑图如下：<br>　<img src="https://img-blog.csdn.net/20160528181016610" alt="调度模式逻辑图" style="zoom:150%;"></p><h2 id="二、FIFOSchedulableBuilder"><a href="#二、FIFOSchedulableBuilder" class="headerlink" title="二、FIFOSchedulableBuilder"></a>二、FIFOSchedulableBuilder</h2><p>　　FIFO的rootPool包含一组TaskSetManager。从上面的类继承图中看出在FIFOSchedulableBuilder中有两个方法：</p><h3 id="1、buildPools"><a href="#1、buildPools" class="headerlink" title="1、buildPools"></a>1、buildPools</h3><p>实现为空:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildPools</span></span>() &#123;</span><br><span class="line">    <span class="comment">// nothing</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>所以，对于FIFO模式，获取到schedulableBuilder对象后，在调用buildPools方法后，不做任何操作。</p><h3 id="2、addTaskSetManager"><a href="#2、addTaskSetManager" class="headerlink" title="2、addTaskSetManager"></a>2、addTaskSetManager</h3><p>　　该方法将TaskSetManager装载到rootPool中。直接调用的方法是Pool#addSchedulable()。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">addTaskSetManager</span></span>(manager: <span class="type">Schedulable</span>, properties: <span class="type">Properties</span>) &#123;</span><br><span class="line">  rootPool.addSchedulable(manager)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Pool#addSchedulable()方法：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> schedulableQueue = <span class="keyword">new</span> <span class="type">ConcurrentLinkedQueue</span>[<span class="type">Schedulable</span>]</span><br><span class="line">...</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">addSchedulable</span></span>(schedulable: <span class="type">Schedulable</span>) &#123;</span><br><span class="line">    require(schedulable != <span class="literal">null</span>)</span><br><span class="line">    schedulableQueue.add(schedulable)</span><br><span class="line">    schedulableNameToSchedulable.put(schedulable.name, schedulable)</span><br><span class="line">    schedulable.parent = <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>将该TaskSetManager加入到调度队列schedulableQueue中。</p><h2 id="三、FairSchedulableBuilder"><a href="#三、FairSchedulableBuilder" class="headerlink" title="三、FairSchedulableBuilder"></a>三、FairSchedulableBuilder</h2><p>　　FAIR的rootPool中包含一组Pool，在Pool中包含了TaskSetManager。</p><h3 id="1、buildPools-1"><a href="#1、buildPools-1" class="headerlink" title="1、buildPools"></a>1、buildPools</h3><p>　　在该方法中，会读取配置文件，按照配置文件中的配置参数调用buildFairSchedulerPool生成配置的调度池，以及调用buildDefaultPool生成默认调度池。<br>　　默认情况下FAIR模式的配置文件是位于SPARK_HOME/conf/fairscheduler.xml文件，也可以通过参数spark.scheduler.allocation.file设置用户自定义配置文件。<br>spark中提供的fairscheduler.xml模板如下所示：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">allocations</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pool</span> <span class="attr">name</span>=<span class="string">"production"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">schedulingMode</span>&gt;</span>FAIR<span class="tag">&lt;/<span class="name">schedulingMode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">weight</span>&gt;</span>1<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minShare</span>&gt;</span>2<span class="tag">&lt;/<span class="name">minShare</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">pool</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pool</span> <span class="attr">name</span>=<span class="string">"test"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">schedulingMode</span>&gt;</span>FIFO<span class="tag">&lt;/<span class="name">schedulingMode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">weight</span>&gt;</span>2<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minShare</span>&gt;</span>3<span class="tag">&lt;/<span class="name">minShare</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">pool</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">allocations</span>&gt;</span></span><br></pre></td></tr></table></figure><p>参数含义：<br>（1）name: 该调度池的名称，可根据该参数使用指定pool，入sc.setLocalProperty(“spark.scheduler.pool”, “test”)<br>（2）weight: 该调度池的权重，各调度池根据该参数分配系统资源。每个调度池得到的资源数为weight / sum(weight)，weight为2的分配到的资源为weight为1的两倍。<br>（3）minShare: 该调度池需要的最小资源数（CPU核数）。fair调度器首先会尝试为每个调度池分配最少minShare资源，然后剩余资源才会按照weight大小继续分配。<br>（4）schedulingMode: 该调度池内的调度模式。</p><h3 id="2、buildFairSchedulerPool"><a href="#2、buildFairSchedulerPool" class="headerlink" title="2、buildFairSchedulerPool"></a>2、buildFairSchedulerPool</h3><p>　　从上面的配置文件可以看到，每一个调度池有一个name属性指定名字，然后在该pool中可以设置其schedulingMode(可为空，默认为FIFO), weight(可为空，默认值是1), 以及minShare(可为空，默认值是0)参数。然后使用这些参数生成一个Pool对象，把该pool对象放入rootPool中。入下所示：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pool = <span class="keyword">new</span> <span class="type">Pool</span>(poolName, schedulingMode, minShare, weight)</span><br><span class="line">rootPool.addSchedulable(pool)</span><br></pre></td></tr></table></figure><h3 id="3、buildDefaultPool"><a href="#3、buildDefaultPool" class="headerlink" title="3、buildDefaultPool"></a>3、buildDefaultPool</h3><p>　　如果如果配置文件中没有设置一个name为default的pool，系统才会自动生成一个使用默认参数生成的pool对象。各项参数的默认值在buildFairSchedulerPool中有提到。</p><h3 id="4、addTaskSetManager"><a href="#4、addTaskSetManager" class="headerlink" title="4、addTaskSetManager"></a>4、addTaskSetManager</h3><p>　　这一段逻辑中是把配置文件中的pool，或者default pool放入rootPool中，然后把TaskSetManager存入rootPool对应的子pool。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">addTaskSetManager</span></span>(manager: <span class="type">Schedulable</span>, properties: <span class="type">Properties</span>) &#123;</span><br><span class="line">   <span class="keyword">var</span> poolName = <span class="type">DEFAULT_POOL_NAME</span></span><br><span class="line">   <span class="keyword">var</span> parentPool = rootPool.getSchedulableByName(poolName)</span><br><span class="line">   <span class="keyword">if</span> (properties != <span class="literal">null</span>) &#123;</span><br><span class="line">     poolName = properties.getProperty(<span class="type">FAIR_SCHEDULER_PROPERTIES</span>, <span class="type">DEFAULT_POOL_NAME</span>)</span><br><span class="line">     parentPool = rootPool.getSchedulableByName(poolName)</span><br><span class="line">     <span class="keyword">if</span> (parentPool == <span class="literal">null</span>) &#123;</span><br><span class="line">       <span class="comment">// we will create a new pool that user has configured in app</span></span><br><span class="line">       <span class="comment">// instead of being defined in xml file</span></span><br><span class="line">       parentPool = <span class="keyword">new</span> <span class="type">Pool</span>(poolName, <span class="type">DEFAULT_SCHEDULING_MODE</span>,</span><br><span class="line">         <span class="type">DEFAULT_MINIMUM_SHARE</span>, <span class="type">DEFAULT_WEIGHT</span>)</span><br><span class="line">       rootPool.addSchedulable(parentPool)</span><br><span class="line">       logInfo(<span class="string">"Created pool %s, schedulingMode: %s, minShare: %d, weight: %d"</span>.format(</span><br><span class="line">         poolName, <span class="type">DEFAULT_SCHEDULING_MODE</span>, <span class="type">DEFAULT_MINIMUM_SHARE</span>, <span class="type">DEFAULT_WEIGHT</span>))</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   parentPool.addSchedulable(manager)</span><br><span class="line">   logInfo(<span class="string">"Added task set "</span> + manager.name + <span class="string">" tasks to pool "</span> + poolName)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="5、FAIR调度池使用方法"><a href="#5、FAIR调度池使用方法" class="headerlink" title="5、FAIR调度池使用方法"></a>5、FAIR调度池使用方法</h3><p>　　在Spark-1.6.1官方文档中写道：</p><blockquote><p>如果不加设置，jobs会提交到default调度池中。由于调度池的使用是Thread级别的，只能通过具体的SparkContext来设置local属性（即无法在配置文件中通过参数spark.scheduler.pool来设置，因为配置文件中的参数会被加载到SparkConf对象中）。所以需要使用指定调度池的话，需要在具体代码中通过SparkContext对象sc来按照如下方法进行设置：<br>sc.setLocalProperty(“spark.scheduler.pool”, “test”)<br>设置该参数后，在该thread中提交的所有job都会提交到test Pool中。<br>如果接下来不再需要使用到该test调度池，<br>sc.setLocalProperty(“spark.scheduler.pool”, null)</p></blockquote><h2 id="四、FIFO和FAIR的调度顺序"><a href="#四、FIFO和FAIR的调度顺序" class="headerlink" title="四、FIFO和FAIR的调度顺序"></a>四、FIFO和FAIR的调度顺序</h2><p>这里必须提到的一个类是上面提到的Pool，在这个类中实现了不同调度模式的调度算法。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> taskSetSchedulingAlgorithm: <span class="type">SchedulingAlgorithm</span> = &#123;</span><br><span class="line">  schedulingMode <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">SchedulingMode</span>.<span class="type">FAIR</span> =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">FairSchedulingAlgorithm</span>()</span><br><span class="line">    <span class="keyword">case</span> <span class="type">SchedulingMode</span>.<span class="type">FIFO</span> =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">FIFOSchedulingAlgorithm</span>()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>FIFO模式的算法类是FIFOSchedulingAlgorithm，FAIR模式的算法实现类是FairSchedulingAlgorithm。</p><p>　　接下来的两节中comparator方法传入参数Schedulable类型是一个trait，具体实现主要有两个：1，Pool；2，TaskSetManager。与最前面那个调度模式的逻辑图相对应。</p><h3 id="1、FIFO模式的调度算法FIFOSchedulingAlgorithm"><a href="#1、FIFO模式的调度算法FIFOSchedulingAlgorithm" class="headerlink" title="1、FIFO模式的调度算法FIFOSchedulingAlgorithm"></a>1、FIFO模式的调度算法FIFOSchedulingAlgorithm</h3><p>在这个类里面，主要逻辑是一个comparator方法。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">comparator</span></span>(s1: <span class="type">Schedulable</span>, s2: <span class="type">Schedulable</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> priority1 = s1.priority   <span class="comment">//实际上是Job ID</span></span><br><span class="line">  <span class="keyword">val</span> priority2 = s2.priority</span><br><span class="line">  <span class="keyword">var</span> res = math.signum(priority1 - priority2)</span><br><span class="line">  <span class="keyword">if</span> (res == <span class="number">0</span>) &#123; <span class="comment">//如果Job ID相同，就比较Stage ID</span></span><br><span class="line">    <span class="keyword">val</span> stageId1 = s1.stageId</span><br><span class="line">    <span class="keyword">val</span> stageId2 = s2.stageId</span><br><span class="line">    res = math.signum(stageId1 - stageId2)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (res &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果有两个调度任务s1和s2，首先获得两个任务的priority，在FIFO中该优先级实际上是Job ID。首先比较两个任务的Job ID，如果priority1比priority2小，那么返回true，表示s1的优先级比s2的高。我们知道Job ID是顺序生成的，先生成的Job ID比较小，所以先提交的job肯定比后提交的job先执行。但是如果是同一个job的不同任务，接下来就比较各自的Stage ID，类似于比较Job ID，Stage ID小的优先级高。</p><h3 id="2、FAIR模式的调度算法FairSchedulingAlgorithm"><a href="#2、FAIR模式的调度算法FairSchedulingAlgorithm" class="headerlink" title="2、FAIR模式的调度算法FairSchedulingAlgorithm"></a>2、FAIR模式的调度算法FairSchedulingAlgorithm</h3><p>　　这个类中的comparator方法源代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">comparator</span></span>(s1: <span class="type">Schedulable</span>, s2: <span class="type">Schedulable</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">   <span class="keyword">val</span> minShare1 = s1.minShare <span class="comment">//在这里share理解成份额，即每个调度池要求的最少cpu核数</span></span><br><span class="line">   <span class="keyword">val</span> minShare2 = s2.minShare</span><br><span class="line">   <span class="keyword">val</span> runningTasks1 = s1.runningTasks <span class="comment">// 该Pool或者TaskSetManager中正在运行的任务数</span></span><br><span class="line">   <span class="keyword">val</span> runningTasks2 = s2.runningTasks</span><br><span class="line">   <span class="keyword">val</span> s1Needy = runningTasks1 &lt; minShare1 <span class="comment">// 如果正在运行任务数比该调度池最小cpu核数要小</span></span><br><span class="line">   <span class="keyword">val</span> s2Needy = runningTasks2 &lt; minShare2</span><br><span class="line">   <span class="keyword">val</span> minShareRatio1 = runningTasks1.toDouble / math.max(minShare1, <span class="number">1.0</span>).toDouble</span><br><span class="line">   <span class="keyword">val</span> minShareRatio2 = runningTasks2.toDouble / math.max(minShare2, <span class="number">1.0</span>).toDouble</span><br><span class="line">   <span class="keyword">val</span> taskToWeightRatio1 = runningTasks1.toDouble / s1.weight.toDouble</span><br><span class="line">   <span class="keyword">val</span> taskToWeightRatio2 = runningTasks2.toDouble / s2.weight.toDouble</span><br><span class="line">   <span class="keyword">var</span> compare: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">   <span class="keyword">if</span> (s1Needy &amp;&amp; !s2Needy) &#123;</span><br><span class="line">     <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!s1Needy &amp;&amp; s2Needy) &#123;</span><br><span class="line">     <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (s1Needy &amp;&amp; s2Needy) &#123;</span><br><span class="line">     compare = minShareRatio1.compareTo(minShareRatio2)</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     compare = taskToWeightRatio1.compareTo(taskToWeightRatio2)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (compare &lt; <span class="number">0</span>) &#123;</span><br><span class="line">     <span class="literal">true</span></span><br><span class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span> (compare &gt; <span class="number">0</span>) &#123;</span><br><span class="line">     <span class="literal">false</span></span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     s1.name &lt; s2.name</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>　　minShare对应fairscheduler.xml配置文件中的minShare属性。<br>（1）如果s1所在Pool或者TaskSetManager中运行状态的task数量比minShare小，s2所在Pool或者TaskSetManager中运行状态的task数量比minShare大，那么s1会优先调度。反之，s2优先调度。<br>（2）如果s1和s2所在Pool或者TaskSetManager中运行状态的task数量都比各自minShare小，那么minShareRatio小的优先被调度。<br>minShareRatio是运行状态task数与minShare的比值，即相对来说minShare使用较少的先被调度。<br>（3）如果minShareRatio相同，那么最后比较各自Pool的名字。</p><p>原文链接：<a href="https://blog.csdn.net/dabokele/article/details/51526048" target="_blank" rel="noopener">https://blog.csdn.net/dabokele/article/details/51526048</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark调度模式-FIFO和FAIR：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark调度模式-FIFO和FAIR&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark中parallelize函数和makeRDD函数的区别</title>
    <link href="http://zhangfuxin.cn/2019-06-02-Spark%E4%B8%ADparallelize%E5%87%BD%E6%95%B0%E5%92%8CmakeRDD%E5%87%BD%E6%95%B0%E7%9A%84%E5%8C%BA%E5%88%AB.html"/>
    <id>http://zhangfuxin.cn/2019-06-02-Spark中parallelize函数和makeRDD函数的区别.html</id>
    <published>2019-06-02T03:30:04.000Z</published>
    <updated>2019-09-16T01:39:52.469Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark中parallelize函数和makeRDD函数的区别：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark中parallelize函数和makeRDD函数的区别</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><p>我们知道，在<a href="https://www.iteblog.com/archives/tag/spark/" target="_blank" rel="noopener">Spark</a>中创建RDD的创建方式大概可以分为三种：</p><p>（1）、从集合中创建RDD；</p><p>（2）、从外部存储创建RDD；</p><p>（3）、从其他RDD创建。</p><p>　　而从集合中创建RDD，<a href="https://www.iteblog.com/archives/tag/spark/" target="_blank" rel="noopener">Spark</a>主要提供了两中函数：parallelize和makeRDD。我们可以先看看这两个函数的声明：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p>　　我们可以从上面看出makeRDD有两种实现，而且第一个makeRDD函数接收的参数和parallelize完全一致。其实第一种makeRDD函数实现是依赖了parallelize函数的实现，来看看Spark中是怎么实现这个makeRDD函数的：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  parallelize(seq, numSlices)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​        我们可以看出，这个makeRDD函数完全和parallelize函数一致。但是我们得看看第二种makeRDD函数函数实现了，它接收的参数类型是Seq[(T, Seq[String])]，Spark文档的说明是</p><blockquote><p>Distribute a local Scala collection to form an RDD, with one or more location preferences (hostnames of Spark nodes) for each object. Create a new partition for each collection item.</p><p>分发本地scala集合以形成RDD，每个对象具有一个或多个位置首选项（spark节点的主机名）。为每个集合项创建一个新分区。</p></blockquote><p>原来，这个函数还为数据提供了位置信息，来看看我们怎么使用：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> iteblog1 = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">iteblog1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> iteblog2 = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">iteblog2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">11</span>] at makeRDD at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> seq = <span class="type">List</span>((<span class="number">1</span>, <span class="type">List</span>(<span class="string">"iteblog.com"</span>, <span class="string">"sparkhost1.com"</span>, <span class="string">"sparkhost2.com"</span>)),</span><br><span class="line">     | (<span class="number">2</span>, <span class="type">List</span>(<span class="string">"iteblog.com"</span>, <span class="string">"sparkhost2.com"</span>)))</span><br><span class="line">seq: <span class="type">List</span>[(<span class="type">Int</span>, <span class="type">List</span>[<span class="type">String</span>])] = <span class="type">List</span>((<span class="number">1</span>,<span class="type">List</span>(iteblog.com, sparkhost1.com, sparkhost2.com)),</span><br><span class="line"> (<span class="number">2</span>,<span class="type">List</span>(iteblog.com, sparkhost2.com)))</span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> iteblog3 = sc.makeRDD(seq)</span><br><span class="line">iteblog3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at makeRDD at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"> </span><br><span class="line">scala&gt; iteblog3.preferredLocations(iteblog3.partitions(<span class="number">1</span>))</span><br><span class="line">res26: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>(iteblog.com, sparkhost2.com)</span><br><span class="line"> </span><br><span class="line">scala&gt; iteblog3.preferredLocations(iteblog3.partitions(<span class="number">0</span>))</span><br><span class="line">res27: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>(iteblog.com, sparkhost1.com, sparkhost2.com)</span><br><span class="line"> </span><br><span class="line">scala&gt; iteblog1.preferredLocations(iteblog1.partitions(<span class="number">0</span>))</span><br><span class="line">res28: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>()</span><br></pre></td></tr></table></figure><p>我们可以看到，makeRDD函数有两种实现，第一种实现其实完全和parallelize一致；而第二种实现可以为数据提供位置信息，而除此之外的实现和parallelize函数也是一致的，如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq, numSlices, <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">String</span>]]())</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">val</span> indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq.map(_._1), seq.size, indexToPrefs)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>都是返回ParallelCollectionRDD，而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。</p><p><strong>转载自过往记忆（<a href="https://www.iteblog.com/）" target="_blank" rel="noopener">https://www.iteblog.com/）</a></strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark中parallelize函数和makeRDD函数的区别：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark中parallelize函数和makeRDD函数的区别&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习之路 （二）Spark2.3 HA集群的分布式安装</title>
    <link href="http://zhangfuxin.cn/2019-06-02-Spark%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%20%EF%BC%88%E4%BA%8C%EF%BC%89Spark2.3%20HA%E9%9B%86%E7%BE%A4%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85.html"/>
    <id>http://zhangfuxin.cn/2019-06-02-Spark学习之路 （二）Spark2.3 HA集群的分布式安装.html</id>
    <published>2019-06-02T02:31:04.000Z</published>
    <updated>2019-09-16T02:02:45.545Z</updated>
    
    <content type="html"><![CDATA[<p>** Spark学习之路 （二）Spark2.3 HA集群的分布式安装：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （二）Spark2.3 HA集群的分布式安装</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="一、下载Spark安装包"><a href="#一、下载Spark安装包" class="headerlink" title="一、下载Spark安装包"></a>一、下载Spark安装包</h2><h3 id="1、从官网下载"><a href="#1、从官网下载" class="headerlink" title="1、从官网下载"></a>1、从官网下载</h3><p><a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">http://spark.apache.org/downloads.html</a></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420084850299-679933183.png" alt="img"></p><h2 id="二、安装基础"><a href="#二、安装基础" class="headerlink" title="二、安装基础"></a>二、安装基础</h2><p>1、Java8安装成功</p><p>2、Zookeeper安装成功</p><p>3、hadoop2.7.5 HA安装成功</p><p>4、Scala安装成功（不安装进程也可以启动）</p><h2 id="三、Spark安装过程"><a href="#三、Spark安装过程" class="headerlink" title="三、Spark安装过程"></a>三、Spark安装过程</h2><h3 id="1、上传并解压缩"><a href="#1、上传并解压缩" class="headerlink" title="1、上传并解压缩"></a>1、上传并解压缩</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ ls</span><br><span class="line">apps     data      exam        inithive.conf  movie     spark-2.3.0-bin-hadoop2.7.tgz  udf.jar</span><br><span class="line">cookies  data.txt  executions  json.txt       projects  student                        zookeeper.out</span><br><span class="line">course   emp       hive.sql    log            sougou    temp</span><br><span class="line">[hadoop@hadoop1 ~]$ tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz -C apps/</span><br></pre></td></tr></table></figure><h3 id="2、为安装包创建一个软连接"><a href="#2、为安装包创建一个软连接" class="headerlink" title="2、为安装包创建一个软连接"></a>2、为安装包创建一个软连接</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop1 apps]$ ls</span><br><span class="line">hadoop-2.7.5  hbase-1.2.6  spark-2.3.0-bin-hadoop2.7  zookeeper-3.4.10  zookeeper.out</span><br><span class="line">[hadoop@hadoop1 apps]$ ln -s spark-2.3.0-bin-hadoop2.7/ spark</span><br><span class="line">[hadoop@hadoop1 apps]$ ll</span><br><span class="line">总用量 36</span><br><span class="line">drwxr-xr-x. 10 hadoop hadoop  4096 3月  23 20:29 hadoop-2.7.5</span><br><span class="line">drwxrwxr-x.  7 hadoop hadoop  4096 3月  29 13:15 hbase-1.2.6</span><br><span class="line">lrwxrwxrwx.  1 hadoop hadoop    26 4月  20 13:48 spark -&gt; spark-2.3.0-bin-hadoop2.7/</span><br><span class="line">drwxr-xr-x. 13 hadoop hadoop  4096 2月  23 03:42 spark-2.3.0-bin-hadoop2.7</span><br><span class="line">drwxr-xr-x. 10 hadoop hadoop  4096 3月  23 2017 zookeeper-3.4.10</span><br><span class="line">-rw-rw-r--.  1 hadoop hadoop 17559 3月  29 13:37 zookeeper.out</span><br><span class="line">[hadoop@hadoop1 apps]$</span><br></pre></td></tr></table></figure><h3 id="3、进入spark-conf修改配置文件"><a href="#3、进入spark-conf修改配置文件" class="headerlink" title="3、进入spark/conf修改配置文件"></a>3、进入spark/conf修改配置文件</h3><p>（1）进入配置文件所在目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/spark/conf/</span><br><span class="line">[hadoop@hadoop1 conf]$ ll</span><br><span class="line">总用量 36</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  996 2月  23 03:42 docker.properties.template</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 1105 2月  23 03:42 fairscheduler.xml.template</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 2025 2月  23 03:42 log4j.properties.template</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 7801 2月  23 03:42 metrics.properties.template</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  865 2月  23 03:42 slaves.template</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 1292 2月  23 03:42 spark-defaults.conf.template</span><br><span class="line">-rwxr-xr-x. 1 hadoop hadoop 4221 2月  23 03:42 spark-env.sh.template</span><br><span class="line">[hadoop@hadoop1 conf]$</span><br></pre></td></tr></table></figure><p>（2）复制spark-env.sh.template并重命名为spark-env.sh，并在文件最后添加配置内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ cp spark-env.sh.template spark-env.sh</span><br><span class="line">[hadoop@hadoop1 conf]$ vi spark-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_73</span><br><span class="line">#export SCALA_HOME=/usr/share/scala</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apps/hadoop-2.7.5</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop/apps/hadoop-2.7.5/etc/hadoop</span><br><span class="line">export SPARK_WORKER_MEMORY=500m</span><br><span class="line">export SPARK_WORKER_CORES=1</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop1:2181,hadoop2:2181,hadoop3:2181,hadoop4:2181 -Dspark.deploy.zookeeper.dir=/spark"</span><br></pre></td></tr></table></figure><blockquote><p>注：</p><p>#export SPARK_MASTER_IP=hadoop1  这个配置要注释掉。<br>集群搭建时配置的spark参数可能和现在的不一样，主要是考虑个人电脑配置问题，如果memory配置太大，机器运行很慢。<br>说明：<br>-Dspark.deploy.recoveryMode=ZOOKEEPER    #说明整个集群状态是通过zookeeper来维护的，整个集群状态的恢复也是通过zookeeper来维护的。就是说用zookeeper做了spark的HA配置，Master(Active)挂掉的话，Master(standby)要想变成Master（Active）的话，Master(Standby)就要像zookeeper读取整个集群状态信息，然后进行恢复所有Worker和Driver的状态信息，和所有的Application状态信息；<br>-Dspark.deploy.zookeeper.url=hadoop1:2181,hadoop2:2181,hadoop3:2181,hadoop4:2181#将所有配置了zookeeper，并且在这台机器上有可能做master(Active)的机器都配置进来；（我用了4台，就配置了4台） </p><p>-Dspark.deploy.zookeeper.dir=/spark<br>这里的dir和zookeeper配置文件zoo.cfg中的dataDir的区别？？？<br>-Dspark.deploy.zookeeper.dir是保存spark的元数据，保存了spark的作业运行状态；<br>zookeeper会保存spark集群的所有的状态信息，包括所有的Workers信息，所有的Applactions信息，所有的Driver信息,如果集群 </p></blockquote><p>（3）复制slaves.template成slaves</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 conf]$ cp slaves.template slaves</span><br><span class="line">[hadoop@hadoop1 conf]$ vi slaves</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop1</span><br><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br><span class="line">hadoop4</span><br></pre></td></tr></table></figure><p>（4）将安装包分发给其他节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop1 apps]$ scp -r spark-2.3.0-bin-hadoop2.7/ hadoop2:$PWD</span><br><span class="line">[hadoop@hadoop1 apps]$ scp -r spark-2.3.0-bin-hadoop2.7/ hadoop3:$PWD</span><br><span class="line">[hadoop@hadoop1 apps]$ scp -r spark-2.3.0-bin-hadoop2.7/ hadoop4:$PWD</span><br></pre></td></tr></table></figure><p>创建软连接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~]$ cd apps/</span><br><span class="line">[hadoop@hadoop2 apps]$ ls</span><br><span class="line">hadoop-2.7.5  hbase-1.2.6  spark-2.3.0-bin-hadoop2.7  zookeeper-3.4.10</span><br><span class="line">[hadoop@hadoop2 apps]$ ln -s spark-2.3.0-bin-hadoop2.7/ spark</span><br><span class="line">[hadoop@hadoop2 apps]$ ll</span><br><span class="line">总用量 16</span><br><span class="line">drwxr-xr-x 10 hadoop hadoop 4096 3月  23 20:29 hadoop-2.7.5</span><br><span class="line">drwxrwxr-x  7 hadoop hadoop 4096 3月  29 13:15 hbase-1.2.6</span><br><span class="line">lrwxrwxrwx  1 hadoop hadoop   26 4月  20 19:26 spark -&gt; spark-2.3.0-bin-hadoop2.7/</span><br><span class="line">drwxr-xr-x 13 hadoop hadoop 4096 4月  20 19:24 spark-2.3.0-bin-hadoop2.7</span><br><span class="line">drwxr-xr-x 10 hadoop hadoop 4096 3月  21 19:31 zookeeper-3.4.10</span><br><span class="line">[hadoop@hadoop2 apps]$</span><br></pre></td></tr></table></figure><h3 id="4、配置环境变量"><a href="#4、配置环境变量" class="headerlink" title="4、配置环境变量"></a>4、配置环境变量</h3><p>所有节点均要配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 spark]$ vi ~/.bashrc </span><br><span class="line"><span class="meta">#</span><span class="bash">Spark</span></span><br><span class="line">export SPARK_HOME=/home/hadoop/apps/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><p>保存并使其立即生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 spark]$ source ~/.bashrc</span><br></pre></td></tr></table></figure><h2 id="四、启动"><a href="#四、启动" class="headerlink" title="四、启动"></a>四、启动</h2><h3 id="1、先启动zookeeper集群"><a href="#1、先启动zookeeper集群" class="headerlink" title="1、先启动zookeeper集群"></a>1、先启动zookeeper集群</h3><p>所有节点均要执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /home/hadoop/apps/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">[hadoop@hadoop1 ~]$ zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /home/hadoop/apps/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure><h3 id="2、在启动HDFS集群"><a href="#2、在启动HDFS集群" class="headerlink" title="2、在启动HDFS集群"></a>2、在启动HDFS集群</h3><p>任意一个节点执行即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ start-dfs.sh</span><br></pre></td></tr></table></figure><h3 id="3、在启动Spark集群"><a href="#3、在启动Spark集群" class="headerlink" title="3、在启动Spark集群"></a>3、在启动Spark集群</h3><p>在一个节点上执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ cd apps/spark/sbin/</span><br><span class="line">[hadoop@hadoop1 sbin]$ start-all.sh</span><br></pre></td></tr></table></figure><h3 id="4、查看进程"><a href="#4、查看进程" class="headerlink" title="4、查看进程"></a>4、查看进程</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420201419188-161981735.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420201445760-578558622.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420201503589-1845421183.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420201519992-501293445.png" alt="img"></p><h3 id="5、问题"><a href="#5、问题" class="headerlink" title="5、问题"></a>5、问题</h3><p>查看进程发现spark集群只有hadoop1成功启动了Master进程，其他3个节点均没有启动成功，需要手动启动，进入到/home/hadoop/apps/spark/sbin目录下执行以下命令，3个节点都要执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~]$ cd ~/apps/spark/sbin/</span><br><span class="line">[hadoop@hadoop2 sbin]$ start-master.sh</span><br></pre></td></tr></table></figure><h3 id="6、执行之后再次查看进程"><a href="#6、执行之后再次查看进程" class="headerlink" title="6、执行之后再次查看进程"></a>6、执行之后再次查看进程</h3><p>Master进程和Worker进程都以启动成功</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420201942655-1416446127.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202004959-38463551.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202024084-1987751130.png" alt="img"></p><h2 id="五、验证"><a href="#五、验证" class="headerlink" title="五、验证"></a>五、验证</h2><h3 id="1、查看Web界面Master状态"><a href="#1、查看Web界面Master状态" class="headerlink" title="1、查看Web界面Master状态"></a>1、查看Web界面Master状态</h3><p>hadoop1是ALIVE状态，hadoop2、hadoop3和hadoop4均是STANDBY状态</p><p><strong>hadoop1节点</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202404904-1216450970.png" alt="img"></p><p><strong>hadoop2节点</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202505998-2125653978.png" alt="img"></p><p><strong>hadoop3</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202714859-448500440.png" alt="img"></p><p><strong>hadoop4</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420202843592-757375998.png" alt="img"></p><h3 id="2、验证HA的高可用"><a href="#2、验证HA的高可用" class="headerlink" title="2、验证HA的高可用"></a>2、验证HA的高可用</h3><p>手动干掉hadoop1上面的Master进程，观察是否会自动进行切换</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420203250642-1623659016.png" alt="img"></p><p>干掉hadoop1上的Master进程之后，再次查看web界面</p><p><strong>hadoo1节点</strong>，由于Master进程被干掉，所以界面无法访问</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420203421981-757224448.png" alt="img"></p><p><strong>hadoop2节点</strong>，Master被干掉之后，hadoop2节点上的Master成功篡位成功，成为ALIVE状态</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420203548607-1978973082.png" alt="img"></p><p><strong>hadoop3节点</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420203751677-1108153453.png" alt="img"></p><p><strong>hadoop4节点</strong></p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180420203824105-812085082.png" alt="img"></p><h3 id="1、执行第一个Spark程序"><a href="#1、执行第一个Spark程序" class="headerlink" title="1、执行第一个Spark程序"></a>1、执行第一个Spark程序</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop3 ~]$ /home/hadoop/apps/spark/bin/spark-submit \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --class org.apache.spark.examples.SparkPi \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --master spark://hadoop1:7077 \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --total-executor-cores 1 \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> /home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 100</span></span><br></pre></td></tr></table></figure><p>其中的spark://hadoop1:7077是下图中的地址</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421115203993-483927862.png" alt="img"></p><p>运行结果</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421115300933-1021926480.png" alt="img"></p><h3 id="2、启动spark-shell"><a href="#2、启动spark-shell" class="headerlink" title="2、启动spark shell"></a>2、启动spark shell</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ /home/hadoop/apps/spark/bin/spark-shell \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --master spark://hadoop1:7077 \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --total-executor-cores 1</span></span><br></pre></td></tr></table></figure><p>参数说明：</p><blockquote><p><strong>–master spark://hadoop1:</strong>7077 指定Master的地址</p><p><strong>–executor-memory 500m:</strong>指定每个worker可用内存为500m</p><p><strong>–total-executor-cores 1:</strong> 指定整个集群使用的cup核数为1个</p></blockquote><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421115600861-294197793.png" alt="img"></p><p>注意：</p><p>如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。</p><p>Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可</p><p>Spark Shell中已经默认将SparkSQl类初始化为对象spark。用户代码如果需要用到，则直接应用spark即可</p><h3 id="3、-在spark-shell中编写WordCount程序"><a href="#3、-在spark-shell中编写WordCount程序" class="headerlink" title="3、 在spark shell中编写WordCount程序"></a>3、 在spark shell中编写WordCount程序</h3><p>（1）编写一个hello.txt文件并上传到HDFS上的spark目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ vi hello.txt</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -mkdir -p /spark</span><br><span class="line">[hadoop@hadoop1 ~]$ hadoop fs -put hello.txt /spark</span><br></pre></td></tr></table></figure><p>hello.txt的内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">you,jump</span><br><span class="line">i,jump</span><br><span class="line">you,jump</span><br><span class="line">i,jump</span><br><span class="line">jump</span><br></pre></td></tr></table></figure><p>（2）在spark shell中用scala语言编写spark程序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.textFile(&quot;/spark/hello.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;/spark/out&quot;)</span><br></pre></td></tr></table></figure><p>说明：</p><blockquote><p>sc是SparkContext对象，该对象是提交spark程序的入口</p><p>textFile(“/spark/hello.txt”)是hdfs中读取数据</p><p>flatMap(_.split(“ “))先map再压平</p><p>map((_,1))将单词和1构成元组</p><p>reduceByKey(<em>+</em>)按照key进行reduce，并将value累加</p><p>saveAsTextFile(“/spark/out”)将结果写入到hdfs中</p></blockquote><p>（3）使用hdfs命令查看结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop2 ~]$ hadoop fs -cat /spark/out/p*</span><br><span class="line">(jump,5)</span><br><span class="line">(you,2)</span><br><span class="line">(i,2)</span><br><span class="line">[hadoop@hadoop2 ~]$</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421131948389-32143897.png" alt="img"></p><h2 id="七、-执行Spark程序on-YARN"><a href="#七、-执行Spark程序on-YARN" class="headerlink" title="七、 执行Spark程序on YARN"></a>七、 执行Spark程序on YARN</h2><h3 id="1、前提"><a href="#1、前提" class="headerlink" title="1、前提"></a>1、前提</h3><p>成功启动zookeeper集群、HDFS集群、YARN集群</p><h3 id="2、启动Spark-on-YARN"><a href="#2、启动Spark-on-YARN" class="headerlink" title="2、启动Spark on YARN"></a>2、启动Spark on YARN</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 bin]$ spark-shell --master yarn --deploy-mode client</span><br></pre></td></tr></table></figure><p>报错如下：</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421172825817-1328970589.png" alt="img"></p><p><strong>报错原因：内存资源给的过小，yarn直接kill掉进程，则报rpc连接失败、ClosedChannelException等错误。</strong></p><p><strong>解决方法：</strong></p><p><strong>先停止YARN服务，然后修改yarn-site.xml，增加如下内容</strong></p><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Whether virtual memory limits will be enforced for containers&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Ratio between virtual memory to physical memory when setting memory limits for containers&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><p>将新的yarn-site.xml文件分发到其他Hadoop节点对应的目录下，最后在重新启动YARN。 </p><p>重新执行以下命令启动spark on yarn</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 hadoop]$ spark-shell --master yarn --deploy-mode client</span><br></pre></td></tr></table></figure><p>启动成功</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421173924763-194192730.png" alt="img"></p><h3 id="3、打开YARN的web界面"><a href="#3、打开YARN的web界面" class="headerlink" title="3、打开YARN的web界面"></a>3、打开YARN的web界面</h3><p>打开YARN WEB页面：<a href="http://hadoop4:8088" target="_blank" rel="noopener">http://hadoop4:8088</a><br>可以看到Spark shell应用程序正在运行</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421174140672-405904964.png" alt="img"></p><p> 单击ID号链接，可以看到该应用程序的详细信息</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421174305638-985452065.png" alt="img"></p><p>单击“ApplicationMaster”链接</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421174408279-1529306592.png" alt="img"></p><h3 id="4、运行程序"><a href="#4、运行程序" class="headerlink" title="4、运行程序"></a>4、运行程序</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> array = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">array: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(array)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">res0: <span class="type">Long</span> = <span class="number">5</span>                                                                  </span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421174609129-1153563069.png" alt="img"></p><p>再次查看YARN的web界面</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421174716496-1159210602.png" alt="img"></p><p> 查看executors</p><p><img src="https://images2018.cnblogs.com/blog/1228818/201804/1228818-20180421175002608-557719051.png" alt="img"></p><h3 id="5、执行Spark自带的示例程序PI"><a href="#5、执行Spark自带的示例程序PI" class="headerlink" title="5、执行Spark自带的示例程序PI"></a>5、执行Spark自带的示例程序PI</h3><p><a href="javascript:void(0);" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --master yarn \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --deploy-mode cluster \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --driver-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-cores 1 \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> /home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 10</span></span><br></pre></td></tr></table></figure><p>执行过程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop1 ~]$ spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --master yarn \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --deploy-mode cluster \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --driver-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-memory 500m \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-cores 1 \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> /home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar \</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 10</span></span><br><span class="line">2018-04-21 17:57:32 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2018-04-21 17:57:34 INFO  ConfiguredRMFailoverProxyProvider:100 - Failing over to rm2</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Requesting a new application from cluster with 4 NodeManagers</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Will allocate AM container, with 884 MB memory including 384 MB overhead</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Setting up container launch context for our AM</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Setting up the launch environment for our AM container</span><br><span class="line">2018-04-21 17:57:34 INFO  Client:54 - Preparing resources for our AM container</span><br><span class="line">2018-04-21 17:57:36 WARN  Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line">2018-04-21 17:57:39 INFO  Client:54 - Uploading resource file:/tmp/spark-93bd68c9-85de-482e-bbd7-cd2cee60e720/__spark_libs__8262081479435245591.zip -&gt; hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005/__spark_libs__8262081479435245591.zip</span><br><span class="line">2018-04-21 17:57:44 INFO  Client:54 - Uploading resource file:/home/hadoop/apps/spark/examples/jars/spark-examples_2.11-2.3.0.jar -&gt; hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005/spark-examples_2.11-2.3.0.jar</span><br><span class="line">2018-04-21 17:57:44 INFO  Client:54 - Uploading resource file:/tmp/spark-93bd68c9-85de-482e-bbd7-cd2cee60e720/__spark_conf__2498510663663992254.zip -&gt; hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005/__spark_conf__.zip</span><br><span class="line">2018-04-21 17:57:44 INFO  SecurityManager:54 - Changing view acls to: hadoop</span><br><span class="line">2018-04-21 17:57:44 INFO  SecurityManager:54 - Changing modify acls to: hadoop</span><br><span class="line">2018-04-21 17:57:44 INFO  SecurityManager:54 - Changing view acls groups to: </span><br><span class="line">2018-04-21 17:57:44 INFO  SecurityManager:54 - Changing modify acls groups to: </span><br><span class="line">2018-04-21 17:57:44 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()</span><br><span class="line">2018-04-21 17:57:44 INFO  Client:54 - Submitting application application_1524303370510_0005 to ResourceManager</span><br><span class="line">2018-04-21 17:57:44 INFO  YarnClientImpl:273 - Submitted application application_1524303370510_0005</span><br><span class="line">2018-04-21 17:57:45 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:45 INFO  Client:54 - </span><br><span class="line">     client token: N/A</span><br><span class="line">     diagnostics: N/A</span><br><span class="line">     ApplicationMaster host: N/A</span><br><span class="line">     ApplicationMaster RPC port: -1</span><br><span class="line">     queue: default</span><br><span class="line">     start time: 1524304664749</span><br><span class="line">     final status: UNDEFINED</span><br><span class="line">     tracking URL: http://hadoop4:8088/proxy/application_1524303370510_0005/</span><br><span class="line">     user: hadoop</span><br><span class="line">2018-04-21 17:57:46 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:47 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:48 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:49 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:50 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:51 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:52 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:53 INFO  Client:54 - Application report for application_1524303370510_0005 (state: ACCEPTED)</span><br><span class="line">2018-04-21 17:57:54 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:57:54 INFO  Client:54 - </span><br><span class="line">     client token: N/A</span><br><span class="line">     diagnostics: N/A</span><br><span class="line">     ApplicationMaster host: 192.168.123.104</span><br><span class="line">     ApplicationMaster RPC port: 0</span><br><span class="line">     queue: default</span><br><span class="line">     start time: 1524304664749</span><br><span class="line">     final status: UNDEFINED</span><br><span class="line">     tracking URL: http://hadoop4:8088/proxy/application_1524303370510_0005/</span><br><span class="line">     user: hadoop</span><br><span class="line">2018-04-21 17:57:55 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:57:56 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:57:57 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:57:58 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:57:59 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:00 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:01 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:02 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:03 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:04 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:05 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:06 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:07 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:08 INFO  Client:54 - Application report for application_1524303370510_0005 (state: RUNNING)</span><br><span class="line">2018-04-21 17:58:09 INFO  Client:54 - Application report for application_1524303370510_0005 (state: FINISHED)</span><br><span class="line">2018-04-21 17:58:09 INFO  Client:54 - </span><br><span class="line">     client token: N/A</span><br><span class="line">     diagnostics: N/A</span><br><span class="line">     ApplicationMaster host: 192.168.123.104</span><br><span class="line">     ApplicationMaster RPC port: 0</span><br><span class="line">     queue: default</span><br><span class="line">     start time: 1524304664749</span><br><span class="line">     final status: SUCCEEDED</span><br><span class="line">     tracking URL: http://hadoop4:8088/proxy/application_1524303370510_0005/</span><br><span class="line">     user: hadoop</span><br><span class="line">2018-04-21 17:58:09 INFO  Client:54 - Deleted staging directory hdfs://myha01/user/hadoop/.sparkStaging/application_1524303370510_0005</span><br><span class="line">2018-04-21 17:58:09 INFO  ShutdownHookManager:54 - Shutdown hook called</span><br><span class="line">2018-04-21 17:58:09 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-93bd68c9-85de-482e-bbd7-cd2cee60e720</span><br><span class="line">2018-04-21 17:58:09 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-06de6905-8067-4f1e-a0a0-bc8a51daf535</span><br><span class="line">[hadoop@hadoop1 ~]$</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** Spark学习之路 （二）Spark2.3 HA集群的分布式安装：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （二）Spark2.3 HA集群的分布式安装&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>模板</title>
    <link href="http://zhangfuxin.cn/2019-06-02-%E6%A8%A1%E6%9D%BF%20-%20%E5%89%AF%E6%9C%AC%20(4).html"/>
    <id>http://zhangfuxin.cn/2019-06-02-模板 - 副本 (4).html</id>
    <published>2019-06-01T02:30:04.000Z</published>
    <updated>2019-09-16T01:53:35.632Z</updated>
    
    <content type="html"><![CDATA[<p>** 模板：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （一）Spark初识</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1官网"><a href="#1-1官网" class="headerlink" title="1.1官网"></a>1.1官网</h2><p>官网地址：<a href="http://spark.apache.org" target="_blank" rel="noopener">http://spark.apache.org</a></p><h3 id="1、什么是Spark"><a href="#1、什么是Spark" class="headerlink" title="1、什么是Spark"></a>1、什么是Spark</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 模板：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （一）Spark初识&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>模板</title>
    <link href="http://zhangfuxin.cn/2019-06-02-%E6%A8%A1%E6%9D%BF.html"/>
    <id>http://zhangfuxin.cn/2019-06-02-模板.html</id>
    <published>2019-06-01T02:30:04.000Z</published>
    <updated>2019-09-16T01:53:35.632Z</updated>
    
    <content type="html"><![CDATA[<p>** 模板：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （一）Spark初识</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1官网"><a href="#1-1官网" class="headerlink" title="1.1官网"></a>1.1官网</h2><p>官网地址：<a href="http://spark.apache.org" target="_blank" rel="noopener">http://spark.apache.org</a></p><h3 id="1、什么是Spark"><a href="#1、什么是Spark" class="headerlink" title="1、什么是Spark"></a>1、什么是Spark</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 模板：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （一）Spark初识&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>模板</title>
    <link href="http://zhangfuxin.cn/2019-06-02-%E6%A8%A1%E6%9D%BF%20-%20%E5%89%AF%E6%9C%AC%20(9).html"/>
    <id>http://zhangfuxin.cn/2019-06-02-模板 - 副本 (9).html</id>
    <published>2019-06-01T02:30:04.000Z</published>
    <updated>2019-09-16T01:53:35.632Z</updated>
    
    <content type="html"><![CDATA[<p>** 模板：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （一）Spark初识</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1官网"><a href="#1-1官网" class="headerlink" title="1.1官网"></a>1.1官网</h2><p>官网地址：<a href="http://spark.apache.org" target="_blank" rel="noopener">http://spark.apache.org</a></p><h3 id="1、什么是Spark"><a href="#1、什么是Spark" class="headerlink" title="1、什么是Spark"></a>1、什么是Spark</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 模板：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （一）Spark初识&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>模板</title>
    <link href="http://zhangfuxin.cn/2019-06-02-%E6%A8%A1%E6%9D%BF%20-%20%E5%89%AF%E6%9C%AC%20(7).html"/>
    <id>http://zhangfuxin.cn/2019-06-02-模板 - 副本 (7).html</id>
    <published>2019-06-01T02:30:04.000Z</published>
    <updated>2019-09-16T01:53:35.632Z</updated>
    
    <content type="html"><![CDATA[<p>** 模板：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （一）Spark初识</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1官网"><a href="#1-1官网" class="headerlink" title="1.1官网"></a>1.1官网</h2><p>官网地址：<a href="http://spark.apache.org" target="_blank" rel="noopener">http://spark.apache.org</a></p><h3 id="1、什么是Spark"><a href="#1、什么是Spark" class="headerlink" title="1、什么是Spark"></a>1、什么是Spark</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 模板：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （一）Spark初识&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>模板</title>
    <link href="http://zhangfuxin.cn/2019-06-02-%E6%A8%A1%E6%9D%BF%20-%20%E5%89%AF%E6%9C%AC%20(6).html"/>
    <id>http://zhangfuxin.cn/2019-06-02-模板 - 副本 (6).html</id>
    <published>2019-06-01T02:30:04.000Z</published>
    <updated>2019-09-16T01:53:35.632Z</updated>
    
    <content type="html"><![CDATA[<p>** 模板：** &lt;Excerpt in index | 首页摘要&gt;</p><p>​        Spark学习之路 （一）Spark初识</p><a id="more"></a><p>&lt;The rest of contents | 余下全文&gt;</p><h2 id="1-1官网"><a href="#1-1官网" class="headerlink" title="1.1官网"></a>1.1官网</h2><p>官网地址：<a href="http://spark.apache.org" target="_blank" rel="noopener">http://spark.apache.org</a></p><h3 id="1、什么是Spark"><a href="#1、什么是Spark" class="headerlink" title="1、什么是Spark"></a>1、什么是Spark</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;** 模板：** &amp;lt;Excerpt in index | 首页摘要&amp;gt;&lt;/p&gt;
&lt;p&gt;​        Spark学习之路 （一）Spark初识&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://zhangfuxin.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://zhangfuxin.cn/tags/Spark/"/>
    
  </entry>
  
</feed>
