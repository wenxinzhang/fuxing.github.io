<!DOCTYPE HTML>
<html lang="zh-CN">

<head><meta name="generator" content="Hexo 3.9.0">
    <!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="福星">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <meta name="theme-version" content="1.2.3">
    <meta name="root" content="/">
    <link rel="dns-prefetch" href="http://zhangfuxin.cn">
    <!--SEO-->

<meta name="keywords" content="Spark">


<meta name="description" content="** Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本：** &lt;Excerpt in index | 首页摘要&gt;
​        Spark学习之路 （十五）...">


<meta name="robots" content="all">
<meta name="google" content="all">
<meta name="googlebot" content="all">
<meta name="verify" content="all">
    <!--Title-->

<title>
    
    Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本 |
    
    福星
</title>

<link rel="alternate" href="/atom.xml" title="福星" type="application/atom+xml">


<link rel="icon" href="/favicon.ico">

    

<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7">
<link rel="stylesheet" href="/css/font-awesome.min.css?rev=4.7.0">
<link rel="stylesheet" href="/css/style.css?rev=@@hash">
    
<div class="hide">
    <script type="text/javascript">
    var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
    document.write(unescape("%3Cspan class='cnzz_stat_icon_1263868967 hide' %3E%3Cscript%20src%3D%22https%3A%2F%2Fs95.cnzz.com%2Fz_stat.php%3Fweb_id%3D1272564536%22%3E%3C%2Fscript%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1263868967%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
    </script>
</div>




    

<script>
(function() {
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    } else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

</head></html>
<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->
<body>
    <header class="main-header"  style="background-image:url(
    http://snippet.shenliyang.com/img/banner.jpg)"
     >
    <div class="main-header-box">
        <a class="header-avatar" href="/" title='福 星'>
            <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block">
        </a>
        <div class="branding">
            <!--<h2 class="text-hide">Snippet主题,从未如此简单有趣</h2>-->
            
            <img src="/img/branding.png" alt="Snippet 博客主题" class="img-responsive center-block">
            
        </div>
    </div>
</header>
    <nav class="main-navigation">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                        <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="navbar-brand" href="http://zhangfuxin.cn">
                        福星</a>
                </div>
                <div class="collapse navbar-collapse" id="main-menu">
                    <ul class="menu">
                        
                        <li role="presentation" class="text-center">
                            <a href="/"><i class="fa "></i>
                                首页</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Linux/"><i class="fa "></i>
                                Linux</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Hadoop/"><i class="fa "></i>
                                Hadoop</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Spark/"><i class="fa "></i>
                                Spark</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Java/"><i class="fa "></i>
                                Java</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/Python/"><i class="fa "></i>
                                Python</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/algorithm/"><i class="fa "></i>
                                算法</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/工具/"><i class="fa "></i>
                                工具</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/archives/"><i class="fa "></i>
                                时间轴</a>
                        </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本">
            
            Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本
            
        </h1>
        <div class="post-meta">
    
    <span class="categories-meta fa-wrap">
        <i class="fa fa-folder-open-o"></i>
        <a class="category-link" href="/categories/Spark/">Spark</a>
    </span>
    
    
    <span class="fa-wrap">
        <i class="fa fa-tags"></i>
        <span class="tags-meta">
            
            <a class="tag-link" href="/tags/Spark/">Spark</a>
            
        </span>
    </span>
    
    
    
    <span class="fa-wrap">
        <i class="fa fa-clock-o"></i>
        <span class="date-meta">
            2019/06/15</span>
    </span>
    
    <span class="fa-wrap">
        <i class="fa fa-eye"></i>
        <span id="busuanzi_value_page_pv"></span>
    </span>
    
    
</div>
        
        
    </div>
    
    <div class="post-body post-content">
        <p>** Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本：** &lt;Excerpt in index | 首页摘要&gt;</p>
<p>​        Spark学习之路 （十五）SparkCore的源码解读（一）启动脚本</p>
<a id="more"></a>
<p>&lt;The rest of contents | 余下全文&gt;</p>
<h2 id="一、启动脚本分析"><a href="#一、启动脚本分析" class="headerlink" title="一、启动脚本分析"></a>一、启动脚本分析</h2><p>独立部署模式下，主要由master和slaves组成，master可以利用zk实现高可用性，其driver，work，app等信息可以持久化到zk上；slaves由一台至多台主机构成。Driver通过向Master申请资源获取运行环境。</p>
<p>启动master和slaves主要是执行/usr/dahua/spark/sbin目录下的start-master.sh和start-slaves.sh，或者执行</p>
<p>start-all.sh，其中star-all.sh本质上就是调用start-master.sh和start-slaves.sh</p>
<h3 id="1-1-start-all-sh"><a href="#1-1-start-all-sh" class="headerlink" title="1.1　start-all.sh"></a>1.1　start-all.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见以下分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3.执行<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/start-master.sh，见以下分析</span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin"/start-master.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4.执行<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/start-slaves.sh，见以下分析</span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin"/star`	t-slaves.sh</span><br></pre></td></tr></table></figure>

<p>其中start-master.sh和start-slave.sh分别调用的是</p>
<p>org.apache.spark.deploy.master.Master和org.apache.spark.deploy.worker.Worker</p>
<h3 id="1-2-start-master-sh"><a href="#1-2-start-master-sh" class="headerlink" title="1.2　start-master.sh"></a>1.2　start-master.sh</h3><p>start-master.sh调用了spark-daemon.sh，注意这里指定了启动的类</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> NOTE: This exact class name is matched downstream by SparkSubmit.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Any changes need to be reflected there.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">2.设置CLASS=<span class="string">"org.apache.spark.deploy.master.Master"</span></span></span><br><span class="line">CLASS="org.apache.spark.deploy.master.Master"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3.如果参数结尾包含--<span class="built_in">help</span>或者-h则打印帮助信息，并退出</span></span><br><span class="line">if [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then</span><br><span class="line">  echo "Usage: ./sbin/start-master.sh [options]"</span><br><span class="line">  pattern="Usage:"</span><br><span class="line">  pattern+="\|Using Spark's default log4j profile:"</span><br><span class="line">  pattern+="\|Registered signal handlers for"</span><br><span class="line"></span><br><span class="line">  "$&#123;SPARK_HOME&#125;"/bin/spark-class $CLASS --help 2&gt;&amp;1 | grep -v "$pattern" 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4.设置ORIGINAL_ARGS为所有参数</span></span><br><span class="line">ORIGINAL_ARGS="$@"</span><br><span class="line"><span class="meta">#</span><span class="bash">5.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">6.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">7.SPARK_MASTER_PORT为空则赋值7077</span></span><br><span class="line">if [ "$SPARK_MASTER_PORT" = "" ]; then</span><br><span class="line">  SPARK_MASTER_PORT=7077</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">8.SPARK_MASTER_HOST为空则赋值本主机名(hostname)</span></span><br><span class="line">if [ "$SPARK_MASTER_HOST" = "" ]; then</span><br><span class="line">  case `uname` in</span><br><span class="line">      (SunOS)</span><br><span class="line">      SPARK_MASTER_HOST="`/usr/sbin/check-hostname | awk '&#123;print $NF&#125;'`"</span><br><span class="line">      ;;</span><br><span class="line">      (*)</span><br><span class="line">      SPARK_MASTER_HOST="`hostname -f`"</span><br><span class="line">      ;;</span><br><span class="line">  esac</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">9.SPARK_MASTER_WEBUI_PORT为空则赋值8080</span></span><br><span class="line">if [ "$SPARK_MASTER_WEBUI_PORT" = "" ]; then</span><br><span class="line">  SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">10.执行脚本</span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start $CLASS 1 \</span><br><span class="line">  --host $SPARK_MASTER_HOST --port $SPARK_MASTER_PORT --webui-port $SPARK_MASTER_WEBUI_PORT \</span><br><span class="line"><span class="meta">  $</span><span class="bash">ORIGINAL_ARGS</span></span><br></pre></td></tr></table></figure>

<p>其中10肯定是重点，分析之前我们看看5，6都干了些啥，最后直译出最后一个脚本</p>
<h3 id="1-3-spark-config-sh-1-2的第5步"><a href="#1-3-spark-config-sh-1-2的第5步" class="headerlink" title="1.3　spark-config.sh(1.2的第5步)"></a>1.3　spark-config.sh(1.2的第5步)</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">SPARK_CONF_DIR存在就用此目录，不存在用<span class="variable">$&#123;SPARK_HOME&#125;</span>/conf</span></span><br><span class="line">export SPARK_CONF_DIR="$&#123;SPARK_CONF_DIR:-"$&#123;SPARK_HOME&#125;/conf"&#125;"</span><br><span class="line"><span class="meta">#</span><span class="bash"> Add the PySpark classes to the PYTHONPATH:</span></span><br><span class="line">if [ -z "$&#123;PYSPARK_PYTHONPATH_SET&#125;" ]; then</span><br><span class="line">  export PYTHONPATH="$&#123;SPARK_HOME&#125;/python:$&#123;PYTHONPATH&#125;"</span><br><span class="line">  export PYTHONPATH="$&#123;SPARK_HOME&#125;/python/lib/py4j-0.10.6-src.zip:$&#123;PYTHONPATH&#125;"</span><br><span class="line">  export PYSPARK_PYTHONPATH_SET=1</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<h3 id="1-4-load-spark-env-sh-1-2的第6步"><a href="#1-4-load-spark-env-sh-1-2的第6步" class="headerlink" title="1.4　load-spark-env.sh(1.2的第6步)"></a>1.4　load-spark-env.sh(1.2的第6步)</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">2.判断SPARK_ENV_LOADED是否有值，没有将其设置为1</span></span><br><span class="line">if [ -z "$SPARK_ENV_LOADED" ]; then</span><br><span class="line">  export SPARK_ENV_LOADED=1</span><br><span class="line"><span class="meta">#</span><span class="bash">3.设置user_conf_dir为SPARK_CONF_DIR或SPARK_HOME/conf</span></span><br><span class="line">  export SPARK_CONF_DIR="$&#123;SPARK_CONF_DIR:-"$&#123;SPARK_HOME&#125;"/conf&#125;"</span><br><span class="line"><span class="meta">#</span><span class="bash">4.执行<span class="string">"<span class="variable">$&#123;user_conf_dir&#125;</span>/spark-env.sh"</span> [注：<span class="built_in">set</span> -/+a含义再做研究]</span></span><br><span class="line">  if [ -f "$&#123;SPARK_CONF_DIR&#125;/spark-env.sh" ]; then</span><br><span class="line">    # Promote all variable declarations to environment (exported) variables</span><br><span class="line">    set -a</span><br><span class="line">    . "$&#123;SPARK_CONF_DIR&#125;/spark-env.sh"</span><br><span class="line">    set +a</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Setting SPARK_SCALA_VERSION <span class="keyword">if</span> not already <span class="built_in">set</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">5.选择scala版本，2.11和2.12都存在的情况下，优先选择2.11</span></span><br><span class="line">if [ -z "$SPARK_SCALA_VERSION" ]; then</span><br><span class="line"></span><br><span class="line">  ASSEMBLY_DIR2="$&#123;SPARK_HOME&#125;/assembly/target/scala-2.11"</span><br><span class="line">  ASSEMBLY_DIR1="$&#123;SPARK_HOME&#125;/assembly/target/scala-2.12"</span><br><span class="line"></span><br><span class="line">  if [[ -d "$ASSEMBLY_DIR2" &amp;&amp; -d "$ASSEMBLY_DIR1" ]]; then</span><br><span class="line">    echo -e "Presence of build for multiple Scala versions detected." 1&gt;&amp;2</span><br><span class="line">    echo -e 'Either clean one of them or, export SPARK_SCALA_VERSION in spark-env.sh.' 1&gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line"></span><br><span class="line">  if [ -d "$ASSEMBLY_DIR2" ]; then</span><br><span class="line">    export SPARK_SCALA_VERSION="2.11"</span><br><span class="line">  else</span><br><span class="line">    export SPARK_SCALA_VERSION="2.12"</span><br><span class="line">  fi</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<h3 id="1-5-spark-env-sh"><a href="#1-5-spark-env-sh" class="headerlink" title="1.5　spark-env.sh"></a>1.5　spark-env.sh</h3><p>列举很多种模式的选项配置</p>
<h3 id="1-6-spark-daemon-sh"><a href="#1-6-spark-daemon-sh" class="headerlink" title="1.6　spark-daemon.sh"></a>1.6　spark-daemon.sh</h3><p>回过头来看看<strong>1.2第10步</strong>中需要直译出的最后一个脚本,如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/spark-daemon.sh start org.apache.spark.deploy.master.Master 1 --host hostname --port 7077 --webui-port 8080</span><br></pre></td></tr></table></figure>



<p>上面搞了半天只是设置了变量，最终才进入主角，继续分析spark-daemon.sh脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.参数个数小于等于1，打印帮助</span></span><br><span class="line">if [ $# -le 1 ]; then</span><br><span class="line">  echo $usage</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">2.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">3.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见上述分析 [类似脚本是否有重复？原因是有的人是直接用spark-daemon.sh启动的服务，反正重复设置下变量不需要什么代价]</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> get arguments</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Check <span class="keyword">if</span> --config is passed as an argument. It is an optional parameter.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Exit <span class="keyword">if</span> the argument is not a directory.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4.判断第一个参数是否是--config,如果是取空格后一个字符串，然后判断该目录是否存在，不存在则打印错误信息并退出，存在设置SPARK_CONF_DIR为该目录,<span class="built_in">shift</span>到下一个参数<span class="comment">#[注：--config只能用在第一参数上]</span></span></span><br><span class="line">if [ "$1" == "--config" ]</span><br><span class="line">then</span><br><span class="line">  shift</span><br><span class="line">  conf_dir="$1"</span><br><span class="line">  if [ ! -d "$conf_dir" ]</span><br><span class="line">  then</span><br><span class="line">    echo "ERROR : $conf_dir is not a directory"</span><br><span class="line">    echo $usage</span><br><span class="line">    exit 1</span><br><span class="line">  else</span><br><span class="line">    export SPARK_CONF_DIR="$conf_dir"</span><br><span class="line">  fi</span><br><span class="line">  shift</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">5.分别设置option、<span class="built_in">command</span>、instance为后面的三个参数(如：option=start,<span class="built_in">command</span>=org.apache.spark.deploy.master.Master,instance=1)<span class="comment">#[注：很多人用spark-daemon.sh启动服务不成功的原因是名字不全]</span></span></span><br><span class="line">option=$1</span><br><span class="line">shift</span><br><span class="line">command=$1</span><br><span class="line">shift</span><br><span class="line">instance=$1</span><br><span class="line">shift</span><br><span class="line"><span class="meta">#</span><span class="bash">6.日志回滚函数，主要用于更改日志名，如<span class="built_in">log</span>--&gt;log.1等，略过</span></span><br><span class="line">spark_rotate_log ()</span><br><span class="line">&#123;</span><br><span class="line">    log=$1;</span><br><span class="line">    num=5;</span><br><span class="line">    if [ -n "$2" ]; then</span><br><span class="line">    num=$2</span><br><span class="line">    fi</span><br><span class="line">    if [ -f "$log" ]; then # rotate logs</span><br><span class="line">    while [ $num -gt 1 ]; do</span><br><span class="line">        prev=`expr $num - 1`</span><br><span class="line">        [ -f "$log.$prev" ] &amp;&amp; mv "$log.$prev" "$log.$num"</span><br><span class="line">        num=$prev</span><br><span class="line">    done</span><br><span class="line">    mv "$log" "$log.$num";</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">7.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">8.判断SPARK_IDENT_STRING是否有值，没有将其设置为<span class="variable">$USER</span>(linux用户)</span></span><br><span class="line">if [ "$SPARK_IDENT_STRING" = "" ]; then</span><br><span class="line">  export SPARK_IDENT_STRING="$USER"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">9.设置SPARK_PRINT_LAUNCH_COMMAND=1</span></span><br><span class="line">export SPARK_PRINT_LAUNCH_COMMAND="1"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> get <span class="built_in">log</span> directory</span></span><br><span class="line"><span class="meta">#</span><span class="bash">10.判断SPARK_LOG_DIR是否有值，没有将其设置为<span class="variable">$&#123;SPARK_HOME&#125;</span>/logs，并创建改目录，测试创建文件，修改权限</span></span><br><span class="line">if [ "$SPARK_LOG_DIR" = "" ]; then</span><br><span class="line">  export SPARK_LOG_DIR="$&#123;SPARK_HOME&#125;/logs"</span><br><span class="line">fi</span><br><span class="line">mkdir -p "$SPARK_LOG_DIR"</span><br><span class="line">touch "$SPARK_LOG_DIR"/.spark_test &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">TEST_LOG_DIR=$?</span><br><span class="line">if [ "$&#123;TEST_LOG_DIR&#125;" = "0" ]; then</span><br><span class="line">  rm -f "$SPARK_LOG_DIR"/.spark_test</span><br><span class="line">else</span><br><span class="line">  chown "$SPARK_IDENT_STRING" "$SPARK_LOG_DIR"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">11.判断SPARK_PID_DIR是否有值，没有将其设置为/tmp</span></span><br><span class="line">if [ "$SPARK_PID_DIR" = "" ]; then</span><br><span class="line">  SPARK_PID_DIR=/tmp</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> some variables</span></span><br><span class="line"><span class="meta">#</span><span class="bash">12.设置<span class="built_in">log</span>和pid</span></span><br><span class="line">log="$SPARK_LOG_DIR/spark-$SPARK_IDENT_STRING-$command-$instance-$HOSTNAME.out"</span><br><span class="line">pid="$SPARK_PID_DIR/spark-$SPARK_IDENT_STRING-$command-$instance.pid"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Set default scheduling priority</span></span><br><span class="line"><span class="meta">#</span><span class="bash">13.判断SPARK_NICENESS是否有值，没有将其设置为0 [注：调度优先级，见后面]</span></span><br><span class="line">if [ "$SPARK_NICENESS" = "" ]; then</span><br><span class="line">    export SPARK_NICENESS=0</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">14.execute_command()函数，暂且略过，调用时再作分析</span></span><br><span class="line">execute_command() &#123;</span><br><span class="line">  if [ -z $&#123;SPARK_NO_DAEMONIZE+set&#125; ]; then</span><br><span class="line">      nohup -- "$@" &gt;&gt; $log 2&gt;&amp;1 &lt; /dev/null &amp;</span><br><span class="line">      newpid="$!"</span><br><span class="line"></span><br><span class="line">      echo "$newpid" &gt; "$pid"</span><br><span class="line"></span><br><span class="line">      # Poll for up to 5 seconds for the java process to start</span><br><span class="line">      for i in &#123;1..10&#125;</span><br><span class="line">      do</span><br><span class="line">        if [[ $(ps -p "$newpid" -o comm=) =~ "java" ]]; then</span><br><span class="line">           break</span><br><span class="line">        fi</span><br><span class="line">        sleep 0.5</span><br><span class="line">      done</span><br><span class="line"></span><br><span class="line">      sleep 2</span><br><span class="line">      # Check if the process has died; in that case we'll tail the log so the user can see</span><br><span class="line">      if [[ ! $(ps -p "$newpid" -o comm=) =~ "java" ]]; then</span><br><span class="line">        echo "failed to launch: $@"</span><br><span class="line">        tail -10 "$log" | sed 's/^/  /'</span><br><span class="line">        echo "full log in $log"</span><br><span class="line">      fi</span><br><span class="line">  else</span><br><span class="line">      "$@"</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">15.进入<span class="keyword">case</span>语句，判断option值，进入该分支，我们以start为例</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   执行run_command class <span class="string">"<span class="variable">$@</span>"</span>，其中<span class="variable">$@</span>此时为空，经验证，启动带上此参数后，关闭也需，不然关闭不了，后面再分析此参数作用</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   我们正式进入run_command()函数，分析</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   I.设置mode=class,创建SPARK_PID_DIR，上面的pid文件是否存在，</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   II.SPARK_MASTER不为空，同步删除某些文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   III.回滚<span class="built_in">log</span>日志</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   IV.进入<span class="keyword">case</span>，<span class="built_in">command</span>=org.apache.spark.deploy.master.Master，最终执行</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       nohup nice -n <span class="string">"<span class="variable">$SPARK_NICENESS</span>"</span> <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/spark-class <span class="variable">$command</span> <span class="string">"<span class="variable">$@</span>"</span> &gt;&gt; <span class="string">"<span class="variable">$log</span>"</span> 2&gt;&amp;1 &lt; /dev/null &amp;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       newpid=<span class="string">"$!"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">       <span class="built_in">echo</span> <span class="string">"<span class="variable">$newpid</span>"</span> &gt; <span class="string">"<span class="variable">$pid</span>"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">   重点转向bin/spark-class org.apache.spark.deploy.master.Master</span></span><br><span class="line">run_command() &#123;</span><br><span class="line">  mode="$1"</span><br><span class="line">  shift</span><br><span class="line"></span><br><span class="line">  mkdir -p "$SPARK_PID_DIR"</span><br><span class="line"></span><br><span class="line">  if [ -f "$pid" ]; then</span><br><span class="line">    TARGET_ID="$(cat "$pid")"</span><br><span class="line">    if [[ $(ps -p "$TARGET_ID" -o comm=) =~ "java" ]]; then</span><br><span class="line">      echo "$command running as process $TARGET_ID.  Stop it first."</span><br><span class="line">      exit 1</span><br><span class="line">    fi</span><br><span class="line">  fi</span><br><span class="line"></span><br><span class="line">  if [ "$SPARK_MASTER" != "" ]; then</span><br><span class="line">    echo rsync from "$SPARK_MASTER"</span><br><span class="line">    rsync -a -e ssh --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' "$SPARK_MASTER/" "$&#123;SPARK_HOME&#125;"</span><br><span class="line">  fi</span><br><span class="line"></span><br><span class="line">  spark_rotate_log "$log"</span><br><span class="line">  echo "starting $command, logging to $log"</span><br><span class="line"></span><br><span class="line">  case "$mode" in</span><br><span class="line">    (class)</span><br><span class="line">      execute_command nice -n "$SPARK_NICENESS" "$&#123;SPARK_HOME&#125;"/bin/spark-class "$command" "$@"</span><br><span class="line">      ;;</span><br><span class="line"></span><br><span class="line">    (submit)</span><br><span class="line">      execute_command nice -n "$SPARK_NICENESS" bash "$&#123;SPARK_HOME&#125;"/bin/spark-submit --class "$command" "$@"</span><br><span class="line">      ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">      echo "unknown mode: $mode"</span><br><span class="line">      exit 1</span><br><span class="line">      ;;</span><br><span class="line">  esac</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $option in</span><br><span class="line"></span><br><span class="line">  (submit)</span><br><span class="line">    run_command submit "$@"</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">  (start)</span><br><span class="line">    run_command class "$@"</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">  (stop)</span><br><span class="line"></span><br><span class="line">    if [ -f $pid ]; then</span><br><span class="line">      TARGET_ID="$(cat "$pid")"</span><br><span class="line">      if [[ $(ps -p "$TARGET_ID" -o comm=) =~ "java" ]]; then</span><br><span class="line">        echo "stopping $command"</span><br><span class="line">        kill "$TARGET_ID" &amp;&amp; rm -f "$pid"</span><br><span class="line">      else</span><br><span class="line">        echo "no $command to stop"</span><br><span class="line">      fi</span><br><span class="line">    else</span><br><span class="line">      echo "no $command to stop"</span><br><span class="line">    fi</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">  (status)</span><br><span class="line"></span><br><span class="line">    if [ -f $pid ]; then</span><br><span class="line">      TARGET_ID="$(cat "$pid")"</span><br><span class="line">      if [[ $(ps -p "$TARGET_ID" -o comm=) =~ "java" ]]; then</span><br><span class="line">        echo $command is running.</span><br><span class="line">        exit 0</span><br><span class="line">      else</span><br><span class="line">        echo $pid file is present but $command not running</span><br><span class="line">        exit 1</span><br><span class="line">      fi</span><br><span class="line">    else</span><br><span class="line">      echo $command not running.</span><br><span class="line">      exit 2</span><br><span class="line">    fi</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">  (*)</span><br><span class="line">    echo $usage</span><br><span class="line">    exit 1</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<h3 id="1-7-spark-class"><a href="#1-7-spark-class" class="headerlink" title="1.7　spark-class"></a>1.7　spark-class</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;"/bin/load-spark-env.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find the java binary</span></span><br><span class="line"><span class="meta">#</span><span class="bash">3.判断JAVA_HOME是否为NULL，不是则设置RUNNER=<span class="string">"<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin/java"</span>，否则找系统自带，在没有则报未设置，并退出</span></span><br><span class="line">if [ -n "$&#123;JAVA_HOME&#125;" ]; then</span><br><span class="line">  RUNNER="$&#123;JAVA_HOME&#125;/bin/java"</span><br><span class="line">else</span><br><span class="line">  if [ "$(command -v java)" ]; then</span><br><span class="line">    RUNNER="java"</span><br><span class="line">  else</span><br><span class="line">    echo "JAVA_HOME is not set" &gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find Spark jars.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">4.查找SPARK_JARS_DIR，若<span class="variable">$&#123;SPARK_HOME&#125;</span>/RELEASE文件存在，则SPARK_JARS_DIR=<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/jars"</span>，否则</span></span><br><span class="line"><span class="meta">#</span><span class="bash">SPARK_JARS_DIR=<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/assembly/target/scala-<span class="variable">$SPARK_SCALA_VERSION</span>/jars"</span></span></span><br><span class="line">if [ -d "$&#123;SPARK_HOME&#125;/jars" ]; then</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/jars"</span><br><span class="line">else</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5.若SPARK_JARS_DIR不存在且<span class="variable">$SPARK_TESTING</span><span class="variable">$SPARK_SQL_TESTING</span>有值[注：一般我们不设置这两变量]，报错退出，否则LAUNCH_CLASSPATH=<span class="string">"<span class="variable">$SPARK_JARS_DIR</span>/*"</span></span></span><br><span class="line">if [ ! -d "$SPARK_JARS_DIR" ] &amp;&amp; [ -z "$SPARK_TESTING$SPARK_SQL_TESTING" ]; then</span><br><span class="line">  echo "Failed to find Spark jars directory ($SPARK_JARS_DIR)." 1&gt;&amp;2</span><br><span class="line">  echo "You need to build Spark with the target \"package\" before running this program." 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">else</span><br><span class="line">  LAUNCH_CLASSPATH="$SPARK_JARS_DIR/*"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Add the launcher build dir to the classpath <span class="keyword">if</span> requested.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">6.SPARK_PREPEND_CLASSES不是NULL，则LAUNCH_CLASSPATH=<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/launcher/target/scala-<span class="variable">$SPARK_SCALA_VERSION</span>/classes:<span class="variable">$LAUNCH_CLASSPATH</span>"</span>，<span class="comment">#添加编译相关至LAUNCH_CLASSPATH</span></span></span><br><span class="line">if [ -n "$SPARK_PREPEND_CLASSES" ]; then</span><br><span class="line">  LAUNCH_CLASSPATH="$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> For tests</span></span><br><span class="line"><span class="meta">#</span><span class="bash">7.SPARK_TESTING不是NULL，则<span class="built_in">unset</span> YARN_CONF_DIR和<span class="built_in">unset</span> HADOOP_CONF_DIR，暂且当做是为了某种测试</span></span><br><span class="line">if [[ -n "$SPARK_TESTING" ]]; then</span><br><span class="line">  unset YARN_CONF_DIR</span><br><span class="line">  unset HADOOP_CONF_DIR</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">8.build_command函数，略过</span></span><br><span class="line">build_command() &#123;</span><br><span class="line">  "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"</span><br><span class="line">  printf "%d\0" $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Turn off posix mode since it does not allow process substitution</span></span><br><span class="line">set +o posix</span><br><span class="line">CMD=()</span><br><span class="line">while IFS= read -d '' -r ARG; do</span><br><span class="line">  CMD+=("$ARG")</span><br><span class="line"><span class="meta">  #</span><span class="bash">9.最终调用<span class="string">"<span class="variable">$RUNNER</span>"</span> -Xmx128m -cp <span class="string">"<span class="variable">$LAUNCH_CLASSPATH</span>"</span> org.apache.spark.launcher.Main <span class="string">"<span class="variable">$@</span>"</span>，</span></span><br><span class="line"><span class="meta">  #</span><span class="bash">直译：java -Xmx128m -cp <span class="string">"<span class="variable">$LAUNCH_CLASSPATH</span>"</span> org.apache.spark.launcher.Main <span class="string">"<span class="variable">$@</span>"</span></span></span><br><span class="line"><span class="meta">  #</span><span class="bash">转向java类org.apache.spark.launcher.Main，这就是java入口类</span></span><br><span class="line">done &lt; &lt;(build_command "$@")</span><br><span class="line"></span><br><span class="line">COUNT=$&#123;#CMD[@]&#125;</span><br><span class="line">LAST=$((COUNT - 1))</span><br><span class="line">LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Certain JVM failures result <span class="keyword">in</span> errors being printed to stdout (instead of stderr), <span class="built_in">which</span> causes</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the code that parses the output of the launcher to get confused. In those cases, check <span class="keyword">if</span> the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">exit</span> code is an <span class="built_in">integer</span>, and <span class="keyword">if</span> it<span class="string">'s not, handle it as a special error case.</span></span></span><br><span class="line">if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then</span><br><span class="line">  echo "$&#123;CMD[@]&#125;" | head -n-1 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $LAUNCHER_EXIT_CODE != 0 ]; then</span><br><span class="line">  exit $LAUNCHER_EXIT_CODE</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">CMD=("$&#123;CMD[@]:0:$LAST&#125;")</span><br><span class="line">exec "$&#123;CMD[@]&#125;"</span><br></pre></td></tr></table></figure>

<h3 id="1-8-start-slaves-sh"><a href="#1-8-start-slaves-sh" class="headerlink" title="1.8　start-slaves.sh"></a>1.8　start-slaves.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find the port number <span class="keyword">for</span> the master</span></span><br><span class="line"><span class="meta">#</span><span class="bash">4.SPARK_MASTER_PORT为空则设置为7077</span></span><br><span class="line">if [ "$SPARK_MASTER_PORT" = "" ]; then</span><br><span class="line">  SPARK_MASTER_PORT=7077</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5.SPARK_MASTER_HOST为空则设置为`hostname`</span></span><br><span class="line">if [ "$SPARK_MASTER_HOST" = "" ]; then</span><br><span class="line">  case `uname` in</span><br><span class="line">      (SunOS)</span><br><span class="line">      SPARK_MASTER_HOST="`/usr/sbin/check-hostname | awk '&#123;print $NF&#125;'`"</span><br><span class="line">      ;;</span><br><span class="line">      (*)</span><br><span class="line">      SPARK_MASTER_HOST="`hostname -f`"</span><br><span class="line">      ;;</span><br><span class="line">  esac</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Launch the slaves</span></span><br><span class="line"><span class="meta">#</span><span class="bash">6.启动slaves，</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/slaves.sh"</span> <span class="built_in">cd</span> <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span> \; <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/start-slave.sh"</span> <span class="string">"spark://<span class="variable">$SPARK_MASTER_HOST</span>:<span class="variable">$SPARK_MASTER_PORT</span>"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">   遍历conf/slaves中主机，其中有设置SPARK_SSH_OPTS，ssh每一台机器执行<span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/start-slave.sh"</span> <span class="string">"spark://<span class="variable">$SPARK_MASTER_HOST</span>:<span class="variable">$SPARK_MASTER_PORT</span>"</span></span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin/slaves.sh" cd "$&#123;SPARK_HOME&#125;" \; "$&#123;SPARK_HOME&#125;/sbin/start-slave.sh" "spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT"</span><br></pre></td></tr></table></figure>



<h3 id="1-9-转向start-slave-sh"><a href="#1-9-转向start-slave-sh" class="headerlink" title="1.9　转向start-slave.sh"></a>1.9　转向start-slave.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.设置CLASS=<span class="string">"org.apache.spark.deploy.worker.Worker"</span></span></span><br><span class="line">CLASS="org.apache.spark.deploy.worker.Worker"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3.如果参数结尾包含--<span class="built_in">help</span>或者-h则打印帮助信息，并退出</span></span><br><span class="line">if [[ $# -lt 1 ]] || [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then</span><br><span class="line">  echo "Usage: ./sbin/start-slave.sh [options] &lt;master&gt;"</span><br><span class="line">  pattern="Usage:"</span><br><span class="line">  pattern+="\|Using Spark's default log4j profile:"</span><br><span class="line">  pattern+="\|Registered signal handlers for"</span><br><span class="line"></span><br><span class="line">  "$&#123;SPARK_HOME&#125;"/bin/spark-class $CLASS --help 2&gt;&amp;1 | grep -v "$pattern" 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">5.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">6.MASTER=<span class="variable">$1</span>,这里MASTER=spark://hostname:7077，然后<span class="built_in">shift</span>，也就是说单独启动单个slave使用start-slave.sh spark://hostname:7077</span></span><br><span class="line">MASTER=$1</span><br><span class="line">shift</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">7.SPARK_WORKER_WEBUI_PORT为空则设置为8081</span></span><br><span class="line">if [ "$SPARK_WORKER_WEBUI_PORT" = "" ]; then</span><br><span class="line">  SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">8.函数start_instance，略过</span></span><br><span class="line">function start_instance &#123;</span><br><span class="line"><span class="meta">#</span><span class="bash">设置WORKER_NUM=<span class="variable">$1</span></span></span><br><span class="line">  WORKER_NUM=$1</span><br><span class="line">  shift</span><br><span class="line"></span><br><span class="line">  if [ "$SPARK_WORKER_PORT" = "" ]; then</span><br><span class="line">    PORT_FLAG=</span><br><span class="line">    PORT_NUM=</span><br><span class="line">  else</span><br><span class="line">    PORT_FLAG="--port"</span><br><span class="line">    PORT_NUM=$(( $SPARK_WORKER_PORT + $WORKER_NUM - 1 ))</span><br><span class="line">  fi</span><br><span class="line">  WEBUI_PORT=$(( $SPARK_WORKER_WEBUI_PORT + $WORKER_NUM - 1 ))</span><br><span class="line"></span><br><span class="line"><span class="meta">  #</span><span class="bash">直译：spark-daemon.sh start org.apache.spark.deploy.worker.Worker 1 --webui-port 7077 spark://hostname:7077</span></span><br><span class="line"><span class="meta">  #</span><span class="bash">代码再次转向spark-daemon.sh，见上诉分析</span></span><br><span class="line">  "$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start $CLASS $WORKER_NUM \</span><br><span class="line">     --webui-port "$WEBUI_PORT" $PORT_FLAG $PORT_NUM $MASTER "$@"</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">9.判断SPARK_WORKER_INSTANCES(可以认为是单节点Worker进程数)是否为空</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   为空，则start_instance 1 <span class="string">"<span class="variable">$@</span>"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">   不为空，则循环</span></span><br><span class="line"><span class="meta">#</span><span class="bash">         <span class="keyword">for</span> ((i=0; i&lt;<span class="variable">$SPARK_WORKER_INSTANCES</span>; i++)); <span class="keyword">do</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">           start_instance $(( 1 + <span class="variable">$i</span> )) <span class="string">"<span class="variable">$@</span>"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">         <span class="keyword">done</span></span></span><br><span class="line">if [ "$SPARK_WORKER_INSTANCES" = "" ]; then</span><br><span class="line">  start_instance 1 "$@"</span><br><span class="line">else</span><br><span class="line">  for ((i=0; i&lt;$SPARK_WORKER_INSTANCES; i++)); do</span><br><span class="line"><span class="meta">  #</span><span class="bash">10.转向start_instance函数</span></span><br><span class="line">    start_instance $(( 1 + $i )) "$@"</span><br><span class="line">  done</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<h2 id="二、其他脚本"><a href="#二、其他脚本" class="headerlink" title="二、其他脚本"></a>二、其他脚本</h2><h3 id="2-1-start-history-server-sh"><a href="#2-1-start-history-server-sh" class="headerlink" title="2.1　start-history-server.sh"></a>2.1　start-history-server.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">3.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">4.exec <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/spark-daemon.sh start org.apache.spark.deploy.history.HistoryServer 1 <span class="variable">$@</span> ，见上诉分析</span></span><br><span class="line">exec "$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start org.apache.spark.deploy.history.HistoryServer 1 "$@"</span><br></pre></td></tr></table></figure>

<h3 id="2-2-start-shuffle-service-sh"><a href="#2-2-start-shuffle-service-sh" class="headerlink" title="2.2　start-shuffle-service.sh"></a>2.2　start-shuffle-service.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">1.判断SPARK_HOME是否有值，没有将其设置为当前文件所在目录的上级目录</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin/spark-config.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">3.执行<span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/load-spark-env.sh，见上述分析</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"><span class="meta">#</span><span class="bash">4.exec <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin"</span>/spark-daemon.sh start org.apache.spark.deploy.ExternalShuffleService 1 ，见上诉分析</span></span><br><span class="line">exec "$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start org.apache.spark.deploy.ExternalShuffleService 1</span><br></pre></td></tr></table></figure>


    </div>
    
    <div class="post-footer">
        <div>
            
            转载声明：
            商业转载请联系作者获得授权,非商业转载请注明出处 © <a href="https://github.com/wenxinzhang" target="_blank">福星</a>
            
            
        </div>
        <div>
            
        </div>
    </div>
</article>
<div class="article-nav prev-next-wrap clearfix">
    
    <a href="/2019-06-16-Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本.html" class="pre-post btn btn-default" title='Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本'>
        <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
        <span class="hidden-xs">
            Spark学习之路 （十六）SparkCore的源码解读（二）spark-submit提交脚本</span>
    </a>
    
    
    <a href="/2019-06-14-Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器.html" class="next-post btn btn-default" title='Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器'>
        <span class="hidden-lg">下一篇</span>
        <span class="hidden-xs">
            Spark学习之路 （十四）SparkCore的调优之资源调优JVM的GC垃圾收集器</span><i class="fa fa-angle-right fa-fw"></i>
    </a>
    
</div>

<div id="comments">
    

<div id="vcomments" class="valine"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="/assets/valine.min.js"></script>
<script>
new Valine({
    av: AV,
    el: '#vcomments',
    appId: 'UckE9LEIQ8aoa3MH1Kio27rB-gzGzoHsz',
    appKey: '7HC9xCVYQdshKqFRDmULFm5G',
    placeholder: '说点什么吧',
    notify: false,
    verify: true,
    avatar: 'mm',
    meta: 'nick,mail'.split(','),
    pageSize: '10',
    path: window.location.pathname,
    lang: 'zh-CN'.toLowerCase()
})
</script>


</div>

                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">
            文章目录
        </h3>
        
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#一、启动脚本分析"><span class="toc-text">一、启动脚本分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-start-all-sh"><span class="toc-text">1.1　start-all.sh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-start-master-sh"><span class="toc-text">1.2　start-master.sh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-spark-config-sh-1-2的第5步"><span class="toc-text">1.3　spark-config.sh(1.2的第5步)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-load-spark-env-sh-1-2的第6步"><span class="toc-text">1.4　load-spark-env.sh(1.2的第6步)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-spark-env-sh"><span class="toc-text">1.5　spark-env.sh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-spark-daemon-sh"><span class="toc-text">1.6　spark-daemon.sh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-7-spark-class"><span class="toc-text">1.7　spark-class</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-8-start-slaves-sh"><span class="toc-text">1.8　start-slaves.sh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-9-转向start-slave-sh"><span class="toc-text">1.9　转向start-slave.sh</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#二、其他脚本"><span class="toc-text">二、其他脚本</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-start-history-server-sh"><span class="toc-text">2.1　start-history-server.sh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-start-shuffle-service-sh"><span class="toc-text">2.2　start-shuffle-service.sh</span></a></li></ol></li></ol>
        
    </div>
</aside>
                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>
<a id="back-to-top" class="icon-btn hide">
    <i class="fa fa-chevron-up"></i>
</a>
    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="busuanzi">
    
    访问量:
    <strong id="busuanzi_value_site_pv">
        <i class="fa fa-spinner fa-spin"></i>
    </strong>
    &nbsp; | &nbsp;
    访客数:
    <strong id="busuanzi_value_site_uv">
        <i class="fa fa-spinner fa-spin"></i>
    </strong>
    
</div>
            </div>
            <div class="col-sm-12">
                <span>Copyright &copy;
                    2018
                </span> |
                <span>
                    Powered by <a href="//hexo.io" class="copyright-links" target="_blank" rel="nofollow">Hexo</a>
                </span> |
                <span>
                    Theme by <a href="//github.com/shenliyang/hexo-theme-snippet.git" class="copyright-links" target="_blank" rel="nofollow">Snippet</a>
                </span>
            </div>
        </div>
    </div>
</div>



<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<script src="/js/app.js?rev=@@hash"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":200,"height":350},"mobile":{"show":true}});</script></body>
</html>